# Governing Scientific AI at Scale: Safety, Evaluation, and Institutional Readiness

**India AI Impact Summit 2026 ‚Äî Day 5 (2026-02-20)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 09:30 ‚Äì 10:30 |
| üìç **Venue** | Bharat Mandapam | West Wing Room 4 B |
| üìÖ **Date** | 2026-02-20 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/l6a4tUtXs64?feature=share) |

## üé§ Speakers

- Dr. Geetha Raju, CeRAI - IIT Madras
- Dr. Shyam Krishna, RAND Europe
- Dr. Suryesh Kumar Namdeo, Indian Institute of Science, Bangalore
- PT Nhean, AI Safety Asia

## ü§ù Knowledge Partners

- RAND Europe

## üìù Summary

As capabilities scale, existing assurance and risk management approaches are often stretched by real-world scientific and institutional contexts. This session examines how safety, evaluation, governance frameworks can be adapted and stress-tested through scientific AI use cases, and how emerging ecosystems can build evaluation capacity, manage high-impact risks, and contribute to globally robust scientific AI safety practices.

## üîë Key Takeaways

1. As capabilities scale, existing assurance and risk management approaches are often stretched by real-world scientific and institutional contexts.
2. This session examines how safety, evaluation, governance frameworks can be adapted and stress-tested through scientific AI use cases, and how emerging ecosystems can build evaluation capacity, manage high-impact risks, and contribute to globally robust scientific AI safety practices.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/l6a4tUtXs64/maxresdefault.jpg)](https://youtube.com/live/l6a4tUtXs64?feature=share)

---

_[‚Üê Back to Day 5 Sessions](../README.md)_


## üìù Transcript

key area. Should we think about it as a governance uh uh data governance problem uh uh problem in model design or uh should it be more on a uh verification or uh compliance angle.

&gt;&gt; Thanks uh thank you very much uh Sham for having me and uh good morning to everyone and welcome to this session. Uh um so uh I think okay let me maybe just start with saying that I'm not an AI or

AI safety expert. So whatever I say take it with a pinch of salt. Uh my work is on biocurity and that's the angle I will come from. um uh I think uh all of those things whether it's uh model model

evaluation and other things those are there and those are very very important uh factors and that those are the things that we need to keep in mind but on top of that there is also a deep structural

change that is happening uh uh for example in the field of life sciences uh historically whatever uh risk and risk governance things that we had were very much linked to the uh the physical

infrastructure and lab facility ities and uh you know uh facility inspection and material transfer control and things like that but that seems to have changed and seems to be changing very rapidly

now with the kind of AI bio design tools as as well as LLMs that are uh emerging. So I think uh uh I think uh Rand also did a study on this but uh there are more than probably 1500 bio design tools

uh that are out there and those are totally transforming how uh uh life sciences but in general sciences science is done. Um now what kind of the change that we are seeing is uh uh with these

capabilities now it's much easier to uh engineer proteins optimize uh DNA sequences to do things that we want have better pathogen host modeling interaction modeling and things like

that. Now these capabilities uh are because of AI becoming partly decoupled from the physical containment measures which were usually used in the life sciences. So we have a lot of this uh

risk landscape shifting a little bit more upstream to the design side when it comes to uh uh at least biological uh uh side of things. Uh so yes uh data governance things matter uh model

evaluation and red teamings are essential and we should be doing that. uh but also it is very important that uh you know um especially for country like India where

we have a very very vibrant scientific ecosystem but that is also very uneven right uh how we can use this uh uh AI enabled science which is rapidly evolving uh into the existing mechanisms

to some extent but also at the same time develop those capabilities have more people uh with the core capabilities and uh uh AI bio security or AI chemical security, AI nuclear security and things

like that. So we need to train more people uh on those things. So integrating in from again going back to the life sciences. So integrating uh AI evaluation into biosafety system

strengthening the uh institutional readiness we uh some places there are information some labs and some institutions have information security uh uh labs uh or information security

offices. how we can get them better prepared for these new emerging risk that are coming due to AI. Some places they have uh biosafety officers or biocurity officers. how we can enable

them better to address the AI risk is what the direction that we need to uh move towards and have more adaptive oversight mechanism that is not only based on the limited to this once in a

while inspection that happens but have that that that's that goes more with the with the rapidly evolving things that we are seeing with the AI models coming up and uh um I think uh just uh in terms of

paradigm change that we we are seeing and that you mentioned is the there there need to be more decentralized checks and balances and oversight mechanisms. Uh if there is one authority

sitting somewhere in Delhi and trying to do everything that's not going to work. So that is one of the things that we we have to collectively think about how do we de how do we decentralize these kind

of oversight systems uh to some extent. uh for example how as I was saying how we can empower the information security or biocurity offices uh uh and create uh what in in the field of disarmament

where I have worked on uh called a web of prevention one measure is not enough it's not sufficient you'll have you need to have a number of number of measures in place which collectively can help

prevent something bad from happening thank you &gt;&gt; thank you that's very insightful and uh I think we've already touched on some areas that uh you know that would be

follow-up questions. Uh uh PT uh focusing a bit more on uh open science uh uh where high risk domains especially in biological uh data and a capabilities as uh Sury was mentioning uh how do we

preserve the benefits of open science while preventing uh the destabilizing diffusion of capabilities that you know we were just discussing about. &gt;&gt; Um thank you thank you for having me

today. Um, so I I guess like I would love to be able to give like a binary yes or no answer, right? I think we all want to have that, but unfortunately that's not quite the case. Um, so we

need to to find a way to balance the openness and also the restrictions as well. So I guess my answer here would be sort of like a tiered access and and uh contextual norms. I think those are

really important and um I I think uh Rand Europe has done a really great job at establishing um the global risk um index on AI enabled biological tools and also just generally looking into AI

safety in general where they uh do this thing where they call the pre-eployment assessment with um structured rubrics and um I'm a huge fan of that because I think that um when you release like uh

very frontier models and frontier tools um the the danger is already out there once released, right? It's really hard to withdraw the danger, but however prevention, right? Like there's this

window before you release where you can do a pre-eployment assessment. So, I think I'm I'm I'm a really huge fan of that and also the same way that I'm a big fan of like uh KYC, you know, like

know your customers. Um and I guess this princ principle also pretty much applies where it's like in in the case of biocurity right where we um we differentially um allow the development

of uh you know medical countermeasures and also the defensive uh measures that is necessary um for the research but also like don't limit the um the researchers from actually innovating

either and I guess um my point here is that um uh uh uh the non safeguarded access um like private access to uh credential researchers um where necessary for like defensive research is

absolutely um uh necessary and then you know like open source tools it's it's it's necessary like we can't we can't turn away from being open source like any governance structure that conflates

open source with danger makes a huge mistakes because that also is a very critical development point especially for lower resource setting. So we cannot afford to to to conflate that alto

together. So I guess a very long way to answer this and then to summarize my answer is is that um differentiated governance at capability level is always better than blanket restriction at

access level. Yeah, &gt;&gt; I think that's a very structured answer and I think you know there's a start of uh uh very valid framework level conversation that's already happening

there. uh Gita turning to you uh thinking more about institutional gaps in you know enabling some of the uh solutions that we are discussing a potential solutions uh what are the most

im immediate gaps that you see uh in evaluating a systems uh technical capability regulatory uh and coordination largely from the policy angle that you work in.

&gt;&gt; Uh thank you Sham uh good morning everyone. So on the technical capabilities right the most fundamental thing I see is the AI readiness aspect of uh deployment. So uh in general when

we see India stands uh or ranks third globally and when we see the Southeast Asian countries I think Indonesia is around 49 and uh so there we see the gap right so whatever we do from the western

context or in the Indian context can never be catered to the needs the unique needs of the south Southeast Asian countries and moreover what uh there is the end user perception where we see

that we have to build lot of capacity uh for uh creating awareness among the end users who are actually going to use the products and uh from the policy perspective I would like to give you

certain aspects where we think about uh the socioultural aspects uh that is relevant to the deployment environments. So uh in general uh the large language models are usually trained on the

western data and uh the very recent research work maybe I'll cover a bit of both tech and policy here. So there is an Southeast Asia uh related benchmark safe safety benchmark which says that

all these leading uh large language models have failed well when they evaluated for more than 20 to 30% of the risk in the biological settings. So which means that uh we did not have

enough safeguards which will protect people from encountering all these risk. And moreover uh so this lets us know that we have to build in more socioultural evaluations and assessments

which will cater to the harms that is more particular to that particular deployment environment rather than just having a highle evaluation strategies and this cannot come just from the

policy side. Right? So we need to bring in all the participatory approach which will bring in the end users the different stakeholders involved in the uh using all these AI systems uh be it

model right right from the uh requirements uh definition right so when we assess whether we need an AI system or not generally now there is a perception saying that for whatever we

are going to build or the problem that we are going to solve uh by default we Assume that we need a large language model which [snorts] will not cater which is not even possible to have it

deployed in a low resource setting. Right? So we need to think about small language models which will enable edge deployments at the low resource settings and also consider all the multicultural

and socioeconomic diversity that exist in these regions so that your model doesn't hallucinate is still fair and also establish some governance and accountability frameworks which will

make the developers more accountable and also [clears throat] um because having the developers more accountable will enable them considering more safeguards,

right? and also uh create more awareness about the main fundamental thing is that they will be uh expected to document whatever uh testing that has been gone through and uh uh on the policy side

there is one more aspect which is the Indian government also uh endorses right uh the self-regulation and voluntary commitments on managing and mitigating risk that comes out of all these AI

models so uh I think uh we have to have a unified framework which can still be adaptable to different deployment settings. Yeah. &gt;&gt; And uh I think we already getting a

diversity of perspectives here and you know it's very useful to hear uh moving ahead and thinking about institutionalizing these uh kind of capabilities uh in scientific AI

context. Uh PT turning to you uh should uh independent evaluation and red teaming of AI systems from a technical kind of uh uh solution perspective for this problem that generate biological

outputs especially you know thinking biocurity you know given your perspective on this should it become a norm uh and part of the global scientific specialist infrastructure and

uh if so how would we go about that &gt;&gt; um so I I guess um like a a good example to to use here is probably we're thinking of like nuclear um like weapons, right? Um which is uh

which falls under this organization called uh the International Atomic Energy Agency. Um the IAEA um now from my perspective I I think um uh fistal materials um correct me if I'm

wrong. um they're very scarce and they are to a certain degree technically trackable and they are also uh more than anything else highly regulated. Um whereas biology on the other hand is

everything but that. Um it it's it's diffused, it's dual use by nature and it's also nearly impossible to trace and also most importantly commercially available. Right? And so um in the

recent study actually this was done by this organization called secure bio uh where they actually uh tested frontier large language models against expert veriologist and it turns out that

chatb03 actually outperform expert veriologists by 94% at troubleshooting vetlap protocols so that is that's a very shocking number right and then um I mean obviously you mentioned earlier

that there's a a very concentrated effort that is happening between the US UK and China like the global superpowers basically. Uh and I guess um there's we in in the recommendation for from the

rand Europe that I was uh you know um helping out with is that we recommended that um governments and also independent researchers um do this six monthly um ritual of of monitoring and also

assessment of risk um on a a continuous basis. And they we also suggested obviously like using AI as um an automation tool to increase the efficiency of of this risk monitoring

system. But I think uh to your point I think stuff like this stuff like that is non-interactive uh methodology that doesn't require u you know researchers to actually query directly with uh the

dangerous systems is actually already in and of itself a very meaningful you know safeguard. But that is not enough. you know, we need um we we we need something that is is much larger than that that is

the integration into like you know, institutionalizing it. Um and but I would argue that like a six monthly you know ritual um uh that refresh caden it it for it to be delivered it's it's

going to require a very significant investment from the government at a multilateral level right and so we can't um we can't go without any investment at all. So my suggestion would be um to

actually implement this um AI safety or security institute model that we've been applying where largely um it is technically credentialed it's independent but also has a very formal

relationship with the government and something that I would caveat from the bio side is that for the institution to have some kind of anchoring around biological weapons convention or um the

WHO um because right now that relationship is not quite not not quite there yet and I think um you know back to my point of like pre-eployment assessment I think um that is definitely

needed and then the result has to be shared then across the you know the credential network with tiered confidentiality that um rather than being kept you know

as a proprietary um to the different state yeah &gt;&gt; and uh I think it's a kind of a middle level problem solving it at the level of multilateral institutions solving at the

level of developers. It's a middle- level problem. It's a that's an interesting uh uh uh position. Uh PT uh Sur uh thinking more about uh safety measures at large, how can how can we

make sure that they remain uh rigorous uh at feasible within research ecosystems uh that you're quite familiar with, you know, from a biocurity angle if you will uh but largely also uh in

the larger scientific ecosystem. &gt;&gt; Thanks. uh Sham I think uh first here first thing that we need to understand is how that ecosystem is and then see if certain measures will work there or not

right uh one of the uh hallmarks of let's say Indian uh uh scientific ecosystem is there's a lot of heterogenity there are some places which are really extremely well performing and

there are other places who are not well resourced or have other all kind of challenges so understanding the how the ecosystem is what kind of reg regulation within the institutes that are there

what kind administrative measures that are there, what kind of uh safety teams these kind of institutes might have. All of those things are uh uh extremely important, right? Uh the governance

capacity, compliance, culture and technical expertise very wide varies widely in Indian situations. Um and I I believe this is true for many other countries in the global south as well.

So it's not uh uh something very particular to India. um we have challenges related to different kind of resources and even when the resources are there sometimes uh it's problem uh

it's also problematic to use them uh efficiently enough. Now if we in given that that context if we just uh you know import safety frameworks that are developed in a let's say in a well

resourced uh place in in in a western country or any developed countries I don't know if that those would be a very good fit for uh the kind of system that we have here. those might become more

performative than functional to some extent. Uh another challenge uh that also uh Py mentioned to some extent is that uh you know the speed and scale of AI is huge right and we need uh uh this

this traditional review mechanisms that institutes have for safety audits and all of those things not going to work. We need uh something which is uh far more uh you know adapted uh adaptive and

quick uh and also this here what we had traditionally is this periodic paper based facilitycentric kind of measures and those are those are very much outdated in the in the era of AI that we

live in now um so what uh now the question becomes how do we design proportionate capability aware uh safeguards that would be better uh matched for the for the for the for the

challenges that we have. One of the major challenge as I think a lot of us realize is that uh there is limited awareness about AI safety when it comes to scientific issues even among the

scientists right so uh a huge number uh a large majority of scientists just don't know what they are putting let's say in in chat GPT might be harmful or what they are getting out of bio

designign tools could be harmful to some extent um so there is some understanding about the privacy related issues but safety and security is still a big uh gap in understanding of of even the

scientific experts that are there. Now also uh regarding AI, I think there needs to be a a tiered uh risk classification. So not everything is highly risky. There are certain virols

for example that are trained in in a in virus data. those we'll put in a higher risk category compared to something which is just working let's say on on on on certain animals which are not not not

dangerous. Now also um the the safety measures as I was mentioning earlier this should as the risk has moved a bit upstream. it has come more on the design side. We should also have more safety

measures moving upstreams and uh uh as as PT was me mentioning that uh you know certain kind of evaluation that are before launching uh AI AI tools are necessary but also integrating um AI

evaluation modules into grant review processes creating uh cross trainined AI biosafety review panels. So panels specifically for uh AI bio safety at at at from the from the bottom up side

instead of from having them from the top uh top down approach investing more in a domestic evaluation capacity having more AI safety uh institutes like Gita's uh uh home uh home institute at IT Madras.

So we need uh a lot more of that. And u lastly I think uh um what we have in the US and UK are uh these uh a lot of AI safety work is being done there right and as I

was mentioning importing that directly might not work and we in the global south are largely the users and importer of this technology. So we have to see from the bottom up

side where do we put those safety measures? uh do we like when it comes to import what kind of what kind of uh when when the data is being transferred is there is there certain places where we

can put those kind of safeguards also uh how we can use some some tax sovereignity measures uh in this context right that tech sovereignity measures are used for number of things but uh AI

security is something AI safety and security is something where those could also be used to some extent so yeah I would stop here and then we can discuss &gt;&gt; thank you uh thank you and I think

lot of useful thoughts here for us to explore a bit more. I think we've we've just crossed uh the midmark and I'm going to use Gita to kind of like bridge between the next two topics by combining

two of your questions. Sorry about that. Uh so uh just as uh Sur just mentioned will the emerging scientific powers you know global south middle powers uh would be would they be able to shape

governance uh in this context especially you know a enable science uh or will they continue to inherit uh the frameworks and if they were to show leadership what would that look like in

scientific AI and research ecosystems and you know you've already been working on some of this so I'm looking forward to kind of hearing uh concrete measures that you know are happening.

&gt;&gt; Yeah. Uh sure. So in general what I think is uh definitely the emerging powers right they are uh putting on all efforts to uh bring in all the tools and uh frameworks that are respons that that

are required for governing these AI systems and for example uh so India's uh strategy towards uh all these emerging texts is that they are trying to create sandboxes which are uh highly essential

for deploying ing or evaluating safety uh aspects for the models, right? So, uh they do it for healthare systems, they do it for radiology systems and whatever, right? So these type of tools

and frameworks come from uh Indian settings will actually help the um other underdeveloped countries to learn from the strategies that we use and then uh build something of their own or uh

something is some something which cannot go cross border can still happen through learning and collaboration right so uh for example we are going to launch a global south network for AI trustworthy

AI which will enable all these mechanisms to happen, enable people to uh develop and deploy AI systems uh which will be deployed in the low resource settings. And the other uh

initiative which is going to uh give a very big leap in evaluating AI safety is coming up with an AI safety comments for the global south. Okay, that is part of the uh safe and trusted AI pillar that

is one of the pillars in this uh impact summit and uh I think uh in another one or two years we'll have safety comments which will help us evaluate and uh assess how these AI data models and

systems work for different uh deployment settings. Um another important thing is that as um Suresh mentioned about um uh the audit frameworks um so when when we come with uh when when we focus on the

kind of risk and audit mechanisms that we have here uh we still have it from a organization perspective and not from the end user perspective right so uh at CI we have come up with an incident

reporting mechanism and a framework that caters to the Indian settings so uh it it tells you how to operationalize incident AI incident reporting in the Indian settings which is completely

different from the western settings and here we uh we have to get uh the harms that the people experience in the marginalized communities which will never be recorded everywhere right so

how so how do we enable all these things so uh since it is all about all these sunbased systems right uh even those things will have certain impacts to the mass communities which may be an

indirect impact but how do they uh uh how do they are are knowing about such things are happening to them right so those kinds of gaps we should mitigate um by building more awareness creating

more AI literacy to all these people and uh the final thoughts about combining all these things is that uh we have to bring in some kind of collaborative work between the different stakeholders who

are involved in developing and deploying these systems and uh the governments have already given certain prompt [laughter] um knowledge about how to enable all

these things through the technolal framework and guidelines that was recently published and the AI governance guidelines uh which was recently published by MI. So um

the Southeast Asian countries can learn from the developing countries like India and then have a curated or more tailored approach towards their unique needs. Right? So that is what I think. So

whoever has an opportunity to or a willingness to have more uh things that will actually help them use or leverage these technologies can learn from whatever learn from the mistakes as well

as the experience that the other countries have which is now openly available through all these summits. Yeah, &gt;&gt; that's uh that's very useful and I'm

looking forward to following up on uh it' work in this uh front as well. Uh going to Suresh for uh kind of the last uh um question in this series really. Um should

you know safety measures evaluations uh primarily focus where should the focus be at the model level and you talked about upstream uh quite a bit? Uh should there be more broader socio techchnical

uh uh readiness measures, misuse uh uh considerations? Where do you think it should be? Um thanks Sham. So I think uh performance metrics that that are there

that tells us tell us that whether a model works or not. Then bias assessment tell that whether it works in equitable fair manner or it does not. But neither of them tell us if it is safe to be used

within certain context and c certain socioeconomic or scientific ecosystem related context. Right? And scientific AI as we know is not uh uh uh deployed in a vacuum. So it it works within the

within the scope of how uh uh science ecosystem is there. So that includes things like uh funding structures uh publication incentives also uh from the startup side, startup ecosystem and

regulatory environments uh that uh that that is there. So a model that is technically robust uh can still generate certain kind of systemic risk if it is uh deployed in in places without

adequate oversightes or or dual use literacy and also uh very importantly incident uh uh response capacity. So what you see in many countries in the global south those kind of incidents

response mechanisms are not there. So if something bad happens how do we how do we report it? uh so those things are missing and those are those are very very important uh uh concerns now in the

countries like India where as I was mentioning the scientific ecosystem is while it's scaling rapidly and uh there's a lot more funding that is slowly coming up now it is still uneven

and institutional readiness for AI varies significantly so a tech so when we have a a a a well resourced national lab uh it is not the same kind of readiness or uh capabilities compared

to let's say in a smaller private college in a tier 2 tier three city in India right um and also uh very importantly how we have to also see it from the context of uh you know people

doing their own thing DIY uh kind of science that happens and also smallcale commercial activities which are not fully under the oversight mechanism of the government right so the polic

considering all of these points right The policy evaluation must expand uh from model ccentric assessment to sociotechnical assessment and this would include uh uh you know evaluating things

like how much how much capability uplift relative to the government capacity that is there. So government has certain capacity uh to manage or do oversight but these AI tools how how are they how

are they changing that incentive structures very very important uh that shape the uh model deployment also uh the the diffusion of risk across borders so it's all of these things don't

respect national borders right so how how it's going to spread uh if people using VP and other things a number of other things that are there so integration uh lastly the integration

with existing biosafety and research security systems as I as I had already mentioned. So um um briefly uh um like performance evaluation is necessary but government governance relevant uh

evaluation must be systemic and otherwise risk we risk auditing algorithms while ignoring the institutions that operationalize them and that is very very important how we

how we focus on that institutional uh uh level mechanisms. Thank you. &gt;&gt; Thank you. Uh PT kind of uh the last of uh structured question before we move into a bit more of an open conversation.

Um AI becomes embedded not just in you know new new cap capacities but also uh existing uh programs like biosurveillance public health systems and uh so there's a mix between emerging

uh uh u kind of scientific knowledge with uh more uh legacy let's call uh engineering you know knowledge as well. So how do we make sure that safety evaluation interoperability all of that

exists with in this uh divide without fragmentation happening across the ecosystem because you know you can easily imagine everyone's doing their own uh AI u you know safety evaluation

and not necessarily talking to each other. &gt;&gt; Yeah. &gt;&gt; Um thank you Sham. I think this is a very important question. Um and it's

also a topic that I'm really passionate about as well which is bio surveillance. Um uh to to to to your point, I think um you know countries are already deploying um AI enabled uh biosurveillance systems

that are you know either syndroic uh uh surveillance or it could be uh uh you know genomic sequencing pipelines or outbreak modeling. the countries are already doing that but um they are not

building on uh the sort of like the unified data standards. So they're basically building on like a very incompatible data standards with with with very different legal regimes across

the borders like we've seen that in Southeast Asia. We've seen that even like countries like for example uh Singapore to Malaysia you you'll see different legal regimens um on how they

monitor like um the the data and also like the bio surveillance and so the fragmentation risk is actually not a technical risk I would argue because it's it's not just a technical risk

because um we've seen co um I feel like if anyone is anybody saying I think we all were a little bit traumatized by it. Um, we've seen how data hoarding and incompatible reporting actually cost

lives and um, I saw that especially happening um, across the region in the lower resource settings like countries like Cambodia for example. Um, uh, AI systems that are trained on

non-representative data obviously are going to perform much worse and and and and guess what happens? the the when they perform worse, the region that is most affected is the region that needs

the help the most. And because of that and also that region is also the same region with the least data infrastructure and so I I guess to sort of like answer your question and I and

what I think uh we need to do I think there are three things to to be addressed here. The first one is the obviously the data standards harmonization. Um currently we don't

have that. I think um we we would need not like a global um overhead um standard that enforces on every country but more of a federated interpretability that applies um frameworks that applies

to different countries. So I can think of like HL7 um uh HL7 FHIR which is the federated healthcare interpretability resources that are attempting to address this very

specific issues. It's on like clinical data but this one would be adapted for you know public health surveillance and uh the second point is the legal um safe harbors for basically just kind of like

crossber sharing of data for like public health emergencies that are negotiated beforehand because and this is important beforehand because if you negotiate during an outbreak people are going to

be freaking out. People are going to be like I'm not going to share my data to you. What what are you going to do with that data? So this needs to be done beforehand. And uh the last point is and

also the most politically challenging point is actually to have some kind of shared evaluation criteria across the board between different countries um that are embedded into the national

surveillance systems. And for example like Singapore data um infrastructure environment might not apply to countries with like different climate data or like different demographic data. So this

needs to be applied into you know the the the national surveillance systems and um what I noticed I guess like the last message is that um what I noticed um uh the AI governance framework often

thinks of biosveillance as like an edge like a niche edge case and then people in biocurity frameworks like doing biocurity frameworks thinks that AI governance is like a tool and the these

people don't talk to each other and that gap that gap right there is where the risk um happen. So yeah, we just need to talk to each other more. &gt;&gt; It's uh easier said [laughter] easier

than &gt;&gt; Yeah. So uh I think I'm just about to close with maybe uh 5 minutes or just under that for audience question 10 second final thoughts on you know each

of you uh from the panel. Uh Suresh &gt;&gt; um just wanted to very quickly uh we we need to also keep in mind that how AI could help solve some of these AI safety challenges right how agentic AI could be

used let's say when people are trying to develop vaccines uh SEPY has developed this platform where Agentic AI is being used to uh check if there's someone who is trying to jailbreak or who someone

who is trying to you know uh misuse the the tool that is there. Uh second very quick point uh uh also uh with all that what I said there is still a gap to transfer things from from digital to

physical uh uh what is called digital to physical barrier. So even if you have everything you still can't just uh you know develop modify viruses without having a proper physical infrastructure

and there are still some ways to control that. Thank you &gt;&gt; GA. Yeah, I think we should move on transforming from issues to intelligence

like learning from the risk that happens and feeding it back to the model training and other assessment activities to mitigate the risk in real time. Right. So that is where we need more to

we need to move towards bringing in more people uh into evaluations and then making it safer for people to use. &gt;&gt; Sure. Yeah. &gt;&gt; Thank you.

&gt;&gt; Um I guess I'll make it quick. Um uh the point that I want to make here is that uh Shash to echo his point I think um you're right that we should not shoot oursel in the foot especially for

developing countries I think it's really important um and so my message for the last message here is just kind of like um while we are forging ahead in innovation and while we are innovating

ahead in whatever domains of scientific domains that we're doing um uh we need to be conscious of the impact that we have and I think um in the AI impact summit is is one of the really good

place to jump start that kind of conversations and break the silos. Thank you. &gt;&gt; Sure. &gt;&gt; And uh thank you everyone. I'm just

going to take uh probably one minute to kind of summarize key points evaluation. I see largely a systemic question safety measure systemic question. I especially like the point on incident response not

being uh uh already there and a couple of points on uh the crossber uh solutions and problems. We already have that uh uh discussion on open science. We talked about uh uh how uh managed

access safeguards and uh comparing government capacity to manage that versus letting it out for more DIY oriented science which you know was a good good uh term I I really like that

uh that's a key uh area and uh for emerging scientific powers of course collaboration is key tailored approach that's something that I'm I'm again waiting to see from uh IT Madras as well

their contribution on this and uh some uh uh uh crossber work on uh legal safe harbors data standard harmonization uh PT that you mentioned really land well uh from this panel I'm going to stop my

summary right now and you know more of this would be kind of put together in a blog at some point in the uh nearby future uh perhaps uh we can go for questions uh first uh yes please I think

I can give you mine &gt;&gt; thank you so much for your wonderful insights. I really enjoyed this session as a researcher in safety of AI at the University of York. So I focus on

psychological harms of AI. And so what I want to ask particularly Gita is um when it comes to the definition of harms and traditional safety engineering, they're catering more to physical harms and now

we see the whole u spectrum of harms expanding beyond that. So I would love to know the work being done by Karai and you in this area and and and in fact enrich my research with it.

&gt;&gt; Yeah sure. Uh so when we actually assess harms and impacts right we do we have to do it from the different two different perspectives. One is on the functional side where we assess all these al

algorithmic risk and other stuffs from the human centric perspective like you said we can keep doing everything from the psychology perspective and other ethics and other stuffs right. So here

at CRI uh we do work on assessing bias generate uh determining whether the model is stereotypical or not and how do we generate explanations for uh the uh highle scientific models and all. So uh

from the perspective of the psychological things right uh there is this cognitive science or cognitive capabilities of AI models which will actually uh enhance or degrade the

capabilities of humans. Right? So those things are uh we are trying to do some assessments from the incident perspective. Right? So if you go to go and read the incident reporting

framework that we have we have a taxonomy of risk and harms and also the impacts. Right? So uh from the kinds of harms that we have defined we have categorized it as physical psychological

cyber cyber incident based harms that and moreover we have all the generic kinds of harms like algorithmic harms socioeconomic harms the environmental harms and all. So we are coming trying

to come up with a taxonomy that will cater to uh the different hierarchies that will uh be applied to these kind of harms and impacts which will again be model specific use case specific and the

domain specific aspects. Yeah. So that that is where we are trying to work on and we also have a healthcare based tool a toolkit which will enable people to actually assess the perceptions of how

they treat these models uh how they see whether these AI applications are helpful for them or not and then come up with some capacity building programs for different uh roles in which they are

working on and this has been done with uh CMC Wellure Hospital and we have been assessing the perceptions of healthcare workers and then come up with a training module which will enable them to uh use

AI models or tools more confidently rather than say being resistant or not relying on them for so much. Uh last probably last quick question maybe keep it short on the responses as well please

sorry. Yeah. &gt;&gt; Hi. Uh so uh my question is about uh like we we discussing all the geographical barriers right the

modalities geography when we change the change the geography the models tend to perform poorly. Are we are we concerned about the temporal modality as well as when the when we go forward in time the

data is going to change eventually and that is going to affect modeling and uh how do we plan on like you know mitigating such a problem if if it if and when it arises. Yeah.

&gt;&gt; Yeah. Uh so this comes under the model monitoring uh the system monitoring approach where we consider the data drifts the out of distribution aspects of the data and model. So definitely

this is one of the criteria where you assess safety and evaluate the impacts of it. &gt;&gt; Anyone else want to add uh before we move on?

&gt;&gt; Ah uh sure yes I think last question. Thank you so much for the insightful discussion. Really appreciated the expertise that you're bringing to the topic. Um and thanks PT for bringing up

CO because I was my question is about that. As we learned from CO biocurity risk can quickly become a crossber existential threat. So what would a successful web prevention and incident

response framework look like and who are you looking up to in this space like who's doing it well in the space? &gt;&gt; Okay, I can start maybe PD can add. Um so I think uh as I was mentioning uh it

will have to be uh more decentralized but at the same time integrated to the leadership. Uh so I think there needs to be more empowerment of people who are like biosafety officers in the lab or

who are institutional biosafety committee members who are people who are working on the ethics and research security side at the institutes. So those are the people who need to be

empowered. So there need to be more capacity building of those people and at the same time there needs to be a mechanism established so they can report those incidents to the to the very top

and there is top leadership sitting in the capitals they can in some way get a get an overview or monitor the situation as it is going on at at different institutes level. Thanks.

&gt;&gt; I can add a little bit to that. So um in Singapore we actually have different agencies responsible for this. So we have the national environmental agency and then we have uh the MO obviously the

ministry of health and then we also have a different smaller agency like communicable disease agency and also like a prepare agency where they responsible for different task but I

want you to envision this as almost like the way that Singapore is trying to establish itself. I think it's it's trying to establish itself almost as a firefighter. So when there's an incident

where there's a crisis, who is actually doing what is very clear, but it's always not always clear across like different countries. For example, um in Laos, um Vietnam might be looking very

different, but I think um having a very coordinated response across the different agencies on who is doing what. Like for example, National Environmental Agency is responsible for wastewater

surveillance. So monitoring how the sickness is increasing or spiking or not, those are the people Yeah. that you would look up to. I and uh I think that's the last word

right it all comes down to prevention and preparedness even in this much like anything else with uh bio context uh thank you uh everyone for the question and thank you to my uh brilliant uh uh

panelists uh Suresh Gita and uh PT uh this was a very insightful discussion on the screen is the work from Rand Europe with uh CLTR some of what was uh referred to by uh PT and other panelists

as well some aspects of what we were discussing about risk uh uh uh typification you' probably get some ideas there as well and with that I close I'm supposed to hand over these

momentos to apparently including me so let us do that now thank you [applause] &gt;&gt; thank you so Thank you. &gt;&gt; Perhaps the

&gt;&gt; You can take a picture and send it now. Thank you. It's going to be recording the entire day.
