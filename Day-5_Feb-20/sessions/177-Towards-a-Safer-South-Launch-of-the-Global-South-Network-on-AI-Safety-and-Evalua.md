# Towards a Safer South: Launch of the Global South Network on AI Safety and Evaluation

**India AI Impact Summit 2026 ‚Äî Day 5 (2026-02-20)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 16:30 ‚Äì 17:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 18 |
| üìÖ **Date** | 2026-02-20 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/pZd2-Pkh-Xg?feature=share) |

## üé§ Speakers

- Ambassador Philip Thigo, Government of Kenya
- Dr Amandeep Singh Gill, United Nations
- Dr Balaraman Ravindran, IIT Madras
- Dr Fabro Steibel, ITS Rio
- Dr Rachel Adams, Global Centre on AI Governance
- Dr Rachel Sibande, Gates Foundation
- Dr Urvashi Aneja, Digital Futures Lab
- Mr Abhishek Singh, Ministry of Electronics & IT, GoI
- Mr Amir Banafetmi, Cognizant
- Mr Vilas Dhar, Patrick McGovern Foundation
- Ms Chenai Chair, Masakhane African Languages Hub
- Ms Natasha Crampton, Microsoft

## ü§ù Knowledge Partners

- Digital Futures Lab

## üìù Summary

The session will launch the Global South Network on AI Safety & Evaluation ‚Äî a global alliance of research and civil society organisations working to advance context-aware AI safety research and practice rooted in realities of the Global South. It will explore how AI safety priorities differ across Global South contexts, where South-South collaboration is most needed, and how evidence from deployment contexts across the Global South can strengthen global AI safety efforts.

## üîë Key Takeaways

1. The session will launch the Global South Network on AI Safety & Evaluation ‚Äî a global alliance of research and civil society organisations working to advance context-aware AI safety research and practice rooted in realities of the Global South.
2. It will explore how AI safety priorities differ across Global South contexts, where South-South collaboration is most needed, and how evidence from deployment contexts across the Global South can strengthen global AI safety efforts.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/pZd2-Pkh-Xg/maxresdefault.jpg)](https://youtube.com/live/pZd2-Pkh-Xg?feature=share)

---

_[‚Üê Back to Day 5 Sessions](../README.md)_


## üìù Transcript

Uh, good evening everyone. My name is Orvisha Nea. I am the founder and director of digital futures lab and I am so excited to see all of you here and um and to have you all here for the launch

of this network. Um so it's a it's a real pleasure to welcome you to the launch of the global south network for trustworthy AI here at the India AI impact summit on behalf of digital

futures lab and our other founding partners sir from IIT Madras the global center for AI governance IT Rio international innovation corpse um thank you all for being here and we're

especially grateful to Mr. Mr. Abishek Singh the and ambassador Philip Tego and Mr. Quintin Chiao. Uh and to all our distinguished speakers and guests for also joining us today.

Across across the global south um AI systems are being rapidly deployed in critical social sectors such as healthare, education, judiciary and in government.

And while the opportunities are immense in many of these context, many of these contexts are also marked by low institutional capacity, deep societal inequities, populations with low levels

of literacy. So while the potential is immense, the risks and harms are also immense. And so it's it's particularly important that we figure out ways to make AI safe and trustworthy in these

contexts um to ensure not only that we protect the populations and we ensure and to ensure that we don't exasperate existing harms but also to ensure that we build the infrastructure for safe and

inclusive AI adoption. Unfortunately, global south organizations, global south communities, global south states remain under reppresented in global safety and

governance infrastructures. And many countries in the global south are actually unlikely to even have in the near term their own safety or oversight institutes. And there's a real

risk therefore that the concerns and priorities of these countries of these communities remain under reppresented in the global safety infrastructure. And precisely it's those countries that are

that have the most potential or the most opportunity to leverage AI. Independent civil society organizations are uniquely positioned to address this gap. Their proximity to real world

deployment context enables them to surface risks that are invisible to lab-based evaluations or testing. The form of grounded evidence that civil society organizations can bring can

inform global safety benchmarks, standard setting processes and risk assessments, providing corrective signals to technical and regulatory institutions.

The global south network for trustworthy AI works to advance exactly these objectives to evaluate the real world impact of AI systems to build the trust and oversight mechanisms localized to

different linguistic cultural and infrastructural contexts and to elevate global south perspectives in global AI governance forums. It is particularly encouraging I think that this initiative

also aligns closely with the recently announced or announced yesterday New Delhi Frontier AI commitments which focused on multilingual AI and say and evaluation of multilingual AI systems as

well as the importance of real world evaluation of AI systems. The network that we the global south uh network for trustworthy AI brings together some of the leading research

institutions from across the global south. We are joined by a community of organizations from Asia, from Africa, from Latin America. Um, and the names of which you see displayed behind you.

I also want to take this opportunity to highlight um some of the the key activities that we're going to be doing as part of the network. I think one of the key things that we

want to do as part of the network is to really build an independent evidence base to generate communityinformed analysis of the societal, ethical and distributional risks of AI systems

across diverse contexts. We also want to do real world deployment assessment to conduct contextual and public evaluations of models and applications across diverse social contexts. We also

want to push the field of evaluations, push the science of evaluations where we say that benchmarks are very important but benchmarks as they stand today do not necessarily capture all the societal

risk that we see in the global south. So how do we ensure that the evaluation work that we're doing also capture some of those harms. In some sense what we want to do with

the network is field building. We want to bring together global societies, global south civil society organizations to pull in their collective intelligence, to pull in their

capacities and to advocate together for the representation of global south concerns on global governance forums. So what we're trying to do here is field building within the global south around

AI safety um and around building that trust infrastructure. And eventually what we hope that all of this amounts to is collective advocacy. We see an important role that the network will

play in creating a connective tissue between the global governance architecture, between the global safety infrastructure and what's happening on the ground. We hope the network can

provide that that visibility to real world impacts to technology companies who are designing tools who are designing safety infrastructure as well as to governments and international

organizations who are building the architecture of global AI governance. Um, so with that, I want to thank you all. Oh wait, I have one more thing to share with all of you. I'm not ready to

thank you yet. Uh I also want to I also want to showcase some of the projects that we'll be doing in the coming year. Um picking up on yesterday's commitments, uh one of the things that

we'll be doing is building bench benchmarks for multilingual AI. Uh this is with our network partners, the collective intelligence project and CARA and and we're really excited to start

this work. Um we're also going to be doing work on gender and safety. This is with our partners, the GXD hub and the global center for AI governance to build a taxonomy of gender harm so that we can

start building a more robust incident reporting database when it comes to gender related harms and really advance gender safety in digital spaces. The third piece that we're going to be

working on this year is around procurement. All of the evaluation work that we do, all the benchmarks that we build, all of that has to eventually feed into public policy. Um and so we

hope that some of this work can support procurement and procurement we think is a really important lever for countries in the global south to shape markets for responsible innovation. I think we've

all heard a lot about the kind of third way uh of AI governance that India brings to the global governance landscape and procurement can be an important lever of of making that third

way a reality and shape and setting the bar for what responsible innovation looks like. Like I'd mentioned earlier, we also want to push on the science of evaluation. What does good evaluation

look like? What are the kind of methodologies that we need? What are the kind of methodologies that reflect the concerns and the capacities of communities in the global south? So,

we're very excited to be doing this work with its Rio, who's also one of the founding partners. And specifically to to to advance this discussion on evaluations, we'll be looking at labor

market impacts in the global south. Um, and finally, we're going to be looking at evaluations of health health information systems. Do these do the existing generative AI tools and large

language models that we see do do they deliver for clinicians? Do they deliver for doctors? H what more can they do to support the needs of healthare professionals in the global south? So,

those are the five kind of big flagship projects that uh we're going to be launching within the coming year. Um, we're going to be very busy as you can see. We have a lot that we're going to

try and get done and we're really excited to uh be on this journey with all of you and uh would would love to engage with all of you post the launch and see how we build this civil society

and research infrastructure together. Um so with that I am delighted to welcome our keynote speakers. Um first and I would like to give the floor to Mr. Abishek Singh. Sir, thank you for

your continued support for the uh network and uh for your leadership on the India AI summit. Over to you sir. [applause] &gt;&gt; Thank you. Thank you. And uh first and

foremost like to congratulate the all the team the network which has brought this together this global south network for trustworthy AI because a few months back when we started discussing this

concept with Urvashi with Kalika with my team we felt that how do we go about it because safe and trusted AI is something that is that nobody disagrees with. Everybody says that whenever AI

innovation is happening but we must ensure that we must protect ourselves. we must kind of secure ourselves of the harms that can come from from misuse of AI or from the risks that frontier AI

poses. So yes, we did have the Yeshua Benji's report, the scientific panel report which has been part of all the impact summits, the action summit and the Bley Park summit in which it has

kind of identifies the risks that Frontier AI model poses. But what we do believe is that just identifying the risks is not is not sufficient. We need to think of how do we address those

risks and for addressing those risks you need to first have the technical tool the capacity to identify those risks. What are the benchmarks on which you will evaluate them? Some of which Rashi

identified like how do how do various models perform on how is on multilingual benchmarks because very often most models are evaluated on benchmarks which are predominantly in English language.

But if you look at India a diverse country we have 22 official languages and multiple other dialects. How do we evaluate how a model performs on various uh domains in prompts given in those

languages? So we don't have specific linguistic benchmarks. The same applies to many countries of the global south. So it felt that while limited expertise exists in some institutions where

research is going on. Sarai is one of it where professor Balam Rendan is leading it. There are many labs of course whether it's Microsoft research or whether other labs wherein such work is

going on. The AI security institute in UK is doing some work in this direction. The OECD has been doing some work. But how do we ensure that we we enable the access to such resources, such tools,

such studies to the larger global majority. So with that this whole concept of bringing a creating a global south network for trustworthy AI came in and then we immediately had this

conversations with all the key stakeholders, partners. We got a lot of support from all almost all stakeholders and along with that the conversation for the new Delhi frontier AI commitments

was also going on which Kalika from my team was leading it and luckily we were able to announce it in which all models committed to to those two commitments about sharing usage data as also

multilingual performance benchmark benchmarks. So that was a huge achievement and I feel that the launch of this global south network for trustworthy AI is a further step in that

direction. How do we enable compliance to those commitments? How do we ensure that how this data will be shared? How do we create tools for evaluating models in various languages? How do we build up

capacity in all countries, the global south? How do we share resources? How do we share knowledge across? So this this is just the beginning and I feel that with support from all industry

organization the frontier AI labs the research organizations governments across across uh the world this can really really grow into a resource that can be a global utility. So I compliment

the entire team which is involved in doing that. The launch of the network is the first step but how do we action it out? How do we make it functional? How do we ensure that we get necessary

support from all stakeholders? Because very often whenever we talk about trusted AI, whenever we talk about safe AI, some people think that we are trying to stifle innovation. The objective is

not that. We always said that while the primary objective is to ensure diffusion of AI, primary objective is to ensure that more and more users benefit from the usage of AI. But at the same time,

we need to do that in a responsible manner. We need to do it in a safe manner. we do need to do in a trustworthy manner to limit the harms that can be caused. So this global south

network for trustworthy AI which is being launched will work in that direction. It will be an institution that will support not only India but the entire global south and I'm sure with

the just the presence of all the speakers who are in present in the session the strong commitment that all industry and all countries and all multilateral organizations are showing

to this initiative I'm sure this will get further strengthened in the days to come there lot of work that urashi and team are taking on their own but we will be there to provide all necess support

from India mission and we'll work towards ensure ensuring that you gain the same level of support from every participating country which is here. So thank you once again and congratulations

for this launch and look forward to working towards the objectives and [applause] reach them. Thank you. &gt;&gt; Thank you sir for your remarks and most importantly for your support. Um I think

it means a lot to us to be working so closely with the India AI mission and um and we're really excited to be able to deliver on this promise. Um, it's now my honor to invite Ambassador Philip Tiggo,

the special envoy on technology from the Republic of Kenya to share his reflections. Thank you so much for for this opportunity to share my reflections and

I noticed that this is really a women's uh network. So again, congratulations and and Rachel putting this together. I think before we celebrate um the launch of the network, I think we must

acknowledge um and I say that deliberately because we must acknowledge the structural problem around the safety conversations um and the infrastructure that has been carried carrying safety in

the last kind of three years. I think the global south has always been excluded from this conversation. I say this from a position of strength because Kenya is the only Kenya I think we're

the only member of the international network of AI safety institutes. Um and so there's a challenge there. [clears throat] Um and so I think um that model that is

not inclusive um to a global majority that in most cases bears the brand and the impacts of AI is not acceptable and so this network in my sense is timely but also linked and so there's almost an

urgency that we need uh to work very kind of closely in how we scale up what this network does. The second part of course is as I mentioned that a lot of the global majority countries are the

ones that not just bear the brunt of the models but bear the adverse societal harms of the models. Uh Kenya is one of the countries that uses one of the models. Uh and and from the use cases we

see that um they use it for the wrong reasons emotional support or companionship. It's not necessarily for anything meaningful or productivity. And so as a as as the world advances then it

it it therefore behooves us that we uh work with these frontier model companies to ensure that their models are safe uh beyond secure but also most trustworthy. The second part of course is that um

part of model evaluations assumes um access. Uh we know we now know that a lot of um my colleagues who are doing model evaluations are doing it from an external point of view. So we need to be

very clear that global majority countries and by this when I say global majority countries we also have a new global south in AI uh because it's just not the global majority we know in the

global north of artificial intelligence is two countries and a few companies. So we must beyond this extend uh to also include other colleagues whether it's from Europe or Eastern Europe or other

or Latin America. Uh safety must also go beyond technical towards social technical issues. We look at AI uh in the context of Kenya from minds to models and so safety must also include

environmental harms, biases, misinformation, disinformation but also harms to water environment. Uh and so we need full life cycle accountability. It's good to evaluate the models but

also it's good to evaluate the footprints of the model quickly. Um there are four structural gaps that we see and this is why I love this network and the networks that are I think one is

yes you want global majority folks to evaluate the models but we have red teaming capacity gaps so I hope that this network will look at this secondly I think is also issues of access to

compute uh we can't have a global majority researchers trying to evaluate models without necessarily having access to compute to do that um third part of course has been mentioned by uh I think

he's left uh issues around linguistic and cultural mismatch. So we need to do that. Um the other part of course is benchmarks as governance power also benchmarks are not neutral. Uh sometimes

I think I I like to be honest because that's what evaluation needs to do. Uh and so we need uh in most cases to to ensure that um only a handful of institutions should not define what risk

what risks are measured, what harms are prioritized and what safe performance means. governance is about power and we must dis deconentrate that power even if it's unintentional. Finally, I think for

me um uh evaluation is also about agency um and and and we must have a question of agency or notion of agency around around these models um but also including sovereign capability as you

know a lot of your countries are are trying to build sovereign models but also sovereign capabilities across the tank. What should this network deliver in my view? And I'll humbly kind of make

these quick suggestions. One, I think yes, good to have the network, but can we have regional nodes for this uh so that because Africa is I speak for Africa, Africa is not a country, it's 54

countries expanded to have nodes. Secondly, uh include multilingual benchmark data sets. Uh could be an interesting annual red teaming exercise. uh could be potentially why not publish

an global south AI safety report with an ex expansive definition of what safety is and I will be remiss if I don't say how do we fit this into the multilateral process we already have a global uh UN

scientific uh UN scientific panel on AI and there's a global dialogue on AI governance uh I'm one of the champions for this so hopefully we will get this in there finally let's close the

accountability loop how do all this ultimately matter for citizens. We can evaluate all we want, but if they don't translate real gains and real opportunity for citizens, I think we'll

be failing. Thank you very much. Thank you ambassador for highlighting the urgency of this work and also uh you know reframing the safety conversation for the global south and and just to say

we are planning to have regional hubs and um we do and I think the the point about how we engage with the multilateral system is very important and you know we will have the Indian AC

as part of our steering committee and we hope we can work with the government of Kenya as well and of course we have professor uh Ravindran who is part of the scientific council so we will be

relying on uh [laughter] on him as well. Um but thank you thank you for your uh remarks and um with that I' I' I'd like to call our final keynote speaker for the day who represents the UN office for

digital and emerging technologies. I'm pleased to invite Mr. Quentin Chao Lambert the chief of office and AI lead to deliver the next keynote address. &gt;&gt; [applause]

&gt;&gt; Thank you very much. Um, thank you to the digital futures lab and congratulations to you and your partners for the launch of this global south research network on AI safety. And um

I'm here to offer a few perspectives um from the kind of tracking of the conversation at the international level on AI safety issues uh and think about how this initiative

could connect to those because this is just the kind of initiative that's needed if AI really is to benefit the vast majority of humanity 80% of which uh lives in the global South as

ambassador Tiggo said you know 99.9% of which are not AI technology developers themselves but will be impacted in some ways and um so in my remarks I'll just speak

a little bit about um why this kind of initiative can be very helpful based on the trends in the discussions in the technical landscape and also institutionally

[clears throat] and then come back to how it might link link in to some of the UN mechanisms that the ambassador mentioned. So looking at the kind of trends over

the last three years um the concept of AI safety uh has kind of morphed a bit and it's morphed with who's been involved in the discussions

and it's morphed with how the technology has evolved including regarding open source. And I'd start off by saying that from the UN's perspective and I'm borrowing

here from the work of the secretary general's high level advisory body on artificial intelligence uh several of whose esteemed members are actually speakers also today. Um that safety can

be thought of as a relational concept not an absolute concept. Uh when we talk about safety, we always think about who is who is safe uh and who is protected from which

threats. And so it's a vulnerab vulnerability based approach. And at the earlier discussions of AI safety, a lot of the focus was on so-called existential

risks. And the claim was that somehow everybody on the planet was at risk or under threat from some developments in AI technology. And of course some of the ideas going around

were around uh you know um the more kind of Hollywood ideas of AI wiping out humanity or becoming too too smart and then somehow you know turning everyone off. uh then it became more around kind

of biological weapons and weapons of mass destruction that could be enabled and harm everybody. But in any case, those earlier discussions um around the Bletchley Park time which had about only

30 countries involved were mostly focused on these kind of extreme frontier capabilities and the extreme existential risks to everybody from that. But then uh open source kind of

came along and the futility of containing these technologies within few hands became apparent. And so um I would also say you know from the commercial perspective one of the

earlier kind of impulses was around safety as a way of scaling uh a single model to a large user base and spreading the fixed costs of very heavy fixed costs of developing

concentrated models across a massive user base and so thereby making it more economically viable. But as we've heard in this summit and in the opening ceremony, the the minister mentioned

that you know from a developing country's perspective, 90% of the use cases could be handled by small language models tuned to a local context operating much lower cost uh both in

terms of development and you fine-tuning and also in terms of electricity and infrastructure. And most of the cost in that context is probably spent on um tailoring and

fine-tuning the model into a local situation. So the what does that mean for the concept of safety? Well, and here we could think about two different kind of

versions of it. I mean there's still the safety that applies to everyone and we can think of you know safety as accuracy and reliability of the models. If a model is inaccurate when it

diagnoses illnesses and you have false negatives and people have late treatment and they harm they their health is harmed or if a model is unleashed to make decisions on financial transactions

and it's sending money to the wrong people because it's not accurate and then the individual is harmed through that. or if people are being denied benefits inappropriately because the

model is inaccurately classifying them or judging them in a kind of court case and putting them in the wrong punishment because they didn't do it but the model inaccurately said they did. These are

the kind of risks that could impact most people if the model is applied in that way. But there are also increasingly um a focus on the sensitivity to contextual harms. These are harms

which may not be uh dependent on the model's reliability or accuracy but rather on the model's sensitivity or appropriateness to a local situation. And here we can think of things like the

cultural appropriateness of text or images or respect for religious uh beliefs. um and the ways in which models and linguistic differences, the way that

which models treat um interactions in different cultural contexts and all of this in a global south context in a much more resource constrained situation where there is less um perhaps

infrastructure or um energy connection to go around. So the concept of AI safety becomes less of a um or or it kind of edges into this more contextual field and that's where this kind of

local perspectives field tested um examples can be very helpful to surface which we're missing and I'd say um the idea of AI standards as technical standards don't solve that issue because

a one-sizefits-all standard will not be contextually sensitive. So moving from this kind of scaling a small very a very concentrated highly expensive model across a massive user base to more

tailored small language models to context turns the issue of AI safety into a more fuzzy kind of discussion and one which really needs empirical evidence. And I think the trends in the

institutional discussions from Blechley Park to Sol where there were also around 30 countries signing the declaration to Paris where you had 60 plus and now here over 100 countries engaging. We now have

the United Nations global dialogue on AI governance which will include a whole 193 member states um informed by analysis from an independent international scientific panel on AI uh

which will look at the risks and also opportunities and impacts of AI. And so as the conversation in these summit settings and in the international level has widened to include more countries

and more people and covered more of humanity, the focus um has through the open-source uh developments been allowed to become much more uh focused of encompassing other perspectives and

that's why um to close and to echo ambassador Clego uh you know these kinds of networks will play a crucial role in connecting and bringing examples of um cases of threats from various sources to

local people into discussions so that international discussions uh do not ignore or omit or discount the perspectives of the vast majority of

people on the planet. Thank you. &gt;&gt; Thank you, Mr. Chowo, for those remarks. I'd now like to call our panelists onto the stage. Miss Natasha Krompton, vice president and chief responsible AI

officer, Microsoft. Dr. Rachel Sabande, senior program officer, AI for Africa at the Gates Foundation. Before you sit, we're going to take one quick picture. [laughter]

Miss Chennai chair, um, I don't see you. Oh, there you are. Yes. Okay. Director of the Masakani African Language Hub, Mr. Amil Banatami, uh chief responsible AI officer at Cognizant. And last but

certainly not least, uh Dr. Balaram Raindran, uh head center of responsible AI at IIT Madras. [clears throat] &gt;&gt; Yes. And can we get the keynote speakers as well?

&gt;&gt; [laughter] &gt;&gt; photos. &gt;&gt; And thank you Okay. So,

as with all good things in life, we're short on time. Um, [laughter] but so let let's get started. Rachel, I'm going to start with you. Um, where do you where where according to do

to you what according to you or where according to you do you feel like we still lack clarity on how safe and reliable AI systems are when they're deployed in real world context in the

global south? [clears throat] &gt;&gt; Um, thank you. So couple of things maybe two three things. Number one is we need to redefine what is safe and what is harmful in as far as AI models or

applications are concerned according to the social cultural contexts that they are deployed in. And that means that having models or applications that are great at understanding the data or the

patterns to generate content is not enough if they do not understand the social norms, the gender dynamics, the religious beliefs, the political sensitivities or indeed even the humor,

the slang or the tone particularly now that voice is uh a key channel for delivery of AI. So we need to redefine safety and harm in the context in which AI models are deployed. So I think we're

missing that but hopefully we get there. I think the second piece is around language. It's not enough for a large language model to have strong translation

capabilities. Language in itself is not just about vocabulary. It's also about the lived meaning, the lived experiences. I come from a beautiful country called Malawi.

It's also called the warm heart of Africa. Now, if you're deploying a model for pregnant mothers to access advisory messaging there, if a mother says their waters have broken, which clinically is

a critical incident, that should warrant that mother to be referred to a health facility. [clears throat] But if you translate that from the local language to English which is where most

of these uh large language models and applications have been benchmarked on that will literally mean I have thrown away water. So if the model is not trained to

understand that context then you will miss um that flag and then finally I wanted to say that we also need to understand harms that eage as people use the AI models.

Currently I think much of the benchmarking is done on the content and predefined metrics. So final example, personally I use my AI companion as my therapist. So it's the one persona that

knows a lot about my personality from all spheres as a mother, as a career person, my finances, all of that. But at what point can we then be able to track whether I'm substituting my cognitive

capabilities with that AI model or application or that I'm becoming overly emotionally dependent? So, I think there are those three areas that we're missing and hopefully we can get better at it.

Thank you. &gt;&gt; Thank you, Rachel. And thank you also for those powerful examples because I think we've been saying some of this at almost a theoretical level, but I think

those examples really bring home um the the gaps in terms of where the current safety conversation is. Um, Chennai, um, from a from a civil society perspective, what what do you feel companies or

developers often miss about the safety implications of deploying AI systems in the global south? &gt;&gt; Thanks. Is that on? &gt;&gt; Hello.

&gt;&gt; Green light. &gt;&gt; Thank you. Um, that's one thing they miss, [laughter] the user experience. Um, so on a more serious note, thank you Vashi. So this

is great to actually piggyback from what you've said and I was like are we reading the same notes? Um so I think what really is missed when people are deploying some of these solutions is

around the context in which they're deploying the tool and this is particularly looking at an example where on the African continent there is high levels of gender inequality um a very

youthful population with young people often unemployed and also older people forgotten in actually the development of technologies. So I don't know who we're developing for but sometimes we actually

don't consider that diversity and the inequalities that exist. So you can find that sometimes when these tools are deployed they actually further exacerbate a situation of inequality.

And I'll give you one example where perhaps an agricultural tool that has a voice system uh on it to provide farmers who are women information on what to plant may actually have um a male

sounding voice. And if in that context there's high issues of gender- based violence or lack of trust and communities the community members were not consulted in the design process what

it actually leads to is just exacerbating an already existing situation and that is an example that actually did happen when people were deploying internet solutions uh for a

community. Then secondly also thinking about who gets left behind in deploying these solutions. This is where language um as Rachel was mentioning comes in. So on the African continent, we have over

2,000 languages that have been documented. Uh Masakani is only working on 50 of those African languages to build up quality data sets. So what you then find is when people are deploying

technologies, even if they deploy them in something like Kisui, which now has a large number of data sets, people just don't speak Kiswahili in across East Africa. And particularly in Kenya if you

go to Nairobi um the kisahi spoken in Nairobi will be shang then you go to it's not even kisili as I'm being corrected [laughter] um and then if you go to the coast in

Mombasa it will be completely different so we have to actually take into account the context and nuance of what is being deployed and then lastly the way in which this technology is actually used

if deployment doesn't take into around um the whole ecosystem of the end user, it can actually result in misuse. And I want to specifically say that there's two forms of misuse here. There

could be people who unintentionally actually carry out a problematic harmful act online based on how they're interacting with the technology. And we know that content particularly if it's

in their own language. And we know that content moderation for the global majority is not sufficient or people are underpaid as we've seen the cases that were coming out about um content

moderators in Kenya. Then there's actually intentional misuse. Now this is where we find gender disinformation, the use of deep fakes to discredit people particularly around election period. And

now with increased open AI that people can actually just type something and get something back. We are seeing that um high level of deployment without thinking about what is the after end

impact to close it off because I'm talking about AI as if it's coming later. Air tags when they were deployed it was great. I can track my missing bag on a flight. They've now been put in

women's bags or children's bags by people who they do not know and they track them. That's already an act of surveillance that was if people had been consulted it might have been mitigated

against. Yes, I do know want want to know where my bag is but I don't want to be tracked unknowingly. &gt;&gt; Thanks. &gt;&gt; Thanks Jennai for for that and also for

pointing out the the bringing the gender dimension on the table and highlighting these issues around what seems like useful technology how quickly it can become surveillance surveillance

technology. Um, I'd like to now bring the industry perspective into this conversation. So, Natasha, maybe I can start with you. Um, as as you scale systems globally, what are some of the

hardest constraints that you as a company face in in ensuring context contextsensitive safety? &gt;&gt; Well, thanks for that question and congratulations to everyone on the

establishment of the network. I think it's a really important step forward. So when I think about uh Microsoft, I think about um sort of Microsoft scale and our our mission is really to try and empower

every person in every organization the world to achieve more. And so one of the challenges I think that we face with scaling up our efforts here is how do we take the very um deep, careful,

thoughtful, communityled um evaluation work that animated a project like Samishka which uh the Kaya uh organization as well as the collective intelligence project and

Microsoft research worked on together which really developed you very contextaware evaluations that were appropriate for the use case. And how do we um take that thoughtful work and

really scale it up because really we want to do that type of work for thousands of languages and in probably millions of different cultural settings. And so um I think we want really need to

think about the system of how we are going to build multilingual and multicultural evaluations that we can really run broadly. I think sometimes we think

evaluations um we don't sort of understand how sustainably they need to be run a as in you can't just do it once before you release a product. You need to run the evaluations on an ongoing

basis to understand how there might have been shifts. And so I really think for us we need to think about the system. How are we going to build a sustainable grounded community-led system of

scalable evaluation? &gt;&gt; Thanks Adasha. And I hope in in some sense also the network can actually play at least part of that function is in building that kind of coherence to the

space of evaluation and helping us at least build a shared vocabulary and a shared set of methodologies um together as um as organizations. Um I'm here. Um what what do you think needs to change

whether it's internally within companies or externally in terms of the ecosystem that we're operating in to make such grounded evaluations the kind that Natasha was talking about u become the

standard practice for industry? Should they be the standard practice and if so how how do we get there? &gt;&gt; Uh thank you for that question and uh first congratulation and happy to be

also part of this network and support it. Uh I think Natasha mentioned part of the foundational questions and I think from a I'm putting my hat of cognizant chief responsive officer. We work with a

lot of companies and governments into deploying new scenarios we call it systems or applications or anything else. The concept of safety I was mentioned is is diffused is not very

clear what we call safety. So evaluating the the underlying element that needs to be changed or to be addressed is not obvious. When we talk about models, models are not just one things that you

deploy. It goes into an applications, there's a system infrastructure, there are network access, there's API connectivity, data, data access. All of them are contextually different that was

mentioned before. And then the problem, one of the problem is that you didn't ask me about the problem, but there's a problem issue is that there's a lack of imagination. people that are building

systems have no awareness about the context in which those situations occur and how they occur and what's the causes and what's the likelihood of solution to happen. So absent of that all this

context which language is part of it culture is part of it is not captured. So without that there is very little capability to address that from a regulation or incentive perspective. uh

safety on the other hand is not costed right into into financial systems and so forth. There is no penalty of not being safe. So as long as there is no constraint to put safety as a cost

structure which strong mandate companies will not pay attention or enough attention. So if it's not part of the the the financial planning and the processes and so forth will happen. So

there's a disconnect between what we do as an enterprises to sure that systems and platforms are properly built and deployed. There is the disconnect between the system in which they are

deployed. At the same time there is a talent inclusion that is missing. So the inclusion part is that all the talent that is building into those safety conversations are not the talent that

are exposed to those issues. So that absent voice is also a piece that needs to be addressed not just from a skilling perspective but also from an integration perspective. And finally the

infrastructure part the infrastructure is not just systems and models and data is also the tooling and the evaluation and it was mentioned that has to be done differently but if you don't know what

harms or safety means evaluation is going to be different. Um there is probably an opportunity here to um to come up with a series of evaluation tools that are not only built for model

design but also built for system deployment and how we go from pilot to scaling what issues occur and what examples are happening or what incidents are are are deployed and incident

reporting is a huge opportunity here because it would capture nested in the reporting some of the hidden element of the consumer issue. issues data access uh regulation uh um absence or anything

else. Finally, there is a latency issue um in global north and you mentioned probably correctly that there's only a handful of company countries in the global north and probably the new slot

is much bigger. Um there are institutional uh framework you have basically the rule of law you have civil society which is very active you have legal framework that basically create an

accurated feedback loop into all this incident safety in most of the global south countries these mechanisms don't exist which delays the feedback loop and basically compounds right the harm

possible harm and everything else so there is probably an opportunity to figure out how we can accelerate the learning capabilities and the speed at which we capture knowledge and data to

be tied with tools that are probably need to be implemented and deployed either on an open source manner or a free access manner and build it with a contextual environment the talent pool

to actually link it together. So the ownership of the global set of all these pieces are important. So the network could actually incentivize those different pieces that could

complete each other to really play a a a role into understanding better where safety issues are, where harm can happen and what corrections can be made in the rhythm that needs to happen because

rhythms are not exportable and what we do from one country to another is not and finally the network would probably help bring it together. Thank you for laying that out and also

just pointing out and how how all the kind of pieces link to each other and we can't just kind of go at it at one one level alone and and to the importance of capacity across across all those um

professor Renderan um AI deployment is accelerating in the global south in India in in many other countries as well but at the same time we don't see or so far we haven't seen as much investment

in the kind of safety and safety infrastructure Um, and would you agree? And &gt;&gt; you're actually asking an academic about investment. [laughter]

&gt;&gt; Sure. Of course, there is not enough money. Yeah. &gt;&gt; Why not? And how do we change it? &gt;&gt; So [clears throat] I I'm going to answer a different

question. &gt;&gt; Sure. &gt;&gt; Perfect. Like a like a true academic. &gt;&gt; I'm sorry. [laughter] &gt;&gt; Like a true academic.

&gt;&gt; No, I I I'll connect it back to what you ask. Right. So um there are a whole lot of initiatives that that are getting announced at the summit and also things that I kind of discovered while having

various conversations that there are multiple networks that are getting launched or that are already in operation. There's a network in Africa looking at capacity building. There's a

network in China apparently which none of us seem to have heard about but uh that's that's being launched on on on AI safety in uh capacity building and that is our network that's getting launched

and that is the UN initiative on building this network of capacity building institutes for the global south u which we had a meeting in the morning as well about that. So there are just

too many of these initiatives that are getting launched, right? And we have to figure out a way how we would coordinate operations among these initiatives as well, right? So I think that would be a

great force multiplier instead of everybody going out and saying okay let me see what small piece of the pie that I can get so that I can do this activities and stuff that if there is a

lot more coordination and if you remember our initial conversations about when we wanted to start this thing was about this would be like this one node in the global a

I can't even say global network of safety institutes anymore can I so they're not even safety institutes yeah so AC inst institutes whatever ACS this should be like one node in the AC

network which kind of represents unheard voices there because almost except for as the ambassador was pointing out except for Kenya so we really have and of course India I presume we don't have

uh uh safety institutes in in in uh in the global south right that can participate in the dialogue so I mean that kind of larger collaboration framework is something that we should

enable so that I mean even if You say we go to Gates, right? And then how many different people how many people how many different networks would Gates want to spend their spread their money to if

there is one way we can say that we are you know there's this whole operation that's happening then that would be great uh way of harmonizing our efforts. So there you go. I kind of it back to

your question. &gt;&gt; Thank you. No I mean I think I think you raised a really important issue of kind of harmonizing these efforts and also that how this network can play a really

important role in the in the larger kind of AC network. Luckily, the S remains the same, so we can still go with the acronym, I guess, on the on the safety network. Um, we're almost at time, so

let's just do one kind of quick rapid fire round with all the um panelists. Uh, and maybe, uh, Natasha, I can start I can start with you. Um, what what is the one concrete step your institution,

Microsoft, uh, could take in the next year to strengthen AI safety in the global south? Well, I'm looking forward to making good on the New Delhi Frontier AI commitments that Microsoft made,

which is going to help advance multilingual and uh multicultural evaluation work as well as share data that will um uh help policy makers make or understand AI adoption within their

countries and make the sort of choices and in policy interventions that help bring that broader access. So, if I can be sneaky and kind of count that as one thing. The second thing I'm really

excited about is we're making um uh large uh infrastructure investments uh across the global south to the tune of uh $50 billion by the end of this decade. Now, that infrastructure as air

and others on the panel have mentioned is essential to being able to building up this scaled system of sustainable evaluation. So, um I'm looking forward to those investments, too.

&gt;&gt; Thank you. Um, Professor, &gt;&gt; so that thing looks like one of the props they use in TV shows. Is that going to explode now that it is blinking and not going to explode?

&gt;&gt; No. Okay. &gt;&gt; It's been safety tested. &gt;&gt; So, so sorry, what was the question again? &gt;&gt; I'll ask you a different question. What

do you think would be the most Catholic investments that would be needed uh to advance? &gt;&gt; What do you think I do? &gt;&gt; WE KEEP ASKING THE INVESTMENT QUESTION.

Do you ask Microsoft? You have gates here. You have cognizant here. Why do I have to answer investment? &gt;&gt; Effort. Not [laughter] what would be the most important step.

Effort. &gt;&gt; Sure. I mean, so I mean one of the biggest things that I'll answer the question you asked Natasha. So, right. So, one of the things that we have to do

as individual &gt;&gt; We're almost done. &gt;&gt; Is that a fire alarm or something? &gt;&gt; No, no, no. It's our it's they're telling us that we have to wrap up I

think. &gt;&gt; Okay, fine. Okay, great. So wrapping up uh so we have to get the work going rolling right so talking about it is one thing uh but actually starting to do

this collaborations and getting these research efforts going would love to you know reach out to partners across the globe in fact I am part of the other UN network as well and we have been talking

about looking at problems that would necessarily require crossber collaboration right as opposed to you know problems that we would have anyway solved in our geography then just

working with somebody else to solve collaborate in two geographies. Okay. But if you can pick problems that will necessarily require people across borders to collaborate, I think that

will certainly drive this and also will, you know, kind of put forth the importance of having the network itself, not just information sharing, but actually problem solving that can be

done only across the network. &gt;&gt; Thank you, Rachel. 30 seconds. We'll do a quick &gt;&gt; Yeah, 30 seconds. I think from the foundation side is to really

institutionalize the evaluation of safety of AI solutions right at deployment cuz what we see now is that safety issues almost emerge post deployment. Thank you.

&gt;&gt; Thank you. &gt;&gt; Um so from the hub side we actually do have a benchmarking initiative that's going on this year. So this will be one contributing to the African benchmarking

work and so that will be our output um in contribution. Amazing. Looking forward to that. Thank you, Chennai. And Amir, last but not least, &gt;&gt; um we're working already on uh with our

two labs in one in Bangalador actually and one in San Francisco on um safety evaluations um mostly on incident reporting and we already made it culturally contextual. So our hope that

we are help to basically provide open source tools for evaluation uh to disseminate them and work with network to basically make them available to all all partners.

&gt;&gt; Fantastic. Thank you. Well, we're unfortunately out of time. I think this conversation could have gone on for a long time, but thank you to all of you for joining us and sharing giving us

your time and sharing your insights. And thank you to all of you for joining for the launch. [applause] Thank you.
