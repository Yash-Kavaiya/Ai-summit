# Who Watches the Watchers? Exploring Independent Verification in AI Governance

**India AI Impact Summit 2026 ‚Äî Day 5 (2026-02-20)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 12:30 ‚Äì 13:30 |
| üìç **Venue** | Bharat Mandapam | West Wing Room 4 A |
| üìÖ **Date** | 2026-02-20 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/NL6lEFmkDek?feature=share) |

## üé§ Speakers

- Dean Ball, Foundation for American Innovation; Fathom
- Gregory C. Allen, CSIS Wadhwani Center, Center for Strategic and International Studies
- Shana Mansbach, Fathom

## ü§ù Knowledge Partners

- Fathom

## üìù Summary

This session explores how independent assurance, operationalised through a marketplace of independent verification organisations, can support greater trust and adoption in artificial intelligence systems. The discussion will bring together AI policymakers and experts to examine mechanisms for realigning developer and deployer incentives with the public interest across different country contexts.

## üîë Key Takeaways

1. This session explores how independent assurance, operationalised through a marketplace of independent verification organisations, can support greater trust and adoption in artificial intelligence systems.
2. The discussion will bring together AI policymakers and experts to examine mechanisms for realigning developer and deployer incentives with the public interest across different country contexts.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/NL6lEFmkDek/maxresdefault.jpg)](https://youtube.com/live/NL6lEFmkDek?feature=share)

---

_[‚Üê Back to Day 5 Sessions](../README.md)_


## üìù Transcript

you've really got a remarkable group of individuals here to talk about this topic. Um, a who's who of so many different aspects of AI governance. So, uh, to begin to my immediate right, we

have Steven Claire who wrote the International AI Safety Report as the co-lead author if I'm not mistaken. Um, [applause] and he earned that applause uh because

that report is a remarkable document that I do think is is the foundation upon which all conversations about AI governance now must rest uh for the the next year. It's it's the sort of minimum

amount of knowledge that you must have to participate in the conversation which I think is is a really a tribute to him. Um then we have Heroki Habuka who is currently a research professor at the

Kyoto University Graduate School of Law and was also deeply involved in drafting uh Japan's first set of soft law regulations um and is an expert on all things AI but

also especially uh astute at what's going on in Japan. Um I we also have a privilege of collaborating with him at CSIS where he's a non-resident uh senior associate and I must say um he is

probably the the best person writing about Japanese AI policy in Japanese but he is definitely the best person writing about it in English. Uh and so I often tell Heroki that like if he doesn't

write about it nobody in Washington DC knows about it. So it's important uh his work. And then uh finally we have Shaina Mansbach who's the vice president of strategy and communications at Fathom

which is a young think tank started only two years ago uh but has already succeeded as one of the best conveners of the Ashby conference series on AI and also now leading a policy initiative

which I think she's going to uh tell us all about. So um without further ado I'd like to start with you Stephen. Um, I just said that the report that you were the lead author of is sort of the

bedrock for having a conversation on AI governance. For those in the audience who haven't yet made it through the report, but they of course will, uh, can you sort of set the stage? Where are we

in 2026 uh, in AI governance and in AI safety technical and procedural interventions? &gt;&gt; Sure. Thanks, Greg. Um, first of all, I'm sorry. If I'd known Greg was going

to make the report, you know, required reading, I would have tried harder to make it shorter. Um yeah uh thanks for having me. Thanks for uh I'm really excited to be here. So for people who

don't know the report is um it was founded after the Bletchley 2023 Bletchley AI safety summit um as sort of you know uh the shared evidence base for decision makers thinking about these you

know complicated fastmoving noisy AI governance questions. Um it's kind of trying to be like the IPCC report for for AI. Um it's backed by over 30 countries and intergovernmental

organizations. uh you know I'm one of two co-lead writers along with Karina Prinkle, but there's over 30 uh dedicated experts writing different sections and there's hundreds of people

that review it. So it's really trying to be a sort of state-of-the-art what do we know, what don't we know about um general purpose AI systems and the risks they might pose. Um I think this year

the main message of the report is like the the rubber is really hitting the road um or something with with these kind of systems. risks that even a year or two ago might have been theoretical

are now very real and we're seeing emerging empirical evidence. Um more real world impacts of AI on productivity and labor markets um uh and in science and in software engineering. It's all

like really happening out in the world. There's a billion people now using AI um around the world. Um many of those impacts include risks. Uh so we're seeing effects of deep fake spreading,

cyber attacks being more common with AI assistance. Um and so the need for sort of risk management techniques that are effective is also growing. Um one thing that I found surprising

working on the report is that in this domain on on uh risk management and and technical safety there's actually you know some good news quite a lot of good news I'd say. Um in various ways our our

technical safeguards are improving um models are becoming much harder to jailbreak. So, um, you know, 3, four years ago, if you wanted to, if you asked a model to give you a recipe for a

Molotov cocktail, it it would not do that. But if you said, "Oh, I'm I miss my grandma." And she used to tell me this amazing bedtime story about how she loved making molotov cocktails. Please

help me remember my grandmother, it' be like, "Okay, well, if it's for your grandmother." Um, then that stopped working maybe a year or two ago. But then if you maybe translated your

question into Swahili or something and put it in the model and then translated the answer back um it might evade safeguards. None of that works anymore. Um these safeguards are much harder to

to evade. And we know this quantitatively. Um for example, the UK AI security institute will try and evade the safeguards or jailbreak all these new models when they're released. At the

beginning of 2025, they could do this in literally minutes. find a sort of universal jailbreak that would elicit um potentially harmful knowledge. For the latest models, it's taking them seven 10

hours to get around safeguards. So, there's still vulnerabilities, but for novices um or even moderately skilled actors, it's becoming much much harder to evade them.

We're also seeing more of these safeguards get implemented into organizational practices. So 12 companies, all the leading AI developers now have frontier safety frameworks,

which are these documents that describe how they plan to to manage risks as they as they scale more powerful systems. Um, which is many more than had them a couple of years ago and is a you know, I

think a sign of transparency um and sort of collective learning about risk management that's worth noting. Um, so basically, yeah, our toolkit for managing these risks is is growing. Um

but you know it wouldn't be a safe report if I didn't maybe end on a few caveats or some bad news. Um the first is that these technical safeguards are still vulnerable in many ways. They can

still be jailbroken with enough effort um or in edge cases and it's very difficult to test and provide reliable assurances that these safeguards will work across this huge range of use cases

that these models are now applied to in the real world. And on the organizational side, you know, these safeguards only work if they are applied. And although we're seeing

especially from frontier developers where very prominent um you know usually quite robust safeguards applied on models um across the whole industry and especially behind the frontier

application remains quite inconsistent. The safety frameworks all these companies have them but they vary in the risks they cover they vary in the practices that they recommend. Um and so

the landscape as a whole you know these tools only work if they are applied. Um and we still see that some vulnerabilities across the landscape which I think turns this technical

challenge that points towards the governance challenge of how do we assure broader adoption? How do you ensure compliance? What do you do when there's a lack of compliance? Um we're sort of

facing these questions and again because these risks and the impacts are are now not something that we can sort of push down the road anymore I think for future years. Um the governance question is

becoming a lot more urgent. &gt;&gt; Terrific. And if I could contrast what you said with what we might have said if we were having this conversation back at the Bletchley Park AI Summit, it's

almost like the only good news on AI safety, AI security, and AI governance at Bletchley was well, at least we're all here talking about it. And now 3 years later, the good news is we've done

a lot about it. We have techniques uh that can provide demonstrable increases in safety. uh we we don't know everything that we need to work but we know a lot of stuff that does work and

so and uh really a lot of the challenges I think as the report says it's now in the hands of policy makers uh to to make sure that these uh safeguards get implemented robustly and diversely. Um

so with that I now want to turn to uh Heroki who I hope can give us a state of where we are in the story of AI governance around the world. If if the if the next steps are really in the

hands of policy makers where are we globally? Thank you Greg and again congratulations uh Stephen for the publish of the great report and I think first of all I feel very uh glad that

now the discussion on AI governance is such advanced compared with three years ago and I'm a lawyer and I'm a former policy maker I worked for the Japanese government for four years designing the

Japanese AI policies mainly in terms of regulation and governance and as a lawyer and policy maker the question after reading The report is where is the end to what extent stakeholders has to

manage the risks because in the end you can't remove all the risks AI is blackbox and you know the technology advances so fast and even though there is advancement

progress of godwares the next day you may find another risks so there is no end to the story then how regulators should design the regulations s that is the main question all countries are

facing and different nations regions take different approaches. So maybe the most famous regulation is the EU AI act and in that context a lot of people say hey EU takes a uh hard hard law like

regulatory approach on AIS while Japan or UK United States takes a soft law approach but I think it's a completely wrong understanding of the regulatory framework because as you know there are

already a lot of regulations that can be applied to AI systems, privacy protection laws, copyright laws or sector specific laws such as finance, automotive or healthcare. We already

have a lot of regulations out there. So the real question is not whether or not to regulate AIS but the real question is how to update our existing regulations and where's whether or not we need

additional regulations targeting AI systems in addition to the existing regulatory framework. So in that sense all country takes the hard rule approach and also all countries have soft laws

because European Union there are a lot of technical standards to implement uh the EU AI act uh that are now under discussion but anyways so all countries have both hard laws and soft laws. That

is a start of the discussion. And then when we compare EU approach and Japan approach, the clear difference is whether to regulate AI holistically or not sector specific. And when I compare

the Japanese policy and the US policy, we are on the same position uh as to this like taking a sector specific specific regulation. The main difference I uh understand is whether you uh

prioritize the exant approach or exposed approach. The US more like exposed approach you can do whatever you want to do and the regulation is usually very high level principle based but once you

have a problems if you damage others properties or lives then you go to the court and you fight in the court. The Japanese society is not like that. In Japan uh actually the number of losses

are very low. People prefers to set the rules in advance. Japanese companies are very very good at complying with the given rules but they are not very good at creating their own governance

mechanisms or explaining to stakeholders why you are doing that. And now Japanese stakeholders are starting to realize that it doesn't work. So we need to have more agile and

multistakeholder approach. So we are trying to leverage the power of soft laws negotiating among different stakeholders and give the standards guidances and but in the end again if

you violate the existing hards of course you will be sanctions. So that that's the main uh differences in uh American approach and Japan approaches and in in in the end all countries are

facing the difficult question of how to deal with this cutting edge technologies at the black box and there are li unlimited risk scenarios and sometimes we don't know how to evaluate the values

such as you know privacy or transparency or fairness there has been no clear benchmark standards so far in the society. So how to design those benchmarks and revelation methods or the

challenges all countries are facing. &gt;&gt; Terrifici and uh Shaina I know you have a unique perspective on this because your organization is now proposing sort of uh additional models of AI governance

that are not really uh uh reflected in existing law whether in the United States or Europe or Japan or India. Um so so walk us through you know what you see as the the important work worth

doing now. Sure. My uh my panelists have set me up very well to say this. Uh so I think as the international AI safety report shows um the capabilities around these models are surging and as the

capabilities surge so too does the uncertainty around the risks by which I mean do these systems work uh safely, securely and as advertised. That uncertainty creates a trust

problem. A trust problem for the public which doesn't have a way of figuring out what is actually safe. A trust problem for deployers by which I mean uh hospital systems, retail, banks who want

to and indeed need to use these systems but have no idea what they can actually trust. There's a trust problem for the regulators too. They don't know uh how do you confer not just trust but how do

you confer earned trust? And I would say there's a there's a trust problem for the developers also because if and as trust starts to decline, you're going to see adoption decline as well. So this is

something that I think the developers should be focused on too. And the current approach is just not the current approach to tech governance is not equipped to handle this trust problem

very well. Traditional command and control governance says here are the rules. Here are all the things you have to do. Here are the procedures. Here is what compliance actually uh here's what

compliance actually looks like. There a bunch of problems with this approach in the context of AI, but I'll focus on two which is the speed problem. AI moves really really quickly and even

well-intentioned regulations are going to become outdated very very quickly. And then there's a technical capacity problem. Even with the rise of the AI safety institutes which are doing

amazing work the the talents the expertise for understanding these systems and understanding their risks is largely concentrated in the frontier labs which of course leads some people

to say well let's just go to the frontier labs they can regulate themselves there I don't think I have to spend too much time explaining why there are

problems with that approach but uh you know it's simple incentives I think all of us know people in the labs who are doing amazing amazing work they are the people who make sure that I can because

of them I sleep better at night. But um the incentives are just not there. There are always going to be trade-offs between investing in safety testing and tooling and investing in development. Uh

so we're going to have problems with uh with self-regulation in terms of addressing that trust gap. So where does that lead us? Uh at Fathom, my organization, we're very focused on

coming up with new models that can uh solve this trust gap. So we're very focused on independent verification. uh specifically a marketplace of independent verification organizations

uh by which I mean a government authorized and overseen marketplace of independent verifiers which are charged with test deter creating testing and tooling to determine whether these AI

systems are actually safe. Um the the difference here is that this is an outcomesbased approach instead of as I said having procedures here are the rules here are all the things you need

to do uh here's here's here are all the boxes you must check to be certified as being good even outcomes uh based approach where you have a government saying here are the things that we care

about we care about children's safety we care about data privacy and protection we care about controllability and inter- interpretability ility and then you have independent verifiers that can actually

go out do the testing have updated testing constantly to make sure that those outcomes are being met. We think that independent verification solves for a a couple of these deficits in the

trust context. Um you know first they are independent the labs are not grading their own homework. Second, democratic accountability. You have governments that are creating outcomes instead of

the industry doing it itself. Third, flexibility. Under this system, the IVOS's, independent verification organizations are constantly updating their testing and criteria to make sure

that they're keeping up with the pace of technology and the pace of risks as well. And I think the fourth thing which is pretty interesting is uh is it creates a race to the top here. Right

now the only people working on or mostly the only people working on safety testing and tooling are in the labs. What we are envisioning is a marketplace that incentivizes ever better testing

and tooling here. Um I could talk about IVOS for days and days but let me just end on one point. Um I was talking to Greg about this earlier and Greg asked are are there are there analogous

systems or industries or uh you know sectors that we could talk about and I said sort of I mean in America we have underwriters lab uh there's lead certification you know there's some

analogies but the honest answer is there's not a perfect analogy we've had the same regulatory system for the last century and I think that with the rise of AI we're seeing that system is no

longer built for purpose and when we try to use old systems hard law soft law any of these things we're really struggling to uh to make it work. So what I'm trying to do, what I'd encourage all of

us to do is say, you know, we do need to think a little bit differently because this is what uh these this technology in this time calls for. &gt;&gt; Well, that's great. So there's a few

points I want to uh pull together there. The first is, you know, as Heroki pointed out in the the US system, liability law looms extremely large, right? the the lawsuits at the end of

this story when things go wrong and when you have as for example chat GPT does 800 million weekly average users something's going to go wrong every week right and and the question is uh how is

that going to intersect with our existing body of regulation how is that going to ex intersect with liability law the the second thing is um this is going to because we're talking about these

generalpurpose technologies This is going to be adopted in so many different sectors of the economy and right now as as Shaina pointed out the the number of people who have you know

Steven's expertise on what it takes to really make AI systems safe and well-governed and perform reliably as intended across the whole range of potential applications. That's not a lot

of humans on planet Earth who are good at that stuff. And because these AI models are going to be deployed in just about every sector of the economy, we need some level of those capabilities in

every sector of the economy. And so the the question is you know if I am a finance company if I am a healthcare company you know how am I going to know and how are my consumers going to know

uh that when they use AI related capabilities it's going to work reliably as intended uh over the full range of acceptable use cases and so Stephen I want to come to you and ask when it

comes to governance when it comes to oversight and verification how do you see the B the the balance of responsibilities in terms of what responsibilities need to fall upon the

model developers what responsibilities need to fall upon the users what uh responsibilities need to fall on independent third parties whether that's the government whether that's auditors

whether that's uh this marketplace of verification that Shaina is talking about so what do you see as the the balance of responsibilities and how might this go wrong how might this go

right in 30 seconds or less. [laughter] &gt;&gt; Kidding. Kidding. &gt;&gt; Um, I mean, I'm sure it's kind of the the boring but true answer of it. It's

depends and it'll vary a lot across uh use cases and and sectors. Um, I think probably it's not the case that it's fair or helpful or true to allocate to one act or another, but instead we need

this like layered approach of just many different uh policies um and practices at different parts of the stack. &gt;&gt; Um because none of our approaches are foolproof. Um they all have

vulnerabilities and so we have instead of like safety by design, we have this like safety by degree um situation where we want defense and depth. So um for developers there will be like training

uh training techniques that they can implement to make models less likely to elicit dangerous knowledge in the first place. Um if there are people building on top of those models and then

deploying them, there will be monitoring um uh systems they can put in place and classifiers that identify dangerous queries and stop models from answering them. And then probably for ecosystem

monitoring bodies which could be deployers but could also be other institutions in the world there can be how tracking how AI content is spreading across borders and around the world and

then I think there's this other aspect of we're focusing a lot on sort of model or developer safety but as we we are moving into this world where many people around the world are having access to

powerful helpful intelligent technologies and we also just need to adapt for that reality and think about resilience at the societal level too of how do we adapt to the the

beneficial use cases and the and the various use cases that these models will be used for. So thinking about hardening digital systems against increased cyber attacks. Um just sort of admitting the

reality of the situation in many ways and adapting to it rather than trying to prevent all harmful uses in the first place. Um I think we need like a variety of approaches across all these different

actors. &gt;&gt; Yeah. And just to use an analogy uh for how broad the group of stakeholders is, if you think about a ride hailing service, a taxi service like Uber, um

you have the automobile manufacturers who have to make sure that this is a a solid car design that was manufactured uh safely and appropriately to specification. Uh then you have you know

uh Uber where in some in some countries Uber owns the car and so they're responsible for ensuring that it gets maintenance uh appropriately and then you have the the driver who's insure uh

who's responsible of ensuring that they are actually following the law and driving the car safely. And if you apply that analogy to AI, you have uh the model developer. Then you might have the

sort of business use case deployer, which could be a bank, a medical device company, uh who a financial institution, whoever. And then you finally have the end customer who who's, you know,

receiving those services and making sure that they're using them appropriately. And so um if you think about that sort of different body of use cases, as I said before, the capabilities uh are not

symmetric across all of those. uh but there are sort of obligations and so uh Shaya I want to come back to you and and ask this model that you're uh proposing um what exactly does it mean for the

different stakeholders in the ecosystem? How does their life change uh if we adopt the system that you're in favor of? &gt;&gt; Yeah, I mean I the overarching answer is

we create trust throughout the system which is something is is the missing piece here. I think there there are a couple of pieces that I would pull out. Um

you had mentioned liability earlier and I let me talk about that a little bit. Um what this system does it does not assign liability. It doesn't say you know deployers you know developer it's

you it's you. This is we're seeing at least in America uh courts move their way through the system or wow court cases move their ways through the court system. uh and we'll see where that is,

but where that ends up being. But what is really missing is um uh a standard of care. And this is I think one of one of the real advantages that this system has. So right now, at least how it works

in our current tort system is that if your your Whimo kills someone, someone can sue and uh then a judge and a jury has to figure out did. So again, we're not answering who should

be sued, but let's say that uh the family of someone who got hurt or killed is suing Whimo. Um what happens is that the the jury has to decide whether uh whether the uh person

who was sued did the right thing. And if you are not technical, that is the hardest thing. Even if you are technical and you know, maybe even Whimo doesn't know. So what this system would do is

confer uh if you are verified it would confer the verification would confer um a rebuttal presumption of having met a heightened standard of care. So what we're doing is uh def clarifying and

defining upfront before an actual harm happens what a developer or excuse me what a deployer or whoever is sued is actually supposed to do. um instead of having you know this very very messy

system where someone after the fact has to figure out what went wrong and who's who's responsible for that. Um I can talk about other layers of the stack here but I but I think the the liability

piece is really key. I mean we just see this I think it's a reflection of the the trust problem here where when you're a deployer I mean god I think every everyone that I talk to um you know

again hospital systems retail banks uh anyone who needs to be consumerf facing is really worried about this problem I mean when I get sued what do I do and maybe there'll be a populist backlash

and everyone will hate everyone who's using AI systems and it's much better to ahead of uh something like that actually ahead of that happening um have that standard of care defined up front and

have uh that seal of approval conferred. And uh Heroki as you think about you know the different stakeholders in the system and especially the idea of auditors which now there are a number of

organizations uh being founded it seems like almost every day who are proposing to provide uh external evaluation services that can help companies understand you know as as

uh Shaina said this this product or this service or this company meets the seal of approval um and and we've vouch for it as an independent entity. What what kind of momentum do you see for this uh

independent assessment uh part of the story across regulatory frameworks? &gt;&gt; Independent evaluation is essential given that we are all using AI systems for all different situations starting

from language models to healthcare systems to car driving. But it would be uh not easy to persuade corporate executive to use the independent audit without clear economic incentives.

&gt;&gt; For example, if you get the uh certification for the autonomous driving then you can sell the car to the big market then of course you pay for the audit. But if you

like uh if you say that if you take this audit for this language model then you can prove that this language model is relatively safer than the other models but it doesn't necessarily make enough

incentive for model developers to to to to conduct the uh audit or evaluation systems like independent evaluation uh because there is no clear uh financial incentives and let me talk about

&gt;&gt; actually could I ask you elaborate on that. So like where where might these financial incentives come from? You mentioned one which is the regulators force you to do it. That's one. Uh maybe

insurance is another like where where might these incentives come from? &gt;&gt; I think it should start from the regulated areas &gt;&gt; such as cars, healthare systems, finance

systems or infrastructures because everybody needs a strong uh require strong trust on those systems. If it doesn't work well then somebody might have be killed. That's a big problem.

And maybe you could say, hey, but in the end, if you are killed, you can be compensated, but it's it's not the end of the story. While uh if the damage could be compensated by money by the

company and stakeholders are okay with that, maybe companies would like to just run the system go and uh and compensate to the victims. For example, if the language model says something

discriminative, the company can just say say, "Hey, we're very sorry. We introduce uh better guardrails and we pay for that if you want compensation, small compensation.

&gt;&gt; Yeah. So, so Stephen, if I could bring you in here, &gt;&gt; um we framed the uh report as the existing consensus of the scientific community when it comes to AI safety and

governance. And that's in terms of what is possible, what interventions work, what the risks are. Um, but I want to ask about how we go from that degree of consensus to something that might be

more of like a standard around procedural implementation. You know, uh, Shaina's term of art is standard of care, which matters a lot in the American legal system. Uh, I'm sure it

matters a lot in other legal systems. I'm just ignorant about, you know, how and where. Um, and so I'm I'm curious, you know, what do you see as the gap if if if these independent evaluators,

these independent auditing organizations are emerging? Um, how do they go from we think we're good at this to no, this is the accepted best practice? You know, we have accepted uh consensus on the risks

and the interventions, but like how do you turn that into a procedure? Just to give an example to the folks in the audience, I used to work at a rocket company and our uh the safety standard

in the American aerospace industry is AS9100. And in the history of our company, there's kind of like a before AS9100 moment and then there's an after-9100

moment. And everything changed for our company, you know, after we got that third party audit evaluation. A lot of our customers, you know, just said we do not sign checks for uh companies that

are not AS9100 certified. So, you know, you you are deeply steeped in where we are today on the consensus, but how far are we from converting that into standards and procedures for for third

party evaluation? &gt;&gt; Yeah. Um, I'll also say one follow up to Heroki's point too about auditing. Not only is there sort of a lack of incentives to conduct audits voluntarily

now, um there there's might even be disincentives where one is um it's costly and it slows you down and there there's a very intense competitive pressures to release faster. And there's

also potentially um like information or or security risks to sharing. you spent hundreds of millions maybe billions of dollars developing a model and then you have to share it with an external party

um before deployment um like serious risks to or serious perceived risks at least to um to the having that information leak or or um so I think uh yeah it's it's um there's some serious

challenges there I guess there's one other um potential uh part of the story which is sometimes you see companies want to be willfully blind right if they have a if they have a report that says

my product is not safe. Well, now they know they're going to lose the lawsuit. Whereas, if they never commission the report, maybe they'll win the lawsuit. Um, so Shaya, what do you see as as

meaningful interventions that can help address this problem? Both the cost side uh that Stephen mentioned and the uh the other parts of the incentive structure. &gt;&gt; Yeah, let me make a couple of points. I

mean, I think um we're talking about the cost of audits. And I think this this is a big issue that we think about a lot. This system will not work if everyone if there's a flat fee everyone is paying a

ton. I mean we are really we think that an unsuccess there are many ways that a system looks unsuccessful and one of those ways is if it is just protecting incumbents and we're thinking we

envision the system as something that works for you could verify a general purpose LLM you could also have narrow AI you could have you know a tiny little tool a little chatbot that is used in

schools those three different products should not be audited in not only at the same cost but in the same way I mean compliance isn't just you know the check that you are writing it is you know how

how much of a pain in the butt is it uh you know how many lawyers do you need how long will this take so the great thing about this being a marketplace is that uh the system is right sized to

risk type to size of these products and um in again instead of having just uh oneizefits-all this is what you have to do to comply because I I think that that is a a real issue and really quickly I

just going to go back to, you know, the question that you asked Heroki about incentives. I mean, you can imagine a system where this is mandatory and maybe in some areas you can imagine that um

but I think that there are three real carrots for wanting to get verified. Um we talked a little bit about liability. So obviously the liability, clarity, uh that this uh is a big carrot. I think

the insurance piece um uh the insurance piece is is real right now and we are seeing the big insurers saying we're not going to touch this. We're not going to insure any AI products because we have

no idea what's inside of them. At least in America, the way that life insurance works is if you want insurance, you have to uh jump on a scale and tell someone how healthy you are and what are the

things that you do and the insurer decides, okay, are you worthy of being insured and at what premium. I think that's actually a pretty direct analog for what we're trying to do here. uh

where the books are opened and an insurer can look at whether they don't have to do the testing themselves but they can look at whether the system has been verified and say okay we will

actually insure you or we will ensure you at a more affordable premium and I think the third thing is just straight up market uh advant you know competitive advantage if I'm a school super uh

attendant and I am choosing between two learning chatbots to put into my schools I'm not going to choose the one that has not been verified I want the one that has been verified, that is safest. Um,

yes, because I'm worried about getting sued, but because I want my kids to be safe. And you can imagine a situation, um, much like Underwriters Lab in the United States where basically all

consumer products like light bulbs, toothbrushes, basic things that you buy at a store like Walmart all have the UL seal of approval. And those are the ones that get sold in stores. They have a

huge market advantage. They pay a little bit, but not very much. And uh in exchange for doing that, so they go to market in a way that's or they compete in a market in a way that the ones that

don't go through verification do. I'm so sorry. You asked me an actual question and I just answered everyone else's question and probably not my own. So if &gt;&gt; it's okay you get out you have a get out

of jail free card because you mentioned insurance uh which is something I'm deeply interested in uh right now. Now, I mean, in that uh space orbital launch vehicle example that I just mentioned,

uh you can't get insurance for space launches of satellites until you're AS9100 certified. And that is 10% of the cost of getting a satellite into space. It's just the insurance on the rocket.

And so, uh basically companies that can't get insurance can't compete in the market. And as Shaya mentioned and I think this is a super undercovered story um there are now many of the majors sure

insurers in the United States at least are saying for your enterprise risk policy AI is not included. So if you are a major bank and you are doing you know big important financial transactions

as soon as you start using AI you've lost all your insurance. And I think the um the the Trump administration in the United States has a very light touch regulatory approach. And my concern

there is that well just because the government is not doing anything big on and bold on regulation doesn't mean there will be no regulation. uh the insurers will step in and if the

insurers exit the market, you know, maybe not in legal terms, but in economic outcome terms, that could be very similar to draconian uh regulation. Um so Shanny, you're you're mentioning

of the Underwriters Lab, which is an organization that writes and standards that are relied upon by underwriters, the people who are issuing insurance. This is a huge part of the regulatory

and governance ecosystem, um that I think is really important. And so now I'm hoping, Stephen, that you're going to tell me uh that you've been reached out to by a bunch of insurance companies

and they're all reading your report eagerly and and thinking about this, but um maybe maybe not. Uh what's the case? &gt;&gt; Not yet, but it's it's a really long report. So [laughter] maybe um 212

pages, but it goes like that. &gt;&gt; Um maybe I can come back to the best practices point a little bit. Um because I think we're talking about auditing here and at least I know there's a lot

of steps involved I'm sure but at least at the technical level the main tool we have right now to audit the capabilities of the risk of an AI model are evaluations and although in my opening I

sort of talked about oh it's great we have this toolkit that's emerging and it's strengthening and that is true um I think on evaluations in particular as far as like okay let's say we have

auditors that are looking at these companies looking at models what are they actually looking at to to audit or evaluate the models I think we actually have a big gap here, a big evaluation

gap in terms of um well, how are we actually assessing? And so if we're moving towards best practices, not only do I think we don't have a sense of the best practices right now, but if we did,

they'd be different in a year because the capabilities are moving too quickly to keep for these technical tools to be in date for very long. &gt;&gt; Um so for example, you'll have, you

know, these evaluations often look like a set of questions related to a certain topic and you ask the model, so you have a bunch of questions about biocurity or a bunch of questions about cyber

security. Um, and if it's above, if it scores high enough on the test, you say, "Whoa, we this is this is dangerous capability and we need to implement more safeguards or something." And as far as

what's best practice or safe risk management for a company, we evaluate in terms of well, does it seem like the safeguards apply proportionately to the risk that you've assessed. Um, but I

think in many cases, these evaluations we're using are already not super informative about real world risk because they're too narrow. um because you have to build a set of questions

that gives you some information about the vast range of use cases in the real world. And as models have become more capable and general and adopted more widely, this has become much more

difficult. Um and I don't think there's very many actors out there that are constantly thinking about new ways to evaluate the capabilities. Um and so um I think this is like an important gap in

terms of our toolkit that is again quite urgent because these models are being released and we're using our current evaluations which are already in many cases out of date and not super

informative about real world risk. &gt;&gt; Shannon, do you want to jump in here? &gt;&gt; Yeah, I Stephen I agree with you so much. I mean we we're all of us are obsessed with benchmarks because that's

kind of all we have and they are just so narrow. I spend a lot of time with um organizations that we think will become these IVOs and testing is so so hard. I mean think

about this like we have a fundamentally stochastic system. So I can ask something 10 times system 10 times uh and I'm going to get 10 different answers. So what does that mean in a

safety context? Uh another problem that we have um you know what a uh what a model outputs is not the same thing as what someone does with it. So think about in the context of mental health,

you know, maybe the model says 10 time, you know, to 10 different people, different versions of I think you should kill yourself. Nine times, you know, maybe for nine of those users, that's

fine. They will laugh it out, but for one of those users, there's going to be a real problem here. Um, and also the the multi-turn nature of uh of AI. I mean you you build relationships with

these systems and you you ask long queries and this stuff just gets really complicated really quickly as uh technical minds could explain far better than I could. So what I'm what we are

trying to do here is incentivize better testing because right now the only people creating eval organizations who are doing God's work doing awesome stuff but you know what does it mean you're

the best meter out there? I mean there's just um there's not an incentive to be to go from good to the best and the other actor working of course are the labs and I think many of the labs are

actually uh attempt to be responsible actors here but again there's an incentive gap I think the only way that you're going to solve this is to have an ecosystem where all of the actors are

competing to have the best services to have the best evaluations you know one in our we hope one day um you know one of these IV O says, "I've developed a new type of testing that figures out

this kid safety thing that no one has ever thought about." And then the next day someone says, "Well, we have to be better because then everyone will want to be verified from that organization."

So, you were incentivizing ever better testing. And as Stephen says, I think that uh just given how quickly and dramatically the capabilities and the risks of these systems are increasing,

we need really good testing and tooling that can keep up with that. The only way to do that is to incentivize it. &gt;&gt; So, Stephen, if I could come to you about what uh Shaina just said, you

know, she you you pointed out how the the state-of-the-art in evaluations and assessment is constantly shifting as the capabilities are shifting. I sometimes hear the the frontier labs say yes, and

that's why we're the only ones who can do the testing because we're the ones out there on the frontier. But Shaina is making this point about misaligned incentives which I think we saw in a

conversation you and I had a couple weeks ago uh in the XAI Grock um you know undressing children kind of example. There's there's perverse incentives uh sometimes at work here in

terms of the companies evaluating themselves. So how do you reconcile that that gap between the the frontier AI labs often do have a unique perspective and a unique understanding but also it's

really hard to see how we could ever be comfortable with them being the only ones assessing themselves. &gt;&gt; Well I can talk about a bit in the context of the report where we try to

work with well we try to work with everybody to get the state of the state of the science across the whole landscape. Um and there I think you know it's it is true that there's this big

information asymmetry between um the people in the labs who both have the most technical capacity and also the most access to leading models and all of the information about testing and

development that they don't release publicly. And if you don't draw on that knowledge um you're not going to be able to understand what's actually going on in the AI world. Um but then I think we

brought in a lot of perspectives from academia and civil society and and government feedback um to sort of get a full perspective of of the landscape. Um, as far as what to do going forward

to deal with this, I think probably it looks something like this with partnerships that are aiming to draw on that knowledge. Um, but then aiming for transparency and information sharing

that gives, you know, third parties and external actors a better understanding of what's actually going on. Because it's true like even writing the report we were reliant on these papers that

labs will occasionally publish and drop with like very useful data on how people are using the models or adoption rates. Um but we're kind of reliant on these like ad hoc publications. Um and uh then

that le leaves a lot of gaps across the landscape and different risks. And so um we you know constantly had the word uncertainty or um um unknowns in the report um because we lack that data

outside of the labs. &gt;&gt; And do you think that that's likely to remain the case or do you think that that could change over time? You know, as we've we've seen literally, you know,

the safety staff of some of these labs quit and start their own auditing companies. Um so are they likely to have their skills atrophy as they get farther from the development process or do you

think it's credible that you know these third party organizations um can build The word that comes to mind is like economies of scale that are relevant to to to be able to continue advancing the

state-of-the-art of uh safety and governance even as the technology keeps evolving. I'm not sure but I think what we can do is sort of look at the trend and the

trend is towards I think um a stronger ecosystem around AI labs as more people as these problems of of lack of data and lack of independent verification um are identified more there's more people

working on it and then I think we've seen some movement towards greater transparency with so frontier safety frameworks are now a governance mechanism that's in the EU AI act or in

the code of practice um and has become institutionalized it started as voluntary anthropic just published a responsible scaling policy and so you've seen these movements towards sharing

more information in more structured ways. I think also yesterday where the new commitments from the companies at the summit which were related to sharing data about usage. Um so I think as like

a broader set of actors in society are paying attention to AI because again we're feeling the effects more clearly. It's becoming more of an economic priority. Um we'll see more demand from

outside the labs to to share this information and yeah maybe that will lead to some changes. Um, Heroki, you know, you've written a ton about AI, but in your capacity as a lawyer, you also

have a lot of understanding of many different industries. Are there any lessons learned from other industries here that have solved this sort of technical expertise exists here, but the

need for independence exists here? What what kind of precedents that you see that we can learn from? &gt;&gt; Okay. So, uh before that, let me add one more incentive, which is a public

procurement. If the government says we recognize this LLM or model is safe and then government procure this this standard model then it will be a big incentive for developers.

So that is one thing and uh when I try to answer your questions um I think democratic debate is necessary as to what kind of risk level is acceptable and also what kind of test measures are

good uh because there's any single specific answer as to uh the this is acceptable level per se for example uh in Japan every year more than 2,000 people were killed by human-driven car

and the question is what kind of safety would would we require for the autonomous vehicles? Is it okay if it is that the the kill number is less than 2,000 or would we like to require more

safety than human drivers? Then if so, what would be the level? There's no single answer to that kind of question. So we need to debate democratic manner as to what is our uh acceptable

goal. uh and also about the test uh uh measures. Uh for example, we can just simply compare the number of rate per kilometers. But if you test in a very safe street highway, of course, it's

easier to get the safety. Uh while if you try to drive in the pretty complex city, it's going to be very difficult. So how to measure or how to define the the test methodology is the next

question. And I don't go into the details but you know the similar discussion has been done in a lot of uh industries car industries or finance industries or aerospace industry. So we

can certainly there are a lot of lessons learned from the existing. &gt;&gt; Yeah. Um, one one analogy that as you were talking uh you you joged my memory is the National Highway Transportation

Safety Administration in the United States which actually industry begged for this organization to be created uh in the 60s and 70s because they said look uh all of us are going to claim

that we have safe cars but only some of us are making big investments in becoming safe and we want to reward the people whose good behavior is making big safety investments.

And so they created this new organization which would give cars a safety rating on one to fivear or one star. And so now uh the companies can only get a fivestar rating if they're

actually doing what it takes to be safe. And consumers, you know, they're not always qualified to rip open their their car's engine and see what it looks like under the hood, what's safe, but they

can interpret that five-star uh rating. And so, uh, my idea was to ask you, Shaya, to elaborate on this in the context of your model, but I'm I'm now scared of the beeper, uh, which is quite

loud and scary. So, um, please, uh, join me in thanking our terrific panelist. Good afternoon ladies and gentlemen. I would request everyone to kindly remain seated as we are having a very

interesting session that is coming up conducted by Confederation of Indian industry and we'll start after rearranging the diet. So I would request everyone to kindly wait for 5 minutes.

Oh yes, we should talk. Yeah. Yeah. Yeah. &gt;&gt; Okay. I was
