# Beyond Proof of Concepts: Using 4D-AI to Build Sovereign, Sustainable and Responsible AI at Production Scale

**India AI Impact Summit 2026 ‚Äî Day 5 (2026-02-20)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 11:30 ‚Äì 12:30 |
| üìç **Venue** | Sushma Swaraj Bhawan | Shakuntalam Banquet |
| üìÖ **Date** | 2026-02-20 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/9WteaphGxng?feature=share) |

## üé§ Speakers

- Omeed Hashim, Kainos
- Theresa Yurkewich Hoffmann, Kainos

## ü§ù Knowledge Partners

- Kainos

## üìù Summary

A fast paced, practical session for government and industry leaders who need AI that works in production‚Äînot just in pilots. Join us to explore the four dimensions of Trusted AI and discover actionable strategies for scaling secure, responsible, and sovereign AI across the globe.

## üîë Key Takeaways

1. A fast paced, practical session for government and industry leaders who need AI that works in production‚Äînot just in pilots.
2. Join us to explore the four dimensions of Trusted AI and discover actionable strategies for scaling secure, responsible, and sovereign AI across the globe.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/9WteaphGxng/maxresdefault.jpg)](https://youtube.com/live/9WteaphGxng?feature=share)

---

_[‚Üê Back to Day 5 Sessions](../README.md)_


## üìù Transcript

like for instance Canada we are very strong in uh Poland and various other countries as well. So that's that's what I wanted to say I think if you can move on to the next slide please.

&gt;&gt; Sure. So um just as we're starting we thought we've shared a little bit about who we are and and what we're doing but we would love to know more about who is in the audience. So if you can scan this

QR code um we've got a couple questions there. It'd be interesting to know or we can do a show of hands. Um, who in this space is is working in AI right now? Interesting to know. Are you a

developer? Are you working in innovation? Are you researching? Um, are you in policy? &gt;&gt; Is it working? &gt;&gt; Oh, zoom. the u the picture

is it possible? &gt;&gt; No. &gt;&gt; One moment. Otherwise, we'll just do show of hands. &gt;&gt; I I think uh if you guys come out of the

presentation mode, maybe you can zoom it that way. Yeah. &gt;&gt; Yeah, that's it. &gt;&gt; Yeah. If you could just just zoom that bit.

&gt;&gt; It's working. No, didn't really get bigger. &gt;&gt; If you make the window bigger, &gt;&gt; okay, it's fine. We can &gt;&gt; We can We can do a show.

&gt;&gt; We can do it by show of hands. We didn't need the question. So, &gt;&gt; if you go back to that same slide, please. Okay, let's just do a show of hands. So,

who here is working who here is a student? Student. Um, who here is working as a developer or in engineering? Very cool. Um, who here is working with the

government uh in policy? Nice. And do we have anyone working in research? No. Anyone? What did I miss? &gt;&gt; Innovator.

&gt;&gt; Innovator. Innovation. Nice. Any others that we've missed? No. All good. Um, and who here has created an AI uh AI agent, AI tool? A couple of you. And were they successful?

&gt;&gt; P. &gt;&gt; P. Just P. But not scaled. &gt;&gt; Not reach that stage to scale. Not connected to the customer. &gt;&gt; Fair.

&gt;&gt; Build a P. Now we'll connect to the customer. &gt;&gt; Okay. Sounds good. Um okay well this session will be all around that. So if we can have the next slide.

So what we want to talk to you today is that there are so many um different AI projects and AI pilots happening in the world and a pilot is the same as a proof

of concept. It's an idea that you're testing to see if that idea is something that you can put into implementation later on. And I was looking up a stat of how many um AI pilots are in the world

and that was very difficult to quantify. But what I did find was that only 30% of all the AI projects actually go into production. So what we're finding in the world is that we have lots of different

AI ideas but really a difficulty in translating that into something real. And the point of this session and what I think is the point of the whole AI summit was that one of those reasons is

because we don't have trust. So if we can have the next slide. So if we think about trust that could be an organization's trust that the AI will work. It can be trust you know in us as

individuals around how our data will be shared. Um the outputs that it will give us. It could be trust in terms of the impacts that it will have on people and jobs and how that will work. And with

that, what we're seeing is a lot of these um AI projects are failing to consider that. And I don't know if you're familiar with the OECD AI observatory, but they do um a monitor

where they essentially monitor all of the harms and all of the AI incidents around the world. And you can see that it's been growing exponentially. um in 2025 of December only there were

600 different incidents in the world. So those are 600 different times that people were harmed or that there was some kind of AI hazard that was created through a through a pilot. If we can

have the next slide is just to zoom in. So this is a little bit difficult for you to read now. Um but in that harms monitor you can click on any of them and learn more about them. So some that I

found uh the first one is in Romania um AI was being used to clone people's voices and then scam their their relatives by making them think that they're they were in distress. Um as

well there was an example I believe it was in Cairo so there was a book fair and a lot of the books there were actually produced using an equivalent of chat GBT so using generative AI but

there was no humans included in that project. So the books were printed with the prompts and the AI instructions still in them. So that created a lot of issue of you know creativity and are

these books generated by AI? Are they what we're looking for? Is that what we thought we were buying? Um and then there's several other examples happening all around the world where this is

happening with facial recognition for example. So using that at borders and all of a sudden that might not work equal between different types of people. Um and and all of these really build

towards people losing trust in AI and being fearful of using it. So these are some examples and we'll kind of go into next what we can do about that. So next we're going to look at why do these

proof of concepts fail and how do we shift from just experimenting to actually having impact. So I can have next slide. So I put here um six ideas of why I

think or what we're seeing with the customers we work on is why proof of concepts are not working. Um the first one is between adoption and impact. So a lot of times we'll have organizations

that are working on AI and they've just thought about producing something but they haven't actually thought about how will people use it. Will it have the goal that you're hoping it to have? Or

say for example I'm using a a legal tool. Will it will it actually serve the purpose that I'm looking for? Will it require more work for me to actually review everything it's doing? So there's

a gap there. The second is around governance failures. So I'm not sure how many of you have thought about risk management. How do you identify all of the risks that are coming up? How who's

going to be accountable to solving them? That might be things like um is it treating people differently? Is it biased? It might be things around security for example.

And then there's also a failure around misalignment. So between what you're looking for and society, those might not be aligned. So if you're for example prioritizing

um AI use to automate people, all of a sudden people are thinking what about job loss? So there's not really a link in value there. And that's another reason um we've got three other

challenges. The first one is sovereignty which I think if anyone was around the summit today or this week everybody was talking about sovereignty. So questions around how do we maintain control? Um

who is responsible if for example a foreign government decides to turn off that AI access? Is that something we trust or how do we deal with that? Uh we also have sustainability pressure. So

thinking about um the carbon cost of using AI and and lack of clarity around that. Um and then change management is really all the people. So if we're thinking about these frontier firms or

people are working with agents, what does that work culture look like? Um have we actually thought about how people use AI and have time to test it and practice with it? Have we thought

about the relationship between people and AI and how that works as well? So these are six uh quick concepts and if we can have the next slide is just a point to make is that you know

when we're considering a proof of concept we're really just considering does it function. We weren't considering any of those other six things and if we want to scale AI we need to think about

everything else. So next slide. Um so I guess the sess point of this session is really to think about how do we actually do that? So what we have thought of is calling it AI in 4D. Um so

four-dimensional the idea that you need to look at four different lenses um to build trust in AI. If we could have next slide. And when we're looking at that, we're

thinking if you can look at all these four different lenses, that's really going to help you predict any harms that or challenges that could come with the AI model and actually prevent them so

that you can deploy and scale that AI. There's four dimensions that we're looking at. The first one is sovereignty. So thinking about who controls it, not just data, but looking

at all the security measures behind it. Where does the model come from? Who has access to it? We're looking at green, so that's sustainability. Um can this scale without uh destroying our climate goals,

for example. Uh we're looking at responsibility, so that is thinking about ethics and governance and and bias and fairness and human- centered. and then valuable. So, is this project

actually really going to deliver a realworld benefit to people? So, next slide. Um, this one I think it might be difficult for us to create a poll. So, what we'll do is we'll do it by hand

instead. Um, so if we can just go to next slide. What I thought we could do before we give you more information of those 4D and how to apply them and and break out into groups is we could just

have some quick scenarios and and test uh what your knowledge is of those themes already. So I'm going to give you an example and then we'll do a show of hands of who thinks uh what lens is

missed here. So this example is with the public health company. um they're using AI to read different X-rays and radiology scans and the point of the proof of concept is to help triage

different illnesses or different breaks um things that you might find in a scan and reduce that backlog. So when they actually started mon modeling and and rolling it out, the team realized that

this required more compute than they needed. Um it would exceed actually the available power supply. So there was not going to be ability to use it consistently and that actually there was

a large demand on water because the GPUs needed to be cooled and this is in a a water sensitive area. So there that would be another challenge between people and um and and the planet. So

this program failed uh this hypothetical program failed because it was financially and politically impossible to run. So, who thinks that this is a problem because of sovereignty?

Who thinks that this is a problem with sustainability? Yeah. Who thinks that this is a problem with responsible AI and value?

Yeah, agree. So, I mark this one as sustainability. Um, I think it's an example of the dynamics that we might have in the real world is we want to scale AI, do really great things, but

actually we haven't considered the power or the water usage that that has because we either don't have the information or it hasn't been something that's been baked in the front to think about. And

we will give you some higher level into what this means and how to apply it in a moment. Okay, we have the next one. So, we've got a second one. This is dealing with uh transport. Um so I think

you know we've all dealt with traffic this week. We're looking at this in this scenario here is thinking about this project is to optimize traffic lights across the city and um smooth

congestion. But when they started implementing this project um it was only looking for average commute time. It was diverting traffic into lower income areas and pedestrian safety actually

became worse. So while this met the technical uh triggers that it did reduce and optimize time, there was a lot of community backlash. So does someone want to tell me which one they think this is

a failure of? Um I think this one is actually value. So here what the um ministry had thought was valuable reduce overall time is not what's valuable to the people. What's

valuable to the people is that they want to have safety and walking and what's valuable to them is that you protect communities and you don't have unbiased impact.

Next one. So now we're looking at justice. Um, so here we've got a justice system. Our justice department is building AI to triage different complaints from

citizens and reroute them to the right legal body. So whether it's the courts or commissioner or something like that. Um, in the pilot it performed really well, but later when they started to

prepare to deploy this into production, the team discovers that one, the model is hosted offshore. two, um they don't have a lot of information on when the model will be

updated and they don't have control over that. Uh this government doesn't. Um that different logic within the model could change based on updates that they couldn't control and that they can't

audit the logs. So who do we think what do we think uh this time? &gt;&gt; Yes. Okay. Everyone is sovereignty. Um sorry, did you say something else? &gt;&gt; A responsible AI. I think that could

also be here um because they hadn't thought of maybe all these risks beforehand. But I agree here, especially when you've got a national organization, they need to have control of the model

and how it functions. Not being able to update it or audit it in such a sensitive area like justice is is a real challenge. So sovereignty is a challenge here. And then last one.

Okay, so here we've got a social science agency and they're using AI to determine who's eligible for social benefits and the pilot showed that they were able to progress and reduce the time and have

fewer manual checks. But when they were actually doing this in real life, one they weren't the model wasn't able to explain why it had made a decision. So why it had allocated benefits to someone

versus someone else. um there was no ability to understand how to appeal it. So if you were rejected for example um you couldn't understand why that was and how to change that decision. There was

bias discovered between different groups so age groups or ethnicity or gender. Um it wasn't applying it same to equal equal to everyone and there was no agreed process for how you would

escalate if there was a problem. So this became very seriously harmful. Um and there was a lot of vulnerable citizens who could be impacted. So in this scenario, what do we think between

&gt;&gt; responsible &gt;&gt; and sorry &gt;&gt; and value? Anybody else? &gt;&gt; Mhm. Training data not accurate. Agreed. So I agree. I think this one is a good

one of responsible but also valuable here. Um a responsible AI is thinking about bias. It's thinking about fairness. is thinking about the data that you have. Um, it's thinking about

all of these harms up front and how you're going to deal with them. And then equally with value, people need to see value of why they're using AI in a public system. And if it's actually

harming people, then it's not necessarily a good use case. So, so far everyone is doing good. Uh, I think we can move on. Um, but what we wanted to go through now is how does this work in

real life? What does this actually look like? And so, I'll pass to Omid. Can we have the uh next slide please? Right. So I think it's it's clear you know having had this conversation and uh

the contribution from yourselves that u uh it's not so straightforward because there are different dimensions and this is the point that Rez is making in terms of having to look at different angles.

So over the last two days, it's definitely day before yesterday, I was going around in the summit um hall and I was asking everyone because you you see everywhere it says sovereign AI,

sovereign AI. I was saying asking them what do you mean by sovereign AI and some people were talking about oh um we need to have our data centers here. Somebody was saying or our models need

to be here. Um there were there were different kind of conversations in terms of what sovereign AI actually means in the context of AI um and how it works and how it deploys and so on so forth.

Um but the key thing is that ultimately comes down to control and my view is that it's not even just about the organization, the sector potentially or the nation but also about the people you

know. So where is your data? Who's actually looking at your data? Why are they looking at your data? What will they do with your data? If you don't have an understanding of that, the

likelihood of you trusting that system is very low and therefore it would be susceptible to failure. So it's really really key to understand um the implications of data sovereignty, AI

sovereignty and so on. I mean I was talking to um uh one one one uh country called Serbia and they were saying that you know we we have a view that you know we

need to have control of our own uh environment so we're building new large language models um in our own geography and we are going to have control over what we do and I think that's the key

thing but the important thing is that if the trust is lost in terms of the sovereignty the likelihood is that the system will fail. And I can assure you um that if it's not designed in at the

beginning, you're going to design you're going to test this in under a lot of pressure. you're likely to be in a crisis as well because when you don't know if your health data is um trained

on somebody else's data or you lose using um very commercially available um uh large language models then then the thing is you're actually beholden to those people and therefore you know you

may not be able to uh achieve what you want to achieve as the objective. So it's really really important dimension in terms of um a successful deployment and all of the stuff that I'm going to

go through here um whilst I've seen them through failure but also they're the recipe for success. So you can think of it in both ways. So if I could have the next slide please. Yeah. So uh green AI

I mean this is kind of not dissimilar to what we had before in terms of cloud and uh green computing and um the fact that you know unless you actually look at the environment look at it from a uh

economic viability of the system ultimately what it means that it's going to cost a lot more and it won't scale and if it doesn't scale um and you cannot handle the data volumes and the

amount of usage that you're going to have the likelihood is that it would stop. Now in my mind this is um the the the the approach to take here is to make sure you address both and what happens

is that addressing both the environmental effects as well as the cost actually work very very nicely together. So we had a similar scenario before in how we deploy cloud services

um and the same thing is translating to this now. So um the the more the more economic your system is the more likely that you know it's going to reduce your less greenhouse gases as well and as a

result of that you can sustain their system longer longer term I mean we all know you know people are building now um massive data centers yesterday um there was I think a discussion around um

Microsoft building uh the new data center that consumes consumes as much electricity as all of Los Angeles and Los Angeles is an enormous city. So the environmental effects of what we're

doing are really key and it has a direct link into um the costs that are driven out of that as well. And and I can again assure you I think if there's any takeaway that if a AI system can't scale

sustainability sustainably then it won't scale at all. Um I'm pretty convinced of that. So we can move on. So, so the next one is responsible AI and you know I think a

lot of people here are familiar with that you know in terms of governance assurance are we doing the right things ethically you know is the bias in the system um all of those things fall under

the responsible AI banner and it's really fundamental in terms of giving people that trust that Theresa was talking about in order to use the system in in anger and and kind of really link

you know their kind of lifestyle to that and and so on and so forth. And um as you know now um there are all sorts of other systems now like uh the AI companions that kind of help you achieve

different things whether it's weight loss or uh you know even provide you um counseling and uh help you along in in your life but unless they're done in you know a ethical way and and unbiased way

they're not leading you down you know a particular path um they are likely to fail as well. Now one thing that I wanted to bring to your attention and yesterday uh is it

yesterday or day before Prime Minister Modi is yesterday was talking about this um which is really key as far as I'm concerned in the responsible AI area is the um uh the human- centered design of

AI because when you're actually building an AI system you need to have in your mind you know who you're trying to help and how and what does this actually mean to them when they start to use the

system. Right? So u and I think the example around uh you know the traffic management was a very good one because we all struggled over the last few days with the traffic and and if a system is

put into place which does not take into account what the purpose of what they're doing is then it is likely to fail. I think the the goal of the system itself as well uh is really key in terms of

whether it gets um the right sort of results or not. So there are many systems where um people don't consider that and as a result of that um it it becomes unusable by the people or um it

might have harms bit built into it as a result of that. with the last one, the last dimension is how valuable that AI is um and what does it mean in terms of the outcomes and

what the measures are and and so on. So um couple of days ago I attended a u a session where um we had um uh a a senior executive from UAE. Um they were talking about as a country what they're trying

to do and it's really key for us to understand you know what we're trying to achieve. So they had a very simple kind of thinking in terms of um you know what they were trying to do which made what

they were doing much more measurable. So what was the intention for them? intention was that um there are about 12 million people in United Arab Emirates and um they wanted to effectively be

rather than 12 million people uh with the introduction of AI do as much work as 120 million almost like 10 times the the size and I think that actually is really really key very simple uh reason

as to why you're doing what you're doing and how you measure it and what the value is now if you actually think about that in the context of say India in my opinion that ambition is doesn't give

Indian India the value so to create I don't know lots of agents to replace people's jobs or do more jobs right doesn't actually have the right uh outcome because there's already a lot of

people here why would you do that right so you have to think really carefully about what the value is of the system itself Because without thinking about that you end up building a system that

you cannot measure the value of and then ultimately what it would do is that it would just become a dead weight. Why do we have this at all? Should we be getting rid of it or not? So hopefully

you kind of understand all of the aspects of the different areas at at KOS we deploy systems AI systems into production. So we see a lot of these issues. Um and we are quite um lucky

because our customers which are all the government departments are actually very very clued up as well in terms of what different aspects of what we're doing are and they see value in it. So it's

not it's not just about you know deploying the technology but how is this technology going to affect the UK citizens and where we work in other countries like Canada, US and so on

those countries respectively. So I think that was my last slide. I think if want to hand over to you. &gt;&gt; Um so we had originally intended to maybe do different breakout groups. The

audience is quite small so it's up to you. Um we could either have everyone kind of have a few discussions and talk about what you think is the most challenging or we could use 10 minutes

if we want to do a Q&amp;A if people want to share their thoughts. put your hands up if you want to go into a breakout group and discuss one of the concepts together.

&gt;&gt; I don't know. &gt;&gt; Okay. So, we'll do the we'll do the second. Um, &gt;&gt; nobody voted for that. Uh, so why don't we Yeah, we can have a discussion. It'd

be interesting to hear are you looking at these four challenges? Which do you think is the most difficult? Which do you feel like you've solved? Um, and we can have a little discussion around that

for for a little bit. &gt;&gt; Hi, introduce yourself. &gt;&gt; Hi there. Thank you. Uh, my name is Ami Kotcha. I'm co-founder of Amro Partners. We are a real estate company and we are

now in getting involved in a data spinout. Um, my my challenge is as follows. I'm one of the co-founders of the company. Um and um as as a leader I'm very keen of course that there's AI

adoption there's upskilling etc in the company um uh and and productivity challenges where we have them should be should be addressed uh using this technology. I feel like I am often left

in the lurch to actually literally make all the decisions within the private sector environment whereas I think government needs to step in and make some of these decisions on our behalf.

um in terms of model utilization, where we go, what do we do with it? I mean, we we are good experimenters. So, you know, fortunately, we we throwing capital at experimenting. uh not every company can

a afford to do that or b would want to do that because of the same sort of um issues you mentioned right at the start which are um aligned with just the fear of adopting something that's going to

break your system or you know open yourself up to some kind of cyber attack etc. So how do you see this sort of playing out in the you know the next 6 months 8 year 12 months because

obviously the technology is moving really fast as to what role the government is going to play in saying this is safe to use and this is still experimental and you should worry about

it. &gt;&gt; Fair. I think we're probably seeing different approaches with different government. So I think um in the EU for example we have the EU AI act and that

lists four different levels of risk. So we've got you know if it's a lowrisk system where you're using AI in the back office there's not really any requirement. It's

like go ahead and do it. But then there's um a medium risk a high risk would be something that would be like really critical infrastructure uh or something that's impacting people

directly. And if it's a high risk, then there's a lot of different things that you need to do around transparency with people. Um, there's also prohibited use cases of how to use AI. So, I think

that's one example where some governments are actually saying, uh, this is what we've deemed safe and if it's not one of these uses, then we want to see a lot of other checks. in the UK

we have regulation um that's looking at thirdparty suppliers right now and if they're critical to the infrastructure not of the country then there will be new requirements on AI as well in terms

of like the updates that go in transparency around models explainability um but then maybe you have the US approach where you don't have regulation

yet so I think it really depends on the country I think a lot of what we heard yesterday was around you know for India thinking about ethical and responsive AI but I don't know if you have any

regulation in place around that yet I think uh yeah I think it's very difficult otherwise for a private company because otherwise you're fighting to who gets to the bottom who's

the cheapest who's the quickest and uh this week I was touring around with different businesses and everyone was thinking how do we do agents but no one was thinking about human- centered,

ethical, responsible. So, I think it does need to come from the government to have a base. Um, but I I have noticed that some are maybe more forthcoming with that than others.

&gt;&gt; Yeah. &gt;&gt; I think just before uh uh I just wanted to answer your uh question about the government, there is a data protection and data personalization law that was uh

uh you know legislated last November 2025. it's going to be legal in the next 27 October onwards. They're getting a time of you know around 18 to 24 months. uh after that what you are saying the

addressing of how the data is handled by the person who's creating the data who is like the person who's created who is the principal or one who is the repository all that rules are coming uh

but presently I would say it is.1% of that responsible AI part which is happening but over a period of this two years the preparation is going to happen where it will slowly get into that mode

actually &gt;&gt; sure I was just going to say so she's a high-f flyier um entrepreneur in the UK actually that's that's she's there but I was just going to say in my mind right

there are a couple of things that we should really push the government to do right one is um about smart data so they've been playing around with this for years and years so we've got quite a

lot of open banking applications now but this can be extended way beyond open banking where different organizations can share data um like for instance in the property market you know how do you

go through the cycle of uh all the way from putting an offer in to conveyancing to I don't know valuation to the end right um so that's that's really critical the other side of it is

actually having trust in language models which are built within the UK itself right and and I think most of the even Serbia is doing that right French have already done it with mistro So there is

a lot of examples of this and that's where the government can really help and that's what we should be lobbying them to do in my opinion. &gt;&gt; Any other comment? Oh yeah maybe behind

you or you have Oh sorry you had your hand up first. You go first and then behind you &gt;&gt; next. &gt;&gt; Yeah. So my uh I am building a uh aentic

AI for vending machines and I have been an entrepreneur in the corporate world but uh before 3 years I was just doing physical stuff right doing products innovation in the food and beverage uh

uh you know uh sector. uh one of the challenge which I am seeing is how to uh build value at a platform level rather than an individual customer level right for example if I offer this vending

machine agentic for a PepsiCo they would say don't do it for Coca-Cola right give it to us only and keep it with us so but UPI for example was not a master card or a visa card thing right it was for the

whole country Right. So how do you get that kind of attraction to build a platform instead of one very customized for a customer who might say that don't give it to anybody else right so that is

the key question that I am trying to address and I do not seem to find answers &gt;&gt; I agree I think that is a challenge in the corporate world I used to work at

Microsoft and even there it was if you're using our technology or if we're coming on a panel then we're on a panel but we're not having Amazon on the panel or Google on the panel with us. Um, but

I think like you say that's really figuring out what you have that's so unique and that actually goes to the value lens I think is that if you have something that's really valuable to

people you make the case that it has to be shared but it is it is difficult if you're building it uh with one customer first because that almost becomes their IP that they want to keep right so

something that we are doing when we're working on responsive AI projects is we're looking at all this similarity of requests that come in and we're sort of doing the work on ourselves in the

background and then we're taking elements that we need and exposing them to the different customers and that way we keep that IP but it is very difficult to get multiple customers on board if

they're all competing. &gt;&gt; Yeah. So for example I build a few IP in the area of sustainability like clean air, clean water. I sold to a company but that company is not commercializing

it. I don't want to name that company because it didn't want to commercialize. It wanted to keep that technology right. So that's a big challenge that I am seeing in the corporate world that a

company will buy another company but it won't implement for the society or the for the good right so that is the uh uh challenge that I'm seeing how do you handle that because that is part of the

responsible AI as well as the valuable AI part. Yeah, I I think you're right and I think you you have your own kind of description of this problem, but uh I

was in the US uh few months ago and um I I saw um I don't know whether you're familiar with SVB, but it's basically Silic Silicon Valley Bank, right? And they did a presentation to us uh where

they were talking about where all the funds are going, right? And if you actually see what is going on in terms of this, I think it's about a trillion dollars worth of investment. This

investment is flowing only into a handful of companies. What those companies are doing, they they literally are stifling everybody else, right? This is a this is a commercial reality,

right? Um but if I was to offer you some options, I would say this shouldn't be just a IP. You should be thinking about it more as a service that you could build layers on, right? So you may

retain the IP or you may share the IP. It could be a co-created whatever it is, but it's got to have a service model attached to it because if PepsiCo buys X and then Coca-Cola buys Y, why would

they be buying it? And how would you be able to build on top of that? But, you know, it's very very a commercially uh um challenging problem. It's been there for many years. This is nothing new as

uh as Sherza said exactly like that &gt;&gt; saying UPI beat that right. So uh today UPI compared to a master card or a visa in India everyone is using that right and there are applications which are

attached to UPI whether it's a PTM or a Google or whatever Amazon pay all of them are on the platform of API right so the question I had was why are it companies for example Kynos right or an

infosys or an Axer not looking at the platform approach and looking at the services approach where they can put team manpower and run projects right so I I see this as a challenge I have been

talking to the top management of Infosys Accenture every time I go with the proposal they say just do it for a client you know and we will attach you as an expert I don't want to do that I

want to build a platform there's nobody who is uh really interested in building that sort of a business which is pathbreaking it takes longer time right like UPI came it it happened organically

can these kind of initiatives happen inorganically. That was the question. &gt;&gt; Yeah, I I think they are looking at both. Um yeah, sorry. I think we we should take one more question because we

have very few minutes left. So, we'll we can talk after and I want to get to the person behind you for his question as well and then we'll do a quick wrap up. &gt;&gt; Uh good afternoon. Thanks for covering

those areas in the lectures that were much needed to understand. So, you talked about sovereignity AI and then you talked about value or responsible AI. So there might be few scenarios

where while changing sovereignity we might have to bypass value additions or responsibility for the citizens while the other way around also. So can you discuss about those scenarios where you

value sovereignity more than talking about responsible AI or uh value additions AI and the otherwise also and when they can be parallelly taken into account. Thank you.

So you're asking around responsible AI and valuable AI where they link over &gt;&gt; where one might be more useful than the other. Exactly. Right. Um so I see responsible AI I think it can

actually incorporate as like a lens for everything. Uh but it's much easier to think of it as a separate. I think responsive AI can encompass five things. So like ethics um trust like bias and

fairness human- centered governance and security where I think that distinguishes from value is value is looking beyond financial growth. So a lot of organizations you might look work

with um or when you think of many organizations in the world they're looking at how much money will this save me or how much time or how much productivity but I think valuable AI is

looking at what goes beyond that does it actually create more well-being in people does it give people time back with their families for example um or other hobbies they want to do valuable

is thinking about what's the long-term benefit that this will have in terms of how we change society. Maybe it's going to create um a whole bunch of different jobs now in something else. So I think

actually if you're using responsible AI, it will create value. So I still think they go hand inand but that's probably how I distinguish it. Is that your question? I'm not sure.

&gt;&gt; Maybe Omid has an answer you know also. &gt;&gt; Yeah. So, so I think you're saying what happens when you have to do a trade-off, right, between sovereignty and value and I think this is a very good question to

be honest, right? Um because when you so again uh yesterday I was wandering around in the summit. I keep asking people questions about different things and um um one of the countries that I

spoke to um they know that uh using um GPD models or claude and various other things is a quick route to building what they need to build because they're there. It is there. it is immediate and

it can be done uh you know almost almost like without any issues at all but they're taking the hard route right so they're saying actually we don't want to do that because what if tomorrow we fall

out with them as Europeans are falling out with Americans anyway right so what happens if they turn off the systems right what would we do then right so if you think about it in terms of the speed

the speed and the value is is actually going with what you got, right? But the the more challenging thing which is the value the value is can we actually use this system for our citizens on an

ongoing basis. Is that data something that belongs to us? Are the models aligned with what we are doing? So they want to be able to enable their people in order to deliver the right outcomes

and that would not happen if they just outsource their sovereignty to um the US. So so I think those are the those are the some of very very important factors that need to be explained but

ultimately from a value perspective the spot on I think it's about what is the value in terms of to the people that are going to use that system. So if tomorrow we found this fantastic system like I I

give you example we were stopped multiple times in terms of the traffic because some VIP was coming out of somewhere right and then just literally close the road. So we sitting there for

like half an hour and then we get going again that's happened I think three or four times so far. So if you were to build the system um you would need be needing to think you know what is the

value for those taxi drivers all these public that are going around um and that's the key thing that you need to be able to use AI to achieve right this needs to be measurable it needs to

actually help the people itself so yes it's a it's a very tricky trade-off very tricky &gt;&gt; yeah I think the trade-offs is is really good especially in sustainability as

well so a lot of times Um, organizations might just think, how do we adopt AI as quick as possible, get people to use it as much as possible, but actually every query that you use has a sustainability

impact. Um, and so I think there's a there's a trade-off there because there might be a sustainability impact, but depending on where you are, you might value training people to use AI more. So

you might be okay with that impact because it's more about getting people comfortable with using it. But then if you are an organization that really values sustainability, you have really

strong carbon goals or net zero goals, then actually that might be the trade-off that you have. So I think one thing that we're doing when we're working with organizations is we're

getting them to make that very difficult decision of here's high concern, here's low concern. We map out all the harms that we can think of and all the principles and values that align to them

and they can't put any side by side. and they have to move all of them from high to low concern and very quickly that makes you see what's real for your organization and I've seen a lot of them

put sustainability at the bottom which to me is a little bit concerning um but it does start to think about really understanding your organization and how those trade-offs are going to play and

that's what we're finding in in the human one as well so &gt;&gt; uh so just 10 seconds more adding to yours ranking low to high so out of all these four sustainability sovereignity

responsibility and value addition. How do you rate them as low to high? All the four factors that you have covered in your PPD. How do you rate all these four on a scale of

&gt;&gt; how do I rate them? &gt;&gt; Yeah, you both please. &gt;&gt; I think that's very difficult. I think I'm putting responsible AI at the top because I

think it can actually it's a cheat because it can kind of include sustainability actually and I think it will create value. Um I think So then I would probably put sovereignty lower

than that. But obviously this year has maybe changed that geopolitically. I think I still have responsible AI at the top. I'll make that hard choice. &gt;&gt; What would you say?

&gt;&gt; Um I think I kind of uh agree and and I think Modi, Prime Minister Modi said this himself yesterday about so human- centered AI design is part of responsible AI, right? Um few days ago

me and Treza were talking to someone and they were describing a system right now. Um if you just uh indulge me for a couple of minutes I let me explain the background of the system and then you

see how it's relevant. Um so they were building a system for um kind of the nursing or old people's home, right? So you may know that uh elderly get dehydrated and they forget to drink

water and then that causes a lot of problem for them. So they built a system where you know using AI and vision they were seeing if the elderly were having enough liquid in the day or not. Right

now that's fantastic. Everybody says this is a brilliant idea. Right? But then you think about it, they're monitoring those elderly both in the area where it's common as well as where

they maybe in their bedrooms or whatever, right? So that brings a challenge, right? And then the other challenge was you know what about the people the nurses who are actually uh

hydrating them because that could become a negative effect on them because somebody might be saying you're not doing your job right. Right. Um and what about the family of the uh elderly? What

about the impact on them? So I think it is really important to understand why we build the system, who it affects, how it affects them, and what the long-term benefits are which brings the value.

Right? This is why it's four dimensions. None of these are independent. I think they're all relating to one another one shape or form. &gt;&gt; Yeah. Um, so we'll work towards wrap

wrapping up. Um, because I think we're getting the time check. Can we &gt;&gt; But this one switched cuz it said eight and then it said 17 and now it says 10 and then she told me I had seven.

&gt;&gt; Oh, this one's right. Okay. &gt;&gt; Okay. &gt;&gt; Um, &gt;&gt; well, let's see if there are more questions.

&gt;&gt; Yeah, but we have the takeaways and things to go through also. So, I think we'll wrap up and we can talk to people individually afterwards. Can we skip through some of the slides because we

wanted to ask next one. Next one. Next one. Um, next one I think. Okay. So, we actually wanted to flip that question and ask this to you in the audience as well. Um,

which one would be your top? So, of those four lenses, sovereign, green, responsible AI, which one do you think is an absolute must-have? And you can only pick one. And if this isn't there,

it's going to derail the project. Sh, show of hands. Who wants to say so? Who says sovereignty is the most important? Who says that it's green, AI,

Who says it's responsible and then value? Some people didn't vote. You didn't vote back there. Um but I think it sounds like a lot of people in the responsible

and value is the most important. I think I agree. Um but I think what we wanted to come across is all of these need to come into play as well. Can we do the next slide

of that question though who who has a responsible AI practice in place? Who uses a framework or anything like that? Anybody? Who has a sovereign AI policy in place?

No. And who is looking at None of us. That's a takeaway for all of us. Um, [laughter] so I think we wanted to wrap up with how do I take this forward.

So a couple points I want to make. The first is that we have taken a lot of the learnings and things we've talked about in this and we've turned it into a white paper. Um, there's a link below, but we

can share it with you. We've shared it on LinkedIn as well um to talk about these learnings and we wrapped it up to say for each of those themes here's eight to 10 things that you could do if

you really wanted to take sovereignty green sustainable and responsible and valuable AI forward. So please check out that paper um very happy to talk about it and give you insights. I think the

key takeaways for us here is that no single dimension is the answer. I think that's come out in the scenarios, in the conversations that we've had and in how we're prioritizing is that you can't

really have one just one. You need all of them if you want to scale that project and really make it to to production. The second point is on tradeoffs. So it was really good that

that came up in the conversation is that just being aware of the trade-offs that you will have to make and having something in process to talk about why you made that decision is important. Um

I think a takeaway for everyone here is think about an AI policy which talks about how you'll use AI and what you will prioritize. think about having a responsible AI framework which is

essentially all the questions and things that you want to be implemented across you know ethics or across um trust security and then really think about how you can turn some of this into numbers

so what are the KPIs that you can actually look at for sustainability for users for ethics don't just make them a concept of we will be ethical think about what that actually means for you

and how you're going to measure it That's important if you want to get funding, investment, and show the project is a success. And then finally, think about how you can upskill your

teams to understand these concepts and how you can incorporate diverse views. I think that's probably the most important in um in building out the responsibility.

Um so we will wrap up. This is to say if you want to get in touch with us, here's our details. Find us on LinkedIn, send us an email. Um, we can have a couple minutes after this since I know there

was two questions in the audience that we might not have got to. Um, but otherwise, we hope the session was useful. If you want to give us feedback, here's a bigger QR code. Um, so if you

want to stay in touch, then fill out this. Um, let us know if there's anything we can improve on the session or any questions that you have. We're super happy to hear that. And otherwise,

just a big thank you for your participation. Um, and yes, hope you have a good rest of the day and a good weekend. &gt;&gt; Yeah, I was just going to say I think

great questions there about tradeoff and absolutely the right question to ask because none of these are unique. Sorry, please go ahead. &gt;&gt; Yeah,

&gt;&gt; like you were talking about tradeoffs. uh I just wanted to say okay uh every model have their own uh aspects like uh pros and cons or uh as you said different dimension I have got most of

my answers from the those questions uh but I just wanted to ask uh if we building something and taking this aspects like uh okay taking responsible AI valuable AI uh on them uh but

If we are taking like we'll be missing some uh aspects uh as he said about responsible AI if we are taking uh uh okay like uh accuracy and fairness uh if we'll take uh

uh okay sorry &gt;&gt; if it makes it easier to speak in Hindi I understand &gt;&gt; uh it's okay but uh um fairness. Okay. Like uh if we are

doing uh oops sorry again. &gt;&gt; Yeah. Um I think you know there's no issue at all. I mean you can ask us on email as well. It's not an issue. More

than happy we'll respond to you you know if you want to ask us the question. Exactly. &gt;&gt; Huh. And &gt;&gt; uh hello. Actually, we don't have much

time anymore. &gt;&gt; So, no. &gt;&gt; Yeah, we don't have much time anymore. I think we'll uh you'll discuss it after the session.

&gt;&gt; All right. Okay. So, we thank you uh uh Omit and also Teresa. uh uh from kindness. So we would like to present use uh something we would like to call uh Omit please and we'll hand over the

momento to him. &gt;&gt; Thank you very much. &gt;&gt; Thank you. So now we'll let you call Teresa. You can stand there.
