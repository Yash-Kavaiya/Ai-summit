# Democratizing AI Compute and Digital Data Infrastructures

**India AI Impact Summit 2026 ‚Äî Day 5 (2026-02-20)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 09:30 ‚Äì 10:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 19 |
| üìÖ **Date** | 2026-02-20 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/hGg1tBCKSjs?feature=share) |

## üé§ Speakers

- Chenai Chair, Masakhane African Languages Hub
- Faith Waithaka, Africa Data Centres Association
- Ronnie Chatterji, OpenAI
- Sangbu Kim, World Bank
- Sanjay Jain, Gates Foundation
- Saurabh Garg, MoSPI, GoI
- Yann LeCun, AMILabs

## ü§ù Knowledge Partners

- The World Bank

## üìù Summary

This panel will examine how governments and partners can expand equitable access to the core enablers of AI ecosystems, particularly compute and digital data infrastructure to support inclusive, sustainable innovation. It will discuss practical models such as shared compute, regional infrastructure, and public‚Äìprivate partnerships, alongside investments in skills, data readiness, and governance. The panel will also explore cost-efficient AI approaches and strategies to reduce AI's environmental footprint through energy- and water-efficient infrastructure.

## üîë Key Takeaways

1. This panel will examine how governments and partners can expand equitable access to the core enablers of AI ecosystems, particularly compute and digital data infrastructure to support inclusive, sustainable innovation.
2. It will discuss practical models such as shared compute, regional infrastructure, and public‚Äìprivate partnerships, alongside investments in skills, data readiness, and governance.
3. The panel will also explore cost-efficient AI approaches and strategies to reduce AI's environmental footprint through energy- and water-efficient infrastructure.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/hGg1tBCKSjs/maxresdefault.jpg)](https://youtube.com/live/hGg1tBCKSjs?feature=share)

---

_[‚Üê Back to Day 5 Sessions](../README.md)_


## üìù Transcript

access and energy. Number two, computing power. Number three, data access. Number four, talent building. And number five, credible, responsible AI framework and policy.

Among those five, uh everything is very important, but we are currently struggling with some uh lack of access to computing power and data sets. So that's why today discussion is very

important. Unfortunately, more than 80% of our data set uh in the world are very heavily uh skewed to the developed world high income countries

less than 2% in Africa subs Africa if you just carve out South Africa less than zero something% only for the other uh subs Africas so we see the B gap uh in in this basis so this is pretty uh

important time to talk about how we can really democratize the you know competing power access in this space. So thank you for joining us uh and then I look forward to really good discussion

with all of our panels. Thank you. &gt;&gt; Thank you Sangbu for that opening. So I will start by asking the panelists to introduce themselves in a very short way and I'll

start with myself. I'm Faith Waka. I build the infrastructure that makes AI possible. So I build the electrical mechanical infrastructure in data centers in Africa and I'm also the board

chair of the Africa data center association. So we'll go this way. Yan, please tell us who you are. So I'm Yanuk. I'm the executive chairman of Ami Labs, Advanced Machine Intelligence

Labs, which is a new company I'm I'm building uh to build a next generation AI system. I'm also a professor at New York University still and I'm a just a month ago, I left my position as

chief AI scientist of Meta after 12 years at Meta. &gt;&gt; I'm Sanjay Jan. I lead the digital public infrastructure team at the Gates Foundation.

&gt;&gt; Um I'm Sab Gag. I'm secretary in the ministry of statistics and program implementation in the government of India. &gt;&gt; And I am Chennai chair the director of

Masak African languages hub which emerged from a grassroots community called Masakane focusing on African language NLP. &gt;&gt; Good. So, Chennai

and coming back this way to all my panelists, what is the single biggest barrier? And and and I can imagine that we're all coming from different segments from the

introductions we just did, but what do we feel is the single biggest barrier today to democratizing AI compute? Chennai. Thanks, Faith. So there are over 2,000 documented languages on the

African continent. So our single biggest barrier is the breadth of work we actually have to do to document these languages to ensure they're well represented and also focus on the

communities that actually speak them. um I would say u access to uh models, open models and AI literacy to be able to utilize those models and the reason I say that is uh uh perhaps infrastructure

is something which might get acquired over time and hopefully uh the uh the requirement of the size of that infrastructure may also change and the focus we probably need to focus much

more on models. I would say too much concentration of digitized digitized data only for developed world. Actually also go on the data point

because we believe that AI will scale effectively only when data for everyone is available. So when I can get a personalized service because my personal data is accessible through some

protected means to a model. So then that will allow AI to reach everyone. &gt;&gt; I'll just echo some of some of the things that was said earlier. U certainly the availability of top

performing open models open weight but also open source would be a way to remove the barriers or at least uh if not a sufficient condition at

least a necessary condition. And the problem is that today there is no such thing. The open models are behind. But there is a way to get them to surpass the proprietary system and it's through

data. So uh the access to data was mentioned um if various regions of the world um collect or digitize their cultural data whatever it is and then contribute to training a global model

that would constitute eventually a repository of all human knowledge. then those models would be much better quality than all the proprietary system because a proprietary system would not

have access to that data. And this can be done technically in a way in which regions don't need to actually communicate that data. They can keep ownership of that data and then

contribute to training a global model by exchanging parameter vectors. um don't want to get into the weeds of technicalities there, but it's a form of federated learning and I think this is

the way to open up um access to to AI and it's absolutely crucial for the future because we're going to need a wide diversity of AI assistance for the

reason that u there's a wide diversity of linguistic cultural u differences value systems political opinion ions and philosophies and if our AI assistants come from a handful

of companies on the west coast of the US or China were in big trouble. So we we absolutely need this. &gt;&gt; Okay. So we've had the challenges and there are wide range of them from

inclusion to compute to data sets. What we're going to discuss today is how do we overcome those barriers from the different perspectives and the different angles that we have on this team. So

coming to you Sangbo from a World Bank perspective, what does it mean to democratize AI? And would you please give us one indicator that signals that a country is

moving from consuming AI to actually building it? From the World Bank point of view, uh, democratizing data computing is very important. But

let's think about this. So many people are very easily uh is talk about building data center itself physically and you know securing more GPUs and servers.

From the beginning I agree the fundamental infrastructure is very crucial and very important but more important thing is that how can we use that computing for for what. So we need

to really think about what will be the best way can create which can create demand for computing power that is more crucial part. So without having very clear application and you know you know

some solutions you know nobody can really you know run their own computing uh data center business in Africa. So it is very uh crucial part. So I would like to say we need to

think differently from even though computing power is very important how can we really create the data demand. So in in this regard so the the clear indicator is that how

can we really fully manage the data uh in the local. So one good thing, one good news is that anyhow local data, local context can be fully owned, controlled, managed by local country and

local people. There is a very good news even though we see a lot of you know inequality in the computing uh infrastructure and resources but which what cannot change uh in in even in this

AI era is that people and the local country and local pe uh you know community can strongly hold their context and then hold their data set. So it is really important uh signal and

opportunity. So I would say you know measuring the fully utilizing uh and harnessing the data set in the local will be the key indicator for this. &gt;&gt; Okay. Yan,

you spoke about compute a few minutes ago, open compute, and I would really like to know is the concentration of frontier compute a temporary scaling phase or a structural feature of AI

and where do you see the biggest technical opportunity to reduce compute intensity? It's something that Sangu as well touched on. Okay. So, first of all, I think the the

the computing requirements for uh training modern AI systems is is temporary. It's temporary because the type of AI systems that we build at the moment, LLM

essentially are knowledge storage systems, right? They accumulate factual knowledge and therefore they need enormous amounts of uh memory. The the reason why the models

are so big uh in terms of number of parameters, we're talking hundreds of billions of parameters, which make them really expensive to train and to run, is the fact that they just accumulate

knowledge so that it can be easily retrieved. But um but there's there's another way to be useful in in terms of AI. It's not accumulating knowledge, but actually

being smart. Okay? And you can replace knowledge by intelligence. So current systems are not particularly intelligent but they store knowledge. There is another revolution of AI coming which

actually my new company is built around uh which intends to build systems that are smarter even if they don't necessarily accumulate as much knowledge. So those models will be

smaller. Now the bad news with this is that perhaps at inference time they will be more expensive because they'll reason more than current systems.

Um so we're going to see maybe a shift in the requirements for training. Uh but the the requirements for inference which is really where most of the computation goes uh is still going to be quite

significant. Um now to answer your second question the incentives are there for the industry to reduce the power consumption of uh of AI system. Most a lot of engineers working on on AI in

industry these days uh even in academia are actually focusing on how can I make this model smaller how can I distill it in a smaller model how can I use mixture of experts so I have sort of a ladder of

models that are more and more complex um so that to answer simple questions I can use a simple model etc um all of it is to optimize power consumption why because that's where the money goes

that's where you spend all the money when you operate an AI system. It goes into into power and maintaining your your hardware. So the incentives are there. So that's the good news. You

don't need to have laws or regulations or anything. They're working on it because they need to. The bad news is that it's progressing as fast as it can and it's not fast enough. Okay. But uh

but we're not going to be able to make it faster unless um you know we find some you know technological breakthrough at the fabrication level or the architecture or technology. There's a

lot of mileage to be uh to be had in in those things still. Um the power efficiency is actually making progress really quickly much faster than Warsaw and uh but uh but it's still too slow.

So I'm I'm not expecting like some you know big revolution in hardware design until we start you know building something else than um transistors on on you know cos transistor on silicon

that's not happening for another 10 or 20 years &gt;&gt; 10 or 20 years &gt;&gt; well I mean there's going to be progress you know in the meantime it's not what I

mean but uh but if you want like a real breakthrough like some completely new way of of building computing systems there's nothing on the on the visible horizon that really will allow this, you

know, whether it's carbon nano tube or spinronics or whatever it is. &gt;&gt; Okay, that's very interesting um to think that um the training models will become smaller yet the inference

might be the one that will take up the compute. yet we're also looking at uh bringing inference to devices as close as possible to the people using it. Um so there's a bit of a balance to be done

in that 10 year period. I think 10 years is a lot of time considering what AI has shown us over the last over the past decade and I think in terms of research um we might see it sooner.

Yeah. Serb um you led the other digital ID and now in statistics. How do you see digital public infrastructure enabling AI innovation?

And how can countries expand access to shared AI infrastructure without creating new dependencies or compromising data sovereignity? &gt;&gt; Um thank you. So uh I think two

characteristics of uh digital public infrastructure uh which are key are uh to ensure that not only there is access but also agency of the people. So uh most people would

not like to be just consumers but also be co-creators and I think that's the uh real uh issue going forward. Um for any uh system to be a DPI I think

there are a few essential characteristics. It needs to be uh trusted. It needs to be interoperable and uh sharable and uh obviously reusable is part of it because and

that's what Aadhaar is. it's it's able to bring these characteristics uh onto this and this is what will also ensure that uh innovators focus on solutions rather than trying to get together the

infrastructure uh together and uh in the democratizing AI working group which was one of the seven working groups of this AI summit setup which I had the privilege of chairing along with

representatives from Kenya and Egypt. uh uh one of the outcomes of this uh of course there was a charter on AI uh diffusion uh but one of the outcomes of that is what we are suggesting uh

building initially which might be a digital public good but modularly it will become an infrastructure as we move ahead is the uh methri platform which we've called friendship and methri

standing for multistakeholder AI for a trusted and resilient infrastructure and how we can uh on a in a modular manner add on the four uh which I think uh my fellow panelists have also

mentioned uh components of uh AI compute data models and talent. Uh these are the four aspects and of course governance mechanisms uh would would of course be there. So how we can ensure that uh

different countries are able to contribute in whatever manner to build this uh if I can call it a global platform uh which is in a way owned by all and yet looks at what are the issues

of real uh criticality and uh and I'm sure uh there's a lot of there's a major role for not only countries for private sector and philanthropies uh to be able to build this structure

together which will meet the uh requirements of of countries private sector and the philanthropies because each of them have different motivations uh to it and the private sector would

have a profit motive and that has to be uh kept in view as far as far as the dependencies that's the second part of the question that you uh asked me I think one of the areas is uh that we

need to ensure that we follow a federated structure rather than a centralized structure. I think that would be key uh because uh and and that will also ensure that uh the variety of

languages and cultural contexts that the data sets carry uh to and which will also ensure that ownership remains where uh whoever has contributed with the data and yet uh technology and open systems

exist now to be able to ensure that uh that sharing can be done in a safe and trusted manner. So how we are able to uh ensure that this uh collaboration and cooperation is done based on trust and

what kind of mechanisms we can develop and they could be partly technological and partly u policy uh based or protocol based and a combination of this will ensure that uh we don't generate new

dependent dependencies. Thank you Sanjay. When I said DPR, you nodded your head. So in terms of digital public infrastructure, we've seen it scale because it was

interoperable. How can we ensure that data and AI systems that we build now are interoperable and open by design so that even small startups or

governments like we've just spoken about can plug in and benefit? I actually want to uh go off what Dr. Girk said. Uh broadly uh DPI provides a way for data of all individuals. So their records

their ID the transactions are sort of the system of record on top of which DPI sits. So DPI provides a management layer on that and provides consented access. And so that's something which we have

seen around the world that particularly like for example in India we see this a lot is that now that you have access to all of this data you can actually build on top of that through consented access

lots of applications and that's really where a lot of the value comes in and I think uh Yan mentioned about training data sets again the same model can be applied to allow either consented access

or anonymized access so that you can do a federated learning so that the data never goes to the model but the model comes to the data. And so with uh and India's been looking

at uh this data empowerment and protection architecture which is on that lines and that I think we are now starting to see the structural building blocks come together which would allow

for this underlying data layer to be built but that requires strong DPI and so we do think that there's a lot of reason for uh countries around the world to adopt DPI systems so that citizens

data can be managed in a very trusted way accessed with consent and then We have things like uh MCP coming up which then allow the users context to be taken which then allows AI to be safe. Of

course, as long as the data is the rights on the data are quite clear that they're not going to be stored. So overall, I think we are moving towards this world where we are seeing the

underlying pieces come together. They have to come together at a global scale. I think that's the point that Dr. Gergo was making. And so from that perspective, I think we are in a fairly

good place. But then to make sure this happens we have to I think act in a unified manner. I I mean for example we have to uh work together to fund uh efforts which at the grassroots. So for

example what we are seeing with Masakhan where you're working with countries with the communities so that their languages can be represented so that that context becomes very important because finally

we are going to have to serve users in their languages. So I do think you know I'm very positive that we're moving in the right direction. I just think that u there's still some ways to go. I think

uh there other barriers as well but on this aspect I think DPI provides a way for us to get past the data hurdle as long as of course DPI is implemented in a

responsible manner in the countries in and in the right way. &gt;&gt; Thank you Chennai. You've cautioned against technology

becoming extractive. How should we build data infrastructure that is trusted by communities? And would you please give us an example of what principles would make an AI

project in a village or in a community in some rural place in Africa for example feel empowering rather than extractive. Thank you so much Faith for that

question. Um and I think I have the pleasure of sitting here as a representation of what it means when community is involved in building something. Masakana basically

means we build together translucently translated in isizuru and that was then a creation of a participatory approach in knowledge building as a result of being excluded in spaces. So if we're

going to build um data infrastructure that community trusts is to respond to the realities that they live in and to be participatory. So that's the first example. And just to prove how important

for something to be participatory is that 2019 2020 there were not as many data sets around African languages. I think a source of data was the Jehovah's Witness 300 Bible. Um and they had

translated the the languages for their own purpose which is fine. And then so the community came together, the masakan community came together and brought in everyone, linguists, NLP people, machine

learning people, anyone who spoke the language to actually develop the scripts and do the machine translation work on top of that. And this community that was unfunded doing everything by the

bootstraps actually won a Wikipedia award in 2021 for their participatory action work. And I think that is then crucial to actually show that if you're going to build trust, people have to see

what the end value is and also be recognized. So this paper actually has I think about 20 people on it, a lot of people on it which some people could never have been authors but they

contributed to it and they've got a paper published and that's significant. And then secondly, it's really thinking about commu meeting communities where they are regardless of what their

location is. It's realizing the inequity that we exist with. So, one of the projects that we will be doing at Masakane is called Project Echo. It's designed to be a gender responsive uh

project because gender transformative of transformative is also the north star that we're hoping to get to one day. And in that instance, it understands the realities of gendered inequality on the

African continent regardless of any technological innovation. And what we're doing with in partnership with Gates Foundation um and also working with IDRC who are working on this as well as a

gendered intervention is to actually then create work with tech entrepreneurs working um developing gender responsive use cases that focus on women's economic empowerment as well as health to then

think about how we creating an impactful tool when you add African languages on top that will result in better economic output. puts for them or better information when it comes to health. So

again, it is thinking about designing with the communities and meeting them where they are. And then lastly, um this is to say that this is we love to say this on our team that what we're not

doing is new. The technology may be new, but there are practices that we can borrow from other spaces to actually then ensure this is done. So I would like to reference the community network

models. Last mile connectivity is a significant issue across the continent. Um we've had universal service access funds as an incentive for mobile network operators to do this. But sometimes some

communities are not served well enough. And so then there has been um interventions to actually result in internet connectivity that's localized being developed by the communities. They

in charge of building the masks for their community networks. They're in charge of creating the content that people are going to need. Figuring out what the necessary power is. Do you then

um you know create have a transformative booster in one person's home and then people go and charge their phones there because it's the whole life cycle of this. So if we're going to build

infrastructure that people trust, we have to borrow from what's already been done and then ensure that people are part of the whole life cycle so that they see ownership and also it allows

for sustainability because they're like that's my resource and I'm not going to wait for anyone else to to support it but I am going to be in charge of making sure that it continues to exist.

&gt;&gt; Interesting. I like that community ownership and I don't think we can do that if we don't build small AI. So Sangu, you've written a lot on small AI. What would be our playbook for scaling

small AI responsively? &gt;&gt; So let me introduce what small AI is. uh small AI is not really small but in this very you know uh you know big tech oriented big scale of large language

model point of view it is you know some small or but practical approach. So the small AI is very practical and affordable and locally reliable and relevant model. So sometimes it doesn't

require very heavy data set. It doesn't require any very high-end smartphone capability. Al also it can fully lean on local language. So that is small AI. So why

small AI is very important? Because without clear you know applications and use cases which can provide more value to our people. Yeah, we cannot fully harnessing this

technology only with infrastructure. So we are just do some reverse engineering and you know cultivating the demand first rather than providing some infrastructure. So this is our the new

approach. How can we really create some aggregated demand which can scale up throughout so many people throughout the multiple countries and for the for whole developing world. So this is this is our

uh main approach for small AI. Why small AI need? Why do we need that? Actually if you let's look back the the 20 30 years ago when computer innovation started and

mobile innovation started around 10 15 years ago what I have observed in that area uh era it was still supplier and developer and engineer oriented did

service provisioning in some sense the human customer and users were not in the center because there's so many restrictions uh you know user can uh you know can

restrictions so user cannot fully utilize some uh technology without get trained and learn. So 20 30 years ago we talked a lot about digital literacy and some basic digital skill and how to use

window and explorer etc. That mean that meant it is not very userentric because user had to do a lot of things but now AI is going towards very user centric uh services. So users doesn't need to do

some uh that much. They can only control and ask verbally about what uh they are curious what they need and then it can be automatically provided to the users. That is the the philosophical concept of

AI uh in in my mind. So in in that sense our focus is how to more bring more user centric mindset to this field along with our client because you know we have compared to developed world

we have pretty much big you know context base ground and local datas and so many user interests. So that's our approach how that's how we going to fully harness and utilize for this AI.

Thank you for that. Now that we're speaking about communities and users, Sanji, you've spoken about moving from digital aid to digital empowerment. In the context of AI, what would digital

empowerment look like and what should development partners like Gates World Bank sitting in this forum prioritize so that countries are not just consumers of AI but co-creators.

So the thread I'm going to pick up back is the DPI thread and broadly what we have done in that space is to look at how instead of building systems for countries we sort of have open-source

systems which countries can then adopt to build systems which are adapted to their needs. So when we look at Aadhaar in India that's one thing but then for the rest of the world we're looking at

MOSEP and MOSIP is a modular open source ID platform that we have supported which countries are taking and building with their own policy layers building their own application versions of it and so in

Ethiopia you have FIDA I which is based on MOSIP and it's actually very much customized to what they need. So the idea is you build these pieces of technology which then countries can

adopt and build in a way that suits their needs is governed by them. It's uh local laws work on that. So all of that institutional infrastructure legal infrastructure is then uh sits on top of

the technology layer to do that. Similarly we have supported uh other open source efforts like open G2P for government payments. We have supported digit for healthcare campaigns and so

the whole idea is you build open source let countries and communities take that and adopt it. Similarly with masakane again the same idea is that if you have a way by which uh local communities can

come together and collect data but then make that available for global needs. So we have funded those kinds of efforts in India and in Africa as well. So that these efforts are now there where local

communities are empowered to make sure that AI systems can understand and speak their language and that is again a form of empowerment. So broadly that's sort of the way we think about it is how do

we build open standards, opensource products that countries and communities can use and contribute back to and uh co-create essentially their versions of their systems that then work in a

unified way across the world. And so that is really empowering them to be a part of the community and that is what we would love to see more happen. &gt;&gt; Thank you for that. Now Yan, I can't

help but come back to this world models that um in my mind I was thinking they would increase the compute power necessary so the infrastructure would be bigger. But

from your explanation, it looks like being more intelligent means less compute and we now move the power not on the grid side for the training models but on the infant side on the devices.

So what does that actually mean for the government people, the AI ecosystem, the startups that are in this room? what should be their focus over the next one, five, 10 years if these changes are to

happen and I I I do believe they will happen. &gt;&gt; Um, wonderful question. Thank you. Um, so that there there's going to be another AI revolution, right? We've seen

in recent uh years the the deep learning revolution and the LLM revolution and uh unfortunately the type of AI systems we have access to at the moment manipulate language very well and they it it fools

a lot of people into thinking that we've had it we have it made that we have systems that are in as intelligent as humans because we think of language abilities as properly uh humans. human.

But it's a mistake that generations after generations of computer scientists and and people around them have made in AI uh for the last 70 years of discovering a new paradigm for AI and

assuming that this paradigm will lead us to systems that have human level intelligence. And it's just false and it's false today as well. The our current technology is

limited. Um it's useful. There's no question it's useful. It should be deployed, developed. It's going to help people. I use it all the time. But it's limited like previous generations of

computer technologies and and AI AI systems. So what is the next revolution? It's the revolution of AI systems that understand the real world. And I think there is a lot of applications of that

uh throughout the world for all kinds of uh applica you know all kinds of domains of uh of market segments if we're talking about commercial systems or or just you know helping people in their in

their daily lives. Now it turns out that um and we've known this for a long time that understanding the real world is much much more complicated than understanding language and manipulating

language. It's because language is a sequence of discrete symbols and it turns out that makes it easy for computers to handle. Uh but the real world is messy. It's highdimensional.

It's continuous. It's noisy and it's just much more complicated. So I've been making that joke for many years to kind of try to explain this to everyone that your house cat is smarter than the

biggest LLMs. Um and in many ways that's true. Certainly in the understanding the physical world your your your cat is way smarter than the biggest LMS. Um doesn't

mean the LMS cannot accumulate knowledge about the real world but they but they don't really understand the underlying nature of it. So the next revolution are systems that really understand how the

world works and sort of learn how the world works a little bit like um children who open their eyes and let me give you a interesting number. um LLMs are pre-trained today on basically all

the text available on the on the internet publicly uh which mostly is in English or you know languages spoken in developed countries which of course as panel has pointed out is an issue um but

it's it represents roughly 10 to the 14 bytes okay a one with 14 zeros um that seems like a lot of data and it is because it would take us any of us about half a million years to read

through it. Um, but then compare this with the amount of data that gets to the visual cortex of a young child. In four years, a young child has been awake a total of 16,000 hours. And if we put a

number on how much data gets to the visual cortex, it's about 2 megabytes per second. Do the arithmetics, that's about 10 to the 14 bytes in four years instead of half a million years. And so

it tells you we're never going to get to human level intelligence or anything like that by just training on text which is human produced. We're going to have to have systems that understand the real

world and are trained to understand the real world through sensory input. It can be video, it can be, you know, all kinds of stuff. And by the way, 16,000 hours of video is not a lot of video. It's

about 30 minutes of YouTube uploads. Okay? If you get a day of YouTube uploads, it's about a million hours. And that's about a 100 years of video. And we have video systems that we've trained

that have been trained with that kind of data. They understand a lot more about the real world than any any LLM. They can tell you if something impossible happens in a video that they watch. Um,

so they they they've acquired a little bit of common sense. So my guess is that this is going to make a lot of progress in the in the future. And from those kind of techniques, we can we can build

one models. What is a world model? Given this an idea or representation of the state of the world at time t and an action or intervention that you imagine taking, a world model would predict the

state of the world at time t plus one resulting from this action or intervention. And this is how you can build intelligence system because they would be able to predict the

consequences of their actions before taking the action and they would be able to plan and reason because reasoning is like planning. So everybody is talking about agentic system in systems in

industry uh the way agentic systems are built today is not this way. agentic systems today are not able to predict the consequences of their actions and this is a terrible way of planning

actions. Um so I think um you know again we're going to see a revolution over the next few years based on world models based on systems that can learn from the real world messy data um and uh I'm not

very popular in Silicon Valley when I say this but those are not generative models kind of a different type um and and so um yeah my my colleagues who work on LLM and generative AI are don't like

me very much So for me, I'm I'm I'm I'm really liking this. So I'm going to ask you a number question. What would it take? What kind of money

would it take to make this faster? &gt;&gt; Okay. So there there's a number of different things that need to happen. The first thing is there's a lot of research to be done like academic

research, right? And in fact the what's what's what's interesting as a phenomenon is that this idea of world model and and those non-generative architecture which I call Japa but

there's sort of various incarnations of it um are mostly worked on by academic groups who are interested in applying AI to science and mostly ignored by industry. industry particularly

Silicon Valley which is you know dominant players is entirely focused on NLM and everybody is working on the same thing everybody is stealing each other's engineers and working on the same thing

because nobody can afford to you know do something slightly different and then you know run the risk of falling behind and so that creates kind of a monoculture

um that makes the industry a little blind um and so right now it's in the hands of academia. So, so basically kind of propping up this kind of research in academia uh and

you know preventing LLMs from sucking the oxygen out of every room it gets into I think is uh uh is the first step. Um second step is uh of course there is you know a role for

governments and industry to play there in sort of you know pushing those models once they work and that's what I'm working on that's why I left Meta and created this company which is because I

think it's the time is ripe for you know trying to make this uh real and uh and then uh you know obviously there this there's there's going to be a lot of applications of this um everywhere in

the world Um there there was an experiment that was run a few years ago, a couple years ago by some of my colleagues at at Meta where they gave smart glasses to um farmers in India,

rural India. Um and you could talk to the assistant in in in you know Indic languages in the AI assistant asking them oh what's this what's this disease on my crop or like you know should I

harvest now or wait a little bit what's the weather tomorrow? So there's a lot of things like this that could be useful if the price, you know, could be could be brought down with systems that really

understand uh the world better than than we currently do. And in the future, all of us will be walking around with an AI assistant that will, you know, essentially amplify our our own

intelligence. It's like you know all of us would be sort of a you know the the a leader a manager of a staff of of virtual people who are smarter than us which is a great thing to do by the way

working I'm very familiar with the concept of working with people who are smarter than you it's great the greatest thing that can happen to you so um you know we shouldn't feel threatened by

that um and uh so it's going to you know allow people to get uh more knowledgeable more educated u you make more rational choices, but we need we need systems that basically approach or

surpass human intelligence in certain domains and understand the real world. &gt;&gt; Thank you, Yan. So, we know where Yan is putting his money. Coming back to all my panelists, not

just your money, if I other people's money, &gt;&gt; if if if I had $500 million to give And I'm not asking you for a P&amp;L. I'm

not asking you to give me a profit. I'm just asking you to help me democratize AI and make it accessible for everyone. Where would you each put your money? Let's start with Sanjay.

Incidentally, 500 million is the amount that you're looking at as raising cap catalytic capital to get DPI everywhere in the world because we think that you know getting those underlying systems of

record getting people access to their data in a digital form can actually empower them so much that they can then participate in the AI revolution in the right way with the right uh controls and

structures in place. So, you know, you've kind of just made my case that we would want to think about how we can take that money, deploy it, and bring everyone up to the same level in terms

of digital infrastructure, getting the data, getting the uh ledgers, getting the uh uh health records, all of those digitized so that then they can take benefit of AI for their needs. So,

that's actually what we would want to do. &gt;&gt; Okay. Sangu &gt;&gt; again I would say I will spend that big money uh to develop some more use cases

again and again. So we are identifying agriculture, education, healthcare and some more the government service can be a really promising uh use case field. So developing some more practical and

profitable use case and which adds so many uh so much build value will be the really critical one. On top of that maybe while we are developing the the use cases more important thing is that

some change user mindset and user inspire users because one typical you know problem uh we are facing is that our po the you know lowinccome

users and clients and people are not don't do do not really know what they do know do don't know. So inspire even though they can do something with this type of technology but they don't

clearly understand what they can do. So inspire them that they can really do this with higher productivity with low cost that will be very important uh you know things to remind them.

&gt;&gt; Thank you. given the volume of funds available, I would focus a lot more on uh capability development of people to be able the ability to use AI uh for improving

productivity and maybe if I can add to it just to again stress on models on the need for small domain specific niche models. may not be small may not be the right uh word to use but uh domain

specific and niche models which will ensure that uh they use lot less power lot less infrastructure and uh and and not have the problems of large large language models

&gt;&gt; um so I'm assuming each one of us is getting 500 million &gt;&gt; yes &gt;&gt; so I co-sign on everything in addition I I would say for us what is critical

given the point I mentioned about the breadth of work that needs to be done is actually having open models and also investing in talent. So the open models do allow for people to innovate on top

of them. And an example of this is Crane AI which actually developed a um offline first AI stack focusing on health education and agriculture services and they emerged from the masakana

community. So what happens when we actually can fund a lot of people to think about this and build on on top of open models and then lastly talent. Talent is very important um across the

whole value chain. talent that actually looks at the building of the models, the uptake, the business cases to motivate for people to allow for um sustainability but also the talent to

actually build capacity of the end users to understand so that we create an ecosystem where people are excited for these new technological interventions instead of afraid. And that's sort of

been the biggest narrative of you're either very excited or you're very afraid. And coming from a South African context, everyone is afraid to lose their job to AI. So, how do we ensure

that we're creating that um ecosystem that's favorable for innovation? So, as we come to the end of our panel, with everything that's been said, even with

all the money on the table, free money, we see that it's not a one-sizefits all, we simply can't just focus on one area and leave the rest. We need the talent, we need the compute, we need the data

centers, we need the regulatory reforms, we need everything to come together to make this possible. And with that, um, I'm done with my questions. I have five minutes if

before I even finish my question. So would someone help me with a mic? What I'll do, I'll take three questions. um hopefully to three different people from you guys and then since I see no

one um quite uh good thank you um let's start here thanks v thank you all for such a brilliant session uh my name is Arun Sharma I work with the world bank uh my

question to anyone uh yan specifically what is the lag that we have in the physical physical and the virtual world it's dominated a lot by the machinery. I mean you gave the example of a farmer

wearing glasses but then the seeds or the fertilizer or anything that he orders still run on archaic systems. So obviously there is a lag between the hardware and the software. The software

is evolving much faster. Where do you see that happening and going? And I asked this specifically because in in the Indian system uh where we've not been able to deploy our

resources is in the education space or in the healthcare space where we we still lag in those areas. So thanks &gt;&gt; sorry before you answer that let me take the three questions. Um I would prefer

that you throw the next question to someone else. I'll take the question from the back there. the guy with the specs. &gt;&gt; Yes,

&gt;&gt; thanks a lot. Um Daniel Doash um particle physicist from CERN originally and then a research director for Swisscom. Um you mentioned federated learning technologically this is easy

the architecture of collaboration might be difficult for that. So do you have some ideas which kind of organization could coordinate this kind of collaboration? Thank you.

&gt;&gt; Okay. And one last question I need to get from here. The guy with the red &gt;&gt; uh hi uh thank you sir. My question is to you. uh like you have said that we have the data like 10 to the^ 14 bytes

and the same data that a boy consume likely four to five years of age. So do you think that data is the only bottleneck despite of compute and the architecture to get the AGI or maybe the

humans the super intelligence artificial super intelligence and the next question is when we will achieve AGI what was the benchmark like how we benchmark AGI that like it will be definitely uh smarter

than humans so how humans will evaluate that so yeah that's it &gt;&gt; so yeah &gt;&gt; so quick quick answers I I'll go in reverse U so there's no such thing as a

GI there is human level AI perhaps but human human intelligence is extremely specialized and so calling this general intelligence is complete nonsense okay but we will get to we will build system

that are as intelligent as humans in all domains where humans are intelligent it's just not going to be next year unlike what you know some some colleagues in the industry are claiming

this is going to take a lot longer um it's not going to be an And it's not like we're going to dis to discover one secret that is going to just, you know, unlock uh intelligence. Um it's going to

be, you know, progress is going to be much more difficult than we think. U it's always been more difficult than we thought in the past and it's still the case. So no event for AGI and no AGI,

human level AI, yet super intelligent AI. Yes, we should call it ASI, artificial super intelligence. &gt;&gt; Yeah. Well, it depends. Okay. So that's that's the first thing. Um and you had a

second part to your question. I can't remember where it was. So I'm going to answer the other one. Uh there is a number of organizations that could um so first of all the thing that's needed for

this federated learning uh idea for an open source model uh should be bottom up like it should be people actually kind of you know putting up a GitHub and then collaborating on sort of building the

infrastructure for this. Um, of course we can get help from governments and organizations and that that that's required too, but I think it's uh it's gonna ultimately people need to build

code, write, write code. Um, so there's a number of groups that have already built their own um LLMs that are pretty good quality. There's a group in Switzerland uh centered at AFL and ETH

Aerus. So you probably know it. Um there is a group in the UAE. um uh centered on MBZ UAI. Um there is you know similar models in uh in Korea in various other countries and they all would like I mean

they should all kind of you know get together and basically join join forces and then you know bring in other other countries as well. I think SEM can play a role. I think UNESCO can play a role.

I think uh uh you know Switzerland should play a role right. I mean they they they they have all those organization in Geneva, right? May maybe that's where

&gt;&gt; and the next summit is going to be there. Um so maybe that's the right thing to do. Um and and have a bottom up and and top down. One big organization that can play a role is the AI alliance

which is a group that promotes open source AI. &gt;&gt; Yan, let me cut you short. Um we've run out of time and we would like to thank you all for coming. Um yes,

&gt;&gt; thank you so much for all the speakers. We just have a small momento from the government side to uh just me make this a memorable event.
