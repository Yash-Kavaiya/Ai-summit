# Panel on the 2026 International AI Safety Report

**India AI Impact Summit 2026 ‚Äî Day 5 (2026-02-20)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 11:30 ‚Äì 12:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 15 |
| üìÖ **Date** | 2026-02-20 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/EEQdcNvnXnw?feature=share) |

## üé§ Speakers

- Adam Beaumont, AI Security Institute, United Kingdom
- Alondra Nelson, Institute for Advanced Study
- Josephine Teo, Government of Singapore
- Lee Tiedrich, Inaugural AI Multidisciplinary Initiative Fellow, University of Maryland
- Yoshua Bengio, Universit√© de Montr√©al; MILA; LawZero

## ü§ù Knowledge Partners

- UK AI Security Institute

## üìù Summary

AI capabilities outpace the evidence-base on impacts and safeguards, creating a dilemma for policymakers: How to act early enough to reduce risk while ensuring progress of effective approaches? Turing Award-winner Yoshua Bengio and global experts will present and discuss the key findings of the International AI Safety Report, an initiative backed by 30+ countries and organizations like the EU, OECD and UN, providing a science-based understanding of general-purpose AI capabilities, emerging risks, and mitigation measures.

## üîë Key Takeaways

1. AI capabilities outpace the evidence-base on impacts and safeguards, creating a dilemma for policymakers: How to act early enough to reduce risk while ensuring progress of effective approaches? Turing Award-winner Yoshua Bengio and global experts will present and discuss the key findings of the International AI Safety Report, an initiative backed by 30+ countries and organizations like the EU, OECD and UN, providing a science-based understanding of general-purpose AI capabilities, emerging risks, and mitigation measures.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/EEQdcNvnXnw/maxresdefault.jpg)](https://youtube.com/live/EEQdcNvnXnw?feature=share)

---

_[‚Üê Back to Day 5 Sessions](../README.md)_


## üìù Transcript

continue uh rapidly uh for policy makers across the globe to uh rely on an independent scientific assessment of what AI can do and what it can cause and what we can do already to try to

mitigate this. I'm committed to continue supporting such reporting. Uh as you know, we're heading into a future with many unknown unknowns. uh things that we could not even imagine a year ago like

the psychological effects are happening and there will be other surprises in the future uh and so we must accept the prevailing uncertainty and collectively prepare for all plausible scenarios uh

according to the scientific community. So thanks uh and looking forward for the continued discussion. Okay. Well, thank you, Yosua, for your leadership and for giving us um an

overview of the safety report. And now we're going to dig into the safety report in more detail. And to do this, we've got an amazing panel. Um to my left we have uh we have Minister

Josephine Tio from Singapore who leads the Singapore government's efforts in digital development, public communications and engagement, smart nation strategy and cyber security. Um

we're also joined by Professor Alandre Nelson who holds the Harold F. Flinder chair and leads science, technology and social values um lab at the institute for advanced study um where she's been

on the faculty since 2019 and Alandre also contributed significantly to the report as a senior adviser. Um and then we also have uh Adam Bowmont who is the director of the UK's AI security

institute um the first and biggest governmentbacked organization dedicated to ensuring advanced AI um is safe, secure and beneficial and I'm Lee Tedrich um with the University of

Maryland and I also had the honor of serving as a senior adviser on the report. Um so to get us started um I'll send the first question to Yosua. Um you talked about um how the technology has

evolved quite rapidly and continues to evolve rapidly and you highlighted um you know some of the significant changes but are there any particular changes um that really stand out to you as being

significant um in 2026 as compared to 2025? &gt;&gt; Yes. Uh I think in terms of risk management and uh potentially policy um the advances in agency of the AI

systems is something we should pay a lot more attention. Uh and the reason is simple. Um having AIs that are more autonomous means less oversight. Right? So right now when you interface with a

chatbot of course the human is in the loop, right? It is a loop and then usually you take what the AI is proposing and then you human do something with it.

Agents are a different game where the agents will like work on a problem for you uh for hours, days and they will be given credentials uh they will be given access to the internet. So we need to

have um AI technology that will be much more reliable and and avoid some of the issues we're seeing today I think before this can be deployed in a way that's safe and and accept it because uh you

know businesses and users at some point will be concerned they can't trust this technology with all the credentials that we might give them. And uh and then we're also seeing things that are I

think somewhat unexpected but not yet sufficiently studied which is uh once we kind of let out these agents into the world they start interacting with each other and I think it's a bit early days

but what we're seeing is a bit concerning. &gt;&gt; Yeah I know it's certainly gotten a lot of attention in in the press and I think it highlights the need to increase AI

literacy too so people understand what these agents can and cannot do. Um for Minister Tio, um Singapore has been at the forefront of AI governance from the ASEAN AI um governance guide to the

Singapore consensus on AI safety. And one of the things that Yosua highlighted that the report talks about is um you know the need to translate um some of the evaluation for different cultures

and different norms and also to be able to put it into practice. um based on Singapore's experience, what does it like look like to take the science and actually put that into tools and

practice that people around the world can use? &gt;&gt; Thank you very much. Perhaps I will offer a perspective as a small state uh in a part of the world that has a lot of

interest in um the adoption of AI technologies but um perhaps is still only becoming much more aware of the extent of the risk. Um and so in my interactions with my counterparts, I

often share with them a perspective. Uh they would have visited Singapore. They would have um you know traveled in and out of our air hub. And I explained to them that um Singapore does not own

aircraft technologies. We Boeing does not belong to us. Neither does Airbus. Uh but we have to be concerned about the safety of how these aircraft are manufactured. We have to be concerned

about maintenance, repair and overhaul. We have to be concerned about air traffic management. If we didn't have all of these elements in place, uh it's very hard to see how you can have a

thriving air hub, you know, and be responsible for the lives of millions of people passing through the airport. So that's the reason why we think we have to be invested in the conversation and

the efforts to bring about AI safety. If we want to see wide adoption in our region, then we must equally be aware of um how the the risk can be mitigated. So that's the starting point. Um the second

point I'd like to make is that ultimately as policy makers our objective in understanding the safety aspects must translate into how we can put them into operable guardrails and

very often this would mean standards that are being imposed. This would mean um regulations and laws. uh but we have to do it in a thoughtful way because we still do want to benefit from this

technology. So if we are not targeted in the way we implement these requirements then what we might achieve is not just the impact to the pace of innovation. what we could uh end up is a situation

where we have given a false promise to our citizens giving them the impression that we have protected them when in fact we haven't actually done so. So that's why I think um we uh need to be

thoughtful. Um part of Singapore's interest is also that um uh when the there is clarity about what needs to be done um we want to be able to move very quickly. Joshua talked about the use of

the misuse of AI, for example, to use them for generating images that often target women and children. And um what we did was that um last year we introduced a new law. It imposes

statutory uh obligations on the services that bring these images and make this content available to vast numbers of people. uh they have always said that we are not responsible for the generation

of such content. Uh and so that's something that um we take on board but having been notified of the existence of such harmful content then there is an obligation for you to remove it. So this

new law that we passed uh imposes such an obligation um and um Yasha also talked about the findings in the reports how AI and cyber security is intersecting in very very

concerning ways for example AI being uh used uh to target systems and so AI is a threat. Um now however we also see that AI itself can be a target of cyber attacks and when AI becomes a target of

cyber attacks particularly for multi- aent systems those kinds of risks can easily go out of hand so even as the Singapore government is experimenting with the use of AI we want to be very

thoughtful about how these AI uh agent systems are being architected and what exactly goes into the decision making process as to the agency that is being granted. Is there a way you know to put

guard rails around it? So I would just say that um AI as a threat, AI as a target and uh where we uh really need to cooperate and uh do much better in is using AI as a tool to fight these

threats. So those are the kinds of things that within the Azen community we hope to be able to make progress on. &gt;&gt; That's great and thank you and it's a great um segue to Alandra. um you've

worked um not only in academia but have had high level positions in the United States government and a lot of your work is focused on the relationship between science technology and public

accountability and you know the report um is really intended to inform policy makers and inform the broader community and intentionally does not um take the next step of advising policy makers on

what to do and I'd be interested in your thoughts as to um you know both the structure of the report and and drawing that line and and importantly like what's next like what should policy

makers be thinking about as they as they read and digest the report. &gt;&gt; Yeah, thank you so much. Um and thank you all for being here. So let me just start by thanking Yoshua again because

um I was at Bletchley Park. We were having a conversation and one of the things I said um when I spoke there was that we are going to need new democratic institutions for this moment. One of

those are certainly the ACEs, but one of those is this report, right? Like our ability to have a ground truth as a global community um about the risks are deeply important for um uh the f any

future that we're going to have with AI that's beneficial. So um and I know that takes a lot of work and so thank you Joshua for doing that. And in the course of doing that and serving as a senior

adviser have seen how um they've created a whole new the system. I mean, you know, some of it comes out of CS culture, some of it comes out of research culture that we know, but they

literally have created a new institution to help us kind of think through what's the best information. How do you make evidence-based claims about the state of science in the midst of kind of radical

uncertainty? And that's a new task for researchers of across our our fields and disciplines. So, I just want to tip my hat to you and make sure that people actually know um uh how much work you've

put into this. So, you know, I think the report it's mandated and I think it does a really good job of exactly not crossing that line, Lee, which is to say what what what do we know? Um what's the

best of what we know? What are some I mean the this report I think for the first time uses some OECD scenario. So, it's sort of reaching a little bit to evidence informed kind of foresight and

forecasting um and you know really responds to uh I think the fact that a lot of our information about what's happening in AI comes from journalism. It's a very hard time to be a journalist

right now. So, this is not a knock on journalist, but it's just to say that we don't have globally, you know, the kind of um sort of horizon of information that we really need in the policy space

to make good policy decisions. That said, you know, states will have lots of different policies and concerns that they want to make. So, it's not the um mandate of the report to to sort of

direct how people should think about the evidence, but it is to say there's more than anecdotal journalism here and this is the best of what we know in this moment. Uh Yoshua mentioned there's sort

of updates that are happening. So, the report is uh the team is also getting better at getting the information in more closer to real time. So I think it establishes that ground truth that's so

important for AI particularly in the context not only of uncertainty as I said but of lots of hype um uh that we're sort of reading about and hearing about every day. Um but I would also say

that the report does a good job of of of um at the end of each section making some nods to policy. So these are so what should policy makers make of these scientific insights? And so it does a

very good job at sort of you know steering what the implications of the fact that we have now growing uses of multi- aent systems. How might you need to think about that? How might you need

to think about the fact that there are you know growing sort of biocurity and cyber security risks for example and then lead to your point about what needs to be done. And I think we all know it's

needs to be done and I think I hope that the report because it is you know not anecdotal not whim allows there to be some um stronger political spines and some more political will um to make the

hard decisions that we need to make in the regulatory and polic policy space um both in individual nation states and I think also uh as a global community. So um you know if it can be a resource for

helping people make good policy makers make good and strong and evidence-based arguments um and also I think you know allowing governments to support the funding of the creation of more evidence

um I think it will be all to the good and and obviously moving into the space of of some sort of guardrails and regulatory um regime is is what needs to happen is the next step.

Thank you, Alandre. And I um also it's a great segue over to Adam because the report also identifies what are some of the key research gaps and what are some of the key gaps in the evaluation

ecosystem? And so for you Adam, as the leader of the AC um what jumped out to you in terms of risks and thing and and what are your priorities in terms of how to start addressing those risks going

forward based on the report? &gt;&gt; Yeah, thank you. Thank you very much. And um I wanted to reiterate thanks to Yosha in the panel um and also to call out the work of AC and supporting the

secretary of that for the past couple of years and I know a couple of lead writers in the audience too. So it's really great to see just the collaborative effort that's happened

around the world on that that I think is so important for enabling policy makers to have like an objective independent state of the science report in AC. You asked me about which which kind of risks

um jump out most. It's quite hard from uh our research staff is about hundred. So, it's like naming which is your favorite child. Um but there are a few from my um favorite's a strange word

there. Um there are a few that really jump out to me with my background in national security and Joshua you've spoken a bit about this already um in cyber security and in in biological

capabilities and both of those are very dual use uh but I think in cyber security we've seen such rapid development in the capability of the models even in the

last few weeks and and months um and I think the report does a great job of explaining how that that capability can assist in cyber operations at many different stages in that life cycle or

different tasks. We're not yet seeing that fully autonomous though and I think that that is the area that uh concerns me that and we're trying to research and understand

right now is what does the confluence of some of these risks look like when combined with more autonomy particularly in agentic AI scenarios and some of the things we're doing in AC about that um I

guess we're quite well known for our pre-eployment testing of frontier AI models we also do post-eployment uh you can see some of the impact of that in the model cards the some some of

the companies published. We do a lot of red teaming um and and with that we're trying to strengthen the safeguards of the models that are being provided but also raise the bar for the level of

security research that's happening. So this week we published um research around some of our methods on on how we do that where we want to both responsibly disclose that but also grow

the number of people that are working to help raise the bar. Um we also use grant making and trying to get raise the level of investment happening in this in this space. Um

uh and then we're trying to develop the the way that we do evaluations uh to to adapt to the way that models are improving in capabilities. For example, you get different results if they use

more tokens uh with inference time scaling. So we're trying to make sure that our valuations account for that. Uh or by using cyber ranges rather than just sort of cap capture the flag type

uh scenarios. Uh so I care about all of those different risks that we are researching. Um but the one I'm watching right now is probably on cyber security.

&gt;&gt; Thank you. And uh back to you Josh Yosua. One of the things that you had mentioned in in the overview is that is the jagged performance of the general purpose AI models. And I'd be interested

in your thoughts on how that impacts the evaluation science. If you have a general purpose model and it's good at some tasks but not others, should evaluators be thinking about things

differently? &gt;&gt; Yes. Uh, also I think the general public and the media needs to like escape this vision of an AGI moment because if AI continues to have these

jagged capabilities, it means that we could well be in a world where AI already has dangerous capabilities as dual use for some things at the same time as it might be really weak on other

skills. And so the the the thing that matters at this point is in this world that continues is very careful scientific evaluation of you know per scale per ability risk and capability

right uh by the way that includes capability and intention something I didn't mention too much in my presentation we're seeing a lot of concerns with AIs having goals that we

would not like them to have um and in spite of our instructions acting uh against um their moral alignment training. Um so yeah th this this is we we we can't stay at this very abstract I

mean maybe like a few years ago thinking about AGI was like a reasonable abstraction reaching human level but now it's kind of meaningless because you know we're going to have things that can

be extremely stupid in some ways maybe weak in some ways and already dangerous in the wrong hands in some other ways. So we we we have to be more technical and and more precise in talking about

the risks and also like if you're a business and you want to deploy, you also want to know, you know, is the AI going to be good for what I'm trying to do. Um

I want to add one thing about the report spirit, about the reports rigor. Um that's not directly connected to your question, but but I think it's really important.

There is a central requirement for science which you know when we talk about rigor what does it mean? So what it really means for every scientist

when they put something in writing or something official they should not make a claim that could be false. So if they if you know they should only

be claiming things that they're totally sure about because you know especially in the context where policy makers are going to use that information. You don't want decisions to be taken based on

false claims and of course opinions abound in our world especially because they you know they they impact people's interests. uh and and and this is why it's so important that we can ground our

policy decisions in scientific evaluation. And what it really means is this. It means a kind of humility and honesty even when you know you may be biased in one way or another uh to stick

to those facts and you need a group of people because each of us can be personally biased right I am everyone everyone is is human uh a group of people who can like catch each other's

maybe you know uh going across that red line of rigor and uh not making statements that couldn't be defended very strongly. &gt;&gt; Thank you. And a very very important

point. I I think in addition to the policy makers needing to be able to use this information, I through my work end up talking to a lot of organizations, nonprofits, small and medium-sized

businesses. And what I hear a lot is like it's it's great like you have to start with the science and that is ground zero. But then for some of those other organizations, they need the

tooling. um they're not going to have a whole scientific staff on how do we put that into practice and I'm just wondering from the government's uh perspective uh Minister Tio you know

what are your thoughts on how we might be able to advance some of the tooling to take this great learning and um make it easier for companies um and other organizations to actually deploy

&gt;&gt; I was at a similar session recently and this topic came up um and the way I uh think about it is Um I use Iaya as as as an example. You know when you go to IA uh you buy furniture and IA promises you

this furniture has been tested. So you know if it's a couch uh it has been jumped on I don't know 25,000 times and it didn't break you know and and so your kids are not going to be hurt uh if

they jumped on it too. Well up to 25,000 times. Um and um um if you think about a a a user on the receiving end of this technology, it is I think quite

unreasonable to expect them you know to have to impose safety conditions on their own. They are simply not in a position to do so. They they don't have the power to decide you know what gets

uh sold to them and what does not get sold to them. So we as policy makers must recognize that there is a huge gap between those that we are encouraging to adopt AI

tools, adopt AI technology in various contexts. Um we must think about where are the right points to uh make these requirements uh mandatory when it is you know perhaps not so much uh requirements

that are mandatory but uh it is useful for industries to come together uh uh for example in in in Davos we disc we discussed the possibility of insurance schemes you know creating the right

incentives for uh AI you model developers and I think that there is no easy landing point just yet. Um but if we fail to engage in these conversations in a in a in a in a rational way then I

think we are even further behind and trying to manage the risks. So I would say that um um the the the thoughtfulness has to be applied at many different levels. There needs to be

continued research in AI safety. Um and so I'm very happy that we are continuing to have this conversation through the international scientific exchange second edition in Singapore. Um and we hope to

update uh where are the areas of uh safety research that that should be prioritized. I think this year I certainly agree with you uh multi- aent systems is going to come up quite

prominently. Um but we cannot just stop there. Um we also have an ongoing program. We started um you know by setting aside uh commitments under the our own national AI R&amp;D plan and in

fundamental research. One of the areas that we are very interested in is responsible AI. So you you you need the two to go hand in hand. But can you not have some testing frameworks and uh

toolkits to begin with? We think that that is also not uh helpful. It is more pragmatic to try and to recognize the shortcomings of those testing tools and then to invest further effort in uh

promoting you know more thoughtful ways of uh looking at the risk of these systems and how to mitigate against them. Ultimately we should try and get to a point where the end user has

assurance of safety that they don't have to be thinking so hard about whether you know the proper tests have been applied. We're not there yet. uh but I think we need to find a way you know to work out

the root. &gt;&gt; That's very interesting and you can also think of um analogies in the medical context. We don't always understand how the medicine works but we have assurance

that um you if it's prescribed for us it's going to work well. Um turning uh back to you Alandre you know there's been a lot of um conversation around catastrophic risks and the report is

intentionally broader than just catastrophic risks. I'd be interested in your thoughts as to, you know, whether that was a good place to to draw the line and what some of the benefits are

of broadening our our aperture beyond just the catastrophic risks. &gt;&gt; Thank you. I mean, certainly the um, you know, reason that I continue to be involved with this is because under

Yoshua's, you know, chairmanship of the report that it is attentive to a broader set of risks. So, there's a section of the report that's called systemic risk. And I think what we haven't quite pieced

together is that um particularly if we care about democracy, if we care about social cohesion, it is not the individual risks like we all have our favorites or unfavorites. Adam, to your

point, it is the compounding of those risks together. Like we are we are careening without seat belts in a car, you know, quickly in a society in which all of these risks and harms are

happening simultaneously. So um you know that is you know a very dangerous world for social cohesion that is not a society that's healthy um and that's not you know healthy for democracy and so I

think the attention to the broader set of risks which include you know things like um you know loss of human autonomy what does it mean when you're not in charge of your decision own decision-m

what does it mean uh when um you know sick fancy and other sorts of um I think uh outputs you know mean that you are being manipulated in some way through the use of the tools and technologies.

Uh how do we think about the fact that there might be job loss or job displacement, the anxiety that it creates? I mean, talk about a lack of social cohesion, the anxiety it's

already creating in a lot of societies about people's livelihoods and their abilities to protect them and you know, their families and their well-being. So, I think what the report does incredibly

well under a kind of large banner of safety is to think at a 30,000 foot level. If you take all of the chapters together about what what are those compounding risks? What does it look

like if all of those risks sort of move together simultaneously and therefore it is equally important to think about you know that technology in a health care space that's malfunctioning giving a max

misdiagnosis as important as it is in some ways to think about a biorisk. Um and so I think that's uh important and um and I'm um really gratified that the report continues to be anchored in that

broader aperture of risk. No, I I would agree too because I think a lot of those risks are especially with agents. Um they're here today and they're just going to continue to

increase and we do need to keep the focus on them. &gt;&gt; Just a small comment about the systemic risks. Uh, of course I completely agree, but I want to point out one factor that

makes them uh potentially catastrophic except maybe at a slower pace is because so many people uh are going to be using these systems and the like global dynamics and social dynamics are so

difficult to anticipate uh and could be incredibly uh you know impactful both on the positive and negative side. And I think um Yosua and Alandre's comments tee up the next question for

Adam. You know, these risks are evolving quite rapidly and one of the things that the report emphasizes is we have an evidence gap. Um you know, it's hard for researchers to keep up and it's hard to

do longitudinal studies in a very short period of time. I'd be interested in your perspectives from the ACS. you know, how do you address that as you start thinking about um real world

evaluation today and and how does that impact the approach to evaluation and what might the ACS be able to do to help fill some of this evidence gap? &gt;&gt; Yeah, thank you. And one of the things

I've been really encouraged by this week as I've spoken to different um researchers and organizations from around the world is how many have referenced this report as as providing a

foundation on which to then make policy afterwards. So I acknowledge there's more to do but also do think this is fulfilling an important need. um across the ACS we have been speaking more about

how can we share best practice and how can we understand best practices in evaluation methodologies and together we published a blog post last week that started to talk about uh some of our

learnings and and some of them are quite simple. There are things like if if you're evaluating something, be really clear. What is it you are trying to measure and make sure your evaluation is

actually getting after the thing that you are focused on as as some can be quite misleading in the way that they are organized. But in addition to uh areas where we had

good consensus around best practices, we also highlighted areas where there's still uncertainty or we need more research. And again, we want to communicate that and be very transparent

so that more people can join in. um as we do see this as requiring like many great minds around the world and there just aren't enough safety and security researchers to do that all in one place.

Um but in addition to talking about the the practice of evaluation, we're also trying to provide tooling for other organizations to do that. And one of the things I'm very proud of uh that AC

developed in the UK was the Inspect framework that's been open sourced um and is used really extensively by different companies, organizations in government, outside government. And the

thing I would love to see over this coming year is how we can really grow uh a wide kind of ecosystem of third party evaluators that can offer that independence and bring rigor and

scientific meth um method to the way that we measure these capabilities and then can communicate about them. &gt;&gt; And just um I'm going to ask one quickfire question to uh for the whole

group and then I'm going to open it up for Q&amp;A. So start thinking about your questions. But you know I'm interested Adam and I think it touches on some of the themes of like how do we take the

science and bring it to practice and how do we actually create this evaluation ecosystem. So step one is developing the science. Step two is then figuring out well how do we actually evaluate this

and then there's the you know by whom um and how do you see an evaluation ecosystem emerging? Do you see governments being the evaluator? Do you see this going more like we have with

accounting um where you have third party certified um auditors of of the doing the evaluations? I'd be interested in each of your thoughts and maybe start with uh Minister Tia Minister Tio and

then we can go down the line. &gt;&gt; Well, certainly in the ASEAN context, um I I would um advocate for an approach that deals with uh near and present dangers that everyone is is dealing

with. Um the risk of not focusing on what's most prominent in people's minds today, policy makers minds today, is that um the conversation may feel too theoretical and we may lose interest and

momentum and um we don't even build the foundations of cooperating uh uh in a meaningful way. U and what are some of those uh areas where AI intersect with um AI being used misused for harming

people uh in terms of the content creation? I think that's one area almost every single you know policy maker that I uh come across is is very very uh upset by the fact that they have to

address their constituents concerns about all these um harmful images that are being created with the use of with the help of AI. It's very offensive to our societies and uh if we are not able

to work on these areas in a um meaningful way in in a practical way then I think we risk losing my colleagues attention. Uh so what can we do? um uh we have to then seriously ask

is watermarking the correct approach of dealing with it. Um is there some other way of labeling AI generated content? Is that even the right direction that we should be moving on? The other area is

that I think it will be very prominent and um and and that is the use of AI in in cyber security. I I don't think at this point in time uh AI as a threat um uh is adequately addressed. AI as a

target is even, you know, further from people's minds and uh and in a a a pickup of the conversation in the areas that my colleagues care about, I think stands a better chance of anchoring

their attention and creating meaningful opportunities for us to say here are the ways you can test for it and here are the tools that can be applied. They won't be perfect but they are uh

important start. So I I want to mention maybe totally different aspect that's uh orthogonal to this. As I've been thinking about the process of bringing the science to have

an impact with policy makers, I feel like there is a step in between what we've done and the actual political decision-m and that is using scientifically

uh grounded policy options right so you know the the report doesn't go into recommendations and I think that was a

uh mandate uh that we started from but I think there is something in between taking the policy decisions in this which is uh you know grounded in in in what the scientists see and and the

people like economists and social scientists you know based on this what are like reasonable options for policy makers uh without saying you know you have to take this one but you know you

could do this you could do nothing you know and and and what are the consequences that uh uh are expected based again on the science uh without making an actual recommendation because

in the real world I understand you know policym is hard because you always have a tension between different values and objectives and interests we shouldn't make those choices but we

can help make it easy for policy makers &gt;&gt; I think I would offer we're just getting started with evaluations and assessments and so you know I wouldn't want to put a thumb on a scale and pick one I mean I

think that we actually have to try a lot of different things I also think to the extent that we have um a body of knowledge around evaluation that is coming from uh AC's and and other uh

researchers um that you know I worry that we're going to have a collective action problem and so that everyone's doing their own different kind of evaluation and I think what we will need

to fundamentally do is make as a research community a few choices about what you know something closer to a standard like this is the way that we are the few ways that we're going to

proceed. Um so I think there's that. I do think that it needs to be obviously multis sector. It's a fairly obvious point. How do you do that as a you know is an open question. Um I wrote a piece

in science a few months ago or I suggested that we might think about the LC program for um for human genetics and genomics in which you know 3% of the human genome project research budget in

1990 1991 uh was dedicated to upstream research of potential risks and harm of of human genetics. So that doesn't present risks and harms but it means that you go in upstream to projects

thinking about them as a part of the research and design um often before deployment and it doesn't mean that you can prevent things like someone doing illegal human genome gene editing right

but I think the but you do have a global community that is has thought about it and is ready to have a conversation and knew in the case of um the the human genome the human gene editing that it

was wrong and why it was wrong and we had discussed it. So I think that there are, you know, lots of models that government's deeply important here and that, you know, I think that there are

are schemes that would require, I think, the public sector to, you know, place a little money in the in the space of a sort of common good or a commons for research to understand and and advance

much more in the evaluation and assessment space. &gt;&gt; Yes. you you you asked um who should who should be involved in evaluation or where where should it be done and I

guess my answer to that is um should it be government should it be industry it's kind of all of the above and I really agree with you that we're very early in the in the journey and there's still a

lot of uncertainty um but I do think there's a role for governments to play there's a role for industry there's a role for researchers civil society but also individuals and

we saw that at the start of the year when people are very willing to trade away all their keys passwords anything for for for the for the enjoyment of agent autonomy. And that reminds me a

lot of the early days of cyber security where you know we need to grow ecos individuals have a responsibility as much as governments and I'm I'm sure over time we'll see more institutions

and organizations grow um that help do that but the key to it's got to be uh collaboration. Um so on a practical level things like um uh regulatory sandboxes or like policy lab type things

where you can you can try limited uh pilot approaches seem to be good. We're trying a bit of that in the UK. Things like joint funding programs that bring researchers policy makers together to

kind of iterate options again seems a good idea but strongly agree we're just early in the early in the journey. We should keep options open. &gt;&gt; Thank you. I think we have time for one

or two questions. Um, wow, we have a lot of hands. What I'm going to do is call on uh two people. We'll kind of combine the questions and we'll let the panelists Well, I wish I had more time.

So, we'll um take one here and um one over there. Go ahead. &gt;&gt; Right here in the second row. Can someone bring a microphone over? &gt;&gt; I have a question.

sovereignty like everywhere and like a lot of countries are trying to claim it in some way or another. I would be really curious to hear like how at least in the AI safety field how are you

seeing that impact which are safety concerns like the window. &gt;&gt; Yeah. So I think we should be careful about uh what sovereignty means. Um it doesn't mean uh building walls around

your country. uh it means making sure that your country will retain the ability to you know take its decisions and uh you know succeed economically and politically and often that means the

opposite of uh walls around your country. It means making partnerships with others that increase your uh chances of you know not not not ending up in a bad place and that includes

agreements on safety right because um many of the risks we've discussed uh they you know they're not limited by borders we we can uh collaborate on the safety technology uh with multiple

countries we can have the kinds of agreements that Singapore has been leading where multiple uh parties you know from many different countries agree on principles uh and

eventually we will need international agreements and we will need technology for verification of these agreements we are far from that but that's the only kind of world where you know I would

want my children to live where AI is not used to dominate others and we don't see like reckless behavior across the world &gt;&gt; you I'm so glad that um you know Joshua has offered a view that uh to me is a

very sound approach. Um you said earlier that uh what we want is a world where every country can be at the table not on the menu. Um and and that's exactly how you can preserve sovereignty even with

AI developments. The idea that you get sovereign AI by confining everything to your own shores I think is a gives a false sense of security. Firstly, it's not achievable.

Secondly, the idea that you can do so I think would mean that for many countries um where the the the most sophisticated um applications, you know, will have to originate from elsewhere. that just cuts

you off from being able also to make progress and that puts you even further behind. So how is that sovereignty? So I it has to be a a topic that is dealt with thoughtfully. It's not a term

to be banded about too easily. Alra, Adam, any thoughts? Okay. What? &gt;&gt; Okay. Yeah. So we um unfortunately are running out of time. Um but I would love to

thank our our panelists um for being here today and sharing the report and I hope all of you will read the report and continue to engage with us because as we said there's a lot more work to be done.

Uh thank you very much. &gt;&gt; Thank you all.
