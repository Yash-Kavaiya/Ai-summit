# Trust and Safety as Engines for Growth and Innovation

**India AI Impact Summit 2026 ‚Äî Day 5 (2026-02-20)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 16:30 ‚Äì 17:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 7 |
| üìÖ **Date** | 2026-02-20 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/lNtRBzn5uKw?feature=share) |

## üé§ Speakers

- Alexandra Reeve Givens, IAPP
- Commissioner John Edwards, United Kingdom Government
- Josephine Teo, Government of Singapore
- Trevor Hughes, IAPP

## ü§ù Knowledge Partners

- International Association of Privacy Professionals

## üìù Summary

Around the world governments and businesses agree: trust is the key for accelerating AI adoption. While research has extensively mapped why trust in AI matters, far less attention has been given to how trust will be implemented, including issues around the people, skills and systems required to operationalize trust and safety at scale.

## üîë Key Takeaways

1. Around the world governments and businesses agree: trust is the key for accelerating AI adoption.
2. While research has extensively mapped why trust in AI matters, far less attention has been given to how trust will be implemented, including issues around the people, skills and systems required to operationalize trust and safety at scale.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/lNtRBzn5uKw/maxresdefault.jpg)](https://youtube.com/live/lNtRBzn5uKw?feature=share)

---

_[‚Üê Back to Day 5 Sessions](../README.md)_


## üìù Transcript

And then we're going to dive right in. To my immediate left, I have Alex Ree Given who is the CEO of the Center for Democracy and Technology, one of the leading advocacy organizations in the

world, working on civil rights, civil liberties all around the world. She's based in DC. To her immediate left is Amanda Craig. Amanda is the general manager for responsible AI policy at

Microsoft. Uh to Amanda's left, we have John Edwards. John Edwards is known to many. He is the information commissioner of the United Kingdom. And to John's left uh we have Denise Wong who is the

deputy commissioner of the PDPC in Singapore, the Privacy and Data Protection Commission. Uh welcome to our panelists. So we have uh two regulators, an industry representative and a civil

society representative. Um and uh I come from the IEP. If you don't know the P, we are a global professional association, a notfor-profit, but also policy neutral. We're not an advocacy or

a lobbying body. We bring together the people who do the work, many of them are in the room right now, who do the very hard work of data protection and AI governance all around the world. All

right, let's jump in. The title of the session reflects trust as an engine for growth. Let's think about that just for a minute. uh just a few short years ago, I think it was two and a half maybe

three years ago, this event started in Bletchley Park in England. And in that iteration of the event, it was named the AI safety summit. Right around that time, the EU AI act was being negotiated

soon passed after that. Um but a lot has changed in that two or three years. This event is the AI impact summit. The event last year in Paris was the AI action summit. More recently, we have seen uh

the not yet fully implemented EU AI act become subject to an omnibus package where some of the expectations of that original act are being dialed back a little bit. And we've seen broad

critique of regulatory structures, trust and safety structures that might inhibit growth and innovation in AI. there clearly is a deregulatory mood in the air. Uh in fact, I think it's notable

that there has not been much discussion of law or regulatory initiatives um that might create guard rails to help guide the adoption of AI. So clearly we're in an odd moment and an

odd moment for this panel. But as I walked around the campus of this event, this enormous campus, I noted something that was, I think, quite significant, just about every second

banner or poster. Just about every large um um uh printed word in the show floor somewhere had trust or safety or privacy as part of the messaging. In fact the the sutras and we'll talk about them as

we go through the session the principles announced by the Indian government largely around trust and safety and so what gives what's the the dichotomy here at one moment we are saying it's a

deregulatory move we step back well at the same time we are actively embracing and discussing trust and safety risk management protecting consumers citizens human beings as they engage with AI

So, do we care or not? Are we actually in a deregulatory moment or have we just gotten quiet about the the the need for guard rails and trust and safety in these systems? I would say for business

risk exists regardless of whether there's a law in place or not. And so, businesses have an imperative to respond. I'm going to tell a very quick story and that is that in 1891 when

electricity was first being brought into the White House in the United States, then President Benjamin Harrison and his wife Caroline were actually terrified of flipping the light switch. And so they

hired the electrician from the Edison Company, a man named Ike Hoover, who went on to become the chief usher of the White House. They hired him to flip the light switch. I think the message of

this story is that we won't use it if we don't trust it. And so as AI is being pulled through the walls of our world, as it's creating light and switches and tools for us to use, I think we need to

ensure that we're comfortable flipping those switches. And that is the topic of our panel today. So let's jump in and uh our first question is going to be about just the moment that we find ourselves

in. And I'm going to start with Alex. Why are trust and safety important to innovation and and maybe speak to this dichotomy that I've highlighted? Why is it in this moment that we can't talk

about regulation, but everywhere it seems we're talking about trust and safety? Yeah, first of all, thank you for convening us and it's a pleasure to be here. Uh, I think you really hit the

nail on the head in your in your introduction, which is when we think about the long-term success and sustainability of AI and that is business sustainability for the

companies as well as societal sustainability for all of us. The secret is not just an acceleration, the biggest, fastest, most capable model. The real story is one of adoption and

that has been the overwhelming theme of the summit this year. And for people to adopt this technology, they need to trust it. And that's trust in in in in multiple different facets, right? Is the

tool fit for purpose? Does it work in your language? Is it appropriate for your culture? Will it protect your privacy? Is your data going to be secured? What is the quality of the

information that is grounding that model and those outputs? And I think people are really waking up to this and they're demanding more. This is both as individual users and then of course for

enterprise customers too who themselves are saying we're on the front lines thinking about how to integrate AI into our business operations. We're the ones who will likely be sued if this goes

wrong. Where are the guarantees? So this is where trust really is the fuel of innovation because it is what's going to be the economic driver of these tools being adopted. And the other thing that

I would add is what we see is not only that trust is important for innovation in in the abstract, but this is also where responsible thoughtful regulation can be fuel for innovation as well.

&gt;&gt; Because the same way that we want to be able to drive cars without all of us being experts in how a motor works, &gt;&gt; product liability and good laws around the creation of these tools help make

sure they outsource some of that work for us so that we don't all have to be doing the individual labor of deciding whether we can trust. So many times people will create this false framing of

regulation versus innovation as opposed to thoughtful being regulation being the fuel that actually allows us to sell, buy, and use these tools. &gt;&gt; Excellent. Fascinating. Um, John, I'm

going to jump to you and Amanda, I will come right back, but I'm going to jump to you. The UK doesn't have an AI law in place. It has lots of laws that will apply to AI. I think data protection and

and the GDPR act in the UK is a great example of that. But talk to us a little bit about regulating in the absence of an AI law. What does that look like in the UK? And do you see do you see

organizations exhibiting behavior that that demonstrates that they're focused on the ideas that Alex suggested that trust and safety matter regardless of the regulatory structure that sits over

them? &gt;&gt; Yeah, absolutely. Absolutely. &gt;&gt; Absolutely. There it is. Um no very much so. Uh I mean the data protection laws

apply uh across the board wherever technology touches personal data. So we we have a de facto regulatory regime under the UK GDPR. Um I you know coming back to your comment about trust it's so

important um and that there is a role for regulation actually in uh assisting businesses because businesses are trying to uh deliver that trust proposition to consumers but by what metric

&gt;&gt; right &gt;&gt; so and that's I think where um regulation can provide a common standard so uh you know we require it's a regulatory tool that uh you have to do

data protection by by design, you have to do data protection, impact assessment, you know, we expect privacy by design. We expect um risk assessment. So all of these things are regulatory

requirements, but they are also tools that help intermediate between businesses and the consumers to demonstrate that there is a basis for trust and and and an organization like

the ICO uh is there for both sides to see well there's someone actually overseeing that and that's a role that uh that that we do discharge to to your point about the absence of prescriptive

regulation. in the UK on AI. We don't see that particularly as a deficit. I mean, I think there's a lot of policy work going on in areas where you policy makers and regulators do need to step

in. Uh that's ongoing and I won't comment on that, but you know, there are ongoing issues about um the distribution of uh of um proceeds from the use of creative materials and the like, right?

That that carries on. Um but in the absence of an explicit rule, it's incumbent on my office to deliver uh safety and confidence and metrics uh for industry and to deliver certainty over

what is, you know, can be seen as an uncertain law. So we've um gone out and said, well, here's how we see the the technology neutral general principles of the GDPR apply

when you train a model. For example, uh we see for example the um EU AI act in article 10 talks about the need for fairness. Well, we've been able to articulate uh those obligations by way

of guidance uh linking it back already to the uh GDPR principles. So, you know, there's a there's a mapping. I don't think at the moment for the available applications of artificial intelligence

technologies that there is a lacuna. It's it's there with the uh GDPR and we are there to provide confidence and certainty about how you apply that how you improve your products with it and

how by doing so you engender that trust that you described at the outset. &gt;&gt; Excellent. Okay. So Amanda, tell us, do you agree that that there's that there's not a need for additional guardrails uh

uh traffic indicators in AI? Is John right that the existing regulatory structure is really providing enough guidance or or is it the case that Microsoft is using internal principles,

frameworks, standards that you might adopt to to build programs and services that you think um meet the expectations of trust and safety of the marketplace. &gt;&gt; Thank you. Um, from a Microsoft

perspective, we are focused on implementing our responsible AI governance program and see opportunity for lots of different governance models that governments could pursue in terms

of implementing existing regulation, developing additional regulation that complements that existing regulation. I think the the through line for us, the bottom line is very much what Alex

started us off with, that we do very much see, we've seen through multiple generations of technology. We're not going to have adoption. We're not going to have use of this technology without

trust. And we need to have governance programs at technology companies. We need to have governance efforts by governments that are ensuring that we have an evolving conversation about

trust. Because if I pull the thread on the analogy you started us with like how do you flip on a light switch and that can be scary when you've not done it before. I think the other thing that is

very true about this technology is that it is also very dynamic. It is evolving very quickly and people might even be scared that like they won't know where to find the light switch next week and

that that brings a whole different set of challenges and so that requires not just confidence and and how you are able to sort of trust the technology today but also that there's trust in a

governance process that will continue to iterate and evolve alongside the technology. &gt;&gt; Excellent. Denise, help us then here. I I know Singapore has released guidelines

um standards around AI. Tell us about the Singaporean experience in thinking about regulating trust and safety in AI. &gt;&gt; Thanks so much Trevor. Thank you to the AP for putting this together and for

having us. Um maybe I'll answer that question uh by linking some of the concepts that we've talked about and and that sort of underpins our philosophy. Trust and safety is the outcome that we

want. you know we want to create the necessary conditions for the society to thrive for the public and the enterprises to use the technology with confidence so AI for that public good to

do that we need governance uh we need a framework of thinking about how we can govern the technology and we've been doing this for all sorts of technology AI is but one

&gt;&gt; regulations are a mechanism a type of governance mechanism that you use when the necessary and correct conditions exist. Um and so those that map of that concepts informs how we think about um

our governance approach. So on issues that are very clear where there are clear harms, we have stepped in to regulate. An example of this is um elections regulations that we put in

place where we uh prohibited the use of AI deep fakes to represent candidates. uh it was time limited. It was for the period of elections but we stepped in and put a law in place for that. We also

have laws for AI um creating online harms as well as AI um in deep fa uh in scam situations. Um so that is the the part where we regulate uh for clear and present harms. Uh for the rest of it a

lot of it we leave to sectoral regulations where there's already a web of uh existing regulations. uh and on specific issues as well. John and I and many of us are in the data protection

field where as John has said there are already existing laws uh that can be tapped on, updated, reviewed in order to deal with this new technology that has come about. So where we have done AI

governance frameworks and and tools that you've mentioned is where we've seen a need to create some sort of horizontal principles and platforms to think about the uh sector agnostic general issues on

transparency uh on what what model governance for corporates could look like. Uh we haven't seen the need to regulate that horizontal layer just yet. Um but

certainly a need to articulate some of these principles. Um and that also allows us to create more certainty for the market to give them some direction that actually this can be um a

marketdriven assurance system that uh has demand has supply and has um I'll call them proto standards early early days of standards about what good looks like. Um so that's the work that we've

been doing in trying to create and seed an an assurance ecosystem that sits I would say adjacent and complementaryary to regulations where they're needed. &gt;&gt; Fantastic. Please please.

So just to jump in on that, one area that I think is proving very important and people are discovering this across jurisdictions is even where existing laws apply, there is a problem where AI

systems make it hard to know whether or not those laws are being broken. &gt;&gt; So this is where that transparency layer you were articulating really becomes important.

&gt;&gt; Give us an example. &gt;&gt; Yeah. And I'm going to make it US- ccentric just because this is an it's the one that's top of mind. So forgive the forgive the the bias here. So in the

US we have equal employment laws. it is against the law to discriminate in the course of hiring. Uh so in theory, a piece of software that perpetuates discrimination against particular

candidates, for example, not considering the resumes of people over a certain age, is violating an existing law. So people will say, "We don't need any further regulation. We're done." The

problem is you can tell in a human system where it was just a bad apple in the HR department. It's been historically easier to prove that case. Now, when it's AI powered software

making that decision, it is really hard as a worker who's just put in a resume and never got an answer back to know if something was going wrong. If you actually get up your courage and file a

case, it is really hard to prove your case that there is discrimination. And so without some type of disclosure regime that requires transparency in these high-risk scenarios, high impact

scenarios, to have transparency and disclosure about the system that is being used, impact assessments to make sure that that discrimination isn't happening, you actually don't get the

remedy that people really need under existing law. And so that's where I think this this horizontal piece can complement the sector specific vertical laws in a light touch way, but actually

gives meaning to the laws on the books. So, I think that's a great example of the the harm trigger that Denise described that we identify a clear harm and that may be a place where additional

regulatory structure might be helpful. I think we heard pretty significant consensus across our our panel. Trust and safety is good. That's good that we're there. That's that's a great

consensus to achieve. um and not complete consensus on the idea that additional regulation is needed yet uh with the exception perhaps of a few scenarios in which we can identify high

risk or harm. Let's go out to our audience for a second. Help us describe the relationship between innovation and regulation in AI. If you think it's a great relationship, thumbs up. If you

think it's a bad relationship, thumbs down. If you think it's complicated, make it complicated. What do we think? Oh, I see a lot of complicated. What's What is our panel think? Is I

think it's a good relationship between innovation and uh and appropriate regulation. Fascinating. I thought we have a very strongly opinioned audience here. That's great. Um let's talk about

regulation again and dive in just a bit deeper. I think one of the things that's tremendously challenging is prescriptive regulation. Trying to understand harms that might occur before technology is

fully adopted broadly in the marketplace. Um I'm a veteran of the privacy world going back to 1995 1996 and in the late 1990s we were talking extensively about cookies and how do we

regulate cookies and the privacy issues associated with cookies. Guess what? We're still talking about cookies often and I know for many of the privacy and data protect they're nodding already.

They're crying a little bit because it's so painful to implement implement many of the cookie banners and cookie consent mechanisms that we have. And I'm not entirely sure we might get John to admit

this even that that you know those cookie banners are actually driving the outcomes that we hope for. that we identified the biggest and worst harm or concern and dedicated resources

appropriately to that. Amanda, I'm going to jump right to you. Um, talk to us a little bit about identifying those harms. Alex gave us one which is perhaps u AI reviewing HR submissions, resumes,

CVs and language in those CVs may actually create results that were not intended that create bias that you know in in a human-driven system would be easier to

find in an AIdriven system just much much harder to find. That's a great example. How do we identify those prescriptive harms? those harms that we're not quite sure about yet but may

emerge. Do we do it through p principles through ethics through through what? I think all of the above to some extent. Um part of why why we start with principles in our governance program is

I think it's helpful to orient towards what do we care about right as we then try to build a program that realizes those outcomes. I think we also can look at you know existing law that reflects

where there are harms like in the employment context where people are could be mistreated or treated unfairly that we know we care about. Um and there's been a lot of effort and

regulation to define high- risk high impact. At Microsoft we have something called um the sensitive uses um sort of scenarios where you know we have three categories where the technology could

have like a an impact on um someone's life opportunity or consequential life impact something like employment or um education opportunities for example or how someone's treated under the law

otherwise all sort of fit in that context. We h we have the second big category uh of harm um that we have defined is around sort of the risk uh for psychological or physical harm. Um

so think about vulnerable populations there. Think about the use of AI and critical infrastructure. Um and then the third uh category is the use of AI that impacts human rights. So you know we

have our way of of of defining what is really high impact. You know a lot of governments again have have taken different routes. I think the other thing that we've seen um is the kind of

emergence of a conversation around sort of technology itself that um poses specific high risks for example highly capable models that have a whole other set of risk um that are the risks that

are being defined um and that's one thing that I just want to draw out as we think about this um and drawing upon where I feel like you know I didn't grow up in the privacy world I grew up in the

cyber security um world um and one of the things that I think a lot about um as we um work on defining these harms and figuring out what to do about them that we can learn from the kind of

decades of work on cyber security is the challenge of thinking about how to address risk across the supply chain and I think it's a slightly different conversation in AI than it has been

traditionally in security with software and and cloud technologies um but there is like a common principle or approach that I think we should really look at closely which is you know we we are

oftentimes um in the context of AI thinking about risk and harm um where the technology is actually used right and then what's difficult is figuring out what do we do across the whole

supply chain to manage that risk and have that be cohesive um and one of the the things that in the cyber security context we know with the risk or harm it's it's much simpler it's security

risk right that we care about um but we have the same challenge in terms of like how do we manage that risk across the supply chain and One of the the challenges over decades of work in in

the cyber security context is sort of wanting to put emphasis on one part of the supply chain or the other at any given moment instead of like really dealing with the really hard governance

challenge that it is everything at once. Um and so I think when we you know think about the complexity of defining harms in the AI space um that's that's important work to do and also um in the

context of managing risk for any of those harms being realized we also need to think really hard about looking across the whole supply chain at once even though it's hard from a governance

perspective that's going to be most important for for managing the risk ultimately. &gt;&gt; Fantastic and I misspoke. It's prospective, not pres uh prescriptive

regulation. But John and Denise, maybe talk to us a little bit about that and let me frame it for you both. Um and Denise will have you start. Clearly with data protection regulation, we have had

the GDPR now for over seven years and the effect of that standard on the global policy environment has been enormous. We now have over 120 countries that have privacy laws in place. Many,

many, many of them have genealogical lines that point back to the GDPR. And yet, we haven't seen that in AI yet. The EU AI act has not taken off around the world. We don't see a Brussels effect

happening on AI. Is it because the challenge of identifying harm, the challenge of uh prospectively um uh trying to identify what might happen is a real barrier or is there

something else in play? &gt;&gt; You always ask me the tough questions. &gt;&gt; Oh, that's um I think firstly on the harms question because I think that's relevant to the the regulation question

that you're asking. Uh I think the starting point must be that every country has a unique context and and it's the job of the government to figure out what's harmful what's harmful to

their society. I think there's going to be a huge amount of overlap but at the end of the day what's harmful in one context uh that's harmful in India may not be the same as what's harmful in the

US and the cultural context matters. uh that said I think there's actually increasing consensus I feel about what harms or archetypes of harms there are uh visav AI and we see that for example

the international AI safety report is that starting to anchor some of this taxonomy and uh sort of um buckets and archetypes of harm and we also see that uh beginning to happen at ISO for

example those conversations are happening uh how does that link to prescriptive regulation legislation. I think that if the harms are still being coalesed and formed, it's quite

difficult to be very prescriptive uh about how you deal with those harms. Uh because that by definition is um sort of changing and still coalesing. It's still quite nent. That's not to say we should

step back. I think we just probably need a slightly more agile way of thinking about that broader concept of governance. So in the social media context in Singapore, we did it via um

codes of practice. So we have a broad sort of umbrella legislation that creates a legislative frame for which these codes of practice apply but the codes of practice can be updated more

easily. Same thing actually with our data protection law the PDPA which is structured quite differently from the GDPR. Uh we it our our PDPA is actually very not prescriptive. It's outcome

driven. It's fairly broad but most of the guidance that PDPC provides and these are for compliance is done in advisory guidelines. So I think there are regulatory mechanisms you can use um

that are less prescriptive than primary legislation uh and that gives you enough levers uh it's tools in a toolkit basically to be able to deal with the harms and with the problems that the

society is facing. &gt;&gt; Excellent. to dispute you a little bit on the lack of a Brussels effect. Um I would say I mean going actually exactly to Denise's point. So not only is there

some harmonization happening around the scoping of the harms I think that that certainly is happening but also on potential points of intervention. So for example one of the key elements of the

EUAI act is looking at high-risisk scenarios and having their mitigations in place. We have similar consideration uh laws under consideration in multiple states in the United States. One on the

books already in Colorado. They would never say it as a copycat. It came from its own origins. But it is lawmakers thinking what is an appropriate rightscaled intervention rights

intervention to that particular risk. &gt;&gt; You can look at the recent transparency laws that were passed in California and New York. Very similar discussions to the code of practice for general purpose

AI models that came out under the EU practice. You can look at the EUAI act's provision for regulatory sandboxes and this notion that we want small and medium-sized enterprises and others to

be able to innovate and get a little bit of forgiveness or wiggle room under the laws as they figure out how the regulations apply. That law just got passed in Utah. So there are these

glimmers where we are seeing smart solutions to very specific problems and people learning from each other. the the point's very well taken and I think it's notable from Bletchley Parks and now

there has been a focusing of the dialogue um many of the existential risks are not on the agenda this week you know that were discussed actively at Bletchley Park just just two plus years

ago there has been a focus on some of the the real harms John you had an example of a real harm in AI just recently in the UK with the Grock scenario and your office announced some

actions as a result of that. Maybe use that as an example to talk about how data protection was used in an AI context, but also how your office did or didn't coordinate with OCOM and others

in response to that. Because I think in the absence of that umbrella AI standard, that interaction with fellow regulators across disciplines and domains becomes really important or I

will ask you does it become really important? &gt;&gt; Yeah, it is. It's hugely important that we coordinate. Um, you know, these are new challenges that we're all facing.

Uh, on the Grock issue, obviously it's under investigation, so I won't be able to say too much about it, but um, you know, we we're interested in, uh, what, you know, how models are trained, what

data they're trained on, uh, what output filters are included, um, what kind of safety mechanisms. um interested in what kind of uh ingestion there is of data when it's used at that level. But

there's some complexity in in that case as well because you know you've got users using a tool uh that's amplified by social media. I don't know whether the same functionality is available in

any other image generation uh tool that just hasn't got the same media because it's uh it's not amplified by a social media platform. Um but you know very early on I think I was back home in New

Zealand actually on about the 5th of January and um started to see this and I I messaged back to the office and said what are we doing? What's Ofcom doing? How are we connecting to our

international colleagues &gt;&gt; and that's so important. Um and so we've you know we've messaged into GPA uh we've coordinated very closely with OPCOM. uh and you know we have to cope

with the fact that uh regulation is a little bit fragmented. &gt;&gt; Yeah. &gt;&gt; So Ofcom is uh responsible for administering the online safety act in

the UK. Now that is legislation that um seeks to regulate the kinds of harmful content that can be delivered to a child's device for example. Right? I see this thing. Is that regulated by online

safety? If so, it's Ofcom. How did that get to me? Well, that depends on how the underlying data was processed. That becomes an ICO um you know, GDPR issue. So, we need to be working very very

closely and we are. But um also with the uh Croc issue, one of the very early things we did was to um reach out to our colleagues in the GPA uh the global privacy assembly and say who else is

looking at this? let's make sure that we're not sort of treading on each other's toes or at least that we're sharing information that we've got the same ideas that we think the same way

and that can be tremendously powerful whether or not you can point to um a regulation that uh that app or that platform is uh is clearly in breach of uh to describe a set of expectations

about harm mitigation across a coord coordinated group of global regulators I think can be quite powerful. Uh and you know just to see how you know the the the the alternative

for some of these platforms is not necessarily being investigated and finded by the ICO. It's it's like what I noticed the first day that I was here when I went to flip Tik Tok on and saw

this is not available in this country. So if the offering in a particular jurisdiction does not meet the standards and norms of that jurisdiction, these organizations need to understand that

they can be switched off, that they are not actually all powerful. &gt;&gt; Yeah, I just have the image of the UK information commissioner doomcrolling Tik Tok in my head now. Yeah,

&gt;&gt; it's my Italian sandwich. Great. Um, let's let's do a quick round and and please do keep your answers short, but innovation is not limited to technology, is not limited to business practices.

It's it's also um uh very powerful in the privacy-enhancing, safety enhancing tools that we use inside organizations. It's um uh in regulatory structures. Uh Denise has mentioned regulatory

sandboxes or maybe it was Alex but we've heard regulatory sandboxes mentioned. What is the one innovative idea in trust and safety that you think holds real promise? And I'll let you do one

sentence to explain it, but this is a speed round. So we'll start with Amanda and then work down and come back to Alice. Oh, &gt;&gt; one uh one sentence. Okay. Is that my uh

Um I think about provenence tools as an area of innovation. Um again this is calling upon my cyber security background but I think you know something like aentic AI is an area

where there's um a lot of interest concerned governance um uh momentum uh and one of the challenges is being able to look at something that is u fundamentally not just like one

technology it's a bunch of very dynamic components models platform tools services um applications all working together And um while that feels like a really new hard challenge, we actually

can draw upon what we know of software to actually be a set of dynamic components as well. And one of the ways that we've figured out how to govern that or we're working towards um

figuring out how to govern it is with software build materials. Something that really allows you to have the ability to track those dynamic components. And I think that's something we can apply to

to agents. &gt;&gt; So it increases transparency. It tells you, you know, which algorithm or which system this might have come from. It it helps with accountability broadly. Yeah.

Excellent. John, what's the most promising trust and safety innovation that we have? &gt;&gt; Uh well, the I think you you challenged us with one word, so I'm going to go

with agency. &gt;&gt; Agency. &gt;&gt; Uh and I think it's for me it's a word that you know so much of our world is dominated by consent, which is

&gt;&gt; I won't say broken, but it's under strain as a as a as a useful concept. agency I think has capacity to recognize that uh the objective is to maintain and to restore and maintain an individual's

agency as it uses any product &gt;&gt; and that's consent. It's actually making sure that um provenence is delivered for example, right? You can't have agency if you don't know the origin of the data

that uh is delivering this agentic uh miracle to you. &gt;&gt; Uh it it it it gives you tools at the other end. You know, consent is always a conceived of as a as a front end

&gt;&gt; authorizing concept, but agency says, "Okay, I've done that. Now, where's my delete everything button or my I don't want to do this anymore button. So I think if developers can be thinking

about how they uh deliver the best possible service in a way that uh restores and maintains the agency of the consumer. I think that will go a long way to addressing some of the harms that

we're concerned about. &gt;&gt; Fantastic. I had a law professor uh years ago now who described burden shifting wrenches in the law. And I I think consent is a burden-shifting

wrench that moved much of the burden to the data subject to the individual agency. I it sounds to me is an idea to move back to those who might be accountable and have them have fiduciary

or stewardship responsibilities for that person. Denise, &gt;&gt; I would pick privacy enhancing technology. I think that's a very interesting technological way to deal

with at least one part of the problem which is how do we secure the data? How do we make sure that the the private the personal information is well protected and it's advancing so quickly. So two

years ago we were looking at federated learning for training of AI models and no one could figure it out. I think it's actually being done in production now. So there is I'm a lawyer so I can say

this sometimes the law cannot solve the problem. Um but actually maybe another technology can. &gt;&gt; Fantastic. Uh Alex &gt;&gt; well staffed empowered independent

regulatory bodies that can help represent the public interest. &gt;&gt; Wow. And because in some countries those are under attack right now where that is not available well-resourced technically

informed independent civil society that can play that role in the interim. &gt;&gt; Fantastic. Yeah. The importance of having uh watchd dogs um yeah entities that are watching and observing

commenting enforcing really powerful. So there are four great innovations. Provenance, agency, privacy-enhancing technologies, and well-funded regulators or civil society. Um, uh, well done. I

think that is a great start. Let's do another audience poll. Um, how many of you here in this audience are responsible for AI or AI governance, AI ethics, AI safety inside your

organization? Hands up. It's almost the whole room. Keep your hand up if you're also responsible for something else in addition to AI or it's just AI. &gt;&gt; It's

more um I think I think it was a pretty complete overlap almost a complete overlap. There's at least a significant percentage that were responsible for more than one thing and one of those

things was AI. I think that's an example of the complexity that we see inside organizations today. John described the coordination necessary between Ofcom and the ICO in the Grock investigation which

is ongoing um because there was not a single place where regulatory authority existed to address that concern. This is a really complex environment. The number of harms or issues span from children's

safety to intellectual property, from bias and algorithmic discrimination all the way through deep fakes and other things. Alex, how do we how do we put that all into a pot and make it

something meaningful? Well, what if you can't put it all in a little pot apart from AI is a common denominator in all of those things, but AI is a tool that touches everything.

Um, so I really do think you actually need a nuanced approach that looks at a particular risk, what those mitigations are for that risk, uh, and then goes from there. The privacy considerations

when you are sharing your most intimate concerns and questions about the world with a chatbot is very different than these questions about deep fakes and fraud and incarceration. and it's just

you need to have a different legal regime. I think some of the the common elements that run through one is that transparency and uh rigorous approaches to risk mitigation really matter and

that can either be through regulation or through principles and best practices with meaning and standardization and watchd dogs reading those disclosures. Um and the second is this this burden of

the user. So um when Trevor introduced me, you described my organization. We represent users rights around the world. I am all for user empowerment and also we cannot put the burden solely on users

to navigate this moment. &gt;&gt; Indeed. &gt;&gt; Uh and that is the major lesson of the cookie example you were saying before. We didn't misdiagnose the harm, we

misdiagnosed the remedy which was shifting wrench. It's a burden shifting wrench that the burden on individual users when we don't actually have market choice nor the time or mental energy to

just read a whole bunch of disclosures and act alone. And so solutions that acknowledge the harm are tailored but also take that burden off individual users. So you're empowering users but

not burdening them or leaving them to essentially defend themselves unprotected. &gt;&gt; We have to think about that approach. &gt;&gt; Okay. Uh sadly we are at the end of our

time but we have one more pop question for all of you and we're going to let this be our close. We have gone through the AI impact summit, the AI action summit, the AI safety summit. Five years

from now, what is the AI summit going to be called? What's the word that's going to be in the middle there? So, this is a one-word answer again. What's it going to be? I know it's a tough question. So,

Denise, I'll start with you because you're able to handle the toughest questions. Ah, the AI Trust Summit. Okay, John. &gt;&gt; Nostalgia.

Linda &gt;&gt; driving AI traveling. Okay, &gt;&gt; I'm gonna cheat for the people by the people. It's more words in front of the people

to get on a poster. Here's what I know. I know that there is incredibly hard work that needs to be done to bring trust and safety to this ridiculously powerful technology that I

think as Sundar Pai says will be more profound than electricity. That hard work happens every single day inside organizations that are implementing these tools, inside civil society that

are watching and guiding that behavior, inside regulatory offices that are navigating to ensure that marketplaces around the world that the broad digital economy gets this right. I feel better

because people like this are doing that work every day. And I hope you'll join me in thanking them. Thank you very much. Excuse me. &gt;&gt; Oh, you want a picture? Sorry.

&gt;&gt; Do you want a handshake? &gt;&gt; Sure. Thank you so very much. &gt;&gt; Well done. You were fantastic as expected. So what is the with your I

and I fly to London.
