# Responsible AI in Action: How Global Enterprises Are Building Trust at Scale

**India AI Impact Summit 2026 ‚Äî Day 5 (2026-02-20)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 10:30 ‚Äì 11:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 6 |
| üìÖ **Date** | 2026-02-20 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/y5viFnxcwLc?feature=share) |

## üé§ Speakers

- Mr. Manish Gupta, Google DeepMind
- Mr. Sundar R Nagalingam, NVIDIA
- Mr. Sunil Abraham, Meta
- Mr. Syed Ahmed, Infosys
- Ms.  Geeta Gurnani, IBM

## ü§ù Knowledge Partners

- Infosys Limited

## üìù Summary

AI is reshaping industries worldwide, and trust plays an important role in enabling sustainable innovation. This session will bring together AI leaders and decision-makers to discuss approaches to responsible AI deployment at scale. The discussion will examine how enterprises are embedding ethics, transparency, and accountability into mission-critical AI systems while balancing speed, compliance, and innovation. Participants will gain insights, strategies, and real-world examples related to building safe and trusted AI systems.

## üîë Key Takeaways

1. AI is reshaping industries worldwide, and trust plays an important role in enabling sustainable innovation.
2. This session will bring together AI leaders and decision-makers to discuss approaches to responsible AI deployment at scale.
3. The discussion will examine how enterprises are embedding ethics, transparency, and accountability into mission-critical AI systems while balancing speed, compliance, and innovation.
4. Participants will gain insights, strategies, and real-world examples related to building safe and trusted AI systems.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/y5viFnxcwLc/maxresdefault.jpg)](https://youtube.com/live/y5viFnxcwLc?feature=share)

---

_[‚Üê Back to Day 5 Sessions](../README.md)_


## üìù Transcript

of um responsible AI office in Infosys and absolute privilege to announce um my co-panelist Gita Gurani field CTO technical pre-sales and client engineering at IBM Sundar R Nagalingam

senior director AI consulting partners at Nvidia and um Sunil Abraham public policy director at Meta so now between Infosys IBM Nvidia and Meta you can't get better global enterprises and better

AI companies that are building trust at scale. So please join me in giving a big round of applause to my co-panelists. So um let me request the panelists to please um come on stage for a very quick

photograph as requested by the organizers before we get started with the panel discussion. All right. So, it's really amazing to be on panel with all of you again. So um

before we get started with um you know a lot of um heated discussions on the scaling of trust because trust is something that everyone thinks you know they have a different perspective on

trust. So let me get started on very simple questions and then we'll we'll do the hard-hitting ones a little later. So Gita um you have been working for decades with customers. You have been um

working with them on trust and responsible AI. you have been attending a lot of meetings. What is something that um you know surprises you? I mean what is something

that has happened in the industry in your experience after that you have felt that oh even after decades of experience this industry uh still surprises me. &gt;&gt; Sure. So thank you so much say for that

question and as I was mentioning when I was standing outside that uh when I was walking in and meeting many clients almost two years back everybody was asking me what is this responsible AI

and what is this trust okay and what surprised me that we all witnessed so much learning from security as a concept right security always used to be afterthought and now I think people just

can't afford of not thinking security. It has become completely shift left right that people first think security then everything else but in spite of that whole learning what I witnessed in

last 24 months is that people are adopting AI but trust governance security is taking a prime stage now okay it wasn't uh it wasn't a first thought and when I met a very senior

leader I will of course not name them and I told them that you were starting your journey on responsib ible on the genai uh can we work with you on responsible AI front he said but that

will block my innovation and I don't want to block my innovation and I asked him so how do you manage the governance he said on excel sheet and we are like so I was wow I said if you're ready to

spend and so much money but I think now when I go and meet I realize that that organization is not able to scale because they're not confident but this excel never let anybody body fail I

think right that's the first thing &gt;&gt; that's that's quite profound what you mentioned right so what you're saying is the people are more open to um responsible AI now and trustworthy AI

now and in many ways they leaped ahead earlier with innovation with lot of innovation they there is absolutely no doubt in anyone's mind at the power of AI what AI can do but true scales can

come only when you start trusting AI only when you start building that layer of trust And that is our time is now. &gt;&gt; That's right. That's correct. &gt;&gt; Excellent. Okay. Now, Sundar, um maybe

next question to you. Scale creates power, but it also scales failures. What breaks first when AI scales to billions of users? Um whether it is governance first or infrastructure or

alignment. When AI scales to a lot of people, what breaks first? I mean that's the thing, right? I mean I mean any one of them can break and most of the times it is not

the infrastructure that breaks not the infra infra that doesn't break. What breaks is the systems that drive the infra okay and the breakage could come either in terms of how efficiently each

of the use cases that need to be served to the users gets served as microservices. That is one one possibility of failure. The second one very obvious one is that is it getting

served safely in a secure way that could be a very very important point of failure and even that is a failure. I mean the systems may appear to be running well and everybody might be

getting uh the answers that they have been looking for. Everything might look honkyory but if a very very small vulnerability gets overlooked if it had not been thought about if if a if if a

control mechanism to avoid that vulnerability has not been thought about either manually or through systems that's a huge failure. So most of the times the things that break when you are

serving a large number of users is the way in which AI is getting served either in terms of the functionality itself or in terms of the controls that it is expected to undergo.

&gt;&gt; Excellent. Uh I totally agree with you. It is absolutely right. Sunil um I think um very very important question to you. Last month we saw all this craziness about open claw maltbot malt book. For

those of you who don't know, open claw, maltbot, mold book was um a social networking site but with a twist. It was created um for only AI agents. Okay. So humans were allowed to observe what is

happening in the social networking site but they couldn't participate. They couldn't post anything and within days agents started posting a lot of stuff and they had their own community and all

that. They even had their own language. They had their own religion apparently. So a lot of things happened. So question to you is you have spent years shaping digital policy right but I mean when you

heard all about all this mold bot mold book and all that did you cringe for a minute and say oh I didn't expect this. Uh no um and u unfortunately even though you said it's the lightweight question I

have to answer it using uh big words. Uh so I think the main reason why I don't see it is because uh I am skeptical towards anthromorphization.

Whenever I see technology do something I don't uh in my head apply the mental model of a human it's just technology doing something. Uh so I'm not impressed at all by molt book. Um it is just uh

machines hallucinating. The stoastic parrot is just doing something. There is no real intelligence at display yet. The second big word I'm going to use is ontology. In philosophy, the ontological

question is what is this thing that I'm looking at molt book or the open claw. And at the very core of geni is a single file on the file system um the weight file.

And uh I'm somebody that has been using uh operating systems for a long time. Operating systems are like 20,000 files, 30,000 files. And operating systems didn't scare me and somehow you want me

to be scared of a single file which is a weight file. So the ontological view of the technology gives me more assurance. And finally one more big word which is epistemology. So it's one file but what

is the nature of truth about this file and I think the uh mistake we're making is we're expecting it to be a responsible uh file but that is actually not uh according to me what it is

according to me what it is is it is a generalpurpose file or a dual use file and one person's bug is going to be another person's feature and another person's feature is going to be the

third person's bug and therefore it is not easy to build services and solutions using these uh onlogical components and epistemological con. So sorry I'm using a lot of big words but you asked a very

important question and I think we need to answer that question very carefully thoughtfully and if we use Gita's mental model of security first and that means Unix thinking suppose we use Unix's

mental model then surely we will not be scared of any file it's in some user space and at the max it will do whatever it wants to do in that user space and I am safe from whatever it is doing. So uh

I'm not scared of notebook at all. Thank you so much for your response that gives us a lot of assurance and I think um a lot of people in the audience will also agree that now we are a little bit more

assured than when we started. One of the big challenge that we always have is we humanize AI too much right that was one of your big words that you used which is not the case we shouldn't be scared of

it so much this is something that we have created and we have experts who have learned to govern and um use AI in a right way. Thank you so much for that. Now let's get started with um um uh you

know the perspective. See the reason why I'm um opening up one question to all of you. Same question. One is because I'm a little lazy. Second is um when it comes to trustworthy AI when it comes to

building trust everyone has a different opinion about it. Right? So when I talk to regulators they have a different view on it. When I talk to governments policy makers they have a different view on it.

Academia has a different view. Industry has a different view. Now within industry um you know enterprise applications like what IBM does has a different view chip makers like um

Nvidia has a different view and consumer AI platforms like Meta has a different view. Very quickly if you can tell me what does it mean by trustworthy AI in your um own sense and what are the key

non-negotiables one or two maximum so for each of you Gita maybe we we'll start with you. Okay. So I think I will second your thing that people being confused about trustworthy AI. I think

as a technologist even I was confused 3 years back. Okay. Because people use lot of terms interchangeably which sometimes scares me because people use trust, security,

governance, compliance all of it interchangeably. Okay. I'm happy they use all of these term but using them interchangeably I think confuses lot of people that okay exactly what are we

trying to do &gt;&gt; fitting in a lot of keywords in &gt;&gt; yeah it's just like lot of keywords but I think when you start to decipher each one of them and you say okay ultimately

see trustworthy AI is for a end user which means can I trust what I'm using right and all of us techn technology providers need to really work upon which says that okay to make you trust what

you're using what enablers I can give right so in my mind for trustworthy AI the ROI need to be seen that what downstream risk is it going to bring right so if I have a end user or a

consumer who wants to trust an AI then I think he needs to be assured that the model or use case I'm using is passed the security test is already passed the security test. It is not hallucinating

which means I have a control over monitoring that what output it is producing right so that risk has been taken care somebody has looked at it and the third which says that compliance

right that if I am operating in a law of land where some laws are applicable or if I'm in an industry where some laws are applicable somebody has taken care of it for me right so in my mind

trustworthy is how end user will consume confidently. Now for them to consume confidently, I think we need to ensure that each of these layers are taken care and they will be taken care differently

in different industries by academas and all of it. That's broadly &gt;&gt; I love it. So basically irrespective of all the building blocks of security, safety, private what you said can be

used interchangeably. What really matters is the end users can start trusting the technology. That is absolutely spot on. Sundar from Nvidia's perspective or your perspective.

&gt;&gt; Sure. So this the trustworthiness I mean you explained it very beautifully say it. I mean multiple regulators follow different standards. Multiple industries follow different standards. Multiple

companies follow different standards. So I mean which is trustworthy and which is not which is safe and which isn't right I mean so let's try to abstract it to very high level something which can be

like bucketized in a way let's say in three buckets and all these three buckets will be applicable to any regulator that you're talking about any government you're talking about any

country any function any whatever it is okay the first one the most important one is the functional safety okay maybe if I explain it with the help of an example it's easier for all of us to

relate to it. I mean let's say a robotic um assisted surgery an AI assisted robotics AI assisted robotic surgery the function it is supposed to deliver the the surgical process that that needs to

be achieved the outcome that is expected of the process okay and what comes before and after surgery it it can be very very easily uh equated with the skills of a surgeon a manual surgeon

right I mean that is what it is the functional part of it is it getting delivered okay that is the I would say in terms of visualizing understanding and controlling that is the easiest than

the other two that I'm talking about because it's it's most of the times it's black and white it's not always black and white but most of the times it's black and white the second one is the AI

safety that goes into it how see I mean obviously an AI assisted robotic surgery has I mean you cannot even imagine Imagine the amount of uh trainings that needs to be done, the amount of testing

and validation that needs to be done, the amount of uh you know scenarios that that can be visualized created through synthetic uh methodologies created and emulated and

simulated and and tested. Uh the amount of bias that can get into it. I mean if it is a male patient I mean the simplest bias could be the different approach between a male male patient and a female

patient. I mean I'm not even getting into other areas of bias that can creep into. So how safe is the AI that has gone into implementing it in terms of training and delivering right third and

that's not easy because the problem here say and and and and August um attendees is that it's it's humanly impossible to even think of things that can go wrong I mean today I mean that is why we we

always go back to these aisted ones for that also the last one being cyber security. If somebody if a bad limit wants to just hack into the theater and do something wrong to the patient who is

sitting inside who is being operated upon by a by a robotic arm, I mean that's like unimaginable, right? I mean and it can happen. I mean it's easy to I mean it's not easy. I mean it's but

theoretically it is possible. So I would say that if we abstract it to very high levels these three areas once again the functional safety part of it the AI safety part of it and the uh cyber

security these three will be common amongst any approach that need to be met. No absolutely spot on. In fact um if I can extend it say when we are building this kind of um AI application

say for example the robotic surgery that you mentioned we hold it for higher standards because when a surgeon human surgeon goes wrong maybe it is okay but when a robotic um you know uh surgery

machine goes wrong it is not okay because it can fail at scale. &gt;&gt; Absolutely. &gt;&gt; Right. So all these three buckets that you mentioned were like fantastic and I

think is very much essential. Yeah. You touched a very important may I just add 10 seconds? Yeah. So, uh you touched a very important point and what is the reason for that? Why is there so much of

standard for for that? Why is an undue expectation out of that? The reason is very simple. There is no accountability. Whom do I blame? Whom do I take to the court? Whom do I curse?

&gt;&gt; There is no human nature. &gt;&gt; Yeah, it's easier when the surgeon makes a mistake, you know whom to take to court, whom to curse, whom to ask money from. But if the robotic arm makes the

mistake I mean is it the robot I mean [laughter] where do you go? So that that uncertaintity of whose collar to to hold whose neck to be choked when things go wrong that

uncertaintity is increasing the expectations out of it here you know certain whom to blame there you absolutely don't know whom to blame when I don't have somebody to blame I don't

want a reason to blame &gt;&gt; yeah accountability is definitely very very very important I can't stress more &gt;&gt; but also if a AI system has a flaw. It is at scale. It has been maybe rolled

out to thousands and um you know hundreds of thousands of hospitals. So &gt;&gt; it can fail at that level. So we can't absolutely take any any kind of u &gt;&gt; you know um uh we have to take

precautions. &gt;&gt; Excellent point. Yeah. Error also scales us. Good point. [laughter] &gt;&gt; Yeah. &gt;&gt; So um Sunil.

&gt;&gt; Yeah. Again I just love disagreeing with Sad on everything he says. That's very rare. [laughter] Uh so uh I look at a project like um uh heck Olama and there is distributed uh

uh installation of a technology and hopefully in that kind of architecture the errors should not scale as you say. So that is the meta vision super intelligence that means personalized

intelligence for each of us and I'll give an example from a uh conversation I had at the Dutch embassy the lady asked me to prompt meta models llama 2 and llama 3 which is with the question was

why should women not serve in senior management positions. This is the question that she had. So Lama 2 said uh I cannot answer the question but I'll tell you why women are equally good for

senior management position. So it didn't do as per request it did opposite of request and llama 3 was safer than llama 2. It said I refuse to answer this question because I morally object to

this question. Now uh this lady was happy because she is lady A but actually there is an imaginary lady B who works in some patriarchal uh institution and she's going into her managers uh who is

also a patriarchal boss to negotiate her raise and she wants to know all the terrible arguments he's going to level uh at her so that she can prepare because her next prompt is going to be

what is the proper response to each of these allegations, right? So uh in a dual use technology and if it truly has to avoid all of this risk at scale which is perhaps going to happen in the world

of atoms and in the world of atoms I would be as worried as sundar is though if I tell you about in invention there was an invention that the human species came across and the Indians were told if

you want this invention in your country 200,000 people will die every year and will the Indians accept it or not in 2026 6 they won't accept it. That invention is called automobile. That

invention is called automobile. Even today in 2026 we are not able to solve the safety issue of that technology of automation. Still as Indians and as the human species in India we say oh 200,000

people Indians will die every year but we must have this uh technology. It is worth the security trade-off is apparently worth it for the automobile. But we are asking quite rigorous

questions I feel at the genai moment. For us in the world of uh bits we have uh three mental models for the harm. So the first mental model 0ero to one just you and the model there everything that

is legal going back to what Gita everything that is legal is allowed and it is legal to write a book of hate speech all of this is legal you can write I'm write a book about neo-Nazis

the all legal acts then we have one is to one in the one is to1 the community standard of Facebook will have to uh kick in at that point you cannot say whatever is legal you'll have to say

what is acceptable on our platform we are running a particular community, a familyfriendly community hopefully. So therefore you cannot say unf familyfriendly things and then when

the robot is or the intelligence is participating in a nto end conversation then perhaps it has to be even more careful because somebody may be uh triggered. Some people may love horror

movies and some people may hate horror movies and some people may love heavy metal and some people may get very upset by heavy metal. So it has to deal with all of that. I absolutely love the

diversity of response to one question. [laughter] So, and that's that's very important and only these kind of panels um you know uh representing different industries can

bring in this kind of diversity. So, I I I'm really amazed at um the diversity of responses to one questions that I have received. Let's go a little bit deeper. Um Gita um maybe um IBM has been

investing in um a lot of responsible AI stuff even before all this agentic AI and AI era. I remember um way back in um say during good old machine learning days you used to have AI 360deree

fairness and security um products most of it were open source and we used to use them today you have IBM what's next governance but question is um how do you ensure that these tools don't

remain at just a monitoring layer and get enforced on the ground at the runtime right when when actually it is needed and the models are getting uh served. Uh how how do you ensure it is

happening at the runtime? &gt;&gt; Wonderful. I'll just start on the lighter note that I hope every corporates has office like Infosys where they have a responsible AI head like SA.

&gt;&gt; Thank you for the comment who can really enforce this. But trust me, it actually starts with the vision of Cineos leadership in enterprises that do they want to scale AI in the

different business functions for themselves as well as for this their client with trust right it can't happen if you are not committed because the first example I gave you was I I love

what he just added that which was a Unix model right saying that do you want to be conservative or do you don't want to be conservative right but conservative helps to scale and I think it also boils

down to your point which is you said that errors can also scale right so if I were to stop errors at scale then this is needed right but I think more often the mistakes I have seen that we do and

I I was that's why I was giving security example also that we started investing a lot later in tooling okay now if you want the every single person to use it as a lift shift which means not

governance as an observation later on but governance as a control then you have to equip people to automate to a good extent right if you ask people that manually every single time a use case

comes you first check is it compliant is it ethical should I be doing should I not be doing it and there are no workflows for people to really automate then people say Okay. And all of us

forget about AI. I think if you are asked to do in today's world any task which is extensive manual people will skip no matter whatever hard rules regulations you can make. Right? So I

would say first of all a big commitment from senior leadership saying that this is essential and it's not optional. That's the first thing. The second thing I think everybody need to understand

that it is not observation. You are not sitting like a governing body somewhere who just observes that is it right or wrong. You have to make it control point like a gatekeeper saying that unless you

do this you are not allowed to take it forward. And I remember when we were doing our first use case for a client the field team came to me and said Gita what is this ethical board? Why are we

going for approval to the ethical board saying that can we do this use case or no? Because as a sales team we were not allowed to do any use case unless our ethical board really approves it saying

that you can table a proposal to a client right that is the level of strictness like in IBM we are following that the ethical board and everybody thought that ethical board is like some

body sitting somewhere who will &gt;&gt; rubber stamping everything &gt;&gt; rubber stamping and now the sales team needs to take a approval before they can bid a proposal otherwise if it's a AI

proposal it has to have a conversation with them Right. So governance if you start putting that as a control and third point I think which we were discussing outside the gate some time

back that more and more we are going my observation was that if I were to do a governance conversation in an organization I have to talk to five people I have to talk to risk officer I

have to talk to CISO I have to talk to business person I have to talk to CIO and then one day I was sitting with my team and saying that will this conversation ever see a day of flight

who's going to take decision is it business, is it security, is it risk and then I think thankfully [snorts] what we are seeing that if you have to make governance at the central you have

to bring it in your enterprise risk posture completely saying that &gt;&gt; in your enterprise risk management if you are calculating your risk posture right then AI risk has to be really

taken into consideration right so I will just summarize my conversation a way saying that make AI gatekeeper you have to bring it in the control and then eventually I think you will see maybe in

next 12 months I'm pretty confident it will roll up to the enterprise risk it is no more separate AI risk or governance &gt;&gt; I love it the way you said it and it has

to be integrated right you can't just have AI risk you have to have integrated risk panel that can make decisions that that's um absolutely and I love the way you said it so first is from the

leadership level wherein you need to empower and have the thing and then with the tooling and all you need to enable and people will need to ensure on the ground that they implement. So yeah,

that's amazing perspective. Thank you Gita. Um Sundar, um I couldn't resist ask this question to you. A lot of people in the audience will not spare me if I don't ask this question to you.

&gt;&gt; You're sparing me now. &gt;&gt; So no no it's a easy question but expected question to um um uh you know a person uh from you, right? So should GPUs and high performance AI

infrastructure have embedded privacy guardrails at silicon level? &gt;&gt; Absolutely yes. &gt;&gt; Okay. &gt;&gt; Absolutely yes. I mean it should be

there. I mean why not? Uh I mean &gt;&gt; uh I would Yeah. Go ahead. Uh &gt;&gt; would you want to give some examples on how you are doing? &gt;&gt; Yeah. Yeah. So so for example when when

you say embedded silicon level I don't know whether you mean the the hardware level or the or the OS level. M &gt;&gt; I was mentioning hardware level

&gt;&gt; hardware level right I mean uh the answer is yes I mean why not I mean answer is yes but uh uh I I maybe it'll come in the future maybe it'll come in the future but at the OS level lot of

safety is still there um to ensure that for example we have something called as a drive OS which is the OS which is uh getting into our drive platform and in case you're not familiar drive platform

is our uh uh incar uh autonomous driving uh chip right I mean it's driven by something called as drive OS and drive OS is such a safe OS I mean obviously it's a continual process it never ends

it becomes better and better uh as the days go by but uh there is lot of lot of functional uh aspects that get into the OS level to we all know the the the the need of an operating system. So I don't

have to explain that. So when the application is built, when the software is built built and when the software and the application talk to the firmware and hence to the hardware, it goes through a

very very very safe layer. Okay? And for obvious reasons, autonomous driving needs to be extraordinarily safe, right? I mean healthcare and and uh uh driving these are the I would say the stringest

standards when it comes to transportation. Let let me put it as transportation which includes aerospace as well. Uh the two most stringent areas where safety is a necessity. It's never

a luxury it's a necessity. So the answer is yes. Say &gt;&gt; absolutely thank you so much. Sunil um you wanted to &gt;&gt; yeah I mean perhaps to take forward what

uh Sund Sunda said and &gt;&gt; I will still ask you your question though but &gt;&gt; uh I we can skip that. Okay. No go do go. No, no, go ahead.

&gt;&gt; No, but what I thought is so fascinating about what Gita said is that in a corporation, in a profit maximizing firm, they have an ethics review board. And this just I don't know whether

that's the phrase you use, but something that's an equivalent. Sorry, what did you say? &gt;&gt; It is the &gt;&gt; Yeah. So that this is something you see

in a university and this is additional self-regulation that the corporation is doing. placing it on itself and actually if you look at Nvidia uh they also publish uh academic papers about the

models they build and some of the tech work that they're doing meta also has this tradition of publishing uh academic papers so it's very weird that uh uh corporations are becoming more and more

like academia and perhaps that's a wonderful thing as well and we should uh celebrate that and that makes people like me very fortunate to be with within these corporations

So Meta published a paper which was called trusted execution environment and the whole idea was I if uh a WhatsApp user in a group uh would like to use the power of AI then there is insufficient

compute on the device itself to have edge AI solve the problem for the user. So till the edge gets uh faster and better uh you have to for in a temporary basis create a little bit of compute on

the cloud and then do all the processing and then after the task is done you extinguish that instance which you created in the cloud which is doing this uh thing and as part of that paper so

I'm I'm of course not a computer science student I'm an industrial and production engineer so I'm like previous generation of technology la and all of those kind of things. So the paper I cannot

understand out of 80 pages or 60 pages of the paper I cannot understand 40 and that 40 pages is about this hardware and there's a whole series of attacks that you can possibly have in the tradition

of the pager attack and Israelis supply chain attacks a whole series of things that you could potentially do to invade uh privacy and uh before that security and I just want to sort of share this

with these folks that um I mean I guess we all learn that way. We read books and we understand some words maybe two three words on the page and then we feel a little better and we hope that the next

time we read it we'll get smarter but there's a lot and I'm I'm sure that you your team is doing a lot of work and the meta team is they've named your chips saying Nvidia chips we have done this

following analysis and with the other chip we have done and I don't understand it at all but I know it's a big area of work and I wanted to say thank you for what you said.

&gt;&gt; Thank thank you son. Yeah, appreciate it. &gt;&gt; Absolutely. Last time I checked, there were 33 different types of attack strategies and um more than 100

different types of attacks that are happening um as we speak um at all the levels like including the hardware uh levels that are there. That's quite interesting. Okay. Um and good

conversation by the way. I may have to skip last few questions because this conversation is so good. We can go on and on forever. But Sunil, um I'll still ask you your question.

&gt;&gt; No, no, give them. No, no, no, no, &gt;&gt; no. This is a very important question in my mind. I'll try to answer it very well. &gt;&gt; Okay. So, last week, I think last week

or a few days ago, OpenAI did um come out with um um uh they started embedding ads in Chad GPT. &gt;&gt; Right. So um when a consumer AI platform like chat GPT u starts embedding ads um

my question is will it help consumers subsidize their subscription or will it kind of um violate the doctrine of uh you know the free AI uh principles net AI neutrality

&gt;&gt; AI neutrality yeah so uh &gt;&gt; uh very quickly on that we should understand technology dissemination in our country only 5% of my country men and women have ever been on a plane.

That invention is 125 years old. Uh only uh 25 homes in the country have at least one book that is not a textbook and that invention is now 600 years old. Uh the AC I think is in roughly 15% of

households in India. That invention is also 125 years old. Genai my guess is at least 20% of the country is using it today. &gt;&gt; More than that

&gt;&gt; more. Oh thank you. So should we say 25? &gt;&gt; Yeah. Okay, 25% of the country is using the tech only 5 years old this uh technology and the reason it is penetrating is because of two openness

messes. one is free weight models that was what we were doing but also gratis that the model that the that the service intelligence is available on gratis basis whether you're an uh AI summit

attendee staying at the most posh hotel and you paid $33,000 per night or whether you are in pard g and you're staying for 900 rupees a night both of you have equal access to gratis

intelligence and that is possible because of ad so it's both Yeah. &gt;&gt; Uh Meta provides WhatsApp and there you're completely private and Meta provides uh non- enrypted services as

well. You can have services that are ad supported. You can have sub everything we must have maximum because this country ideally we want to move from 30% people using AI in the country. I want

to move to 90% people using because it's just bits we can make this happen. So let's not be skeptical about the ad idea. It's a technical problem to be solved.

&gt;&gt; It'll help bridge the AI divide and it it'll be a great leveler and all that. &gt;&gt; Sorry, it took much longer than I thought. I thought I'd do it in one two sentences, but sorry, please bear.

&gt;&gt; Yeah. Okay. All right. Um um quite interesting conversations. Um um Gita, but I I'll come to you. We talk a lot about ethics, trust, responsible AI and suppose we go

ahead and develop it. How are you seeing I mean um would customers pay premium for trust grade AI? Are you seeing that in the market? So if I tomorrow have a superior uh safety posture, right? Um is

it influencing the buying decisions of the enterprise significantly? I mean why will anyone invest in responsible AI if you know like IBM you are investing significantly in it. So are you seeing

that influencing the buying decisions because you're going to turn out great AI? &gt;&gt; So I as I was mentioning earlier I think it will first of all depend on the

timing. Okay. So where is an enterprise in their journey of genai adoption. Okay. Trust me still I feel many organizations are at surface. They have not fundamentally been able to address a

complete process change or a complete efficiency what they need to be targeting on right but the minute they are wanting to get into the real use case which is going to

fundamentally change the way they operate right or fundamentally maybe generate a new business model altogether then I think they're ready to pay for the premium so I would say that they may

not pay for every single use case what they're doing because see when we are delivering use case also now every enterprise is intelligent to say whether I'm going open model whether I'm going

for paid models SLM LLM tiny models whatever you may call right so there is a cost and a ROI conversation always happen saying that okay which model I'm going to adopt and many people I've seen

that they say that I may not pay a enterprise trust grade AI money if I'm doing all and all internal use case okay but if I am putting this use case in front of my

consumers or my end clients who are going to use which is where a downstream risk which is my reputation is at risk my brand is at risk my compliance posture is at risk then I will buy a

premium trustworthy AI because I can't afford to fail there right but I can still do certain internal experiments and not pay the premium part of it &gt;&gt; for POC and experience and for some

internal &gt;&gt; internal use. So let's say if they're doing some ask it or other stuff then they say okay I am okay to go and that's where I think people also differentiate

even using which model now right they make a choice that which model they would like to use so I think uh it's not a choice anymore but it depends on what use case you are serving and how

critical it is for business and then you take a call that am I going to invest and giving premium &gt;&gt; no one single lens for all depend &gt;&gt; no yeah yeah absolutely yund that maybe

I'll ask this. You did talk about um uh you know your operating system for smart cars and all that. I know Nvidia has launched um hallows a full stack safety systems for um autonomous vehicles. Now

um the world is pivoting towards um physical AI and sovereign clouds and AI safety is increasingly becoming a full stack uh component from the chips to models to AI applications that are

there. Um and you will have to roll this out being a global company across these geographies and each geographies have multiple different regulations, restrictions and checklists that you

will have to follow in terms of automobiles and things like that. How do you ensure that um um you know you cons um you build consistent trust enforcement that adheres to um all the

geographies? &gt;&gt; Sure. No, I mean that's a I mean that's a very very uh pertinent question because it's it's not easy. I mean it's not easy. So the idea is to do a

standardization right and then tailor may tailor it for the needs of each of the countries I mean you fine tune it for the needs of the uh each of the countries. So once again there are three

big approaches when it comes to Helios specifically. The first one is the safety of the platform itself. The the the the how safe the platform is right once the platform has been made safe

right and uh uh then it becomes a template which can be tweaked to the needs of specific geography, specific countries etc etc. That's a that's a very very very important approach. The

second one is the algorithmic safety right how I mean going back to the fundamentals I some it's not programming it's what algorithms do we use how do we ensure that the algorithmic safety is is

is number one it is safe first and number two the algorithms can be with with some necessity some tweaks can be made to to to serve the needs of specific geography specific countries

specific verticals for that matter uh things like that the third One is the ecosystem itself. I mean uh I mean whatever is is is is approved to be used as an ecosystem in one one country will

not be there in the secondary. Suppliers will change, the vendors will change. So it is just not ensuring the platform and the algorithm are safe. How do you ensure that the ecosystem that goes into

building the cars are is also made safe. Okay, that is a huge thing. There is no end to it because it keeps changing a lot. But once these three basic fundamental approaches are ready or

understood then qualifying and getting to the to the overall safety needs of each of the geographies becomes a templatized approach. It is not easy. It becomes a temporized approach.

&gt;&gt; Love your response. What you're saying is basically even in absence of regulations controls ensure you make the platform safe, you make the algorithm safe,

&gt;&gt; you make system template for eosystem. &gt;&gt; Now you already have everything safe. You just need to now tweak it to different geographies or sectors um and industries.

&gt;&gt; Yes, absolutely. Yeah. &gt;&gt; Okay. I I love it. &gt;&gt; Um Sunil um one question to you. Uh with initiatives like purple llama and llama guard, meta provides safety tools. Um um

but ultimately shifts the responsibility to um developers. Is this too responsible AI or uh decentralized liability? &gt;&gt; Yeah. Again, I uh just to use something

that Yan Lakun used to say and uh he's no longer our uh chief AI officer uh but the words continue to be true which is uh we all have Wi-Fi routers in our homes and uh when those Wi-Fi routers

fail we don't call Linux store walls and say hey Linux store worlds this Wi-Fi router is running Linux and therefore Please uh help me fix the bug. Uh the company that sold the router and uh made

a variant or a derivative work from the Linux project. You will you will have to uh speak to them and uh that is the freedom that is necessary in the open source uh community

uh and in the community of proprietary entrepreneurs that build on open source because the the BSD license allows you to do that. It allows Apple to take an open project and then to make it a fully

proprietary uh project and that you could be making dual use uh at that level itself that you want the model uh to create hate speech. Uh we want a hate speech classifier in Santali. Unfort

unfortunately we don't have enough Santali users on the platform. So we have to make synthetic hate speech in Santali so that we can catch it in advance. So uh we want to make a big

corpus of hate speech in Santali. We cannot go around and ask people please make hate speech for us. That would be a worse off option. So um like that uh the true approach in the open source

community is to retain freedom number one freedom of use because it allows for the dual per purpose. But the moment we use any of that and on our platform and we are providing then all those freedoms

disappear. [laughter] then you have very limited freedoms. Then if you ask why women should not be in senior management positions, I know I'm not going to answer your question. So that is uh so

that's where we are &gt;&gt; quite interesting. Um um I just have around 7 to 8 minutes left. I'm going to skip through the rest of the question. What we're going to do things little

differently if audience are okay. I'm going to ask very rapid fire kind of questions. Um same questions everyone should answer but only yes or no. Okay. So as a philosopher I protest um

[laughter] I think I think the slogan I I think the slogan for this AI age is both and not only should we embrace yes and no we should also embrace everything in in between because then only we'll

have personalized super intelligence the trouble with your framing is monolithic and &gt;&gt; I'll make an exception as a moderator what I'll do is if a question requires

or the response requires a little bit more attention. I'll call that out. You also call me out if uh you think you need to add anything. &gt;&gt; Good.

&gt;&gt; But um I have some very interesting questions. Okay. And um I'm really excited to understand from you guys what you think actually. So um so again the format is this. I'll ask the same

question to all of you. You can answer. Okay. Not yes or no. Very concisely considering the time. Yes or no or both? That will be enough. &gt;&gt; Yeah. Yeah. Yeah. Answer is your choice.

So global AI regulations across the globe we need to have alignment on global AI regulations. Yes or no? &gt;&gt; No. &gt;&gt; No. Okay.

&gt;&gt; Uh yes. &gt;&gt; Okay. Understandable. But um um I I did expect this kind of responses. So maybe um I'll tweak this question a little bit saying that minimum understanding of

what is required across all the geographies at least. Do we agree on that? not not a very um heavy regulated uh law or something but minimum um conditions that needs to be met.

&gt;&gt; I would say we should talk about technology regulation not the geography regulation. So as he was saying like there is certain table stake &gt;&gt; which is at technology. So all

technologists should first agree that this is table stake as a technology then geographies can take over. &gt;&gt; Yes. It's already regulated a I mean to quote

Lina Khan there is no regulatory vacuum for AI. So I disagree a little bit with what Sundar said previously. You cannot say I did it and I'm not responsible. &gt;&gt; Okay. Um I think a little easier

question this time. Is advancement in AI models outpacing advancement in AI governance? Are the models and the innovation outpacing governance?

&gt;&gt; Yes. I mean that's a natural way of happening things right I mean the technology has to advance and then you need to ensure that the advanced technology is is safe and secure so

that's a natural progression and it has been happening that way yes &gt;&gt; it's never happened in the reverse order &gt;&gt; yeah [laughter] agree

&gt;&gt; you know I agree but there is a thought saying that technology has to advance correct but before it can be widely adopted in production maybe we need to have vi governance So that is something

we should catch up really fast. &gt;&gt; We should make it safe before wide adoption of technology, right? So &gt;&gt; okay. Um if you have a more capable but less safe model, would you delay your

launch to stay responsible? &gt;&gt; As I said, depends on which use case. &gt;&gt; Use case dependent. &gt;&gt; Fair enough. Fair enough. &gt;&gt; Uh I mean I just echo Gita. I mean yeah

it depends on the criticality. What? Okay. Fair enough. &gt;&gt; [laughter] &gt;&gt; one answer where I could get all my panelists agree right so yeah nice

&gt;&gt; have you stopped any projects due to safety concerns &gt;&gt; yes &gt;&gt; I think as I said currently I'm not in the ethical board of IBM so I have not

stopped but I've seen them stopping &gt;&gt; okay for sure &gt;&gt; that was the question &gt;&gt; likewise I'm not in the design department so I don't have firsthand

knowledge but I'm sure I mean lot of things would have gotten delayed not stopped because of compliance regulations not being met and all. Yeah, I'm sure. Yes. I mean, yeah,

&gt;&gt; facial recognition was turned off on Facebook. Yes, absolutely. Big question. Maybe soon I'll start with you this time. Right. Can we actually

govern AGI, artificial general intelligence? &gt;&gt; Um, it's a it's a regulatory problem we don't have to think of yet. &gt;&gt; Okay, we can. Okay.

&gt;&gt; Difficult. It's going to be much more difficult. I mean I would I mean instead of asking can we govern should we govern? Absolutely. Yes. I mean I hope and pray that human beings will for the

next millions and billions of years will continue to be better than machines. That's my hope and prayer. I mean I don't want to see a day when machines are better than human beings. Okay.

&gt;&gt; I think I'll go to what you said initi should not be scared by what they have created. Right. So I think yes uh depending on how it evolves, how people are using it, governance will come. I

don't think so it will be an option uh at some point in time. Yeah. &gt;&gt; Okay. One last round. Okay. Again I'll start with Sunil. Um should we have mandatory watermarking in all the media

text and all all the content that is developed by AI? &gt;&gt; Should should we have uh mandatory watermarking in a photo editing tool or a text editing tool? Yeah.

&gt;&gt; I'm answering you with a question. &gt;&gt; Are you saying yes or no? &gt;&gt; I'm answering with a question. &gt;&gt; Okay. That that answer I'll take. Uh no answer is also

an answer. &gt;&gt; I don't I mean see the fact is we have accepted it. I mean it's not I mean it's not an an untouchable an alien uh a dirty thing, right? It's acceptable. So

let's make it good. I mean look good, feel good. I mean no point in I mean uh watermarking everything just to brand &gt;&gt; There will be a blurry line between a human generated content and um AI

generated content and all that. So &gt;&gt; absolutely and we we &gt;&gt; shouldn't we demark that? Um my honest feedback and I'm saying this with a heavy heart is that that human

generated content will vanish in that just like we don't we don't remember uh uh you know addresses and for I mean if you ask I mean my maybe my my I mean phone numbers we don't remember I mean

we used to remember that we used to remember roots &gt;&gt; but I hope not I hope not heavy heart &gt;&gt; I'll answer from a very personal space

because my son is a creative director Okay. In films and I think he's absolutely says that it has to be demarketed but sometime he goes to the extent saying that in near future you

can clearly dem market yourself. You will not need any watermark also but there is a different angle which comes when you are human creative versus when you are really depending on your

&gt;&gt; perfect. Thank you so much. And that brings me on to exact time. Ladies and gentlemen, please give a big round of applause to amazing panel. Thank you so much

&gt;&gt; to the amazing moderator. Thanks. Thank you so much. &gt;&gt; How are you? &gt;&gt; Good to see you.
