# Safe and Trusted AI High-Level Event: Responsible Practices and Industry Standards

**India AI Impact Summit 2026 ‚Äî Day 5 (2026-02-20)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 10:30 ‚Äì 11:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 19 |
| üìÖ **Date** | 2026-02-20 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/kvAYZsmkf1Y?feature=share) |

## üé§ Speakers

- Alexandria Walden, Google
- Ankit Bose, Nasscom Responsible AI Office
- Hector De Rivoire, Microsoft
- Namit Agarwal, World Benchmarking Alliance
- Paridhi Adani, Cyril Amarchand Mangaldas
- Paula Goldman, Salesforce
- Peggy Hicks, OHCHR Thematic Engagement and Special Procedures
- Youchul Kim, LG AI Research

## ü§ù Knowledge Partners

- United Nations OHCHR B-Tech

## üìù Summary

This session convenes diverse stakeholders to exchange perspectives on building trustworthy and socially beneficial AI. It focuses on practical safeguards, shared accountability, capacity building, and alignment around good practices that support innovation while protecting people and communities, encouraging cooperation and actionable pathways for responsible AI adoption.

## üîë Key Takeaways

1. This session convenes diverse stakeholders to exchange perspectives on building trustworthy and socially beneficial AI.
2. It focuses on practical safeguards, shared accountability, capacity building, and alignment around good practices that support innovation while protecting people and communities, encouraging cooperation and actionable pathways for responsible AI adoption.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/kvAYZsmkf1Y/maxresdefault.jpg)](https://youtube.com/live/kvAYZsmkf1Y?feature=share)

---

_[‚Üê Back to Day 5 Sessions](../README.md)_


## üìù Transcript

These are consequential challenges that have impacts in people's lives on a day-to-day basis. And our session is going to address how global standards, collaborative public private solutions,

and rightsbased approaches can enable responsible AI with meaningful real world impact. And we know that these things don't just happen um on their own. It takes deliberation. It takes

thought. It takes engagement to make sure that the products and approaches that we're using in the AI field avoid some of the pitfalls that may be associated with them. And the companies

have are going to share some of the good practices that they're engaging in about how that works in the real world. And we know if they don't engage in that way that the risks are there and real very

much present. And so looking at how we can put in place practical safeguards that ensure that AI works for people not only in advanced economies or or for the dominant platforms but for the people

that we're trying to deliver these benefits for responsible and effective AI governance and clarity of rules for both companies and governments and alignment around the global norms will

help us to get to that point. Companies of course have a responsibility to respect human rights and address the risk to people stemming from their products. Um and human rights due

diligence of course is one of the process-based ways in a pragmatic way to weave this into corporate operations. But of course governments are the ones that also have a responsibility here too

to create a level playing field and we talk a lot about that. We want the incentives for companies to be there so that the ones that are engaging responsibly are actually rewarded for

that responsible engagement as well. Our Btech project at OCHR is aimed at how do we make this this conversation happen. So through convenings like this one, through engaging with companies, pulling

out their good practices and letting all of you hear about them and encouraging others to do the same is what that project is really about. And uh we are we are really looking at and working

with of course how to use tools like the UN guiding principles on business and human rights but also of course AI's uh UNESCO's AI recommendations on ethics and figuring out how we weave those into

the decisions and work that's being done now and as I said bringing this conversation to this summit where there is truly a global and multistakeholder um effort is happening to really look at

AI by innovation and deployment has been incredibly important. So, uh without further ado on that front, I want to hand over to my colleague and and uh co-sponsor here. Uh Tim, over to you.

&gt;&gt; Thanks. Thanks, Peggy. And uh good morning everyone, uh Ambassador Ranchesmar from Estonia, uh and co-chair of the AI dialogue that the United Nations is holding. Of

course, Peggy and dear panelists, it's really wonderful to be here with you today. um to be part of this conversation on responsible practices and industry standards. Um and as we all

know now where AI is moving, you know, from something we discuss in theory to really something that is shaping the decisions in real time in real institutions and of course for real

people. Uh I'd like to thank particularly uh the office of the high commission uh for inviting us to join in on this um and for working with us on organizing this event. It's been a

pleasure. And at UNESCO, we often return to a simple idea that trust is not something technology earns through ambition alone, but really it is earned through design choices, through sa

safeguards and accountability. And that's why the recommendation on the ethics of AI we believe is so important because it does give the world a shared foundation, a first step on how AI could

be built and used in ways that protect people's rights, that promote fairness, and support inclusion. So we've been translating this g global agreement and framework into local realities uh and

through what we call the RAMs the readiness assessment methodology reports which we've now launched in around over 80 countries and uh just two days we did India's uh readiness assessment report

and these assessments provide a kind of cleareyed look at how regional landscapes can evolve uh inviting us to move beyond theory and towards this responsible human- centered deployment

of AI we hear about. And so by grounding in innovation in these evidence-based diagnostics, we hope to ensure that progress remains aligned with those shared values.

But of course, the recommendation only matters if it can be applied by people who are actually catering, creating, and using AI. And so that's the purpose of the initiative I'm going to introduce

today. And I'm very happy uh to say that UNESCO in partnership with LGAI research is uh developing a global massive open online course or a MOO as more commonly commonly known on the ethics of

artificial intelligence. And the course will be developed on Corsera delivered sorry on Corsera with a very clear goal to make AI ethics learning accessible to a wide global audience and to make it

practical for day-to-day work. And so as I mentioned earlier, the key idea behind the the key idea behind the MOO is ethics by design. Um and so in simple terms that we don't

wait until something goes wrong to ask these ethical questions. We should build these questions into the process from the beginning. And the course will help learners think through issues like

fairness, transparency, safety, accountability, and inclusion at the stage when decisions are still being made rather than after systems have already been deployed.

The course of course f is really going to be focusing on practical tools so that we can offer clear ways of thinking and working that can be used in everyday settings. So it'll help learners for

example recognize uh common risks early, ask better questions during development, document the decisions made responsibly, and think through the impact of AI systems on different groups of people.

Um, we're moving beyond a one-sizefits-all approach by and we've done this by collaborating with experts from over 10 countries and five continents

uh with some of the leading minds from the University of Oxford and the Alen Turing Institute. And this global group, this global coalition is really vital because AI of course doesn't operate in

a vacuum. It's shaped by languages. It's shaped by cultural norms and institutional capacities and of of where it is developed and deployed. So by integrating these diverse perspectives,

we're trying to move from the theory again to the live reality. So ultimately this MOO is a capacity building effort with a simple purpose to help more people around the world build

and use AI in ways that are responsible, inclusive, and worthy of public trust. We look forward of course to this continued collaboration with governments, with industry, with

academia and civil society as we take it forward and we hope many of you will engage with the course when it when it launches not only as learners but also as partners in building a cult a

stronger culture of ethical innovation across the world. Thank you very much. &gt;&gt; Thanks Tim Curtis. Uh UNESCO we're all looking forward to it now. Now we have anticipation. We're very fortunate to

have an addition to our program today um with Ambassador Tomsar, the permanent representative of Estonia, who's one of the co-f facilitators for the global dialogue on AI that will be launched in

in July and a big responsibility and he's here to tell us a little bit about where it's heading and how you all can contribute. Please ambassador either use those ones or this one.

&gt;&gt; Thank you. Thank you very much. Uh good morning. I don't know is it morning? Yeah, maybe. So after three days here in in India, I think that I lost track of understanding

is it morning or evening. But uh thank you UNESCO and office of high commissioner for human rights for convening this uh really important discussion and of course to all our

hosts here in India and I also thank partners who contributed to this work. Uh today I'll speak on behalf of uh two co-chairs of uh United Nations Global Dialogue on AI governance. Um and two

co-chairs are from Salvador from Salvador and Estonia. Um the first global dialogue on AI governance um was mandated by all member states

through through a general assembly resolution adopted in August 25. So this is a member statesdriven uh process. Uh it belongs to every country to all member states and its task is very

practical uh while its scope is multilateral. So uh the aim is you know to come together. It is a platform where governments and stakeholders exchange best practices and

experiences and this we believe can strengthen international cooperation on AI governance and ensure human centric AI support sustainable development and reduce

indeed digital divides that are already there. So we've engaged with member states and different stakeholders uh about their priorities and let me uh uh bring to attention four points from

this uh uh priorities. So first they want safe, secure and trustworthy AI systems and the trust here of course is a absolute uh key word. Second they want to close capacity gaps. Many developing

countries need infrastructure, skills and compute to participate fully in AI uh economy and inclusivity and equal access uh are essential here. Third, they want governance approaches that can

work across borders uh and be practical. So fragmentation raises the cost and weakens trust. So interoperability is absolutely key. And fourth and that is uh I think uh quite

actual here they want AI anchored in human rights and international law and this includes protecting vulnerable groups, addressing bias and discrimination and ensuring oversight

and accountability. Now we know human rights are not optional. They are part of a mandate agreed by member states and today's focus on responsible practices and

industry standards response uh directly to these priorities and standards turn principles into action. They shape risk management. They clarify accountability. They guide human oversight and they give

companies and regulators tool they can apply in uh real systems. So let me say that the global dialogue will not and I guess it cannot impose one single model. We will listen. We will identify common

ground. We will build on existing initiatives. Ethics of AI was mentioned here and it's of course one of them. We'll avoid or try to avoid duplication and we will focus on practical value. So

I encourage you to bring your experience into this process. Share what works, share what doesn't work. Help us identify approaches that can scale across regions and level of capacity.

And in best case, if we succeed and failure is not an option, safety and trust will be visible in in how systems are designed, deployed and governed. they will be reflected in real

safeguards and in benefits that reach more people and this is very important for us. So I thank you and wish you a productive uh day uh practical exchanges that move uh our common work forward and

with this uh I give it over to the real experts and panel. Thank you very much. &gt;&gt; Thank you Ambassador. &gt;&gt; Thank you Ambassador. wonderful to have you with us and I think we're all

looking forward we're looking forward to having all of you join us in Geneva uh in July. So with that uh introduction by the three of us, we're really as as the ambassador said going to turn it over to

those who can really inform us about how this work is happening um and and I hope inspire us uh to both uh give support and and emphasis amplification to the work that you're doing and bring more

into the fold around responsible business conduct. So with that introduction, I'd really like to start Ankit Bose with you from NASCAO. Uh we had a great conversation yesterday and

I'd love for you to inform our audience. You know, NASCAO represents the leading Indian tech uh industries and we want to hear more about your work and what you're doing to encourage companies and

help them to ensure a responsible work environment. &gt;&gt; Thank you. Thank you so much for having me here and it's my pleasure here to address the audience. So, so uh NASCOM

has been uh there for almost four decades plus right uh we have been helping uh the tech industry in the country to shape uh and and change the whole you know agenda for the country. I

think that's what we have been doing specifically on responsible AI and trust right I think uh the mission for NASCOM started 2021 so we started with uh a gap that you know we were seeing a lot of AI

was getting developed but again I think we found that there was some missing element that was uh the responsible the trust the human element right I think that that is how the mission started

from that point in time our main core objective has been to develop open assets right uh build capacity build uh you know adoption right and and and I think help all the different components

in the ecosystem right right from the government to the startups tomemes right all of them so we have been trying to help them go up the ladder and and really become aware I think not not only

the the gloomy side of AI but also the bright side if they adopt uh responsible AI governance practices right at the early they can have a big upside I think that's what we have been doing

&gt;&gt; can I ask an how does this work, but you mentioned that full range of companies that are involved. And one of the topics we we spoke a bit about yesterday is is the difficulties sometimes in when you

have big companies, we have some of them represented here, but also startups and small and medium enterprises. How do you differentiate? How do how do you make sure that we're we're engaging across

that that very differentiated group of industry? &gt;&gt; Yeah. So, so I think uh if I segregate, right, I think there the big tech, right? uh then there are the services

companies then there are the middle size small and startup I think all five of them have different sort of uh engagement right the big tech I think are playing on the front foot right uh

the services companies have to follow their contracts right the bigger services companies the medium uh tier companies uh they are really trying to understand uh how they grow their AI

base at the same time build uh you know that uh services or product using right governance principles but again I think the bigger uh support is needed from the uh you know uh the smaller and the

startup right because they are really really fighting for day-to-day right I think uh and and and uh believe me I think a startup founder has to first build a business a tech a team right uh

and also get funding and and and parally focus on you know uh a lot of things around governance right I think in that whole journey they they we have seen they are uh putting it at a second or

probably the side burther which is something which we see is a complete no no if you do that uh you know when you're building a product right uh you might miss when you're

scaling I think that's what we have seen &gt;&gt; great thanks very much I think we're going to turn to the scale side of it now with Alex you're next in line so Alex Walden you've been working on these

issues on the within Google and and I think one of the insights that I've learned from you over the time we've known each other is is really how complex it is to to bring to product

teams and those that are on the techn technologist side some of these issues of responsible business conduct and human rights and give us the benefit of your wisdom about about how that works

and how we can do it better. &gt;&gt; Um, thanks for the question and I love that you said that because I do see a very important part of my role as making sure that the stakeholders that we work

with understand how things are working within companies because that helps us be better and you be better advocates for helping us improve. So, um, anyway, but to your question, because I know I'm

going to be fast, um, I think what where it really starts for us is like from the values perspective. Obviously, we're a company that's founded on values, um, around freedom of expression and

privacy, um, and bringing the benefit of our technology to everyone. And so, that is where it begins. Um, but obviously we have things like ultimately it's the sort of governance inside of the company

that is what permeates throughout the 180,000 people that work at Google to ensure that we are being responsible in the way that we're developing AI. Um, so as a baseline for us, responsibility

and thinking about what responsibility means has to start with human rights and then we can build from there. So we have a corporate policy that says that we have a commitment to respect the UN

guiding principles on business and human rights. Um, and we've also built on that with things like our AI principles that reinforce sort of more of an operational way in which we can manifest those

values in all of the teams that are working to develop the various models or applications of the models in say Google Cloud or YouTube or search. Um, just to maybe hone in a little bit on

the types of standards that we're using because I think that's important because there's so much work being done in our ecosystem. We use the UN guiding principles. We use uh the work happening

at the OECD um the work at UNESCO um and engagement with our peers in industry through the BTE project through global network initiative and this is just a few um but all of the guidance that

comes out of those places and the dialogue that happens there helps us uh ultimately inform how things are working inside the company. Um and then just one layer down then I'll stop. I

think you know having programs and processes like training um and dedicated teams ultimately that's how you operationalize this through getting a product to market. Um and so I can say

more but I think those are kind of the big picture structures for what's required for a company to do this at scale. &gt;&gt; So you know I'm not going to let you off

the hook quite that easily. So &gt;&gt; look we know that this isn't always easy though, right? that there are obstacles to to really convincing people it's worth the time. I'm I've been in the

room where hand ringing is described as the you know no more hand ringing about safety. We're going to we need to just move forward and I'm sure there are pressures that you face and and as the

lead for human rights within this company trying to get your message heard. Tell me a bit about like how you've been able to sometimes surmount some of those necessary challenges that

you might see from from that different perspective on on whether or not these are hurdles or supports for the company to do its mission more effectively. &gt;&gt; Yeah.

&gt;&gt; Well, I mean I think in general we have sort of corporations are incentivized to put products on market that are safe and that are trusted by our consumers. um the people know Google best through

Google search or Gmail um the varieties of consumerf facing ways they're engaging with our products and so we do have an inherent sort of market business reason to put out products that people

trust and deliver um good outcomes um and so we have to have processes inside that that make that real and so what we do is we have model requirements just at the most granular level for before any

product goes to market there are model requirements ments and so that those teams are focused on uh ensuring that they're validating the data and doing testing and doing evaluations and that's

at the model level. And then um at the application layer we have requirements for teams to be again doing testing um additional evaluations setting additional guard rails and focusing on

what mitigations are going to be put in place for again things like Gemini before that gets launched. Um and then we have to have executives review these things. So before anything goes to

market, leadership needs to be understand what the risks are and how we're mitigating them um and have a plan in place to address that. So that is a important part of the of the process for

us. And then last we have post-launch um monitoring because obviously we can do all of the testing in the world but once you've launched a product um there may be novel or new um or residual risks

that arise and so we have to have a process for continuing to monitor that understanding it getting feedback um improving and um improving products over time.

&gt;&gt; Great. That's super helpful, Alex, to understand that that multi-layered approach that needs to happen within companies, including, I think, that executive level that you mentioned. I

mean, the signals from on top will actually inspire all of those other levels to to do what what we're hoping they'll do. And we have another example with us of of some of these practices. I

want to turn to Hector Dvoir, who's the director of responsible AI public policy at Microsoft. Um and we want to hear more about what you're doing to embed responsible uh policy practices within

uh Microsoft's approach. &gt;&gt; Thank you very much Peggy and thanks for having Microsoft here. Um so maybe I want to start with with the inception of our responsible approach which was in

2018 and at that stage you didn't have um codes, directives, regulations, frameworks guiding our approach where we're nearly starting from a blank page and we didn't talk about foundational

models or frontier models at that stage. It was all about like specific AI system and applications such as facial recognition for instance which was very popular. Um but so we forged our AI

principles around you know priorities such as privacy such as re reliability inclusion fairness safety security and like these highle principles the whole challenge was to translate them into

practice afterwards and so it's really on this basis that we forged the office of responsible AI when we created it in 2019 um around around these principles which then became our RAI standard

guiding all our actions across our different programs. Um, one of one of the program that I want to reference here is our sensitive use case program. So, it's a team within uh the office of

principal responsible that is in charge of doing some triage challenging basically sensitive use case coming from our different markets on AI systems and and models um that could actually like

you know violate these principles that was referencing. And so this team analyze um these these use cases and then when it occurs that it's necessary bring them to our ether committee which

is our AI ethics committee uh and and it involve Microsoft board both at the CTO level and the present level and I think the board inclusion is very important in this kind of like internal risk

management framework um and so this work has been informed during the past years by many interesting developments. So the OECDI principles obviously but also the UNESCO recommendation on AI ethics and I

think all these principled approach that evolve or refined nuanced uh with AI capabilities are actually so important and very useful signals for us to refine our own like AI governance program

within Microsoft and &gt;&gt; Hector you've talked a little bit about how you look at it from an internal perspective but we wanted to hear a bit

of how you look externally like what what are the drivers between how you engage across the sector and and with with the government side as well.

&gt;&gt; Yeah. Um and I think we always navigate this this very interesting interplay between best practices and international norms and regulatory standards and and a very good example here is the line of

voluntary commitments that have been signed across DI summits. And so if you look at Bletchley Park in the UK or the South Korea summit happened afterwards, it really helped us as Alex was

referencing to ground our model testing approach especially against public safety and national sec security risk. So when we talk about cyber security for instance or loss of control or CBRN

risks that that really like grounded some very solid testing approach uh with some concrete operational triggers concrete high-risisk domain that we're monitoring at the model level. So that

was that was one. Um the OECD hyper reporting framework the which came out of the Hiroshima process is another very good tool that I was um that was involved in and I want to reference

here. was launched along the lines of the Paris AI summit and actually it's a very good way to understand how risk management transparency works in practice and how you know real world

deployment experience and transparency uh experience can guide upstream uh developments and so it's it's this kind of feedback loop that it creates is very interesting and because we're in Delhi

uh um just to reference the voluntary commitments that were signed yesterday I think that's another very very very good and positive approach that the Indian government have been taking especially

on one of the commitments which basically encourages company to um forge multilingual capabilities approach. So basically build better evaluations against safety risk not only in English

norms but beyond English norms and I think that speaks about you know our principle of inclusion that's that's so important and I'm very happy that they initiated this work. I have to say one

of the contrasts I've been making when I look at uh the what's been talked about here in in Delhi as opposed to prior prior summits is that issue of inclusion and the language issue I think is is so

under reppresented in some of the conversations we have. So it's it's wonderful that you've given that a shout out. We're very fortunate uh to have you with us as well Kim who is the uh vice

president at LG for AI research. Uh so we'd really like to hear more about how you're engaging with these global technical and policy standards. We talked about the UN guiding principles

on business and human rights, the uh UNESCO recommendation on AI ethics and of course the the uh MOO that's being worked on. So give us a sense of how these these frameworks are are being

engaged with by LG. &gt;&gt; Yeah. No, thank you. Thank you for the question. Uh it's a really happy to share our practice the between the global

AI giant. Yeah. So uh I I need see some memos but I have a lot of things to talk about. Uh LJI research is uh the leading AI company in Korea. So we are famous not only for our foundation model

performance or solutions excellency but also we have a lot of activities related on the responsible AI and inclusive AI. So there are two uh examples I'd like talk about. First one is we have uh AI

ethical impact assessment process. The process was made uh inspired by the UNPS and the UNESCO's ethical impact framework. So we build this process and system in 2023 and from the 2024

uh we applied this process all the our R&amp;D project. So as a result uh in 2025 alone we found more than 200 risk set and we try to mitigate uh four out of five of them. Yeah. The second one is uh

uh last year we developed our uh own AI risk taxonomy. we call them chaot. We call that the Korea augmented universal taxonomy. It is grounded in the universal human values. So on things

like the universal declaration of human rights, international human rights treaties and unusual recommendation. So uh this one is uh the result of the taxonomy organizing the four domains the

universal human value, social safety, Korean sensitivity and future risk. So we have more than 200 categories and we found the risk that but the uh the but the the important thing I want to talk

about is the Korean sensibility layer is designed to be replaceable. So any country or region can plug in each their own own context. So we hope our taxonomy system can be a practical reference

especially for the countries in the global south as they built their own AI safety standard that reflects their own realities. And the other one is talk about the

inclusive AI. So we uh run many AI research programs in Korea. So for from the teenagers to 20s uh annually maybe more than 40 uh 40,000 learners every year. So cumulatively we

uh learned more than 140,000 students here. So we our our program is the biggest area liter program in Korea but we don't uh want to stop at our borders. So that's the reason why we collaborate

with UNESCO making a muk. So uh the essential of MOO is for the practitioner the practitioner usually struggling with the same question. How do I actually apply this in my day-to-day work? So we

are focusing on the bridging the gap. So we provide the best standard risk. So uh so we get a lot of risk as Timas mentioned. So we also contribute our own experience. So the I I previously

mentioned about our process and some uh we made also our AI powered data compliance system and also I will mention soon the we have a annual report about the AI ethics activities. So uh I

hope the book can be a good good practice for the everyone. it will launching in the this half. So last one is I want to talk about transparency. So we have a lot of activist

about the AI responsible AI or inclusive AI. So we published our annual accountability report on AI ethics. So uh yesterday uh we released the third edition. So here are some some short

code of that. Yeah, I will spread out after my session. Please left my documents down. Well, yeah. Thank you. Definitely. &gt;&gt; Wonderful. No, I think it's super

interesting to understand both how you've been looking at that learning process within the company, but also how that more global approach working with UNESCO is going to be very helpful and I

think it's one of those areas where we all know so much more needs needs to happen. Um but we've we've heard the the company perspective here and we're very fortunate uh to have with us um from the

World Benchmarking Alliance um Namit Agawa and Namit you know I I think one of the things that we've talked about is how we incentivize the race to the top amongst all of these actors in this

space and you're going to I hope uh give us some some insights based on the the work that the World Benchmarking Alliance is doing about how capital and investment can be used to in uh make

sure that innovation is being approached in a responsible way. Over to you N. &gt;&gt; Thanks for for having me here. U and I'm not representing investors but we do work with several stakeholders including

investors, civil society, governments and companies. Um so we are a nonprofit uh and and we try to strengthen accountability of the world's most influential companies uh so that their

impact on people and planet uh can be sustainable. Um we also assess the world's most influential tech companies uh on whether they are advancing a trustworthy rights respecting and

inclusive digital future um using standards such as the UN guiding principles but also others that were mentioned by uh my fellow panelists here. Our role is to provide comparable

credible and standardized data that our stakeholders can use uh because it's an ecosystem approach. uh so how can they work together um in in doing that u so capital can definitely incentivize

innovation and responsibility but capital alone cannot do that um we published our latest assessments of 2,000 companies at Davos last month uh and particularly from the tech side what

we found is close to 40% of the companies have disclosures on AI principles but just above 10% meet the expectations global expectations on the governance aspect of it and none of

these 200 companies that we assess uh disclose uh their reports on human rights impact assessment and I think that clearly shows that while there is a lot of intent some work is happening um

but governance and accountability are not really there so we need a lot of work to happen there and we believe responsible innovation requires incentives for long-term risk risk

management, clear expectations that are tied to capital allocation and consequences for weak governance because it has to be consequential for companies to move in that direction and I think

that is where investors have a very catalytic role to play. Um we convene a coalition of investors and civil society organizations to promote ethical AI and there are 64 investors representing 11

trillion dollars of assets under management. Um and it's a structured engagement process that we organize. Um and we have seen some results. Um 19 out of 44 companies that we targeted in the

last engagement period published new AI principles. Um 68% of large tech companies responded to investor outreach. Um in some cases we have also heard companies coming and saying uh

that such coordinated engagement uh puts things in motion internally and that's when companies internally start looking at some of these things in priority. This definitely shows that capital

allocation moves things but it's better and faster if it is along with evidence and coordinated action and clear time bound uh asks. &gt;&gt; Yeah. Now, I mean, I think it's so

interesting that we work in a sector that is incredibly based on data, but yet we don't necessarily bring it into this conversation in in the ways that we need to. And that idea of of both

incentivizing the right practices and and leverage within companies, but also, you know, too many conversations sort of focus on the tech industry as a whole and sort of group everybody together as

if they're all engaging in the same way. And so the work that you're doing really helps us to understand those nuances more. Could you go a bit deeper and and look a bit at some of the examples and

concrete suggestions uh coming out of your work as to to how to push that discussion forward more? &gt;&gt; Absolutely. So I think the first thing is engagement and dialogue and I think

that is a very important way and we have been fortunate to have good engagement with both Google Microsoft on this panel. Um but again it's important to build on engagement because it's a

continuous process. Uh it's important for investors to engage with some of the leaders. Uh but also engage with companies who are fence sitters to bring them along faster. The lagards will

definitely uh you know catch up and come on board. But um but for investors and if you want to u you know for capital and finance to incentivize responsible innovation, responsible AI, uh there are

three things that we believe investors should definitely do. Um first is on AI governance and board oversight. uh investors should ask whether there is clear board level responsibility on AI

risk, whether executive incentives are aligned with long-term human rights risk mitigation and whether governance applies across the full AI value chain. Second is on implementation at product

and business model level and we heard some examples just now. Investors need to move beyond policy statements and ask companies how ethical principles are translated into product level

strategies. uh how high-risk use cases are identified and whether there are internal mechanisms and controls to you know identify harms as they emerge and third is robust human rights impact

assessments um and asking whether companies conduct AI specific impact assessments are they publishing meaningful summaries are mitigation measures integrated into product cycles

and I think this is an area where we have seen a lot of gaps &gt;&gt; great thanks now I I wonder if we could actually take that one step further and and get some input from the other

members of the panel on what that looks like in practice because of course um this is a panel that's focused sort of on the company perspective I think we have some of our real partners here on

the civil society side and as much as they understand that that conversation needs to happen I think they sometimes find it difficult to be able to make sure that the way those risks are

assessed really look bring in the voices bring in the experiences of people um and particularly people in different contexts and um different environments in which the companies are are being uh

their products are being rolled out. So those issues of stakeholder engagement, access, dialogue with the civil society side. It'd be great to hear a little bit more about some of the lessons that

you've all learned there. And I I see uh you're shaking your head. Please tell us from the NASCAM perspective how you look at it. &gt;&gt; Uh I I I think I think uh from a

enterprise lens, right? I think when they are trying to implement uh responsible AI or or trustworthy AI right I think the biggest issue is there are different uh groups internally right

the tech group the business group the legal risk group right the finance group right and then all of them are working in silos what what we feel right because the the business want the best you know

uh for the business the tech wants to put the best technology right the risk is very you know uh you know conservative right and and and finance always has a upper limit on what they

want to spend. So, so that's what issue I think uh what helps is if all of them build a a collaboration which can be taken use case by use case right again the high impact use case can have more

investment more uh you know focus versus a low risk right I think that's the first thing the second thing is I think uh what what uh from from NASCOM lens what we are seeing there's a lot of

frameworks which are which are getting developed right every every country every place you go there's a new framework right but from the the you know the framework heavy heavy or or you

know uh the concept heavy to action is not happening I think I think that's a big gap right so so uh if if a technologist is trying to implement responsible governance right a developer

is trying to implement right he will be lost in the frameworks he doesn't know what's actionable right I think what what he should do so I think that's one uh big need I think that's what we are

also driving we are trying to drive uh a multi- you know uh multi- different organizationled approach where we have all different you know different sizes of organizations where we come together

and start discussing collaborating implementing right I think that's the second nugget right I think these are two points I know time is up &gt;&gt; no that's great I mean I think it it

shows that that collaborative effort is going to be super important rather than the siloed approach for so many practical reasons as well that companies can only respond to so many different

frameworks and what they need to do is have the simple guidance and support that they need to to actually implement at this stage. Heck, do you want to say from quick quick comments from the

company side about how you're facing those challenges? &gt;&gt; Yeah, two very quick examples on how we involve civil society and academia in this process. So um our work really sit

at the intersection of policy research and engineering groups and to inform product development uh with our responsible AI principles we regularly publish some internal policies and it's

an iterative process with our research teams with our product teams and as part of this process we actually include um academics who have like a you know a specific rest domain expertise or think

tanks and and civil society organizations which have been thinking very deep about the deployment of one AI system, one AI model in certain context. And so that that really informs, you

know, the product work that we do from the inception. And I think the second example that was raised is you you know like the the the big topic as as a and and and the governance challenge that we

face is the importance of refining AI evaluations. That's the constant thing. And in India for instance, we've been working with some um some NOS's around a project named Samishka um to build some

community-led benchmarks um which is basically a safety tool that we include afterwards in in the system construction um to to really get data sets that are grounded in a community with a specific

like cultural aspect, specific contextual aspects. because if if you just translate safety tools from English to another language, you lose all the context for which this safety tool has

been built. And so that's another example of, you know, really an area where we need more cooperation between civil society and and uh governments and and companies. It's really how do we

build these safety tools uh beyond English norms um such as such as such as in India. &gt;&gt; Oh, that's that's great. and and it takes work to do that and the more we

can spread you've done some of the work you know how to do some of it and and diffuse it amongst other companies that could learn from it that's part of what we're trying to do with BTE but I think

there's a lot more to be done you do you want to come in &gt;&gt; yes uh I agree with his comment the the safety or response of AI we should work together so that's the reason why we

make our annual report because sharing our best practice and also sharing our struggles what what what we uh had a struggle of need that that we think that's a very important thing. So this

is my colleague mentioned about it. Uh there's a African proverb that said if we want to go fast go alone if we want to go far go together. So building a trustworthy and safe AI ecosystem is not

a not a not a sprint. So it's a long journey. So we can go together. Yeah. &gt;&gt; It's a long journey with a lot of sprints happening dayto day as far as I can tell. some of them here at the

summit. But over to you, Alex. &gt;&gt; Yeah, so much sprinting. Um I think maybe just to pick up specifically on the stakeholder engagement piece. Um so a few things. One, I think it's

important for companies to have um a programmatic approach to stakeholder engagement. So we need to have ways in which we're regularly engaging with stakeholders in general, not just on a

specific product question. Um but so I would say first a programmatic approach and then second something that is more ad hoc. So when we need to consult specifically on a product, we need to

have a sort of process and way to do that. Um the other thing is we have programs internally like trusted tester programs where we are working with thirdparty organizations to make sure

that they have early access or pre-launch access um to models or to a product in order to test it so that we can identify potential risks or errors ahead of time and address them before we

launch a product. Um and then last just to highlight something that we do is similar to others. um our research team called impact lab which is part of the overall human rights uh programmatic

work at the company engages directly with communities in doing research to inform how we are improving our products and what we're developing. So that work is also happening through the research

team specifically. Um and they recently launched something called the amplify initiative which is an open source app. This is specifically on language inclusion um that allows uh members of

the public and communities to engage in the fine-tuning work around our language models. So specifically that there is a lot of there's a wealth of information expertise out there that we should all

be benefiting from that we can benefit from um and it's open source so we can also share it with others in industry. That's great to hear and I'm sure more needs to be done on that front, but the

but the the amplification effect is is so crucial. Look, we could probably go on talking all day, but I see the clock is ticking down. Fortunately, rather than us try to draw the conclusions from

this, we've we've welcomed in another speaker to give us some concluding remarks to pull some of these pieces together. I'm very happy to invite uh Parite Adani from Sirill Armachan

Mangala Mangalas uh to help us think through some of these issues. Please &gt;&gt; thank you for that. I think it's partly easy and partly tough because there was a lot to unpack from this conversation.

But firstly, thank you. Uh you've held the conversation beautifully. Um and thank you to UNESCO, United Nations, NASCOM and everybody in this room who brought their knowledge, their conscious

to this conversation. Um actually I just want to talk a little bit about a conversation with the machine. You know, as we were thinking about this topic and we were engaging on this issue, I wanted

to share something that I might uh I feel resonates with a lot of what you've talked about. Um I did something uh in preparation. I decided to ask the tools that we're talking about over here a

question that we avoid asking ourselves. Do you I'm talking to the tool. Do you have ethical limits? Do you understand the difference between what it can do and what it should do? And I'm going to

quote verbatim on conscious at a conscious level. The answer is I don't know and neither does anybody else. Sorry.

The gap is a philosophical uncomfortable position where think of me as having no home inside. Thank you. I really appreciate that. I have no continuous thread of existence

and I cannot verify about myself what you have asked me. I don't have any consequences to bear. Now what came back though unexpectedly thoughtful showed us about restraints

values and what it appears to have internalized. It acknowledged the difference between instruction and conscience a lot of what we've talked about today.

And so I think when we talk about this we said human rights are not optional. We cannot ignore the impact on people and planet. We have to make incentives for good governance. So when a tool

cannot understand this for itself, I think we have to do the job. What we have chosen in India and when we're having this conversation, this location is not ceremonial. It's uh

very deliberate that we have thought about innovation over restraint and we have to think about that being the right choice. Sorry, I've developed some allergy. I'm

so sorry. We allow innovation to be in a safe space without feeling the weight of the regulation. And I think we have a lot to learn from all of you who've been doing

this for so long. The privacy, the safety, and the impact on children and vulnerable groups. The question is whether the people that we're talking about are going to be the subjects of

the transformation or just its objects. An AI system that cannot understand a language or a Hindi woman speaking legal questions is serving a national a narrow slice of

what it calls a universal solution. So any framework for safe and trusted AI that does not express and understand informality, language and gender is not incomplete by accident. It's incomplete

by design. I think the ideas of an interoperable, flexible system is a forward-looking and an inclusive one. I think a lot of what you mentioned Alex

about governance inside the company it's wonderful what you're talking about and I think the voluntary commitments that have been reflected in this summit is also fantastic.

So now we come to the harder work. The ambition is real. The infrastructure exists but ensuring that we don't leave with just good intentions and good ideas but action. Thank you.

Thank you Donnie. It's it's wonderful to hear those perspectives. Um we're coming to the close of this session. Um just a few parting words to all of you. I think we've done enough uh in in this short

conversation to really give a sense of how complex some of these issues are, the dynamics within companies and externally and then globally across different geographies, the challenges

that are faced. But the reality is that all of us have a responsibility to engage on these and we each have different roles. We've heard a bit about what some of the companies are doing.

We've heard a little bit about how we can challenge them and incentivize the actions that they take in the space. Um there are good practices, but they're not univers universally applied. They're

not available to some companies that may want to uh engage in this and we can help them to do this. and NASCAM and I have been we've been discussing a little bit about how we make simplify things

bring in more into the fold of this conversation and of course we're here in an environment where we have governments that are looking at what do we need to do to create responsible business

practices and incentivize them as well. So I hope everybody walks out of the room thinking what can I do to continue this conversation? How can I differentiate between companies that are

thinking about these issues in a way that will deliver for myself, for my children, for my future the ways that we want to see AI innovation will work if there's trust and if the companies that

are delivering it actually invest in in delivering products that will really give those values that will inform and give us human dignity going forward in the future. So, thank you all so much

for joining us. Thank you for fitting this into your schedule today and enjoy the rest of the summit. &gt;&gt; Hi Peggy. Uh Peggy, we would just like to request you to

Exactly. Wonderful.
