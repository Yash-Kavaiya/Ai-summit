# Panel Discussion on Reimagining Responsible AI: Frameworks, Safeguards, and Standards for the Next Wave of AI Innovation

**India AI Impact Summit 2026 ‚Äî Day 5 (2026-02-20)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 15:30 ‚Äì 16:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 9 |
| üìÖ **Date** | 2026-02-20 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/uLz93uh7Vss?feature=share) |

## üé§ Speakers

- Mr. Ashish Tewari, Infosys
- Mr. Kamesh Shekar, The Dialogue
- Mr. Syed Quiser Ahmed, Infosys
- Ms. Arundhati Bhattacharya, Salesforce

## ü§ù Knowledge Partners

- Coalition for Responsible Evolution of AI (CoRE-AI)

## üìù Summary

This panel will examine the next phase of Responsible AI frameworks, safeguards, and standards in the context of evolving AI technologies. It will feature a focused discussion to gather practical inputs on strengthening governance mechanisms, lifecycle oversight, risk assessment tools, and global standard setting, with emphasis on responsible innovation in emerging systems such as agentic AI.

## üîë Key Takeaways

1. This panel will examine the next phase of Responsible AI frameworks, safeguards, and standards in the context of evolving AI technologies.
2. It will feature a focused discussion to gather practical inputs on strengthening governance mechanisms, lifecycle oversight, risk assessment tools, and global standard setting, with emphasis on responsible innovation in emerging systems such as agentic AI.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/uLz93uh7Vss/maxresdefault.jpg)](https://youtube.com/live/uLz93uh7Vss?feature=share)

---

_[‚Üê Back to Day 5 Sessions](../README.md)_


## üìù Transcript

And then we have Mr. Kamesh Shaker, associate director at the dialogue, uh, who will be moderating today's panel. Can we have a huge round of applause for the panel?

uh to to be to kick us off we I'll uh invite uh Mr. Stefani Naguran sir uh to give the keynote address and &gt;&gt; yeah first we'll do the group picture of course

&gt;&gt; I think let's come &gt;&gt; in the Thank you. &gt;&gt; So please good afternoon everyone. I know it's uh

Friday afternoon almost uh end of a fantastic uh global AI summit and uh good afternoon to my fellow distinguished uh panelists. Um I think uh the topic of this particular panel uh

it's probably the apt one uh to wrap up uh this global AI summit because the most important arc in this innovation of AI is making sure the impact of the AI is safe, responsible, ethical, inclusive

and uh explainable right and it has to be holistic at the end of the day. I think there's a lot uh that we have learned over the course of this week listening to uh a number of different

thought leaders uh talking about how uh AI could be uh channeled uh in a manner where uh it delivers the intended impact without uh getting into unintended consequences.

I think there is a significant role the governments uh innovation hubs academia and startups have uh to play in developing this safe and ethical AI right starting with benchmarks must

emerge from deployment reality and not just research labs safety benchmarks fail when developed in isolation the most effective ones come from institutions s building, deploying and

maintaining AI at scale, right? Government innovation hubs sit at this critical intersection between policy intent and operational reality surfacing failure modes and in trust gaps. The

second most important uh element in this uh framework is to ensure these safety benchmarks are co-created with the industry and with the academia and the research institutions. ICOM and the

dialogue developed one of its kind index called the raise index over the last um year and a half that we have been working together which is the first of its kind in uh quantifying

the impact or the quantifying uh the the the value or the quantifying the impact of um AI and deploy within deployment and during

development uh on the safety and responsibility matrix and this is uh you can see up here on the screen the QR code and you can scan uh the QR code and then you'll get access to the entire

framework and you could even test your uh respective uh AI solutions or AI uh systems that you might be developing or you already have in production. test it against that and then see like what the

index comes back and tells you. The third is making benchmarks practical and uh in Telangana we have uh launched Telangana data exchange uh which is first of its kind digital public

infrastructure within uh the realm of AI. It provides startups access to government data sets in a sandboxed environment. This is where benchmarks get validated and uh uh time- tested.

Startups can test their AI systems against actual data, actual use cases, actual constraints before deployment. The third is we all understand and recognize that startups move at a rapid

pace. So when startups are deploying AI solutions you there's number of risks that emerge and we are providing this index again as part and parcel of the whole startup ecosystem that we are

building and as a result they we expect them to detect any early warning signs within uh this framework and continue to improve this. The last is benchmarks and frameworks must be living infrastructure

not static um checklist. Right? AI capabilities evolve faster than regulatory cycles. Static benchmarks become obsolete. Hubs must institutionalize

continuous benchmark evolution. This raise index methodology includes phase-based assessment ensuring benchmarks remain relevant to company maturity stages. So if you take this

broader framework of making sure how do we make sure AI systems are safe and responsible and ethical, the question comes down to how is India leveraging its innovation hubs and its leadership

position in shaping the global dialogue on inclusive and responsible AI. What is interesting is India is uniquely positioned in this global AI discourse. Most global AI frameworks are designed

for high resource homogeneous environments. India operates in the context that most of the developing world shares which is multilingual populations, infrastructure constraints,

massive scale and the imperative to s to serve both economic growth and social inclusion. This is not a limitation. This is a significant competitive advantage that India has in shaping the

global standards. Number two is demonstrating responsible AI in high stakes, high-scale deployments which uh we are offering. ICOM uh the first of its kind innovation u AI innovation

entity out of Telangana with its research and co-inovation pillar helps build AI solutions for healthcare, agriculture, climate, financial inclusion where failures have immediate

so societal impact. When we document how these systems are designed, tested and governed, we contribute frameworks that have been validated under real world complexity, not just lab conditions.

This particular raise index is India's contribution to global standardization. You will notice the more you dig into this index, the index harmonizes requirements across leading global

frameworks. Be it EU AI act, NIST AI risk management framework, the Singapore mass guidelines or the UK AI assurance. We brought it all together into a single portable assessment. Organizations

operating in multiple markets can use one assessment to evaluate alignment with diverse regulatory escalations. The methodology is open and adaptable for other juris jurisdictions.

And I would leave you with last but very important point of institutionalized continuous learning in responsible AI practice. Right? Most frameworks are static standards. ICOM believes in

creating systems with ongoing feedback, tracking system performance over time, updating benchmarks as models evolve, incorporating new research and raise index is designed as an iterative

framework. What we are releasing today is the first edition and it'll continue to evolve through pilot phases, stakeholder consultation and it's not a one-time standard. We all know AI is an

evolving technology and this has to evolve but our intent and goal and hope is this would keep pace with the pace with which the technology is moving. uh and that is very critical and that's a

common responsibility that we all hold be it technologists be it policy makers be it uh think tanks or be it researchers or startups it is we all have to come together as an ecosystem to

ensure the technology that we put out there with the intent of doing benefit for the society does exactly that without any unintended consequences. So I think we are up for a

fantastic panel and um you guys uh absolutely would enjoy the conversation that is going to be held now. Thank you. &gt;&gt; Um thank you so much sir for setting the context and um I think like that deep

like sets the perfect context for us to like pick up the conversation from there. um which is going to be like we are discussing today in terms of like reimagining like responsible AI what's

the what are we trying to like do today in this panel is to like you know understand like what are the shifts that are needed like when it comes to responsibilities with evolving um

innovations and like how we can take the needle forward when it comes to responsibilities I would like to like start with um Miss Arunati Batara here thank you so much ma'am for taking the

time it's absolutely a pleasure to host you And um first first question is to you ma'am is that is like as you are a global enterprise leader um how do you see the balance between the rapid AI

innovation with there is a need for a trust and accountability and customer protection as well. So how do you see that balance happening ma'am? &gt;&gt; Uh so you know uh in u the company that

uh I work for Salesforce uh we started our AI journey in 2014. Okay. And uh in 2014 we also set up within the company an office for the humane and ethical use of technology.

Okay. So this is an office by the way which goes through every one of our products, every one of our processes before it is allowed to make its debut in the market because we realized very

early on that while uh technology and AI could give us many advantages, it would also be used by bad actors for doing things which it was never intended for. And that is true of every single thing

that you know uh we come up with. Whether it be a new medicine, whether it be nuclear energy, whether it be anything that we come up with, it can have its good use. It can also be used

for the wrong reasons. And that is something that we must come together in a global compact in order to defeat and in order to stop. Again, this has to be a global compact. It's not something

that uh one country or one uh organization or uh one uh effort can probably uh ensure because unless and until we have sufficient transparent um uh information exchange, unless and

until we all say together that this is not something that we will allow, it would be very difficult for us to stop the bad actors. It's not easy. Today you see the kind of deep deep fakes that are

there. Stuff that we never thought of in our childhood. Families having safe words amongst themselves. It's not something that was there at all. But today, in fact, I was asking a colleague

uh from the US and he was saying yes, we do have a safe word in the family because we don't know when somebody's going to get a call that's going to sound like me and it's going to say that

I'm in the hospital and I need so much money. please come and get me and it might be somebody entirely different trying to scam you. So we do have safe words. Now imagine that the extent to

which we have gone where we are having to teach children that these are the ways that you can be sure and you can be safe. Now this is not something that we want because obviously AI is also

something that can speed up things like medical research. It can actually speed up skilling. It can speed up many things which en enable us and empower us to come up to potential. So a technology

this powerful should not and cannot be stopped because bad actors are misusing it. And therefore it's up to all of us to come up with a framework a global compact again as I say a framework that

will enable us to ensure that we are all of us together trying to stop the bad actors and ensuring that this is being used for the good of human humanity. Excellent point ma'am like I think like

very interesting aspect is your starting remark in terms of putting together an office on the humane aspect which actually shows that like you know it's not only like technical side can solve

the problem when we talk about responsibility it's also organizational um ethics and organizational ethos that like you know which kind of like brings that kind of essence to it and great

submission on the global compact and like I think like that's something that we should all like strive towards and like I hope the summit will kickstart that process for us as well. I'll come

back to you ma'am. I know you have a hard stop but like I'll do come back to you for one more question. Uh but like now I would like to go to Karna here. Thank you so much um Karna for joining.

Um we did hear from ma'am in terms of like you know what uh what can be actually like um done in terms of from and how how are larger organizations are looking from this but like would like to

like pick up your brains in terms of like you know as a startup and an MSME um what are the operational challenges that you guys face when you are like trying to balance this equation of

responsibility versus innovation and um also you guys are looking at from a forsightedness and like new technologies so any any thoughts there would be you know happy to take it. Yeah.

&gt;&gt; Yeah. Uh thanks thanks for having me. Uh so I am Karna. I represent the company Blue Machines AI. Uh we help uh large enterprises deploy voice agents into their process. The idea is the same

right? We will have so many voice agent demos which we call a PAT demo where you can stitch a easy bot. It will work 80% of the time but it is what the rest 20% of the time which it doesn't work on a

demo stage and that 20% is what enable uh is what breaks and what we focus on is how do we make the AI technology which comes with lot of power be a bit more enterprise softwareish in terms of

compliance governance observability so we that's what we do which means the way we believe is uh if governance looks like a 200 page PDF for all companies, MSMES to figure out. We will see them

struggle and our our idea is uh it should be a part of the core product as a lot of us are building solutions for customers. Governance should be the core product like we believe productize it,

productize it, productize it and that allows mass adoption. And the way we do it is uh so governance to productize it we uh just writing it into prompt is just the first line of defense. It

should be the core part through the entire agentic life cycle which means at the time you're giving it an input and it's reasoning. There are guardrails it checks before it does some tool calling

which is like hey I'm going to write uh to the CRM or I'm going to talk to uh one of your customer on this topic. There is again guardrails before that and even when you do an output there

needs to be guardrail and the guardrails should be a part of the core product and that is important to drive mass adoption and secondly the way we think is knowing we build voice agents for companies. uh

we still believe human in the loop is a first class feature not a failure point which means you should design the system that it doesn't in the intent to give an answer it doesn't give wrong answers

it's okay to figure out when it should transition from a fully autonomous to an assisted agent to a fully auton uh to a human and that principles of using humans in the right place should be the

core part of our product and that that productization has allowed us. So we also have another company upna which is a hiring platform which allows around three lakh companies. Now we because

what we saw m beautifully when we productize a lot of these every year every month in fact 3,000 mmemes are building voice interview agents on their own they not even realizing because we

have productized that at the back of it there are three agents they are creating and training for their recruiting process and they're deploying it and within a matter of five minutes. So and

that has driven to adoption of 30,000 companies who are doing it on their own and uh if we want the entire India all companies to leverage it more and more as software uh agentic software builders

we productize it the better the adoption will be &gt;&gt; that's an excellent point right like I think like this is something that like we kind of like also keep speaking is

that productization of responsible AI from a value proposition perspective right like how can respons ible AI be embedded as a value proposition towards the product that you're building which

also is one of the selling points for like whatever that is like taken that's a great great uh point so I'll definitely come back to you but would like to go to um Ankush and then like

I'll come back to ma'am again um is um like quickly like Ankush like wanted to like understand you guys build AI systems so what are the governance challenges that you see most are like

you know difference when it's for public and private sector Yeah. Uh I think one is the control. I think when it's about sovereign AI so it's not just the data

residency which matters to our client. They want the complete control. No one else other government or uh no other uh party should be able to even see that sniff that audit that. So I think that

is something which our clients uh ask for it and uh that's why though we work with uh almost all the cloud providers um and but we let the decision be with our clients like so which data center

they want us to hold and now uh we see the huge demand of onremise uh solution and that's why now even uh we had seen the need of the edge AI day before yesterday with Nvidia we launched V GPT

desk appliance. So that's a supercomput itself that processes around one paops floating point instructions and you know 4 dB hard disk and uh you know and that can run a model with uh one uh one one

trillion parameter huge right so but our bajp model is just half a billion parameters so means they can do multiple models multiple use cases just one box and uh we'll be announcing that soon uh

we we're working with the defense and now there's a huge need to have not just on not just in India not just on promise it's just in the room on the desk right now when the army is doing critical

meetings so they don't want the data to even go out of that room so even that kind of but with a complete processing complete sovereignity and they also don't want to limit the use cases also

right so they want to start with minutes of meeting a change and the aspirations keep increasing so we needed to have a supercomput thanks to Nvidia was powering our box there. So I think that

that is the major part. Rest we all know about the explanability, inclusivity and privacy and um purpose. So I think this is something where I think that's why many many data centers are coming up in

the country. There is a need of having our own data center here. Yeah, &gt;&gt; that's excellent. Like I think like what you're trying to like underline is the trust over the solutions and that's

coming through the sovereignty of the data. more they have control over it more it is &gt;&gt; that's correct so now tagline is AI with purpose and trust right so now uh trust

is of course important for any relationship client vendor so I think with AI the trust is more important because they are trusting us they are giving us data to create the models

&gt;&gt; so that's why many new companies are coming up you know of course I thank them welcome them to come but I think now the old players are still being valued so the work is still concentrated

here though the deliveries are taking time and all that but uh there's definitely now a need and we need to I think uh my message to all the new uh startups and AI startups is yes

innovation you have to keep showing doing but show the trustworthy part of it you said about observability I think that that's very very important so enterprises want more of trust scale

security than the innovation I'm not don't do the innovation but the trust part is very important especially when AI comes. Yes, &gt;&gt; that's that's a great great important

submission. So, but uh ma'am over to you. I think like you have to leave in five. So, like any closing remarks that you would like to like you know provide us.

&gt;&gt; Uh no the one thing that I wanted to talk about uh was trust because that's what uh was being tr discussed uh in Salesforce. Trust is our number one value. We have five values. The first of

is trust. The second is customer success followed by innovation, equality and sustainability. But trust is definitely number one. Now having said that we are number one in trust. We are also a

cloudnative company. Okay. So we do not have on-prem systems and we also believe that basically it is important for us to adopt asset models mainly because today the need for storage and compute is so

high given the fact that AI is able to handle trillions and trillions of data points and the more you have data points the better your answers will be. Of course, not for everything. You don't

need to boil the ocean for every single thing. But where there are really deep questions that will benefit from the diversity and the extent of the data, it is very important for us to have the

right kind of compute and storage facilities. Now obviously you know if you're going to have that kind of storage and compute facilities that is uh entirely on prem it also means a

pretty high amount of investment into the hardware resource and India is not uh very well known for having deep pools of resources. So given the fact that we necessarily uh uh have to have

capital-like models, it's important for us to find ways and means of ensuring logical security and trust. Okay? And there are ways of doing this. There are several ways of doing this. One of the

reasons why by the way we were behind copilot in bringing our enterprise level uh offerings to the market was because we were working very hard on the trust layer because the trust layer is not

only about access. It's also about ensuring not only that your data doesn't go out but also the fact that your data doesn't have any toxicity that your data doesn't have bias that your data is not

hallucinating. And by the way, the bigger the data u the the amount of data, the more is the tendency to hallucinate. And obviously you don't want something as important as this to

hallucinate and give you a right uh wrong answer. So trust layer actually performs a number of these actions which is all meant towards ensuring that the the results that come are not only

responsible they are trustworthy. It was just evolution for us. We had so much data and we created it. We launched it when we have seen and I'm still not saying we are 100% safe but I have seen

the word is now okay to have inaccuracies right. So, so we we are we we are a bit risk averse right we are not u that risk takers when the whole world was okay because we have the

clients so you see our clients are IRCTC LIC NPCI and army defense and they they used to expect no 99.9% accuracy when the whole world is okay was okay getting wrong answers from these general purpose

LLMs then they got more convinced and uh most of our clients before bajp days. So that was classic NLP. Uh I liked your point where we don't have to answer everything right. So guard is important

but now uh most of our clients have gone to geni not just gen not only gen thing we do composite AI so we still follow the conversation the classic NLP based intent classification entity extraction

you would not believe. So 80% 80 to 90% of our interactions happen classic NLP without geni because we think we all are different we are not right so so when say in one of say IRCTC say 4

million people come to IRCTC if I open the dashboard there are only eight to 10 intense people they have to book cancel change boarding station whatever so 80% use cases if someone is saying I want to

travel from Bangalore to Delhi tomorrow there is no ja involved I just have to call NLU is involved that old model works just because the API gets a data right no geni if someone say hey I have

three pets then how do I do if it is one pet that is a policy that we know if says I have a three pet can I carry in my train right so probably that answer is not there in classic NLP for that we

do the rack based with bat bat GPD so I think if we safety is important I think that should be the core of design and then composite don't do just geni because geni is easily available right

and don't use geni because you have money to buy GPUs and uh you know burn the token so so idea is do purpose-led innovation begin with and end in the mind I've been I have told this line I

think 10 times today so first see what problem you're solving &gt;&gt; correct &gt;&gt; and then you see which solution then which model if model is available use

the available model if not build &gt;&gt; thank you so much &gt;&gt; that's that's an excellent point thank you so much Ankush for making the time and

Quickly moving to Kadnam like you know any any closing remarks that you would have and also like you know whatever you want to like add to your previous point. Yeah.

&gt;&gt; Yeah. So uh I think to the point Ankush was mentioning right like AI technology is fundamentally designed on probabilistic models and we are all used to software working in a deterministic

manner right so it has to exactly do this. Now when it comes to this topic of large processes for large enterprises I think they compliance is one area which is super hard to think about right AI is

probabilistic but compliance you always want it to be correct. So I think what to enable the ecosystem what we believe is we al converting compliance into APIs. So what I mean by that is so we're

deploying voice agent in one of the large mutual fund houses all the compliance for that industry are checkbox. So any every company can pick what compliances they need. They just

need to take the APIs which they want to ensure and that makes the entire ecosystem flourish and these APIs should ideally get open sourced in the market. So there is enough level of validation

across all players that hey this sebi guideline this is an API which you can invoke into your agent and agents will follow it and this has pressured us this takes this burden of ensuring AI works

100% correct uh in all use cases which is not the power of the technology but if we don't think like that then we'll become very restrictive in its application while we work a lot on

making it P99 uh accuracy But there is always the deter probabilistic chance of it. &gt;&gt; And I think the second point we should think about is I think the human state

of mind works well in default versus optional. What I mean by that is whatever is the default selection in any of the things you do has 90% adoption or 80% adoption and whatever is a change uh

is a 20%. So the way we think about it is lot of things should be a default. Yes. So customer data should not be used by default to train LLMs or models. It should be an optional add-on rather than

the other way around which you see right because that's how most startups MSMES businesses uh would otherwise ignore it. And the scale of innovation will not happen if that's not the default state.

And lastly explanable is extremely important because as models are making decisions how do you know why this decision were made? And if we make that more as a core output of the API and not

think of it as oh if something breaks we will figure out how it works. You will not enable your partners to be a decision maker with you when you're designing AI solutions for them. So

that's what we focus on. We focus on how do we make a P80 technology P99 available for enterprises and governance is a prime topic which comes on why what is a missing element to get mass

adoption and that's something which I want the entire ecosystem to embrace can we make it an API can compliance governance be more of an infrastructure rather than a paperwork because if it is

that then we going to see slower adoption in India than maybe the other parts of the world. &gt;&gt; That's that's a great great point. Thank you so much Karna. But uh we have very

few minutes left uh and we have one panelist who has dedicated full time for us. So like you know kudos to that. So um opening up to the floor any questions we I think like we can take two

questions given the time frame. Um any question to Karnam? Anybody? Yeah. They're all very &gt;&gt; hi, good evening. &gt;&gt; Hello.

&gt;&gt; So my question is related to um small language models which are becoming increasingly popular within the developer community. So for businesses like yourself,

&gt;&gt; yeah, &gt;&gt; uh do you see a profitable path ahead for uh SLMs or do we continue depending on these LLMs which I think will be raised to the bottom? Yeah. Yeah. No,

great question. I think uh we think about it a lot and a lot of uh customers of our ask the question, hey, would you be in using SLM? Will we use an LLM? I think the place where we are, we all

will benefit from the flexibility of LLMs because frankly most companies are deploying their first or second actual large scale deployment. I think it is helpful to leverage the power of the

larger models at that time and over time you will learn what actually is needed in it and over time you can transition LLM to an SLM where you get the advantage of sometimes latency,

sometimes cost depending on what your use case optimizes for. But I think in the interest of speed of innovation, it's okay to just use LLM, figure out where the value is getting coming to

your business and then explored through the journey of an SLM model which can give you additional advantages. Um anyone else? &gt;&gt; Awesome. So thank you. I would request

now S to take it over here. Uh thank you so much Kamehsh. Thank you so much to all of our panel members. Uh I think it's been a really really interesting discussion on uh how where

responsible AI is now and its future uh particularly with artificial intelligence going ahead. I'll call Mr. Kazim Risby the founding director of the dialogue to give the closing remarks for

the session. This works. This doesn't work. &gt;&gt; Okay. I think this I think this mic works. Yeah. Okay. Great. Uh thanks a lot Sahesh and uh thank you Kamehsh.

Thank you to all those who stayed back till now. I think our we are crossing the limit of event fatigue. I know a lot of us are quite tired and uh sort of very very sort of exhausted too many

events. But I think the last one week has been fantastic. uh we've had the pleasure and the honor of hosting a few events uh over the last week. Uh but I think specifically on responsible AI as

far was talking in the beginning the dialogue and uh ICOM have developed India's first uh tool to assess responsible AI readiness. So we urge and we encourage and we motivate all of you

guys to sort of look into that. But thank you Kamehsh for moderating. Thank you to all our speakers for joining in. Um I think it's important that we all work towards building responsible AI

practices from from the beginning by design and I think that's something which you know even the tool will encourage. So please have a look at that. Uh but all of you have a good

evening I think for what is left of the AI summit. It's been a fantastic uh summit and uh hopefully all of us got to learn a lot. I did myself but uh look forward to seeing you all soon. uh

dialogue will be hosting multiple conversations on AI policy and we encourage you all to join that. But until then have a good evening, enjoy your weekend and thank you to all our

panelists. Once again also thanks to Inos for collaborating with us. &gt;&gt; Thank you so much. question. Hello
