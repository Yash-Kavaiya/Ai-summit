# Building Trustworthy AI: Foundations and Practical Pathways

**India AI Impact Summit 2026 ‚Äî Day 5 (2026-02-20)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 14:30 ‚Äì 15:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 14 |
| üìÖ **Date** | 2026-02-20 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/Ay6qXXEKdhE?feature=share) |

## üé§ Speakers

- Chanda Grover, Ashoka University
- Dr. Aalok Thakkar, Ashoka University
- Dr. Anirban Sen, Ashoka University
- Dr. Debayan Gupta, Ashoka University

## ü§ù Knowledge Partners

- Ashoka University

## üìù Summary

Building Trustworthy AI is an impact-focused workshop examining how responsible and transparent AI systems can enable inclusive, ethical, and sustainable innovation in India. The session includes an interactive exploration of data consent and privacy in real-world AI deployments, the launch of the research paper ASTRA: AI Safety, Trust, & Risk Assessment, and a multi-stakeholder roundtable discussing practical pathways to advance trustworthy AI across policy, industry, and society.

## üîë Key Takeaways

1. Building Trustworthy AI is an impact-focused workshop examining how responsible and transparent AI systems can enable inclusive, ethical, and sustainable innovation in India.
2. The session includes an interactive exploration of data consent and privacy in real-world AI deployments, the launch of the research paper ASTRA: AI Safety, Trust, & Risk Assessment, and a multi-stakeholder roundtable discussing practical pathways to advance trustworthy AI across policy, industry, and society.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/Ay6qXXEKdhE/maxresdefault.jpg)](https://youtube.com/live/Ay6qXXEKdhE?feature=share)

---

_[‚Üê Back to Day 5 Sessions](../README.md)_


## üìù Transcript

I give this example because I'm fairly confident that when you look it up and when you try it yourself, it'll work. And I know it'll work by the way. Um that is it will fail rather on the

current versions of chat GPT. It will not fail by the way uh in the next generation. I I do some stuff with Google for example. It won't fail in the next generation of Gemini anymore

because they're putting a lot of effort into fixing this one error. They haven't fixed the underlying problem. Right? They saw some presentations of people like me pointing this stuff out, so

they've just put a band-aid on top. Now, we can't run life on band-aids. Band-aids is what? Band-aids is students mugging up one answer before the exam so they get the marks for it. That's not

real learning by definition. The problem is that we've built this system which is our attempt to have general software

and we don't quite know how to handle it. So, let me clarify what I mean. I'm going to say something incredibly stupid and then I'll I'll bring it into place. We were talking about this not too long

ago. A long time ago, you had machines that could do one thing. A hammer is a hammer. A car is a car. A door is a door. You can't use one as one as the

other. I'm saying something that sounds incredibly stupid, but think about it. Why don't you need two separate computers? one to run Excel and one to run PowerPoint. How come both run on the

same machine? This is not obvious at all. We're just used to it. So, it seems obvious, but it wasn't obvious. In fact, the first few computation machines that were made, if you go back

look at all of this, you know, Vanavar Bush and even before that, Charles Babage, all of these names one reads in history books or whatever, you'll see they had differential analyzers and this

that and the other. Oh, this machine it can add that machine. It can solve differential equations. This other machine, it can fit curves. This other machine, it can do this mapping task.

Right? This idea that you could have one machine which could do everything was completely ridiculous because there's only one thing in the universe that we know of that can do that and it's the

human brain. Right? The human brain is a singular object that doesn't that can retrain itself to play billiards, to arrange chairs in a room, to present, to drive a car. It can do all of these

things. So due to a bunch of very clever people like Alan Turing and co, we figured out that wait a minute, we can have one computer, we can build this one machine. I mean, think of it just from a

manufacturing point of view like Jackpot. We can build one machine and it can do all the things. All we need to do is we need to have different software, one for each task. So we'll have one

software for Excel, one software for PowerPoint and the same physical machine will be able to run both. So we built general hardware and that worked for decades and the fact

that we had general hardware led to the computation and information revolution. Now for the first time instead of just having general hardware that is one machine that can run all software we

have general software which is you don't need PowerPoint and Excel separately you can have one software which you tell it what to do and it'll do the job of PowerPoint and you tell it something

else and it'll do the job of Excel also right that's what we're trying to build with AI at the end of the day right going from general software to general general hardware. And

as we know this edit, this ability that we got when we built a general purpose machine before you needed to spend all this money and build separate machines for every task and the moment you had a

single machine that could do all things that led to an absolutely massive revolution. Now that you have general software coming in, right? Once we learn how to do that, think of how the world

is going to change, right? Uh software companies which used to build you you you there's a very interesting graph that um I really should have put here which is if you're manufacturing

something there's a burn rate, right? So you have an increase in the amount of money you have to invest in your company initially and then if you manufacture 10 cars you have a certain amount of uh

money you need to invest. If you want to increase the number of cars you manufacture, I'm talking toy cars. I'm not rich enough to manufacture real cars, but as you increase the number of

toy cars you're manufacturing, your costs go up and sort of linear and there are bumps every time you do a new round of R&amp;D or something. Software companies don't do that, right? software

companies, you have this huge expense at the beginning to build everything up and then once you have that you your your your burn rate is relatively low. Selling 50,000 units of a software and

selling two lakh units of a software isn't going to make a material change in the amount of money you're investing every day. Right? That entire economy is now going to be gone

because you don't need that kind of investment in software anymore. And this has led to multiple real economies collapsing. Right? So I I'll give you two example just off the top off of the

top of my head. Uh web design companies there were thousands and thousands of them all over India. You know group of college students get together they say that look we'll build websites for

people. And these were all micro and medium industries maybe employing anywhere from 10 to 50 people. That economics is just gone. Right? We all learned when we were small right? What

what is the definition of economics? Economics is the study of the allocation of resources under conditions of scarcity, right? What if it's not scarce? There's no economics of air.

Despite the fact again that we're in Delhi, there's no econ. But similarly, now econ writing novels is gone. Right? You saw what happened with sea dance recently, just 24 hours ago. The movie

industry is worried. Who's going why should people invest in making movies if I can write you know what I want a movie like Sherlock Holmes but I want Salman Khan to be the main character and I want

me to be the side character and I want this to be the story make a two-hour movie I press enter movie is done right if that comes to pass then that that entire economics is just gone

right we have seen these are me talking about the future let's talk about right now right Now at this very moment a large portion of the internet is collapsing because what used to happen

is a large portion of the internet used to run on ads right so if I have a recipe website what do I do I put some ads on it you visit my website to read my recipe for blueberry cupcakes or

whatever and you get that ad displayed to you and usually per thousand impressions I get some money from like the Google you've you've seen that at the bottom of pages and so on right so I

get some money off of that. The problem is that now who's going to come to my stupid website? They'll just ask GPT or Gemini for that and they'll get it and nobody's going to come to my website.

Generally speaking, if you got your search engine optimization correct and you were on the first page of Google, your click rate was one in six. Okay, this was the official statistic. That

is, let's say I am a top blueberry cupcake chef. I don't I diff that's definitely not a thing. But let's say I am. I'm very proud proud of my blueberry cupcakes and I've made my website and

everyone agrees it's a great site. So when you search for blueberry cupcake recipes, let's say I'm one of the top 10. And so I would normally have because people don't just click one link, they

usually go to two or three. I would have a one in six chance of getting clicked and I would make some money off of it. That number in the past year has gone from 1 in6 to 1 in,500.

Right? This means that this is multiple orders of magnitude. So all of these websites that Chad GPT and Gemini and DeepSeek and all of these people, they got the

data from these websites only. But now no one will go to these websites and they're all dying. This is even true of open-source tools. So, Tailwind, which is a major CSS platform, had to let go

of a lot of its engineers because what's happening is these tools have eaten all the open- source code and then people are no longer going to the open source libraries to get it. They're just

saying, "Make me this thing that does that and it will do it right." Of course, there are positive sides. There are non-technical people who can now just say things to the system and it

will build them a nice little app, which is great. But simultaneously we are destroying much of the infrastructure and much of the information landscape that made this possible in the first

place. So we have to be exceedingly careful about that. Let me sort of poke on that last sentence that I said and I think that's a really important thing when we talk

about correctness, trustworthiness and all of this, right? Which is in many ways, you know, we had machine learning before 2020 also, right? We were doing classification, we were doing

all sorts of clever things. What really changed with chat GPT was that anyone could use it. It was the genius of the interface. You had this simple chatbot. You didn't need to program anymore. You

could just say things and it would do them right. And it is this ease of that interface which changed everything about how we interact with these powerful AI systems. But there is an inherent danger

in that. What is the danger? Well, we didn't build you know computer languages all their brackets and you know weird expressions. We didn't do that for fun. Okay, we could have had computer if we

could have written computer programs in English and have them run. We would have stuck with that only. Why create all of these complicated looking languages where if I miss miss a semicolon, my

computer's going to turn into a peacock, right? We did it that way because our normal language is too ambiguous. There are too many ways in which we say things where

we assume you already know what I'm talking about. It's too easy to miscommunicate. Right? The teacher told the student that he was going to the fair. Who's going to the fair? The

teacher or the student? Right? This is obviously a very stupid example, but we have thousands and thousands of ambiguities in our language which make it exceedingly difficult to

understand what the other person even wants. That's why we had computer languages in the first place to disambiguate. Now we are saying no need. I will just give the problem

description. This generalpurpose software is just going to basically custom solve it. Think about how useful your instructions are. This is deadly, right? We have literally got stories

about this, right? About how easy or hard our instructions are. We have cautionary tales about the genies and uh uh you know monkeys for story lines, &gt;&gt; Yeah. Yeah.

We We'll switch at 15, don't worry. Yeah. So, when we get to those story lines, we hear that someone says, I want to be the richest person in the world or I want to be the most beautiful person

in the world. And what happens immediately after that? It is it kills everyone else. And it says, I have technically correctly satisfied your query. Everything you said, I have done.

And so when we give a query, we want the machine to basically align with my expectations. That's what alignment that term means, right? That we wanted to align with my expectations of how this

stupid thing is going to act. That leads us to the following conundrum that I have the system. It's going to do certain things. I I worry that it may do certain bad

things. How do I define what is the risk of it getting into this bad thing and doing this bad thing? Do we have a clear way to define risk in our context? And for

that I'll hand over to Anard. &gt;&gt; All right. Uh I can take the clicker. So I will uh keep it slightly brief and I'm going to skip over some slides in the interest of time. We have looked at

different aspects. Three of us are at Ashoka. We work together on different aspects of uh safety risks harm and trying to quantify them and understand them. Uh this is just the map of India

part. I'll get back to India as a question. India is a big nation as we all know but there's a lot of technology and we have a tendency to solve our questions of scale using a lot of

technology that naturally introduces many challenges you on the fifth day of the summit I don't need to tell this to you all of you have seen different examples of how empowering this

technology could be and why it's important to be a bit skeptical about its deployment because that could introduce new kinds of risks but what is a risk to quantify that and

define risks and harms would mean different things in different contexts. Our goal as a team was to understand and try to make sense of these risks. So hard to

define one definition that we've chosen is that the probability of an undesirable outcome characterized by two things. The two things are its likelihood and its severity. Uh I think

the airplane example is just soon up. Okay, it's coming back. But basically uh airplanes are unsafe. All of you know that. Most of you also take airplanes. It's because the probability of

something happening is lower. That's where the likelihood comes in. But airplanes are dangerous. That's why we like watching aircraft investigations because of severity. Those two are just

oversimplifying what I mean here. These definitions also need to be grounded in context. Context such as where you're deploying these systems. So, education, healthcare, uh some of

the many areas that have been discussed in many of these panels and discussions across different halls here. I'm going to keep it brief. Uh but these risks go beyond hype. There are real

real challenges and the real costs that everyone has to pay when such systems are deployed at scale without taking risks into account. Uh some cut off but one example is from Air Canada. There

are many such examples of real people suffering loss of life, loss of liberty, loss of money and property because of AI safety risks. So we have taken a life cycle view of AI safety risks and tried

to create a taxonomy. It's a comprehensive taxonomy uh of 37 different kinds of risks risks. We have launched it uh earlier today and it's now available online. I'm just going to

give you a brief overview of the kind of work we have done towards that. Again, here are some examples of what is a risk in our definition or not. So, what is not a risk is physical destruction of

infrastructure. It is an AI related risk, but we're not talking about that. Our scope is very limited. Uh there are many global frameworks that talk about these kind of things. You

have some coming from Singapore in Asia. We have Europe. We have the US. But they do not take into account the main challenges that we see in India. India has scale, India has linguistic

diversity. But India also has certain problems like low network connectivity. If you for example are deploying AI in a space which is safety critical but you lose network and someone's life depends

on it then it could be another kind of challenge that has to be uniquely defined India. We see that many of these challenges are not covered in international repositories and risk

databases like these. So uh what they have is what we call contextual blindness where they are not realizing the social challenges and the social technological challenges. India

again as you know deploys large amounts of technology. We have larger technology systems than any country in the world. UPI EVMs Aadhaar are just simple examples of that. The safety risk

database that we have launched it's in partnership with Astep Foundation and it's called Astra. It's AI safety trust and risk assessments. We've tried to create a fun acronym that is easy to

remember. Astra is now formally launched. Uh some of us worked on it. Ana who's in the audience is also one of the contributors. It is a sevenstep process and maybe Anan

you could just quickly walk through this process and how was built. &gt;&gt; Yeah. Uh hi everyone. So both Dan and Aloc did a good job summarizing the overall work. So these are a bit of

technical details. I'll probably skip most of it. Basically what what is there to understand is that this if you think about it simply it's basically a risk it's a database of risks right but they

are contextualized in the Indian context heavily right so one formula fits all kind of a narrative does not work in AI safety this is what our claim is and this is in line with many researchers

right many prominent researchers so what we started with was resource identification and here's what you know it our work differs from many of the global frameworks that uh people have

built. So when it comes to resource identification, we had to actually do bottom-up research of how and where exactly these risks occur in the Indian context, right? Uh in we have primarily

education and financial as of now. But um we we started an exhaustive study of how exactly these risks manifest across sectors, right? And the final u uh step of this is uh a comprehensive risk

taxonomy and ontology. Right? So taxonomy is basically categories and subcategories of risks which you will find probably in many global frameworks. But what is there in our database is an

illustrative set of use cases right where you have a use case a risk use case which you can go and click if you are in the financial lending sector right you can go click and see what kind

of risk has happened in the Indian context exactly related to our language our cast our whatever kind of variables we care about right so these are some of the basic steps through which we have

worked on uh building Astra um so there are two parts to it very briefly so one is the causal taxonomy. So one is we we also tell you through this database at which stage the risk has occurred. So it

can occur during development. For example, bias in AI, we all know about it. It happens because of probably biased training data. That is one of the sources, right? So it happens during

development deployment. Let's say you take an AI system which was built in the US and you implement that or deploy that in an Indian solution setup where most of the people speak in Marathi, right?

This is a deployment problem, right? So it manifests in deployment and usage. I take the AI system. It was never meant to disseminate disinformation, but I did that as the user. I actually manipulated

it. So that in usage and then there are stakeholders. Is the AI system primarily responsible for the error or risk or is it that it happened because of a deliberate enduser kind of an action. It

also tells you about whether this risk is intentional or unintentional. Again in no way do we that this database is in any way exhaustive or foolproof right it is currently uh you know advancing it

more and more expanding it to other sectors but the target is to also tell you about these granularities around risk because risk is not just one term like Alo explained they buy explained

right you also have to look at what is the intent behind it so there are two main categories of risk and this is the part that we struggled the most about by the way this Astra you

you should It's currently available on archive and you can probably go and read this paper and you can also take a look at the database whose link is present in that paper but this work took us almost

6 months and again Ana if you could wave so Ana is a primary contributor of this uh uh risk database that we formed and so we c categorized after you know uh looking at the type of risk there are

social risks which are easily quantifiable which you can easily observe for example linguistic bias an AI system trained in English does not answer Hindi queries that well. So this

is a typical risk which comes under social, right? Frontier risks are risks which are very very difficult to observe, right? There are risks that we know could occur. Tomorrow AI could

replace jobs. We all know about it. But how do you quantify it? I mean in many of these risks have haven't even occurred in the Indian context. You know about it because from some remote

western translation you could translate it. We know there's a gut feeling that it might go wrong but we don't we can't quantify them very easily. These are the kinds of risks which come under

frontier. So there are some examples here. I'm not going to the details in the interest of time but there is bias and exclusion toxicity. These are all risk categories right and then in

frontier you have mostly around power seeking an AI system going rogue. I I'll just quickly cite an example right? uh I'm not naming the firm but there's there's this uh uh news on a trading

firm which applied an AI system to do quick trading according to market variables right the AI system performed very well initially and then without the consent of the firm and because they

were not monitoring it properly it went rogue it started doing transactions which were extremely lossy and not just that in a very high volume it started doing that right so this is theam

typical example of power seeking now in India well there might be some examples uh abound but then do you really know whether this kind of a risk can be easily quantified we don't know what

will happen we'll probably deploy and we'll have to watch so those are the kind of risks that we have listed in frontier risk one quick example is also human computer interaction right so we

all know ex I mean I sorry there's a student sitting here but I'm going to say this but in most universities okay students are using AI and we know that it leads to cognitive decline and lack

of critical thinking but again how do you quantify it right it's very difficult difficult. So these are frontier risks, right? Um I'm not going to the details of this. You all know

about cast bias, linguistic bias of AI systems, hallucination we all know about, right? Incorrect outputs by AI and then infrastructure exclusion. So this is one critical example and this

was a came up from a discussion with uh uh the extreep team that let's say there's a AI system that you deploy and a farmer is trying to use it in many regions of India there are connectivity

issues right there is an internet connectivity issue and the entire app starts loading loading and buffering it doesn't work right now this is a typical example of infrastructure exclusion so

again remember the stage of error manifestation is the deployment chat GPT or any open AI for that matter will not care about this. It's not their job. It's our job when we are deploying it in

context. It's our job to take into consideration that our connectivity might be poor. Right? So this is a typical example of uh some examples of social risks.

So quick uh this is one one reason why these social risks manifest at this level. Uh you know with as as you go higher and higher models they have more persuasive power. So they can manipulate

you. uh frontier risks I already spoke about. I'll quickly move on to mitigation. The one quick point I want to make about mitigation is it's an extremely challenging task. So while the

database is the first step as per our AI safety risk framework of Astra, mitigation as Debayan adequately pointed out is the hardest task that we have at hand. So these mitigation measures are

often not effective. They are very context specific and uh there are certain kinds of mitigation measures that also lead to loss of utility. So we have to be super careful about that,

right? You put a very strong mitigation measure but then that leads to lack of utility on the on the users's front. That is not a very good mitigation measure contextually speaking. So

according to this uh work uh you know what we want to carry forward is you know we want to empirically ground these risks going forward. What is the probability of risks really? And finally

we are also trying to include more and more domains. Currently it's on education and u um financial lending. We want to expand it very soon to agriculture and more many more Indian

domains. So this is a quick uh infographic that you can um you know probably quickly look look at and this is basically this defines the categories and

subcategories. It's also there in our paper. So uh with that I think uh we are mostly done. Uh I really implore you to please go and read our paper. It's on archive. It's called Astra. It's the

first comprehensive AI safety risk database completely grounded in Indian context and it's a formative step. We are constantly developing it. But thank you so much for taking out the time and

listening to us. YEAH.
