# Beyond Technicality: Multidisciplinary AI Governance for Real-World Impact

**India AI Impact Summit 2026 ‚Äî Day 5 (2026-02-20)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 10:30 ‚Äì 11:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 9 |
| üìÖ **Date** | 2026-02-20 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/y2fpcrkesGw?feature=share) |

## üé§ Speakers

- Dame Wendy Hall, Web Science Institute at the University of Southampton
- James (Kayliang) Ong, Artificial Intelligence International Institute (AIII)
- Jeanna Matthews, Association of Computing Machinery
- Jibu Elias, Independent
- John Harvard, Tony Blair Institute for Global Change
- Merve Hickok, Center for AI & Digital Policy, AIethicist.org
- Neha Kumar, School of Interactive Computing at Georgia Tech
- Quentin Lambert, UN ODET
- Sara Hooker, Adaption Labs
- Virginia Dignum, Association of Computing Machinery
- Yannis Ioannidis, Association of Computing Machinery

## ü§ù Knowledge Partners

- Association for Computing Machinery

## üìù Summary

This interactive roundtable broadens the AI safety conversation by foregrounding societal, institutional, and ethical dimensions beyond purely technical approaches. It treats AI governance as a multidisciplinary, context-aware challenge rooted in real deployment settings and community experience. Through brief interventions and guided discussion, participants will identify shared priorities and co-develop principles and collaboration areas to embed interdisciplinary expertise, civil society voices, and context-sensitive evaluation into national and global AI policy frameworks.

## üîë Key Takeaways

1. This interactive roundtable broadens the AI safety conversation by foregrounding societal, institutional, and ethical dimensions beyond purely technical approaches.
2. It treats AI governance as a multidisciplinary, context-aware challenge rooted in real deployment settings and community experience.
3. Through brief interventions and guided discussion, participants will identify shared priorities and co-develop principles and collaboration areas to embed interdisciplinary expertise, civil society voices, and context-sensitive evaluation into national and global AI policy frameworks.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/y2fpcrkesGw/maxresdefault.jpg)](https://youtube.com/live/y2fpcrkesGw?feature=share)

---

_[‚Üê Back to Day 5 Sessions](../README.md)_


## üìù Transcript

all the speakers, the ones in the first and the second session. So, if you just want to stand here in front, they want to make a picture of all of us and then we'll start.

Okay. Uh yes, you have to sit there. Okay. Good. Uh good morning everybody. Thank you very much for being here. My name is Virginia Dignam. I will be co-hosting this session with my

colleague Gina Matthews there. We both are the chairs of the uh technology policy council of ACM and today we are here to discuss how to move beyond technical safety uh and looking at

aspects of multiddisciplinarity, governance and real world impact across uh global AI discussion. Safety is too often being framed in technical terms. Model alignment, red teaming, benchmark

performance, frontier containment and so on. These tools matter and they really are f their f I further development is crucial but they don't address the core question or at least one of the core

questions. What determines whether AI systems produce human and societal value or harm in real deployment contexts? That's what we are going to discuss in this session. AI systems like we all

know do not operate in isolation. Their impact is shaped by deployment context by governance capacity by incentive structures and by the lived reality of the communities that use and are

impacted by these systems. As such, AI systems do not fail simply because of flaws in the model architecture or in the data or in the alignment techniques. They fail or they produce harm because

they are imbed in institutional, economic and political systems. So we will have a open discussion with the panelists. It will be two rounds of panelists and I would like to start by

inviting Dr. who is the chairman of the board of the National Institute of Information and Communication Technology in Mosmbique where is at this moment leading the

national strategy on AI for Mosmbique &gt;&gt; please. Yes. &gt;&gt; Thank you. I would like to start by thanking the invitation to join this panel and also to congratulate the

government of India on hosting this AI impact summit. Uh going directly to the topic part of this panel as part of our exercise of crafting the national AI strategy. We look to this topic of

safety and for us safety working as a polit for the policy subject and from the policy formulation point of view for us safety uh we look it as the protection of people not only systems.

So we look AI governance must priorize prioritize human social and institutional impact going beyond technical metrics such as robustness accuracy or algorithmic alignment. We

also look it from the multid-disiplinary governance grounded in real world context of use of AI. Uh for us effective AI policies require input from law, social sciences, education, labor,

ethics and affected communities. So the inclusion of the people and how they will feel safe in using these technologies. We look also from the continuous human oversight and

institutional accountability. People must know what's in the black box. how they has designed if the factors that are affecting their lives the decision made by the algorithms uh have been have

taken into consideration their feelings in the design phase. We also look for the protection of children uh young people and women from the studies that we conduct the women and children and

youth are the first victims of the bad application of the bad application of the AI. We also look for the ethical and social assessment. Mozambique is one of the pilot country adopting the UNESCO uh

principles of ethics in adopting AI and we are looking also for the dimension defined by UNESCO in in this perspective uh sharing what we are doing in the country. Now we in Mozambique we are

drafting as I mentioned our national AI strategy with the support of UNESCO and the thank professor Virginia who is the leading expert in in our team but of contribution of other expert from

UNESCO. uh we are also crafting our data policy and its implementation strategy because we believe that data is a fundamental element for AI system. We are uh reviewing our national cyber

security strategy because data that we're collecting now is that the already cyber security related problems by the use of young use of of of AI model. We just adopted in Mozambique the

regulation for the construction and operation of data centers and also the regulation for cloud computing because we believe that infrastructure is a fundamental and key element for

sovereignity of our country in terms of when it comes to safety but from the policy point of view for the democratic system and all other dimension uh but we also look it from the digital government

point of view so we are reviewing also our interoperability framework that's related to data to make sure that in adopting uh AI in the public administration we address the our main

objective of improve efficiency and efficacy and delivering public services. Uh for us these are the elements that will be contained in the overall digital transmission strategy that if everything

goes as planned will be approved by our government during this this year and we are learning a lot in this summit to and gathering important elements that will help us to uplift and improve our work

in crafting this element. Thank you for the opportunity to to be part of this session. Thank you very much Dr. Shiman. Um I understand that you have to move to

another session. So feel free to leave whenever you need to go. We understand the complexities of program. Now I would like to ask Dame Wendy Hall regist professor of computer science, associate

vice president and director of the web science institute at the University of Southampton and also a former member of the United Nations high level expert advisory body to give us some

provocative statements. &gt;&gt; They will be &gt;&gt; good provocus. I'm fed up with just towing the party line. So, I will um I have to first apologize because I have

to leave at 11. I'm supposed to be on three panels at the moment and I also have a lunch date at midday uh in town. So, that's my morning. Um I want to say I

think three things. One is what's really well four if you know Montipython nobody expected the Spanish Inquisition anyway um this so first of all it's been

wonderful to be in India I love India and I have a lovehate relationship with this summit it's too big there's too much going on and not enough actual real dis debate about there we the core

there's going to be some sort of platitude statement come out today. Yeah. And I'm I've just been come back from the UN um our advisory board and the new scientific panel get together.

They've got a panel going on at the moment. The dialogue that's starting in the dialogue that's starting in in the the AI for good conference in Geneva in July we hope will be a real d

Don't know what form it's going to take yet, but we have to get we have to knock the world leaders heads together. Now I'm now going to say something which also really struck me.

Thank you. Is that working? Yes, at this conference everyone's I love you know in in India AI means all inclusive but 50% of the population weren't included yesterday the women

right that there are no women the CEOs of every country every company there was one lady CEO from Accenture I think there were a couple of ladies on the panels at the end was all men, the alpha

males of this world, men, right? The world leaders that spoke, the CEOs that spoke, that this world is dominated by men. And my mantra has always been in terms of the the lack of women and other

some other diversity points as well, but m mainly women is if it's if it's not diverse, it's not ethical. People don't really understand what that means. That means is if you haven't got

a diversity of people discussing a problem, how are you going to actually sort out the biases? If you haven't got women at the top level making these decisions trying to set up

the guidelines I mean your comment was, yeah, we we want to make sure for the safety of women and children. Well, let's include the women and children in the discussions. I mean the that my

third point um is that we we are watching I mean I'm very into watching these um experiments. I did it all through the web and we need to learn how to monitor what's going on so that we

can say what is the right direction to go in the future means collecting data and evidence and doing longitudinal studies. Um and it takes time but take for example what Australia is doing with

social media we've heard at this conference several other c teenagers several other I mean didn't Macron who was there yesterday Macaron said under 15 in France our prime minister who

constantly changes his mind so I don't suppose it'll happen but he's uh he's talked sorry that's a joke for any Brits in the audience but there aren't many um he he's saying 16 in the UK some other

Spain saying What? There will be unintended consequences of that. Making a ban like that without thinking about the nuances of well, what happens if um well, first

of all, the kids are ingenious enough to get around it. And then they're back on the dark side of things again, even worse than before. U because they're doing it in secret.

Um what happens when they start to use social media? How do we train them to do it properly? My worry about a ban like that I mean it's very brave of Australia to do it first and we can watch and

they're saying six months time they'll have some evidence of how many under 16s are still on social media but the behavioral issues take much much longer to explore than that. And we have to get

over this fact that whilst the technology is going on a pace because the alpha males are driving it without any you know just worrying about technical safety maybe um we have to we

can't say well it's all going too fast we can't do any we have to study this stuff um we have and I think this is what I want the ACM to do I talked at my keynote talk this is my

last point by the way I my keynote talk on whatever day it was Wednesday um on the main stage I talked about two things happening in the UK and actually around the one is our national physical

laboratory which is the equip sort of equivalent of NIST in America um has just launched with government backing a center for AI measurement and the AI security institute in the UK

um and the other security institutes that are growing up around the world. That network is now being called largely driven by the US because Trump doesn't want to call it anything to do with

safety. But I can't believe I just said that. Anyway, but then he was the man that drank bleach in COVID. um they're calling their network

the network for AI measurement and I think this is a breakthrough. I think this is I mean I love AI for science but we need to think about the science of AI and I think and that's a

social it's a social technical and I'm started to call these things social machines as we did on the web that came from Tim Berners the idea of technology and society coming together to create

artifact systems that wouldn't have existed if they had come hadn't come together and the technology doesn't understand society at the moment and society most of society doesn't

understand this technology but together those two systems will create sociotechnical systems or social machines and I want to build a science of studying social machines and it will

be called AI measurement or AI metrology I love that word I've leared to say it is of course it's Greek everything everything's Greek to Arian is I love the yogurt. Don't you love Greek

yogurt? Oh, so sorry I'm finishing there really. AI metrology and we're going to launch an a I'm chair of the ACM publications committee or co-chair he's president.

We're going to launch a journal first journal in this area and start pull and it'll be associate on pulling together work in and the data sharing the data that people are collecting to go with

it. That's me done. Thank you. &gt;&gt; Thank you, Wendy. Very important point. And uh I think you can leave it there. Again, we fully understand when you when you have to leave, you just just leave.

We understand that. So for the the rest of us in the panel uh we start the the day or the the session talking about AI safety needs to be more than just the technical robustness. I love your idea

of the the social machines of this AI methology. &gt;&gt; It just rolls off the tongue. &gt;&gt; Yes, it is. It does. Yeah. &gt;&gt; With me only sometimes probably but I I

did my best now. Yeah, methology. Yeah. &gt;&gt; Anyway, uh I would like to uh bring you into the discussion. How can we both Dr. Shiman and Wendy gave us uh examples of issues that we need really to include in

going beyond this idea of technical robustness. Um even if systems perform exactly as they have been designed and safely designed they will still probably be co causing harm uh which is not

probably just a technical fa failure but also a failure of inclusion a failure of imagination. So I would like to get your opinions from where where you think that we can change the the disc where can we

start changing the discourse of a pure technical approach to a broader inclusive societal institutional approach to the discussion on AI safety on AI measurement and so on. And I would

like to dis start this question which is for all of you starting with professor Yan Yanis Yanidis who is the current president of ACEN and also a professor at the University of Athens.

&gt;&gt; Thank you very much uh uh for having me in this panel. Um uh I'm a technical person uh very sociable but technical that that's that that that's the the my expertise.

So I want to separate uh the issue of um uh uh safety of AI uh and talk about safety of AI use. uh and for me in in my technical mind there is the techn AI technology which

is the algorithms which are the models uh and so on from the use of this technology the use of the software that is on AI and and and we are using this software both in the beginning with the

input that we give it and at the output when we create what is called I have an artificial intelligence I have an agent and so on to do this or that or the other. The technology

is there's no issue there's no social issue in the safety of the technology itself. It's like the car whether it's working or not. There is no issue of safety and innovation in that regard has

to be let free like the human mind and all the innovators to to to progress on that and robustness and not having bugs or not bugs are an issue there but it's it's a day in the

park for us software engineers and computing uh scientists. The use is the important thing and sometimes the key thing that people are talking about is the end result, the

model. Uh we put it in in in the judg's hands, we put it in the doctor's hands. Uh we put it in the uh in the youth's hands uh uh in terms of social media and so on. This we have to work on, measure,

regulate potentially and and in any case all sciences like um it was said before especially the humanities, philosophers, ethicists, legal people uh um cognitive scientists and so on have to come

together to address this. But there is also the input side which is again humans doing it. humans are determining the first parameters where the the systems are are starting to to be

trained. The data that we feed it, it's again humans that are choosing it. And as much as we have to regulate or measure or think uh the end result, the model, the humanoid or non-humanoid

robot uh that is telling us do thing or or that or or the agent. And the same level of importance is that we have to think about what to do with what comes in and humans are using it. Different

humans are feeding it and I think the safety must start from there. We should not grow the input size. We should not let it run for free. Even at that level, we have to have the different sciences,

the different technologies, civic society to be represented there. and having an AI with whatever data we happen to have or whatever data generates billion dollar industries.

These are the data that that we'll use. It's wrong. I mean there is a right and wrong here and and we have to be on the right side of that. So as as a quick uh uh uh wrap up but um so for others to

express their opinion, technology should be running free but both input and output end result should be in the hands of all of us and not the hands of the few.

&gt;&gt; Thank you very much. Um just um turning now to Sorry. &gt;&gt; Yeah. Thank you, Wendy. Thank you, Dr. Shiman. &gt;&gt; See you soon.

Okay. Okay. Let's continue the discussion. Sara Sarah Hooker you are the co-founder and president I believe of adaption labs a very young company I believe uh you

have been before with coer and with other developing organizations what do you think about this balance or tension between the technical robustness

the technical safety measures and the need for understanding more the the environment, the context, the social context in which systems are built and how can uh we technologists uh those

that develop like yourself be developing systems while they are aware of this type of tension and also the ins insertion of the systems in a very concrete real world domains.

&gt;&gt; Hi, so it's lovely to be here. I feel like we got off to a bombastic start to the panel. So that's fun. I feel like more and more people have trickled in as we've gotten more spirited. Um so

Virginia is right. My whole career has been building large systems. I was at Deep Mind. I did um I led the research arm at Coher and now adaption. And typically it's been how do you build uh

extremely large systems at the frontier of what's possible. I think it's interesting. I'll share a few things. So one, I think what Wendy was getting to is that one of the biggest signals of

whether you actually care about safety is uh what the forums of prestige and power look like. I think that's mainly her comment. She's saying, you know, we are at the pinnacle of where we all

gather to discuss these things and the way resources have actually been allocated doesn't show that people are serious, which I think is fair. I think you have to look to the surrounding

environment to understand if people are serious or not about safety or whether it's just a panel title candidly and maybe today it's just a panel title. Um I think in general my philosophy about

these forums is that you have to look six months out to actually get a signal of the what has happened. Um that doesn't mean that they're not critical. I frankly don't know if the expectation

should be anymore that we have universal rules for AI. It's not clear to me that that should be the outcome of these forums. So I think decidedly if you're going in with that expectation, you're

going to be very disappointed because I don't think that's going to happen at this forum or at the next one. But I do think it's worth asking well where are we going as a conversation about safety

and the precision of it because for me that's the most interesting part. Time is very valuable. It's our most precious resource and so for me the more precise the conversation the better. I do think

if I look at the overarching arc from Bletchley to now we've had now four summits we'll have the fifth it's worth asking has it become more precise candidly and thank goodness yes I still

remember Bletchley where it was all about existential risk and six months from now and there were protest and hunger strikes from people who thought machines were taking over but no

precision to the conversation no um accountability for where these timelines were coming from and then I look to now and now we have a very messy conversation about safety. Certainly

everyone has a different view. It's still a blanket term but at least it's more accountable to what is the real world impact of these conversations and the technology that we build because

when I started my career as a computer scientist we were just in research conferences. I mean I think the fact that ACM is so well represented on this panel speaks to the origins of like you

know very narrow group of people who work in a very academic community and now our technology is used everywhere. So it's a much more important conversation to have. So one I think we

have got a more precise but it's still very murky what people mean. Here's the other thing I'll say. I think there's often uh desire in these conversations about where technical meets the

ecosystem to say, "Oh, well, safety has to be everything to everyone." And frankly, that's not a precise conversation either. Because the truth is there are trade-offs. When you build

systems, there are trade-offs. And too often when these conversations enter this arena, there's a misconception about the sheer difficulty of how do you actually impose constraints on these

systems. So the other thing I'll say is um the biggest thing that has to come out is an understanding of what you give up because you give up something. The the big things for me are you know I

work a lot on language. My big ask is just report what languages model providers cover. Uh report essentially like what they say that the safety parameters are not and report what they

don't cover or they haven't tested for. This sounds like a simple ask, but I think this is actually quite precise and what it establishes is what have we given up? What what are you confident

about? What have we given up? There's many versions of this, but too often, and this is my ask, in conversations like this, we end up just circling around and saying we want safety. We

need perspectives of everyone in the model. And the truth is that's also a naive statement because it is almost certainly the fact that there will be some tradeoff. Someone will not be

represented. someone will be represented and actually what I think these forms are very useful for having us all in the same conference is about galvanizing ecosystems where you can make your own

constraints and tradeoffs but also having a discussion about you know for the models that have been shipped that serve billions of people we have these static monolithic models that are served

the same way what are the trade-offs that they have made you know and that's you know as someone who's built these models there are almost certainly trade-offs in play so we need to

understand the state of the world as well as where we want to go. And it's okay if um if there are clearly, you know, things left out. It's more that they have to be stated out loud. That's

my wish list. Yeah. So maybe I'll leave it there and I'll pass it on. I think you are next. Go for it. &gt;&gt; Thank you very much. Thank you, Sara. And indeed. Um next one. Jibu Elias, you

are a researcher, but you are also an activist who ex who examines how technology and institutions. reite recite knowledge, labor and legitimacy. Uh so help us making sense of what it

means safety AI safety for society. That seems to be what you do. &gt;&gt; Yeah, I mean I was most interested in the real world consequences of the panel title. But uh wonderful conversations by

Sara and uh Wendy and all here. Um so I when I look back how technology has shaped my understanding of the world. Um I feel like an idiot because I grew up in a time you know watching this

animated shows like Jetsons and all these futuristic shows believing that the more advanced the technology gets the better uh it gets the it the better our world will be. you know there you

know you know I grew up as this idealist kid who thought oh you know when AI comes there will be no inequality you know that's what I'm saying I was a naive kid back then and nowadays when I

look at these things I mean there have been phenomenal work done by computer scientists like uh you know people present here in panel s and everyone right on technical aspects of thing but

more and more we are seeing AI now becoming more political it's becoming a larger you know sociopolitic construct in general and what concerns me more is its exploitative and extractive nature.

Um I think uh Sara mentioned about the uh uh Bletchley and where you know the talk was all about you know existential risk but now I think we are all at a point where we are all agreeing that the

accumulated risk have become more worrying uh at the same time I've been tracking people who've been using tools uh you know people who've been impacted by and those who were um excluded from

the benefit of this kind of technology right um if you go around states like uh Telanga Ghana, Chhattisgar, Jark and there are big group of tribal population you know their language is not

represented in Germany or anything right and I know everybody wants to impose Hindi on all of us but sorry I still you know Hindi is not the national language of India uh but uh what about them how

do they get access so more and more what I see what I'm seeing is the divide between the socioeconomic divide becoming more wider especially in countries like India and you know it's

fascinating that you know we've been celebrating the data centers we've been building and I you know I had firsthand experience of a data center uh that's very much celebrated in Telangana uh in

a place called make a good I'm not don't want to mention the company associated with it but the how it was built how the people were manipulated how the groundwater being extracted right in a

place where there is a water water scarcity uh you know and and when I I asked the company you know hey this happened you know I have a close association with that organization and

they said we interacted with the community leaders so what I did I reach out to the punch bino punch he has no idea what they mean so essentially there's a lot of you know I mean in

India we know what that means reaching out community leaders bribing the politicians uh but that's the larger things I'm worried about and the people who are using this technology you know

now some people are talking about terms like AI psychosis I don't know how valid those terms are but uh it's fascinating to see that me and my executive director of Musla Foundation has been chatting

about how elderly people are using these models. It's very fascinating and it's it's worrying at the same time you know we we often put our attention on young younger folks but uh I mean it's it's

funny at the same time but still so my larger question is why you know the going forward like yesterday the gentleman from US was uh telling that you know we should everyone should use

the US AI stack um you know I think you know people in Denmark it will be a good idea you know how US rates it's uh strategic partners. Yeah.

Yeah. So, uh so where are so my larger question is where are we headed right? Are we still going to have this extractive nature you know the data annotation workers uh who built building

these models right so I will stop here uh and uh you know like looking forward to the next level of conversation. &gt;&gt; Yeah. Unfortunately uh we have our second round of the panel and like all

the what we all are complaining about it will happen. We all say our thing and the dialogue will uh will be need to be done outside in the corridor and we really hope also to after this meeting

try to combine all the what has been said in some kind of um ask or report. But anyway, now we are moving to the second part of the panel.

You are here. And then we have Roxy for John. We were all going to be in the same panel, but there weren't

enough chairs, &gt;&gt; so we're splitting into two. &gt;&gt; Patience with us. &gt;&gt; Okay, everyone. Uh, thank you so much for being here in the second part of our

session and thank you for all of the panelists who are joining me here on stage. I think we're going to do something a little different than the first panel did. I would like everyone

to just quickly introduce themselves. Um, Niha, would you uh start? &gt;&gt; Hello, check. Okay. Uh, hi everyone. I'm Niha Kumar and I'm an associate professor uh at Georgia Tech in the

school of interactive computing. I'm also uh president of the special interest group on computer human interaction. And so uh this summit is um is really a coming together of many

different worlds. For me I actually I I grew up in Delhi. So uh it's been uh about coming home but also uh a lot of the conversations we've been having are conversations that are really uh very

very active right now in the discipline of human computer interaction HCI as some of you might know it. And uh it's great to see how central um human centricity is to to what we've been

discussing. And third, something that's been much closer to my own area of study is really looking at uh HCI and technology use in the context of social impact. And the this has been named uh

in many different ways over the years. Social good, social impact, societal impact, um public interest, you know, whatever you want to call it. But really it's an area that that we've been

studying for many many years before AI was on the scene. And so um I would say that we're looking at multi disciplinarity in this um uh in this panel. And to me there's there's a lot

of uh learning that could be happening from from many of these disciplines that have been actively looking at some of these agreed that the platform that we're looking at is different uh is

unprecedented in many ways. And at the same time there's a lot that that we have to learn from as well. So I'll stop there and &gt;&gt; thank you Niha where

&gt;&gt; thank you Jina uh Mar Hook. I'm the president and policy director for center for AI and digital policy. We are an independent think tank working globally uh at the intersection of AI policy and

human rights democratic values and rule of law. So I would like to take a more expensive view of uh safety uh and governance at large but more to more to come on that.

&gt;&gt; Thank you Rasmus. &gt;&gt; Yes, &gt;&gt; I think this works now. Yes, my name is Rasmus Anderson. I work with the Tony Blair Institute of Government where I

advise uh leaders around the world at the prime ministerial or presidential level but also at the at the line minister level on navigating AI. how what does it mean for them how uh they

both deliver results to citizens with AI and also avoid harm to their to their citizens and so the question of safety uh comes up uh a lot but it's also usually not the top of of leaders minds

and it's really about for me uh helping them often realize the long-term best interest informed you know self-interest of of what we'll actually what is the world likely to look like in 2030 30 in

2035, how can you best uh make sure that your country and your constituents and citizens are in the best possible position as the world will change very rapidly?

&gt;&gt; Thank you. &gt;&gt; Thank you, &gt;&gt; Tom. &gt;&gt; Is this one working? &gt;&gt; Great. I am not James. Uh I am Tom

Romangh. Uh I'm the director of policy for ACM where I help manage uh the policy committees uh which uh Gina and Virginia uh chair our global committee. Uh we also have regional committees

across the world including United States uh Europe, Asia, India, um Africa and the APEC regions. So what my job at ACM is is to help the computer science folks translate their recommendations on harms

or issues that they see in the technology to policy makers and engage those policy makers on behalf of ACM. Um so before that I was at a think tank in Washington DC. So I worked with Congress

and uh have been working in tech policy for many years now. &gt;&gt; Okay. So, in the interest of time, I'm going to get right to a very uh provocative question, which is we've

been seeing, you know, wellness for all, happiness for all in the presence of a fairly um extractive and exploitive uh potential. Does history tell us that it's going to

be great for everyone just works out? Uh that uh or there have to be some musts, not just good intentions or shoulds. um if we are not seeing things like um recovery uh retribution uh renumeration

we don't see people going to jail when they do bad things with AI are we serious about AI safety &gt;&gt; I'm happy to start &gt;&gt; um so no history does not show us that

it's going to be cool and history is definitely uh not a good indicator uh which means that we need to fight harder uh this time around and try to uh get that level up. Right? So history is

always a story of the powerful of the winner like who gets to decide the narrative and we are seeing that uh again today the narratives around what is safety uh what should be the

evaluations where should the money go uh where should like whether we should regulate or not whether should be it should be should or must uh is always the narrative of the powerful and uh

Wendy uh Dandy Hall mentioned the representation was very much the same kind of people uh throughout the conversations higher level conversations yesterday. So I think the first and more

foremost uh the the narrative needs to change uh in in safety as well. So so far it has been I think it's been an evaluation but so far uh the most important safety issues has been around

nuclear, cyber security, chemical weapons etc. Yes, they might be or existential risk which is another story. Uh yes, maybe we talk about those, we should talk about those but there are

real consequences right now on people's rights, freedoms, ability to live uh with their dignity uh and people's rights to participate in democratic processes. All of these are undermined

and as an organization where those three issues are like in our mission uh we are seeing this uh more and more uh under pressure. So this is the time to get up get your voices up as citizens as

consumers as professionals in your own right and try to change the narrative because otherwise it's going to just be a repeat of history. Well said, uh, Nha.

&gt;&gt; Yeah, I think uh, coming back to something that Wendy said, right, about being all inclusive at the same time as having no women around in decision-m places. I think that that is something

we should really be thinking about. I mean, do we have a history of being inclusive? What inclusivity have we been practicing in our innermost circles? It's easy enough to say that the poorest

of the poor should have access to this AI, but how are we doing on on being all all-incclusive? So, I think there are lessons from disciplines such as feminists and women's studies that we

can learn from to really ask uh the who questions who is making decisions, who is being benefited, who is part of the design process. That's one. Uh second I would say in in learning from design

which is which is one of the the disciplinary um uh disciplines that I've trained in uh thinking about zooming out is great and that's where we have value. So we talk about inclusivity, we talk

about um diversity, we talk about all these great sounding words, but then when we zoom in uh what are we actually doing? And I I think that a lot of the dialogue that we've been having is in

this disembodied state where we talk about infrastructure and we talk about data and we talk about interoperability and we talk about processes but who is benefiting uh and you know the panelists

before me also talked about um aging so people who are uh more vulnerable uh where are they in the conversation and lastly uh with regards to uh development studies thinking about um uh what are

the benefits of development really like we want development and impact and that's what we're talking about here for five days at the summit but um we know from from historical perspectives that

development hasn't worked out so well for so many people and so many countries across the globe and how are we making sure that we don't repeat those same mistakes and I think these these have to

be very much u part of the conversation so that it's it's safety of of the human of the body of our values of um just our our communities our structures social structures that are so critical to us.

&gt;&gt; Yeah, I think we are we're not seeing people go to jail. I I'm not sure we have seen something just as of yet that really where where that that's the case. There are there are lawsuits ongoing on

um suicides uh among young people, etc. But um but I do think that we will see a moment pretty soon where something does go pretty wrong and then we're going to have a decision on what we do with that.

Um you know, some people, this is a very dark parallel, some people said we needed to have World War II, to have the UN and and other systems that were put in place to to avoid that happening

again. And um yeah, and I I think it's a it's a matter of time when when we get something and and uh and and we will have to to make those decisions. And currently, you know, I think uh I'm not

super confident that we will interpret those events correctly, that we will, you know, have a realistic view of what might change and how we might prevent them from happening again. And you know,

it could be it could be, you know, people leveraging them, organized crime. It could be, you know, I mean, we've we've had very recently these uh where we've successfully had Elon Musk uh and

uh and Grock stop allowing people to create, you know, non-consensual uh deep fakes of of nudes, uh which had happened in in the millions. So, we have sort of small those that's not small, but we'll

have much bigger things than that. And um and I do think still when that happens, we will have to think about both pros and cons and costs and benefits. When we regulate things, we

don't regulate risks down to zero. You know, when you get into a car, there's a risk something will happen, but you still need to get places. And and I think it's uh with safety, we do have to

take some of the same lessons as as Maria mentioned from from nuclear, from uh flights. You know, it used to be that when you got on an airplane, you know, something like 200 or a thousand more of

them crashed than than today. And and we've reduced that level of of risk very far down. And I do think that the political level while we need technical inputs the only uh the only force in the

world that can really take all those considerations together and think about the partial perspectives that technical people have that civil society has that uh industry has really only the only

place that comes together imperfectly is is at government and that's why it's so important that we are here however imperfect these summits are. Tom.

&gt;&gt; All right, something a little different. I would like everybody in the room to raise their hand if you think safety is an important aspect of the AI deployment.

Great. Keep your hands up. Keep your hands up. Now, take your hand down if you think that safety should be enforced on the output of AI outcomes. Oh, wow. Okay.

Take your hand down if you think that laws should apply to the outputs of AI rather than AI itself. Okay. All right. You can go ahead and put your hands down. Wasn't as dramatic

as I thought it would be, but um so I'm going to talk a little bit about the 4951% rule. And across all political spectrums, no matter where you are in the world, there's this idea that you

only need 51% of the political willpower to start passing regulations and 49% won't get it done. It applies in the business world as well. You have 51% of the board control or equity in the

company, basically control that company, right? lobbyists have a extreme incentive to not push anybody past that 51 or 49% in order to have an action in the political

space. Right? So across all of our governments in here, there's, you know, private I don't want to say private sector because they're important, but there are private entities that would

like to have in action in the regulatory space. And it's not until 51% of those politicians or that political regulation gets to that threshold that you'll start seeing some changes. And so you you see

examples of that with the example that uh my colleague here mentioned with you know um deep fakes or or nutification applications uh causing worldwide outrage and you started seeing

governments across the spectrum say that's something that at least 51% of our population does not want and so they start moving towards regul regulating or enforcing current laws to punish that

kind of action. And so I say all this because there is also this conversation around moderates, right? We don't know where the technology is going. We have computer scientists. We have civil

society screaming about the need for for action for security within the the stack, right? And the rest of the world are moderates. They're still engaging this AI. They're still figuring out what

it can be doing. And it's not until some kind of action happens, some kind of consequence, some kind of issue happens that people wake up to to the folks who've been screaming about it for

years. And so what I encourage everybody in here is not be a moderate. Pick a side and start encouraging your politicians, your family, your your um you know uh community.

Educate them. figure out ways to to communicate the you know very heady technical aspects of you know security within the AI stack to the common person to the person who can understand it and

that's when you're going to start seeing the regulations start to roll out. &gt;&gt; I think that's a great place to end because I think um we are not going to get uh happiness for all and wellness

for all unless we insist. We're all going to have to insist. it's not going to come automatically. So asking each of us to ask ourselves a question, what are we going to do to

insist I think is a really uh good place to end. Um I think we started this session a little late, but I've been told that they would really like us to try to end on time. Um so I think I will

leave it there, but we would love to uh engage you in conversation uh out in the hall after this session is over. Thank you to all the panelists in the first uh session and also um all of us up here.

Thank you so much. &gt;&gt; Thank you all indeed. I think that there is actually time if I look there to have one question or two questions maybe one now there are too many questions. I have

to to vote. Okay sir there and the lady there. &gt;&gt; So I I would like to A very short question.

&gt;&gt; Yeah. &gt;&gt; One &gt;&gt; I would like uh not it's not a question it's a suggestion to the gentleman uh who has beard on on that side name I

missed. &gt;&gt; Yeah. Ju that go get some life at seram uh setup your agenda of Hindi and other language is going to die very soon. So you have to get some life uh of uh that

Hindi imposition and all those thing. Nobody will impose down the line few ones. &gt;&gt; Yeah, we will we will speak uh each other's language very easily.

&gt;&gt; Go for other questions. Okay lady there question. &gt;&gt; Sure question. &gt;&gt; Sure. Thank you so much for the provocative discussion. This is what I

was hoping to get at the India impact summit. Um my question is about how can regulatory artifacts like data set cards, model cards, system cards, rigorous evaluations, user feedback now

be extended to cover multiple languages, multiple contexts and multiple cultures. &gt;&gt; I think a lot of hard work &gt;&gt; happy to take that. Uh so those were for those of you who don't know model cards,

data cards, system cards are the are documentation that provide us with prominence of where the data came from, how it was processed, how the model was trained and and what kind of evaluations

were taken or or processed on it and what kind of decisions were taken. So it shows both the provenence evaluations, the limitations and the capabilities for those of us in in civil society

researchers as well as consumers. uh it provides us with that documentation of what kind of risks we are adapting if we take these data or models or systems right so it's critical infrastructure

critical uh uh transparency instruments uh but they are mostly in English uh and especially when we're talking about generative AI system language models we are running into this issue of

multilangual multilingual uh capacity limitations especially on the safety So not only these documentations need to be dynamic because the systems are dynamic but it needs to serve the evaluations

the limitations needs to be um adaptive and reflective of the different languages where the systems are being used as well. So it might produce it might perform really good in English but

we know that these systems are not safe or secure or perform that well in many different languages that are not English or as resource intensive as English. So great question. They need to be dynamic

and they need to reflect languages. &gt;&gt; And I will also say just very briefly following up on this is that these are things that governments can require for uh for model providers to release models

in your jurisdiction. Um and uh they so far are are not &gt;&gt; Thank you very much. &gt;&gt; We could insist. &gt;&gt; Yes, we dare to insist.

&gt;&gt; They are they are pro. I mean like California started this. I I just want to just &gt;&gt; sure I I think the we can continue the discussion and I hope we'll do this is

today just the start. I also hope that we will be able together with all the panelists to create some kind of a report setting out the shared pri priorities, the practical issues, the

husks, the methodologies, the the measures and we will uh hopefully facilitate and continue this discussion. I would ask all the panelists of the first and of the second round to stay

here for a momento from the organization and I would like to thank you all for being here and all the panelists again of course. Thank you so much.
