# Preparing to Monitor the Impacts of Agents: Closing the Global Assurance Divide for Safe and Trusted AI

**India AI Impact Summit 2026 ‚Äî Day 5 (2026-02-20)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 17:30 ‚Äì 18:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 6 |
| üìÖ **Date** | 2026-02-20 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/3ob6Qmy1HwM?feature=share) |

## üé§ Speakers

- Shri. Jitin Prasada, Hon. Minister of State, MeiTY
- Chris Meserole, Frontier Model Forum
- Doreen Bogdan-Martin, International Telecommuni cation Union (ITU)
- Madhu Srikumar, Partnership on AI
- Minister Josephine Teo, Singapore Government
- Natasha Crampton, Microsoft
- Owen Larter, Microsoft
- Rebecca Finlay, Partnership on AI
- Rumman Chowdhury, Humane Intelligence
- Stephanie Ifayemi, Partnership on AI
- Vukosi Marivate (confirmed), University of Pretoria

## ü§ù Knowledge Partners

- Partnership on AI (PAI)

## üìù Summary

This 60-minute panel session examines how real-time monitoring and threshold-based safety frameworks can be applied to govern AI agents at scale. It explores inclusive AI assurance approaches that support safe, trusted, and interoperable deployment across diverse global contexts.

## üîë Key Takeaways

1. This 60-minute panel session examines how real-time monitoring and threshold-based safety frameworks can be applied to govern AI agents at scale.
2. It explores inclusive AI assurance approaches that support safe, trusted, and interoperable deployment across diverse global contexts.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/3ob6Qmy1HwM/maxresdefault.jpg)](https://youtube.com/live/3ob6Qmy1HwM?feature=share)

---

_[‚Üê Back to Day 5 Sessions](../README.md)_


## üìù Transcript

ers in 19ish countries. Um, and we're all focused on what does it mean to unlock innovation through trustworthy, responsible, beneficial AI. Uh, and so of course, no surprise, gatherings like

the one that we've had this week are really crucial for the work we do. And with the Delhi declaration adopted yesterday, this is an even more important moment to build on where we

have come from, to lean in and to really get to work around some of the questions of the accountability work that needs to be done, the scientific evidence that we need to build around frameworks and good

policy moving forward. And of course it's extraordinarily important that this is happening in India that it's bringing a whole set of voices and perspectives and leadership that is not optional at

PI. We believe that that is fundamental to building a global community committed to this work and uh it's great to see it in action this week. So thank you all for being here with us. So uh today

we're going to give you an opportunity to see two of our latest papers. These are papers that were begun out of the Paris uh action summit and at that time as we were thinking about moving into

action and invasion we felt that work needed to happen with a good sense of what the assurance ecosystem looked like. So we've had work working groups underway developing these two new

resources. They'll be up on the screen at some point. You'll be able to uh get a QR code and download them. feel free to talk to any of us. The first one is strengthening the AI assurance

ecosystem. It really looks at telling and helping national policy makers if you're building a robust industrial AI strategy, you better have a comprehensive AI assurance strategy as

well. And you need to think about all those actors and what they look like. We're going to hear about one of the experts of course in this as soon as the minister uh comes to join us. The second

piece which is really important we think for this conversation is what does it mean to do AI assurance globally around the world? How do we close the divide that exists? What is different about the

challenges faced by countries in the global south uh versus others? So we're really hoping that these resources not only are good substantive contributions to the work that needs to be done but

the idea is to just catalyze you know sort of plant a number of seeds across a number of ways in which assurance works so that those can grow uh and really come to life out of this. And just two

quick comments on that now that we have have the declaration and so now we can as opposed to earlier in the week uh start to articulate it really leaning in uh with regard to the commitments around

in commitment one around usage clarity around usage data really trying to give some empirical grounding to this work. In 2025 in our progress report around foundation model um uh impact socially

we made exactly this recommendation. We re directly called for frontier AI companies to share usage data. We've been tracking progress and there has been some progress in that regard. So we

are delighted to see uh this particular commitment to come about and to start to see some standards about how that usage data is going to be shared. Um, so we're very pleased to see that work. We're

also very pleased to see the the second commitment around strengthening multilingual and use case evaluations. And you'll see if you do download the report on the global assurance divide

that that is clearly a key piece of work that needs to happen. Um, so this afternoon we are going to give you an extraordinarily uh expert panel that brings a real

diversity of perspectives to this work. And so we want to take the assurance question and apply it to agents because that's where the world is going. We're all seeing them all, you know, in the

news every day. We're seeing them integrated into foundation model systems. So what does it mean to take what we know about assurance and think about uh the applications that a agents

will will add to the complexity of that work. Um so let me begin by introducing our first speaker. Um she's probably been one of the most visible ministers this week because of the extraordinary

leadership that Singapore has taken when we think about AI assurance. Um I know you're going to talk a little bit about that. Uh such a pleasure to welcome you, Minister Josephine Teao. She's going to

come and say some words for us before the panel begins. Thank you. [applause] Thank you very much, Rebecca, and also very much appreciate partnership on AI

for the invitation. Um, when this series of summits first began in Bletchley, um, AI agents were not a thing. Nobody was talking about them. Even just 12 months ago when we had the AI action summit in

Paris, it has barely crept into the conversation. And at the time uh the preoccupation was all around deepseek and what it told us about the capabilities that is emerging out of

China. But today, as Rebecca correctly identified, um agentic systems uh have taken off. Uh they are increasingly being used and uh we need to have a better grasp on how to deal with this

issue because Agentic AI certainly offers transformative possibilities in how we delegate and orchestrate work. When deployed strategically, agents functions as invaluable teammates,

unlocking productivity gains and time savings, which we all want more of. However, however, I should also add that this autonomy, the very nature of how agents can be helpful to us is autonomy.

This autonomy also introduces new risk. The potential for harm increases when systems malfunction and human oversight is no longer present or at least diminished to a very large extent. The

implications may be complex and not fully uh predictable. So the way um my colleagues and I have been thinking about this is that there needs to be a shift. There needs to be a shift in

terms of how we might uh want to rely on reactive regulation to a different kind of stance which is proactive preparation and in Singapore that's what we've been trying to do. We've tried to be

proactive about governing the new risks in uh the era of agentic AI. Um and I think it starts with the government itself being a leader and not a lag guard uh in in using aentic AI. We need

to test it. We need to look at how the solutions can not only enhance public service delivery but we also need to be able to put in place more controls. Um government is high-risisk because the

touch point with citizens are very sensitive. No citizen and no government wants to um and make serious mistakes when they interact with their citizen. Telling them uh things about their

health, telling things about their social security, telling them about things to do with their um uh benefits that that are not accurate and having them not just being told but acted upon.

So this need to uh ensure that we know what we're doing is uh a very high one and the way we are also thinking about it is to try and work with industry. So for example between Google and Singapore

government we have a sandbox on agentic AI. It's one of the ways in which we we think we can in a way eat our own dog food. Try it you know does it taste all right? Does it hurt us in a in a very

significant way? Uh because if we were not able to do so, I don't think we have a lot of credibility uh in terms of how we want to govern agentic AI. Um but we can't wait, you know, for the dog food

to materialize in its consequences for ourselves. Uh in the meantime, my colleagues have put together a model governance framework for aentic AI. It is meant to provide practical support to

enterprises uh so that they can also um deploy autonomous agents responsibly and to mitigate the risk. We know that this is not um a you know complete solution and this document that we put out has to

be a live document. We very much encourage feedback um and and and and as a way for us to keep improving uh the guidance to enterprises. Um can I also just add that um um as we do this work

um what is the meaning and what is the purpose behind it? Ultimately it is to build confidence in the use of agentic AI systems. And we think that at many levels this confidence has to be um

presented has to be demonstrated to boards of organizations to customers to other stakeholders and how do we demonstrate that the risks have been managed well and that is where the

assurance ecosystem that Rebecca talked about comes in. uh it is an an absolutely essential uh part of building trust over the medium to longer term so that there is um a way a foundation upon

which agentic AI systems can be uh you know made more readily adopted and uh available. Um I should also say that for companies that are thinking about it and I see Microsoft here and I'm I'm sure

that there are other companies represented here. Um if we are to trust these agentic systems, um the safety aspects should not be downplayed and I I would venture to say that a company that

is able to give a high assurance on safety will find themsel will find itself being differentiated from their competitor. It's more likely to translate into stronger interest in a

product and service. So rather than think of it as uh uh something that you are uh unhappy to comply with, think of it as a strategic competitive advantage and um that is a way I think will um

that will give us um the the confidence to to put it forward. The question however is that um are we completely uh without experience in this regard and the answer is no. In aviation and

healthcare, there are a lot of measures being put in place to give assurance to passengers. When we board a plane, we usually expect to arrive. When we visit the hospital, we generally

expect to be treated except for disease conditions that are not yet well understood. But the trust in these systems have to be built over time and they don't come without some assurance

being put in place. The question is for AI and specifically agentic AI what would be the components what leads to an assurance ecosystem that uh would be robust enough we think that there are at

least three components the first is that there must be testing we need some way of uh making sure that there are technical assessments of the system to make sure that the systems are robust

they are reliable and they're safe and a lot more work needs to be done in this space developing the testing method methodology building the testing data sets and also making sure that the

testing of agentic systems um take into account that these systems are going to be much more complex the multi- aents and uh for example and it's not just the output but the in between steps how the

reasoning takes place and what is the orchestration that uh is being uh built into the gentic systems so that's the first testing second is that eventually we will need standards We cannot um you

know just define what is good enough. We also need to assure the users that it has met expectations in safety and reliability. And so these are still very early days. Thirdly, we think that this

ecosystem cannot do without third party assurance providers. It's one thing um to claim that uh your AI agentic uh your agentic AI system is safe. It's another thing to have someone attest to the

safety of it. So these could be technical testers, auditors and they provide independence, augment in-house capabilities and also help to identify the blind spots and it's necessary for

us to strengthen this pool as well. So I'm want to conclude my remarks to say that Singapore is actively building these components and we welcome conversations with partners and

colleagues because we know that we cannot do this alone. So we look forward to discussions in the three panels on how we can meaningfully collaborate on assurance for aentic AI. Thank you very

much once again. &gt;&gt; You get to go tonight. You're going Oops. &gt;&gt; So much for &gt;&gt; Thank you.

seat now. &gt;&gt; Yes. &gt;&gt; I know exactly. &gt;&gt; Great. We're [snorts] all here. It's It's the end of the conference and we're

all intact. &gt;&gt; Yes. [laughter] Thank you so much everyone for joining us. Thank you Minister Teo for the keynote. Uh one quick note um before we

dive in. Our panelist Fred has a flight to catch. Um so he need to slip slip away a few minutes early, but Fred, we'll make sure we get your best insights before you escape. Um no

pressure. Um so we are the last session, so we're standing between you and whatever you have planned right after. So I promise we'll make this worth it. We have an incredible panel and a lot of

ground to cover. So before we get started, what do we mean by AI assurance? Because you're going to keep hearing that term quite a bit here. So really put simply, um AI assurance is a

process of measuring, evaluating, and communicating whether AI systems are trustworthy. Are they safe? Do they work as intended? Can the public actually trust them? Um so really think of it

like a safety inspection, but for AI. um you wouldn't want you know you'd want an independent inspector checking a building not just the builder saying trust me it's fine so really AI

assurance is about independent verification as Minister Teo went over and why this panel why now um so the summit unveiled the New Delhi Frontier AI commitments just yesterday and the

second of those commitments is about strengthening multilingual and contextual evaluations so really making sure AI systems work across languages cultures and real world conditions. And

really that's the assurance challenge in a nutshell. And our panel today is about whether we are actually equipped to deliver on that promise globally and not just in a handful of countries. Um so

really our panelists span the ITU, Google DeepMine, um the University of Pritoria um and PI. So we have the range to actually wrestle with this question. So with that, I'm going to get into our

first question for today. Fred, that's going to be you. Um, ITO has been convening on AI governance through AI for good and working on standards that cross borders.

So really when we talk about AI assurance, what does it mean to you? Ensuring that these systems are safe and trusted and how do we think about assurance when 2.6 billion people remain

offline and maybe excluded from the frameworks being designed. &gt;&gt; Yeah, thanks for that great question and thanks for having me here. Um, so I think it's safe to say there's no

shortage of high potential AI for good use cases. Uh, everything from affordable healthcare to education for all, food security, disaster response, and also looking at more applications in

the physical manifestations of AI that you see in robotics, embodied AI, brain computer interface technologies. Uh, the best part of my job at AI for good is I see these use cases coming across my

desk every day. And I can tell you when we started AI for good in 2017, it was mainly in PowerPoint slides like they didn't really exist. But um as we got into say the 2023 with Gen AI last year,

the unofficial theme of AI for good was the rise of the AI agents. Uh bit bit scary Terminator like uh but that that's what people were talking about and where we're really going from sort of the

promise to the pilots to the use cases and and now scaling. Now when you're looking at these use cases, I think one big challenge is is trust. How do you trust them? I mean there's always the

good intention, right? But uh is that trust there? And also are they rep replicable and scalable? And I've yet to see, you know, a hype potential use case developed in Brussels work equally well

in Johannesburg and Shenzen and maybe Panama like it it's just we haven't really reached that yet. And if you look at these sort of fast emerging governance frameworks around the world,

whether you're in the US or or EU or China or everything in between, I think that there's a lot of good intentions, a lot of good thinking, but how do you turn those ambitious words and

principles into actions because the the devil is in the details and I think standards have details. So when you're thinking about how do you especially when you start to get into AI agents and

and you really that trust element is becoming ever more critical. How can you bake in a lot of the common sense things that we've been talking about all week or even for the past years at AI for

good? Are are they trustworthy? Are they verifiable? Um are they secure? Are they safe? Are they designed with uh human rights principles in mind? Are they inclusive? our people from the global

south at the table when we're drafting and developing these standards. So these are not always natural reflexes and at the same time it's hard to turn words into action. So, one of the tools, I

don't I'm not saying it's the only tool, but I think as these solutions start to to scale and businesses start to interact internationally or even internationally, at one point you're

going to need standards. And it's within those standards that you can kind of bake in those common sense principles that we've been all talking about. And um I forget the last part of your

question. &gt;&gt; It was really a question about connectivity. That was &gt;&gt; 2.6 billion people who remain offline. Yeah. Yeah. Yeah. So, you know, ITU's

mission is connecting the world and a third of the world is is still offline and you know, large parts of worlds actually have connectivity, but there's actually no incentive to connect. So, if

there's no content in your local language or or dialect or no access to governance services or or useful applications that are fit for purpose and where you live, you know, there's

why would you connect? So I I think AI can actually help to remove that friction uh where you have a lot of bottlenecks uh for example literacy uh disabilities again like content in your

own language or dialect. So I think one thing is closing the connectivity gap but the other thing is actually using AI to remove that friction and the last thing I would say is I think sometimes

there's a comparison where um if you take East Africa for example and you have the the mobile payment miracle or revolution with Empa right you effectively leaprog decades of

infrastructure legacy infrastructure and there may be a a kind of optimism that well the same thing could happen with with AI in the global south maybe, but I don't think we can take it for granted

that if that happens, it goes in the right direction. It's not a guarantee that just by putting the tool in the the hands of the people that they're going to create value, they're going to use it

responsibly, they're going to use it to solve local challenges, build more cohesion and community, those aren't for granted. So I think that whole AI skilling angle of really educating

people from, you know, grade school to grad school to diplomats and everyone in between, if you don't address that literacy piece, then it's just going to be a crapshoot. You know, we we're not

sure which way it's going to go. Thanks. &gt;&gt; Great. I mean, it's a good transition. Um, speaking of standards, um, Owen, um, Google DeepMind recently deepened its partnership with the UK AI security

institute on safety research. Um so including work on uh monitoring chain of thought and evaluations. So really [snorts] from an industry perspective, you know, what does robust AI assurance

look like um where do you think the gaps and opportunities are between what Frontier Labs kind of do internally and what's needed for broader public trust? &gt;&gt; Yeah, thank you Mu and thank you to

Rebecca and Partnership on AI for convening this really important conversation and a big congratulations to our Indian hosts for a fantastic week uh at the summit um this week. maybe

start talking a little bit about what agents are, we're increasingly excited about them at Google Deep Mind. [clears throat] They're essentially more autonomous systems that instead of just

following basic instructions, can actually achieve goals. So, let's say I want to get my suit dry cleananed on Thursday. Instead of taking an AI system and say find a website for a dry

cleaning company, see if it's open on Thursday, see what the hours are, see if it's within my budget, you can just say to your agentic system, go find a way to dry clean my suit, make sure it's being

picked up by Friday, and it will go and interact with those different websites and try and find a way to to to to meet your um goals. All kinds of fantastic applications already that we're seeing

right across the economy. We're using increasingly agentic coding systems at Google and Google DeepMind to do a lot of our coding. So we have our anti-gravity framework which is

fantastic. You can interact with it in normal natural language and say build me a website, build me a a tracking system to follow a particular bill that I'm interested in and it will really help

you uh achieve these goals. I think you'll increasingly see agents used right across the economy as well. I think we're we're just in the early years of a of a new AI enabled agentic

economy. I think you will have very normal interactions with agents on a regular basis that will pop up on your phone screen and say, "Hey, it's been a few weeks since you bought toothpaste.

Would you like me to go and take care of you um and get some more um toothpaste for you?" You mentioned standards, m which I think is going to be a critical part of getting all of this right.

There's a couple of dimensions to the standards. So firstly, we need to create the the sort of technical protocols to actually underpin this uh agentic uh economy. So we've been trying to

contribute to this conversation. There is the agents to agents protocol that Google has launched. There's the universal commerce protocol. This is basically a way of helping agents talk

to each other and agents talk to websites that you have standardized sets of information. You know, an agent [snorts] will will basically come to an agent or agent will come to a website

and say th this is my ID. These are my capabilities. These are what I'm I'm trying to do. I think in the same way that we developed protocols and standards in the early '9s to underpin

the internet like HTTP, like URL, we're going to have to build these out. There are then also assurance standards which are are related but I think um uh very important as well. We need to make sure

that we're understanding the capabilities of these systems. We need to keep making progress on how we can test for the risks that they may pose and then work right across society to

come up with ways to to mitigate that. I think the work that the safety and security institutes are doing around the world absolutely critical. So, Minister Tio mentioned some of the work that

we're doing uh in Singapore. The UK Air Security Institute has been has been world leading on on this. Um I think this is an area that we're going to see more from the the ACS and the Casey's

right across the world. The US government also through their Casey uh launching an agentic standards initiative uh this week as well. &gt;&gt; Great. And if if you don't mind a

follow-up question, um that's a really important point that you pointed out that we currently need interoperability. We need agents to flourish. we need to find a new way um to kind of imagine

this paradigm. Um but I'm curious if there's a safety challenge when it comes to agents that um yeah that that keeps you up at night. &gt;&gt; Yeah, I think there are there are

definitely risks to be mindful of. So I think agent security is something that we should all be thinking a lot about. If we're connecting increasingly autonomous systems into

different accounts, different email accounts, different bank accounts, I think we want to be pretty careful about how we do that and come up with sec security protocols um that can be

helpful there. We've actually been doing some work with Virus Total, which is part of the the Google security operations um team uh at Google to make sure that when certain agentic systems

are downloading skills or downloading apps from Agentic websites, they're being scanned for for malware or vulnerabilities and are being detected so that um they can be addressed before

people put them onto their their computer. I think there's also a concern that these agentic systems could create new capabilities that could be mis misused. So across the cyber security

domain domain for example I think some of the frameworks that we have already at Google deep mind will be helpful here. So we have our frontier safety framework which we use to to test uh

models before we put them out into the real world. We think about how those models are going to interact with with systems how they might be parts of of agents as we're doing that work.

&gt;&gt; Perfect. Just speaking for myself, I can't wait to use agents. Um I feel like it's a lot of developer communities that have, you know, started playing around with these systems. Um but I imagine

it's reaching lay consumers very soon. Um so um you have built Masakan for African language NLP. Um really building AI for Africans by Africans. Um when assurance frameworks are designed in the

US, UK or Singapore, how well do they translate to contexts where the data, the languages, the deployment conditions are completely different? Um what do we think we're missing?

&gt;&gt; Um yeah, thanks. Uh yeah um well just uh for context I'm one of the co-founders of the Masakani Research Foundation uh which is a distributed uh uh research organization that had thousands uh of

researchers who are working on African languages u yeah uh back 2019 um I think when we're just thinking about whether standardization or thinking about how we're going to measure um like you know

potential impacts or how we measure where data comes from is not to again it's not going to be a one-sizefits-all. Uh I think this is going to be the big one of the biggest uh challenges. Um we

might talk about agents in in specific things. I'm um in and in in and contextually even in South Africa I am privileged. I can like you know I can work uh on a laptop. I can play around

with agents because I I I code. I build um machine learning models and NLP. But for some people, for their use cases, it might just be that in thinking about the skills that those agents might have when

they do then try to use kind of more localized um uh context, they just fail. And what how what does that failing look like? Is it just a jarring experience? Is it something that might be um uh

hurting people's dignity? Those types of things are going to be what we need to think about, right? It's not going to be u using only a lens from the north. uh just like you know in the uh without

better words to say that uh because then if we do that then we on one side we keep on promising um that many will benefit from these technologies but then it's actually going to be a minority um

in that way and that that's the part of like maybe trying to ground what we're talking about here in terms of this assurance um of agents um on the other side in in thinking about the the like

um more of the human side if We're looking at supply chains, right? And in this case, I'll just limit myself to the space I I look at a lot is these data uh supply chains. U we are still having the

challenge of being uh very extractive. All right. in many different ways and uh people are tending to be having to work at a much higher requirement than necessarily what current uh legislation

or policy actually says because you're trying not to like you to you you're trying to preserve the dignity of people uh because the people are the source of the data if you think about language

data again I'm just language is just one of the things machine learning has many many uh different space or or generative AI has different spaces but you want people to uh understand what they're

contributing to. They have their rights. They like you know the as as much compensated um accredited uh for uh for that work and um that is still something we need to work on and I'm hoping that

as we're building up also on the on the supply chain side that we we do get uh to understand that it's a very different thing. I I my my experience has been that there's likely not as much

collection in uh Europe or or North America or or annotation as much as it's happening now in in the global south and but then that also means that it feels like it's further away right it's it's

not where the the the developers are and that then requires um more of this conversation in in in in one place so that again there might there must be um kind of local understanding the Last

piece to that is going to be the capacity and the capabilities of then the policy makers in those countries to be able to understand that part. It it is it it will not be top down. I I

don't believe that it will be them understanding um whether it's labor laws, it's data governance, it's just um monitoring of of systems once they're on. If they there is not that cap

capacity or capability to actually do those things. Again, it's more an autom kind of more automated direction that is not necessarily what u the values of those people actually are.

&gt;&gt; Those are important words right at the end of the conference knowing just how much we have um to get done here. So Steph, over to you. Um PI just released work on closing the global assurance

divide. a lot of what Bukosi just mentioned. What are the concrete gaps you're identifying? Is it capacity to conduct third party evaluations as Minister Teo mentioned? Is it access to

the models being tested or is it something else and what would it take to really close those gaps? &gt;&gt; Awesome. Thanks so much, Mario. And as one of the PI folks, thanks for being

here, everyone. It's great to see you all. I know it's a Friday evening. Uh so we're in between you and cocktails or whatever you have planned. So we we very much appreciate it uh in the last

session of the day. Um so I think it's such a good question and um I think uh your question talks about some things that recognize that those challenges aren't actually just global south

challenge. So I just want to start with the fact that we've released two papers. One is on closing the AI assurance divide and the other is how we strengthen the global AI assurance

ecosystem generally and the question of access is one that uh impacts us all actually. Um in the UK for example the department of science innovation and technology I believe that's what DEST

stands for. um uh has made uh access to models as a means to support insurance a priority for 2026. And so I think that there are a few shared challenges and I'll come back to the point around uh

north south actually collaboration in a second. Um but just thinking about closing the AI insurance divide we released this paper and in it we talk about uh around six challenge areas uh

from infrastructure to uh skills. We talk about languages and risk profiles. So the things that you've heard about from from Vukosi and a lot of the other speakers. So I'll give you a sense of

some of the examples that we have. So on language, we're at the India summit of course and uh India has over I believe 120 languages and 19,500 dialects. Uh when we think about Africa, we have

about 1,500 uh and or 3,000 spoken languages in itself. So when we think about benchmarking and eval uh and designing evals that um uh think about how those

systems are deployed uh in these various contexts it's so important to think about languages and that just generate generally I think demonstrates the complexity of designing evals to meet

the needs of this kind of diverse language ecosystem. Uh Rebecca mentioned at the start that we had the declaration of course yesterday and uh the commitment therefore in the declaration

to multilingual avows is really critical. Of course there's still a lot of work to determine how do we actually do that in practice in the most effective way and accounting for that uh

complex and wide uh language diversity. But that's one uh area that we talk about. The second in terms of uh closing the assurance divide that we need to account for is risk profile.

Interestingly, so in this paper, we actually interviewed a lot of assurance and uh and safety experts uh internationally and one of the things that they mentioned was uh differences

in what they might prioritize when you think about assurance. So uh when you think about the Pacific island nations for example uh they would be thinking about assuring for environmental impacts

differently than uh maybe environmental impacts would be considered as important in uh the US at the moment. For example, uh last year we published a paper on post-appoint monitoring. Um and in that

paper we talk about uh sharing uh uh uh kind of data um from companies. And one of the points uh that we talk about is environmental impacts. And so um it's really interesting that I think in terms

of closing the the divide, it might the starting point or what you put emphasis on might vary. And that's important to note as we're designing things like documentation, disclosure, and what we

kind of focus on. The third I'll just quickly mention is of course infrastructure. I think we've probably all heard a lot about this throughout the summit and this idea of what it

means to be sovereign and which parts of the stack to prioritize and that is really really important. Um but there are trade-offs. So um in terms of importance uh I was looking at a stat

that Stanford's Helm evaluations used over 12 billion tokens and uh they required 19,000 uh 500 GPU hours alone. And so when you think about the kind of infrastructural

uh needs uh it's so uh it creates barriers um for a lot of uh countries in the global south. But I was at an interesting round table actually that that even Carnegie was convening and we

were talking about the fact that how do you um balance uh infra assurance needs um where do you start from across the value chain so um at the moment a lot of the discussion is kind of upstream right

like we need to have that infrastructure in place like that's the point that we need to start with uh but how do you do that in parallel and how much of that resource should be uh put into other

foundational tools for assurance such as documentation artifacts which is another area that we focus on a lot at PI. And so I think there will be a lot of questions around uh how do you weigh up

all these challenges again knowing that even kind of the G7 countries uh the UK uh AI safety institute started with an inaugural hundred million dollars uh alone. So um that prioritization and and

balancing is going to be important. The last thing I'll say coming back to agents and I know we'll talk about this a bit more is the north south collaboration is a real opportunity um

as we think about agents and it's important that um global south countries aren't always playing catchup. I think that's a point that has come through for me from the summit which is that NIST or

the Casey so the center for AI standards and innovation and this is almost like a test for me of kind of saying um these these names of these institutions through this uh panel um but they just

announced a few days ago that they're going to be working on standardizing uh work around agents including um that they've released an opportunity to comment on a paper around agent

attribution and agent identity I believe uh which which is really interesting. Um and uh there's of course a lot of push for um countries to collaborate and you see a lot of the safety institutes

collaborating on questions around assuring agents um in the global north but how do we ensure that global south countries aren't missing from that um that will have implications for how we

attribute agents um how we um test agents and we shouldn't just assume again whilst those upstream points and infrastructure is important that in parallel they're ultimately part of

these kind of thinking ahead uh questions and um frameworks. &gt;&gt; Great. So, I'm going to take the moderator's prerogative and have us do a rapid fire. And by rapid fire, I mean

every answer is a minute and 30 seconds, which let's be honest, it's fairly rapid for AI policy folks. Um I'm going to start with Fred cuz I'm more nervous about your flight than perhaps you are.

So, a minute and 30 seconds. What role should multilateral institutions like ITU play in making globally inclusive AI assurance happen? &gt;&gt; Yes, I think um AI for good has a pretty

ambitious goal, right? It's it's simply put it's to unlock AI's potential to serve humanity. Pretty big. Um but we can't do it alone and no one can. It's not one country, not one institution,

not one NGO. uh that that's why we have you know 50 plus UN sister agencies as part of the eye for good but also making great efforts to bring as many diverse voices to the table uh from the global

south from NOS's from civil society uh it's always been extremely open I like to think of it as the Davos of AI but instead of being very exclusive extremely inclusive right uh so I think

that's a bit the philosophy behind AI for good and um you know I think the AI it's it's just moving so quick. So the focus has always been on practical applications, practical solutions. But

in doing that, you can tease out the next generation of standards of policy recommendations of collaboration and partnerships around the world. So I like to think that in the doing you have the

learning, right? And it's not just about talking and um that's what AI for good has always been all about. Thank you. &gt;&gt; That was incredible. You have 56 seconds left.

Um yeah, I'm going to move us ahead to uh Wakosi. Um so Singapore's aim is test once and comply globally. So from a global south perspective, what would

make that interoperability real rather than a form of exclusion? &gt;&gt; Yeah, that's a hard one. uh uh I think uh going back to I think the other thing that's that's come out

of uh of a lot of the sessions here has been on the evaluations and how evaluations are contextual and because of that contextuality it it's it seems hard to say you're going to come up with

one test right because either on one side it's going to take you a lot of resources to actually either uh put up the evaluation to be so all-encompassing on the other side to run it is going to

be a lot but then uh when it comes down to the user uh which I think was I think our second panels that I was in this week and you're trying to think about personalization if you're going down to

an individual what experience do they do do they actually have and how do you get get to there will be some more um high level um uh like you know safety things that will likely come out and people

will be working on that and maybe that's what I'm thinking Singapore is trying to go for uh but then when we're getting to what the individual experience is given that you have the stoastic systems they

you don't know what is going to happen necessarily. I know we're trying to do that but we don't really know what's going to happen at the individual experience and we

can't model all of that. It's going to then uh require that again you you do have uh closer to uh where the user might be uh things on on what actually that experience was. So one of the hats

I wear is I'm a co-founder of Liilaba AI an AI startup and there you will be doing more testing towards hey we are serving this client we're serving them in this way and then you're trying to

then go in and say where is kind of is your data coming from uh what is the use cases what are we testing for in terms of their operational kind of requirements. It would necessarily not

be just one, but yes, what do you you you might want is just that that that those guidances on how to do it in in such a way that it's repeatable and then people can actually get uh confidence in

what um uh those evaluations are. &gt;&gt; Yeah, that's a great point. Assurance needs to be globally decentralized. Um Owen, um given everything we've discussed, what's one commitment

Frontier Labs should make on assurance um that would actually move the needle? Yeah, good question. Um, I think there's a question of access to the technology which is important here. I think it's

one of the big themes of this conference. Certainly one of the things that I'll be taking away. So, you think the the multilingual part of this is really important understanding,

respecting local cultures. That's important if you're going to have a good product and if it's going to be used broadly. Um, we've been investing in Gemini for for some time now to make it

better, more representative across um different languages. We have uh partnerships that we're doing here in India including with the IIT Bombay to to help um improve performance across

various different Indic languages. It's also really important on the the safety and security front as well to have benchmarks that are that are available um in different languages. Fantastic

work that um ML Commons are doing on this front that we're that we're pleased to support. The other bit of access that I think is really important is having things that are quick and cheap enough

for everyone to use. One of the things of Agentic systems is that they're actually pretty compute intensive to use. We have a range of models that we have developed and uh bringing to to

market at Google DeepMind including our very quick flash models which are relatively cheap, quite efficient, very very quick. We think these can play a really important role in powering

agentic systems. It's also going to be really important if we're going to do effective and rigorous testing of these systems because that could be very comput intensive as well. So thinking

about that that access piece is something we all need to to keep doing &gt;&gt; and it's not an easy you know question really I mean to do it safely and um ensuring that third party assurance

providers um consider the security questions at hand um it's it's an open challenge for the field so Stephanie no bias at all um since we both at PI but I wanted to give you the final word what

what concrete outcomes you know do you think we want to see from the global AI assurance work um in the next 12 months, what would success look like? &gt;&gt; So, uh Owen, now that you said your one

point, by the way, can hold you accountable against this uh delivering on the access question. Um but I think um we in the two papers we talk about uh the need to kind of build a robust uh

assurance ecosystem and one of those things is changing incentives. So um funny enough another session this week um there was a question about whether um we have differences in the way we're

talking about AI safety uh over the last few years and whether there we still have those divergences of whe whether we've converged and there are a few themes that we've actually converged on

which is nice and I think assurance is one of them and this week uh a lot of the discussions we've had are in some of those incentive areas like insurance to support assurance and so what does that

look like how do drive new incentives or or or put some of these structures in place to drive a kind of more mature and robust ecosystem. I think that's going to be really important. Um the second is

professionalization. There are a lot of uh questions around how do you trust the assurer? Um and so how do we ensure that we're thinking about the skills uh uh what does accreditation look like for

assurance uh organizations or individuals and so and that will help I think questions around kind of access um and so that's a kind of second uh piece um but hopefully I think what we're

we're what we're hoping to do and that's just because this is also about agents um I think that there so many of those foundational questions haven't yet been resolved and so I'm hoping that we can

move the dial to start thinking about How do you apply that to some of these future questions? So, just to shout you out, Mardu, Mardu is uh the brains behind our safety work and she came up

with a a paper on real time uh failure detection and monitoring of agents. And what I really like about that paper is it talks about a kind of a tiered approach to assurance as well. So, uh

when you think about agent deployments, do you need to be thinking about assurance based on the risks or the the stakes at hand? So, is it in the financial services sector? Is it in

making about making medical uh decisions? So how do you tie it as close to the use case and the risks? Um that needs to be also linked to reversibility. What are what's the

possibility around reversibility of actions and the the consequences of that? And then third we have affordances. What are the kind of affordances you give to the agents? How

much autonomy do they have? And so how do you design an assurance ecosystem with all of these different components in mind and a kind of tiered approach? And the more that we can advise uh you

know the USKC and a lot of policy makers who clearly are trying to make decisions in this area I think that's what uh success would look like for us. &gt;&gt; This was totally not planned Steph

plugging our work here. But um I can't imagine a better note to end on this. It's a fieldwide challenge but I just want to emphasize it's a fieldwide opportunity. No, you know no one single

organization can get this right. Um, so hopefully that's a helpful reminder as we um end with this summit and move on to the next iteration. Um, so thank you everyone. Hope you have a great um, safe

flight back home. Fred, that's that's tonight for you. Um, and for a closing keynote, I'm going to welcome Natasha Crampton, who's a chief responsible AI officer at Microsoft. Um, and post that

we'll hear from Chris um, who's a CEO of FMF. Thanks everyone. &gt;&gt; [applause] &gt;&gt; Do you want to give it? &gt;&gt; Okay. So, we're going to get momentos.

Sorry. You might want to come back. We don't want to miss this. [laughter] &gt;&gt; Thank you very much. &gt;&gt; All right.

Well, thanks so much Mu and to all of our panelists for um what was I think a very rich and grounded and uh also at times humorous discussion. One of the things that came across

clearly for me today is that uh we need a AI assurance to no longer just be a theoretical exercise but we actually need to build it into an operational discipline and that's a discipline that

really needs to work across borders across languages and cultures and I think increasingly across agentic systems systems that don't just generate outputs but actually take actions.

I heard this panelists focus on the fact that assurance is pretty uneven today. Um it's often strongest where there's access to compute and data and evaluation infrastructure and weakest

where those things are scarce. And as several of uh our panelists emphasized, if we don't address that gap deliberately, the shift towards AI agents is only going to make that divide

um even worse rather than closing it. When I think about the nature of assurance, I think with aentic systems, it does need to change in its emphasis somewhat. Pre-eployment testing has

always been necessary for all types of uh systems and so too has post- deployment testing of course but post deployment testing in an agentic world takes on an even greater level of

importance in my view when systems can plan and they can chain actions they can interact with tools they can adapt over time um assurance really has to move um towards continuous monitor monitoring uh

real time detection and and clear accountabilities for when interventions need to take place. That can be quite a hard technical problem. Um but it's also a governance challenge.

So I know PI is known for convening communities of not just thinkers uh but also doers. And so I wanted to leave everyone with a couple of ideas of um implications that really follow from

some of the insights that we heard today. The first is that it's really important that we build assurance into systems as part of the system development life

cycle and we don't just seek to bolt it on at the end. Um so that means that we need to design systems so that they can be observed and audited and constrained in practice. not just in policy

documents. Second, assurance has to be interoperable. We heard uh Prime Minister Modi speak yesterday about building an in India and delivering to

the world. That I think is absolutely an as aspiration that we should strive towards. But that can only work if we have evaluation methods and documents and signals of risk that are usable

across regions and adaptable to local languages, cultures, and deployment realities. Third, assurance has to be shared. No single company or government or

institution can do this alone. Um and that's especially true for agents given how pervasive uh uh they will are expected to become across the economy. We need shared evaluation

infrastructure, shared taxonomies and shared investment uh in capacity particularly in the global south. So for me this is why uh organizations like the partnership on AI as well as the many

collaborators that have come here together uh in in this week's India AI impact summit um as well as open engagement across the community to make sure that we get this right. Um it's a

really foundational uh uh area for collaboration for all of us. Now my view is that if we do get assurance right and by right I mean it needs to be global and inclusive and also dynamic I think

it really does become an enabler of trust um and adoption as Minister Teao said not a break on progress. One of the key things that I think we need to do as a community is really to

treat assurance as infrastructure. Infrastructure that we need to build together um and put into practice together. Thanks very much. Well, uh what a phenomenal session. um

you know from the the opening and and closing keynotes you know to a really rich and dynamic um panel I cannot think of a better way to close out what has been an extraordinarily rich uh and

dynamic summit uh as well um I have the impossible task of trying to summarize everything that was just said here um so if you'll bear with me I'll just offer kind of three core themes that seem to

to jump out to me um one is that we need to evolve and mature our understanding of of assurance there's a lot of reference to agents here the the kind of coming prospect of multi-agent

environments as well. Um, we need from from eval to mitigations, we need to have a better kind of an evolving understanding of of how to how to do assurance. The second and probably more

importantly, we also heard a lot about assurance as a global effort. Um, here I love, you know, Steph's point about the need for greater north south collaboration. There's a lot of

discussion from Fred and others about the need for global standards and and harmonizing those standards and making them interoperable. Um and then there was also a lot of reference to some of

the the new institutions that we've evolved uh to enable that global um dialogue to happen. Whether it's the the institution that was announced literally just before uh this session an hour ago

um for the the kind of global network um or the international network of ACEs uh that have also been uh you know kind of revitalized recently as well. Um and then kind of the last point I would that

really jumped out at me was the assurance as a shared responsibility. Vicosce I love the point about kind of a bottom assurance as a bottomup effort and I think it's one that you know we

all have a role to play here um whe you know regardless of which sector you are in regardless of what aspect of assurance you're taking part in uh there's a there's a role for all of us

so with that I'm going to leave you with just one kind of final call to action and that is to get involved right we we you know if we want this technology to be safe and secure and trusted we all

have a role to play so download uh the the reports very important thing download the the the great reports that have just come out on this topic. Get involved. Look at the work that PI and

others are doing as well. Um and become a part of the conversation about how we're going to take this amazing technology, but really make sure that it's it's safe and secure and that we

have uh a way to trust it. Um you know, uh in the opening uh remarks, Rebecca kind of used this great metaphor of the seed, right? like one of the goals of the reports that they put out and the

the conversation in this panel was to try and plant a seed about you know to watch kind of assurance grow. So I guess the the parting thought I would I would give you is to say let's let's all kind

of roll up our sleeves and get to work and make sure that the seed grows. So with that thank you and thank you as well for our panelists and speakers. Hey,

Lord. It's the band.
