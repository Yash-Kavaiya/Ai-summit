# International AI Safety Coordination: What Policymakers Need to Know

**India AI Impact Summit 2026 ‚Äî Day 5 (2026-02-20)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 15:30 ‚Äì 16:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 6 |
| üìÖ **Date** | 2026-02-20 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/cTL3XUiugRE?feature=share) |

## üé§ Speakers

- Dr. Eileen Donahoe, Sympatico Ventures
- H. E. Govind Singh Deo, Government of Malaysia
- H.E. Josephine Teo, Singapore Government
- H.E. Mathias Cormann, OECD
- Mr. Cyrus Hodes, AI Safety Connect
- Mr. Jann Tallinn, Future of Life Institute
- Mr. Nicolas Miailhe, AI Safety Connect
- Mr. Osama Manzar, Digital Empowerment Foundation
- Mr. Sangbu Kim, The World Bank Group
- Prof. Stuart Russell, The International Association for Safe and Ethical AI (IASEAI)

## ü§ù Knowledge Partners

- AI Safety Connect (AISC)

## üìù Summary

The race to AGI/ASI is getting more geopolitical and difficult to control and govern with profound consequences for our societies and civilisations. This session will aim to delineate an actionable governance agenda to address that situation, exploring advanced AI technical and governance needs .

## üîë Key Takeaways

1. The race to AGI/ASI is getting more geopolitical and difficult to control and govern with profound consequences for our societies and civilisations.
2. This session will aim to delineate an actionable governance agenda to address that situation, exploring advanced AI technical and governance needs .

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/cTL3XUiugRE/maxresdefault.jpg)](https://youtube.com/live/cTL3XUiugRE?feature=share)

---

_[‚Üê Back to Day 5 Sessions](../README.md)_


## üìù Transcript

that the race towards artificial gener intelligence is no longer a theoretical pursuit. As billions and maybe trillions now of of dollars are getting deployed uh to

push the frontier of artificial intelligence, the technology is now advancing rapidly and safety is not keeping pace with it. There are wonderful opportunities on the

other side of this quest. there also big risks and so that's the purpose that's the reason AI safety connect was founded AI safety connect is there to help shape the frontier AI safety and secure agenda

to towards what I would frame as common sensical AI risk management AI safety connects has been founded to encourage global majority engagement into frontier AI safety

NA safety connects has been connected to showcase concrete governance coordination mechanisms, tools and solutions. So how we do this? We convene at each AI

summit. Um so last year we started in Paris, this year in India, next year we're going to be in Switzerland. But we also convene at the UN General Assembly. Right? We need a faster tempo for this

safety discussion. So every six months we have this global convening. We also do capacity building and we also do trust building exercises at time behind closed doors.

Well, this week in New Delhi has been a an intense one uh an impactful one. Um on Tuesday we had a full day of uh uh panels, conference, solution demonstrations and closed dooror

workshop uh discussions on some specific um nuts to crack to advance AI safety. We for example had the privilege of hosting Prime Minister Dick Shu from the Netherlands on stage to deliver um a

special address on the role of top leadership in advancing AI safety. also engage with industry, engage with academia of India and abroad. &gt;&gt; So we had a extremely busy week beside

our main event. We had this closed door uh discussion that I was mentioning yesterday and today this closed door scientific dialogues. We're going to publish the results soon that brought

together senior industry leaders to discuss share responsibility for AI safety. Well, obviously none of this would happen without partnership and we want to thank our co-hosts the

international association for the safe and ethical AI and its director professor struck Russell to whom I will hand over the floor in a in a few minutes and the digital empowerment

foundation with anchoring us at the grassroots here with us manor we'll close the session later on and we obviously want to thank our sponsors and supporters starting with sympatic

ventures. Eileen Dono will moderate that panel uh and we're thankful for that. Um the future of life institute um Imana and Yan who's been supporting this effort and the Mindu Foundation whose

team is here as well with team and and it's it's great to have your support and we are thankful for that. Yeah. &gt;&gt; So today we're about to hear from his

excellency Matias common who's a secretary general of the OECD. Uh we're going to hear for our excellency Miss Josephine Teo who is the minister for digital development and information at

the government of Singapore. Thank you for your continuous support. Really appreciate that. Same for Yantalin who's the AI investor but also a funding engineer at Skype and the co-founder of

the future of life institute. And last but not least, we also have Mr. U Minister Deo who's going to be with us from Malaysia, Minister for Digital Development and Information. Thank you

minister uh as well as vice president Kim um for digital and AI at the World Bank. So an extremely important conversation to have &gt;&gt; and before you we welcome you to the

stage I would like to hand over the floor to Professor Strat Russell to say a few words and to speak about also what's happening next week in Paris. Thank you so much.

&gt;&gt; Uh thank you very much Cirrus and Nico. Uh so as Nico mentioned the uh International Association for Safe and Ethical AI or ICI the world's worst acronym uh is a global

democratic scientific and professional society. Uh we have several thousand members uh and approaching 200 affiliate organizations. Our mission is to ensure

that AI systems operate safely and ethically. for the benefit of humanity. Uh and as Nico mentioned, our second annual conference will take place in Paris starting on Tuesday. Uh it's still

I think possible to to register uh but uh we're already up over 1300 people coming. Uh it's at UNESCO headquarters in Paris. Um so achieving this mission of uh ensuring that AI

systems operate safely and ethically is partly a technical challenge. How do we even build systems that have that property? Uh but also a governance challenge. How do we ensure that those

are the systems and only those systems get built? uh and this panel is mainly about this second challenge and I think it's one on which global coordination is essential

because the harms whether it's psychological damage to the next generation or loss of human control altogether those harms cross borders

and we must coordinate to make sure that they don't happen or they don't originate anywhere. uh and it's I think fitting that we are having this summit here in India which

has really among other things championed the idea that everyone on earth should have a say and so with that I will hand over to Eileen. Thank you very much. Thank you Stuart. So Dr. Alen Dono is

the founder and managing P part partner of sympatico ventures. She's also the former US special envoy and coordinator for digital freedom and the ambassador to the UNHCR. Eileen,

we welcome the the speaker on the on the floor. Please, your excellency Mr. Maches Corman, Mr. Gobins singo, Mr. Josephin, and Mr. Yan Talin as well as Mr. Sangbukim join us on stage. Thank

you. Okay, given this remarkable panel and the very short time we have, let me very briefly frame our discussion and get right to our speakers. So, we're here to

share views on the opportunity for policy makers to impact international AI governance. As the race towards AGI and super intelligence intensifies, AI safety advocates face a compounding

challenge. The technology is advancing rapidly and being deployed with minimal guard rails. While the risk management processes that do exist are either illadapted to the

magnitude of the risk, fragmented across jurisdictions or insufficiently binding on developers, deployers, investors, and regulators. The result is an unharmonized governance landscape that

fails to shape the behavioral incentives of those building and funding frontier AI. Economies, governments, and societies do not respond well to such mixed signals.

While much of the discourse on frontier AI safety has focused on AI superpowers, there's an urgent need for deeper international diplomacy on the most extreme risks. At this juncture, middle

powers and global majority states can't be seen as peripheral actors in this landscape. Through pulled resources, market leverage, normative influence, and regulatory innovation, they can

shape the direction of global AI practices and safeties. Leading from the middle may turn out to be a more powerful approach than previously anticipated.

Whether or not that collective power is exercised now will determine whether international AI governance moves from the rhetorical level to the real world impact on safety.

This panel will aim to identify present present-day coordination gaps in international AI safety and highlight practical steps policy makers can take in the coming months to close them. So

to our panel, I'll start with Secretary General Corman. The OECD has done remarkable work over the past decade developing consensus on the OECD principles, providing a

definition of AI systems that has resonated internationally and playing an international role in operationalized the Hiroshima International Code of Conduct.

Along with those foundations, we now have the International AI Safety Report and the Singapore Consensus on Global AI Safety Research Priorities. With these principles, definitions, and

frameworks in mind, two-part question for you. First, what are the key lessons learned from the process of building consensus and then implementing these frameworks? And then second, looking

ahead, what's the most critical piece of coordinated frontier AI safety infrastructure we should be building? Now, some have called for an international incident response center

and we're all curious whether you think that should be a priority and achievable. &gt;&gt; Just some small easy questions in in terms of um you know, what is the key to

success? what is the sort of most important lesson on on on like looking back on what we need. Um trust is built through inclusion uh and on the basis of objective evidence and you know what I I

think what we've learned over the last few years is that bringing together all the relevant actors governments companies civil society technical experts is is what we need to do. I mean

each has a different perspective and different imperatives. I mean markets uh reward the private sector for speed, scale and innovation while governments must manage risk and protect the public

interest without stifling progress. But a challenge and it's been mentioned in some of the opening remarks. The challenge for policy makers in this context is that AI is moving much faster

than policy cycles have traditionally moved which easily then creates gaps between innovation and progress and opportunity but necessary oversight, mitigation and management of risk. uh

but you know all sides in this conversation do share an essential common interest and and that is to ensure that the systems that are developing are trustworthy because

without public trust in the end uh even the most powerful AI tools will struggle uh to gain broad adoption. So that means that occasionally and and you know it's not always popular with everyone but

occasionally we should slow down occasionally we should actually uh pause test monitor audit share information and and take the time and invest in building confidence that these systems uh can

work and and as intended and respect fundamental rights. So that that's sort of I guess the first point. Another critical lesson involves international consistency. I mean and

and this is you know part of the reason why these sorts of summits are so important is to really facilitate this conversations among countries and among different

jurisdictions because I mean national priorities can vary quite widely and and and there's of course fragmentation and compliance cost related risks and I mean and at the OECD I mean really what we've

been doing for for six decades now across different policy areas is to um try and reduce fragmentation and and by achieving alignment uh around key principles, building shared evidence and

and facilitating the necessary conversations to to develop a more coherent, better coordinated approach moving forward. And on AI, I mean, we've um developed and and um developed the

OECDI principles which were first adopted in 2019 and which are now adhered to by 50 countries around the world. And that was really the first globally recognized baseline for

trustworthy II. The OECD's life cycle definition of an AI system has since shaped policy frameworks from the EU III act to US executive orders and and we've had just earlier um the meeting of the

global partnership on II code shared by Korea and Singapore. Um we've got the OECDI policy observatory which is sort of essentially the broad gamut of all of the different um policy approaches

around the world. Um to provide countries and industries with data and evidence on on on what's being done, facilitating peer learning and and trying to take some of the politics and

the rhetoric out of it, but really looking at the facts. Now looking ahead um and and you sort of ask a question here about you know what to do about um the risk. I mean the most critical piece

of frontier III cy infrastructure is uh coordinated transparency and incident reporting. I mean the Hiroshima I AI process code of conduct and its reporting framework

launched at the II action summit in Paris last year. you know is that's a promising step and and we've got to continue to develop that. Um since their publication 25 organizations across nine

countries have already submitted detailed reports on how they manage II risks offering for the first time a comparable view of developer practices across jurisdictions and the next stage

is to strengthen information sharing on II failures and near misses. um the GPI um II common framework for incident reporting IMS to help us collectively learn from mistakes before they scale

globally and and over time this could evolve into an international II incident response center coordinating alerts between governments and labs without exposing companies to commercial or

legal penalties for reporting in good faith. Finally, um we do need to scale access to practical safety tools with global partners. The OECD recently launched an open call for open-source

safety and evaluation tools hosted in the OECD.I catalog of tools and metrics to make a trustworthy AI easier to implement in practice. I mean these are some

initiatives um you know to form the foundation of a more transparent datadriven and interoperable AI governance ecosystem and and really about you know building uh shared trust

not just shared rules. &gt;&gt; Thank you. So yeah thank excellent. Uh Minister Teao uh a number of questions for you but let me start with the fact that Singapore occupies a very

distinctive position in the global geostrategic landscape as a pro-inovation advanced knowledge economy with deep commercial and diplomatic ties to both the US and China. As the race to

AGI intensifies and bilateral tensions mount, is there a role for Singapore and other middle powers to play in bridging the coordination gap to keep scientific and safety channels open? And also,

what's the most important step middle powers can take in the next 12 months to help establish a shared minimum understanding of frontier safety? Well, thank you very much um for that

question. Uh I think there is no running away from the fact that uh for smaller states uh and that includes Singapore, uh the technology that our companies, our citizens are going to rely on do not

originate uh from our shores. So they don't necessarily come within our jurisdictions. We don't always get to set the rules. Uh having said that, I do believe that we're not without uh uh

agency. It doesn't mean that uh we take a step back and just let things happen to us. There are still things that we can do. One of the most important things I I think as policy makers is for us to

um think about uh what it takes uh to translate what we know from science into policy. Um and I wanted to just say why this is so important. Um in our case um as as policy makers

the key questions will always be are the policies that we make effective and also policies always come with trade-offs. Um um with the question of effectiveness

uh there is always a need to understand what actually works as opposed to what you know looks good on paper. Uh with a question of tradeoffs. Uh it's about understanding what we lose

as a result of whatever safety aspects it is that we choose to put in place and whether we can minimize them. Can we mitigate them? Now in areas where safety is the objective,

uh we can't just go with gut. We can't just go with speculation. Um you take for example in my previous life I was working on promoting Singapore's air hub and uh we had to

deal with a question of uh aviation safety. We were expanding our airport. um it was going to carry many more passengers um in and out of the country. But we are limited by the number of

runways and in landscap Singapore you can't just click your finger and say let's build a new runway. It's very expensive anyway. Um then there is the question of what do you do when you have

these jumbo jets like A380s because each time an A380 hits the the runway? It creates so much of a um blast that you really need to create more distance between the A380 taking

off and the next aircraft that is scheduled to take off. Now this is not a question that the transport minister can just decide on a whim. Um the air traffic management has to decide on its

policy of how much distance is considered safe between landings uh or rather between takeoffs. And to answer this question you really need to invest in the research. You need

to invest in understanding um the tests. So the science is one part of it but between science to policy you are actually going to need a lot of tests. You are going to need a lot of

simulations and you need to understand whether the distances that you decide are safe works well in you know a a a sun in a in um in a in a thunderstorm a tropical thunderstorm. Does it work just

as well in a snowstorm? Well we don't have snow in Singapore. uh but you think about the airline that operates this if each country that they fly into has a different safety distance that creates

some difficulty. Um so we we we therefore think that um not only is there a need to invest in understanding the science, not only is there a need in understanding what testing looks like,

what good testing looks like, um there is also a need for us to think about what standards uh that will eventually be interoperable. What do they look like? Which is why we think that

international efforts, the collaboration that um you know that is being carried forward by the OECD through the global partnership on AI, the AI safety connect effort and also uh ICI uh where is

Steuart now? that's that those kinds of efforts you you can't do away without uh at the outset there is likely to be a bit of a fragmentation and um the the trade-off with not having

these conversations is that we are not even going to make advances in AI safety and I don't think that that's a very good place for us to be in. it doesn't give us the assurance

that we can deliver to our citizens and um it does not create an a foundation of trust that will eventually help us to push ahead with the use of this technology on a wider scale. So that's

how we're thinking about it. Alen, &gt;&gt; thank you. So let me turn to Minister Goband from Malaysia u and and note that under your

leadership and Malaysia's 2025 Azion chairmanship, Malaysia succeeded in placing AI at the center of Azan's agenda by establishing the Azon AI safety network. Malaysia is now

finalizing its own AI national action plan and Malaysia's AI governance bill is expected in parliament in 2026. So this dual track approach of building national capacity while leading regional

coordination represents a model of middle power agency that other countries are watching closely. So what lessons do you think other middle powers can draw from

Malaysia's experience and on the AI uh Azon AI safety network? Uh we have to note that operationalizing it will require sustained political will, technical capacity and resources.

So what concrete steps must Azion take in the next 12 to 18 months to ensure that this isn't just aspirational? &gt;&gt; Thank you very much. Um the conversation around AI has gained a lot of traction.

uh it is today uh something that um people are concerned about across the globe and I think uh in this uh context uh we can also anticipate that countries will also be uh looking to build

ecosystems uh that prepare uh their uh countries for not just AI uh but uh for next generation technology as well. And when we look at it uh from that angle, we can expect uh that new technology is

going to bring with it uh benefits but also risks uh which must uh be dealt with. Um now in this context uh Malaysia has u looked at u building infrastructure uh that will not just uh

look at how we can develop uh AI but also uh ensure that when uh we um get our citizens to adopt AI and we look towards scaling it uh we are uh sure that um it is safe. This is a

conversation that uh needs to be reflected on. Well, first I think we got to acknowledge the fact that we do not know everything. Uh and this is why uh to Minister Josephine's point, uh it is

very important for us to invest um in expertise. Uh because uh governments consist of politicians, politicians of course run governments. Uh but AI is an area that's developing so fast. uh

people are already talking about what they anticipate next and of course uh the risks uh which will come with that new uh and next generation technology as well. So when you start talking about it

uh from an expertise context uh you also have to start looking at building institutions uh that will allow uh this expertise together resources uh to shape policies that government can implement

uh to deal with the risks of uh next generation technology. Now having said that uh we'll find that many countries are still thinking about how uh that should develop u in their own countries.

Uh similar to Malaysia when we started the ministry ministry of digital just 2 years ago uh we were asked to consider how uh we can develop an ecosystem which deals uh with um AI and the benefits and

risks that it brings together with it. Um now I think what is important is for us to acknowledge the fact that uh when we speak about AI uh we also speak about uh crossborder threats. Now when we

speak about crossber threats we want to make sure that when we talk about ecosystems we have an ecosystem domestically which is secure uh but we also want to make sure that there are

conversations with friends around us and the region and of course eventually globally that understand that there is this problem that needs to be dealt with and it needs to be dealt with

collectively. We all uh understand the risks. We all develop uh domestic systems to deal with this risk. Institutions domestically which deals with this risk. But we want to also make

sure that the standards that we create, the institutions that work for us uh are institutions that can work with other countries around us, standards which are interoperable so that you will

eventually be able to develop standards uh which are acceptable uh regionally and even globally. Now for that to start uh you really got to start strengthening your domestic systems first. Um I think

where you take steps and real steps uh to build um ecosystems um that work uh then from there through those experiences and those lessons you would understand how the discussion the

discussion shapes and when you start having discussions with friends of yours in the region about their ecosystems as well their domestic processes then I think that's where the connect begins

because otherwise the conversation uh becomes uh largely academic uh because you you think about a particular process you are speaking about something that is in your mind but you haven't actually

developed it domestically yet. So Malaysia's moved uh quite aggressively in this context. Uh we have rolled out uh uh several projects uh which are led by the national AI office. What the

national AI office does is quite unique. It really speaks to different industries and tries to understand what risks are involved in different sectors. AI across uh AI across uh different sectors means

that we will look at risks across different sectors as well. Uh so how do you deal with this? Uh there has to be conversations. Conversations need uh uh what do you call to begin and to proceed

and to sustain you know and I think ultimately this is the reason why we have summits like this and these summits are very very important because when we come to these summits then we find that

there's so much more that other countries have actually done which we can do as well. uh and I think uh we've also seen the reverse where other countries ask us what we have done and

how it is they perhaps can also uh try and implement what it is uh we have done based on the experience that experiences uh that we have had. So this of course brings us to uh the AI u the AI safe

initiative. What we want to do is try and uh build on what it is I've just said before. We want to make sure that as a country that understands the challenge that AI brings in terms of

risks, we build ecosystems domestically. Uh subsequent to that, of course, we have groups of experts that talk to us about building standards that perhaps will be applicable uh around us

regionally. Uh but we need to take this conversations uh to uh the the states around us and for us it's ASEAN. I think uh you know the ASEAN member states have been uh very cooperative in looking at

uh collaboration when it comes to building common standards. So this is the reason why we thought that it was important for us to start pushing ahead with u u an initiative which brings it

together so that these conversations do not remain conversations. It's an ambition. We want to create something but I think that ambition has to translate to action. And in order for

that to happen, as I said earlier, we need to have institutions that exist that are able to coordinate, ensure that uh the conversations don't remain just conversations but translate into real

action. And ultimately we have uh what do you call u tangible results in the sense that we see uh institutions in different ASEAN member states being able to work with each other to deal with

threats particularly when those threats uh occur in our domestic system so that we can share this information about what those threats are so that the Assam member states can actually learn from

our experiences and prepare uh for eventualities like that uh like like that in their countries and ultimately also when it comes to larger threats online fraud for example scams. You have

deep fakes today. You have uh um huge concerns about certain vulnerable groups uh that are going to be impacted, children uh the older uh folk and so on and so forth. Right? So this is

something that stretches across uh the region. How do we deal with it uh in a coordinated way and ensure that the conversation doesn't just stop with the government of the day but is a

conversation that expands uh over a period of time with clear policies that we can actually execute. The second layer that I think we need to think about is uh in the event there's a need

for um execution. Um you know when we speak about risks in AI and we speak about how we're going to govern uh these risks um we often talk about uh uh standards. We often talk about

regulation. We even speak about legislation at at times for uh areas that pose uh higher risks. But ultimately it really comes back down to you making sure you have an agency that

can enforce it because you can have the best standards, regulations and legislation but if there is no institution that's really able uh uh to implement those standards to ensure that

they are properly implemented and also to ensure that uh uh rules uh for failure to implement are enforced uh then those uh standards, regulation and policies are really going to be just

strong on paper uh but they're not going to really have uh that impact that you need. So again, how do you build um you know this mechanism across ASEAN where every country strength strengthens

themselves domestically first and then uh moves across uh uh to the Assean member states and hopes uh to lend uh to learn from their experiences so that we can together move ahead in this new

world of AI and I think uh the threats that we anticipate um in future. Now uh the third part to it I think which is really important is also ensuring that whilst this goes on you create those

policies u you have institutions that enforce and the discussions uh persist at an assean level uh I think what is important is also to have uh that expertise looking at what comes next we

must make sure that our countries are prepared for the risks that are to come with the next uh generation technology this is important because you don't want a situation where where new technology

is adopted and they risks that come with this new technology, you're not prepared. I think that's something we want to avoid and that's the reason why I come back to where I started off. Uh

we really need to look at building institutions that have the expertise and of course uh are able to sustain as we go along uh and and to to to build and deliver uh something that's impactful.

Sorry, but that's in short what we are doing in Malaysia today. &gt;&gt; Excellent. Thank you so much. Okay, let me turn to Vice President Kim and talk about the World Bank, which has

been at the forefront of digital public infrastructure, helping countries leapfrog legacy systems. We note that frontier AI systems though are arriving in the global south under very different

conditions from previous waves of technology and governments are under pressure to deploy AI systems quickly often using models that haven't been adequately tested let alone certified

for their context languages or risk tolerances. So, how can the World Bank help global south countries move from being passive recipients of frontier AI to active

shapers of safety and reliability requirements before the systems are deployed at scale? &gt;&gt; Thank you. uh you know one word definitely we need

to uh make our clients well prepared uh before from the scratch uh when they just you know they design the AI uh systems definitely they

need to design the safety you know architecture within the system that's very in general that's pretty correct but real challenge is that nobody can really expect a new type of

new threat especially our you know some some countries uh in in a low capacity it is really hard to figure out what that will be. So

that's the the in order to tackle that type of you know irony uh and dilemma uh we need to very closely are working with very you know developed economies company and government and very high-end

uh you know uh the examples so that we can really well connect those uh good example to the developing world. One partnership is in one of the the good example we are you know helping our

country for example some big tech company uh who is running some red teams so that they are trying very hard to attack their system in advance by fully utilizing AI. So through that type of

you know practice and experiment they can learn how to prevent the AI attack in the future which is um pretty much possible. So in this way uh it is ine inevitable

for our developing countries to keep you know checked on the the new you know trend and new you know uh innovation even in in in this uh safety protection area it is the only way so I have to

admit this constraint but think about there's some some anecdotal story in in East Asia in China and in Korea there's a two merchant who is selling two

product number one is sphere uh which and then they are just keep saying that this sphere is so strong so it can get through any kind of shield. So this is one vendor. The other vendor

is selling shield and then they are they saying that this shield one of the most safe and strong shield. So any no sphere can get through this shield. This is exactly ironical situation. If you think

about AI, AI attack is a the sphere. Now AI is so strong and smart and then really capable so it can get through and and hack any system with high-end intelligence and

and knowledge. But good news is that on the other hand we also can build strong protective system by fully utilizing AI. So this is one good news but we the the constraint

is that we do not clearly know how AI can really evolve to fully protect those big attack uh in the future. So it is in order to you know solve this type of ironical situation

from the developing world point of view and from the World Bank point of view. This is the only way to very closely work and collaborate and learn from the advanced technology and advanced company

and advanced country. &gt;&gt; Thank you so much. Um, last but not least, Mr. Yan Talen. Um, you occupy a very rare position in this landscape as a founding engineer of Skype, an early

investor in Deep Mind and Anthropic, and you're also the co-founder of the Future of Life Institute, which last October released a statement on super intelligence calling for a prohibition

on super intelligent development until two conditions are met. number one, broad scientific consensus that it can be done safely and controllably. And second, strong public

buyin. Uh let's let's just ask the hard question. What would an effective prohibition look like in practice? How could that work?

&gt;&gt; Thank you very much. Uh so I think I'm kind of like a little bit different from the people on this planet on on this plan on this panel and uh most that too I guess um uh that I'm kind of my main

kind of threat vector about my main worries about future like less about like how AI is being deployed and diffused and uh taken into uh practice and my I'm way more worried about what

is happening in the labs in the in the top AI companies uh because they are now in a cutthroat race uh to build something that is smarter than than they are. Uh they are in a cutthroat race to

build super intelligence. Uh and like I mean we just saw yesterday the picture where where uh with a photo with Narend Modi Modi uh Dario Made and Sam Alman refused to link hands. I mean this is

like indicative. We also saw uh both um Dario and Deis Habis uh call for like slowdown in uh in Davos uh last month. Uh they just can't do it alone. Uh and uh I think there are like two reasons

why it's uh like uh kind of unfortunate situation. Uh one is that the US as a country is conflicted. uh they they basically rely on AI uh for their economic and competitive uh power. Uh so

so they're like very hesitant to kind of meddle uh with uh u now cut situation in AI companies and the rest of the world really doesn't understand how big danger they are now. Uh so it's uh uh part of

the reason why we did the super intelligence uh statement is to kind of create kind of awareness that there's increasing political demand to do something uh about this situation. Uh we

now have more than 130,000 uh uh signatures uh which is like many times more than we had than our original 6 months uh post letter had in uh 2023. Uh so so yeah that's that's uh like like

if there was kind of enough pressure I think clearly like the rest of the world is still kind of more powerful the than the kind of leading uh AI uh AI countries uh there are more people

there's more economic power etc. So if there was like enough pressure uh this could be solved. Like the way I put it is that uh it's super hard to do like a $10 billion project it's impossible to

do it uh if it's illegal. Uh so like uh having like this trillions flow into into AI actually makes it easier to govern uh than harder. &gt;&gt; I'm Yeah. So I I'm tempted to follow up

with a question about investors and their potential role in this. Um they are obviously playing a decisive role in shaping the incentives. Um but they're largely absent from the governance

conversation. So what would it take to bring investors meaningfully into the safety conversation? Yeah. So, so yeah, I think the simp answer is there is kind of simple like I

don't think investors play much of a role anymore. Um, because uh the the leading AI companies now are kind of above uh the level where private investors uh can u influence them. Uh

they will now IPO soon. And if you're like on IPO market uh there is a like like sort of sort of level playing field which means that like if somebody's not funding somebody else will uh so yeah I

don't think investors uh investors could have affected things but like five 10 years ago. &gt;&gt; Great. Okay. So since we're running short on time I'm going to ask one

question and ask you all to answer it. Um which is about the 12 month window. the very shortly each shortly um many in the AI safety community believe we have a narrow window perhaps 12 to 24 months

before frontier AI capabilities advance beyond our ability to evaluate and govern govern them. So what would you recommend is prioritized between now and we're basically in the next year to two

years each of you to enhance safety and security. &gt;&gt; Are we limited to one &gt;&gt; one priority? &gt;&gt; As long as you do it quickly, you can do

a couple. &gt;&gt; I think there are two really. Uh I I I I think the AI safety research priorities need to be refreshed because the field has moved so quickly. um the Singapore

consensus identified a set but as soon as they are published uh we we recognize that they will be out of date so we need to refresh it that's why we're going to have um the second edition you know

worked on hopefully in a few months um the second thing I think is that uh we can't just keep thinking about frameworks you know and guidelines at some point we need to uh be able to

introduce better testing tools and until we are able to do so um the companies that are developing and deploying um AI models um they they they they also don't have a very practical way of giving

assurance. So I'd like to see in the next 12 months uh some further advancements uh in those two areas. &gt;&gt; I I'll be really quick. I mean I I know there's always a temptation in these

sorts of conversations but it's the one thing that can sort of fix it all and and the truth is there is not one thing. uh we've we've got to go as fast as we can to apply catchup to a degree, but

we've also got to go as comprehensive and as deep as we can. I mean, there's just no alternative. And I mean, there's catchup to be played. We got to we got to put a real effort and it's got to be

right across the board. And I mean, I I don't think that you can just say there's the one thing that will make us all safe and it's going to be okay. Minister Goind

&gt;&gt; I think as I said earlier we need to start thinking how we can build structures and perhaps institutionalize this entire conversation about building uh security around AI and its

governance. Um in this regard u we have to understand that things are going to move very quickly and you're going to see new technology develop very fast which brings new risks as well. So in

that regard you got to build something that's sustainable and I think in uh in order to do that institutionalizing it um should be the priority. Everyone is really rushing for AI system

development, AI solution development. That means AI is currently AI safety measures currently underinvested. So I really like to urge all of all of us to think about this is not free you

know things. We need to spend some money to protect the system in advance from the scratch. when you design the system. So that means we should allocate some money to fully invest in in this safety

measure in other events. &gt;&gt; Great Yan Holland. &gt;&gt; So slow down. We really need to slow down the the the companies are asking for it and uh if we like instrumental to

that would be basically transparency like uh more people should know what the leaders of AI companies know uh in order to basically understand how crucial the slowdown now is.

&gt;&gt; Great. Well, I believe we have a little bit of a close coming and thank you all so much. I wish we had had a day to talk about all of these issues, but thank you so much.

&gt;&gt; Thank you very much. Thank you very much Eileen and this fantastic panel, excellencies, colleagues, friends. What we've heard today confirms something important. The coordination gap frontier

in AI safety is real and it is urgent and as we've discussed today it is closable and before I hand over the floor to Asama to close off for a few minutes of

remarks and reflection I'd like to invite you all um to the United Nation General Assembly the next edition in New York where we hope to organize the fourth edition of AA Safety Connect and

hopefully with many of the great policy makers and leaders we have heard from today to carry forward that collective effort. Osama, the floor is yours. &gt;&gt; Well, thank you very much and uh we are

one of those absentee co-organizer in this one. So uh you know because being a local but I just want to uh I mean apart from thanking each one of you who didn't get up and uh you know go out of the

room and every one of you who uh gave all the safety remarks before usage of AI. I on behalf of 40 million people that we have reached out in the last 23 years and billions of the other people

whom we are going to work for. I want to uh suggest that the entire safety aspect of AI should be more from please save people from AI right because that's the safety like it's it's a car on the road

you know we have to save people before you teach people how to uh things so we also have to keep a very very strong uh thing how do we save human intelligence from artificial intelligence and how do

we inbuilt in the safety guards and all the ethics and all the all the you know policy uh playbooks. Thank you very much. &gt;&gt; Thanks so much.

Thank you. &gt;&gt; Bye.
