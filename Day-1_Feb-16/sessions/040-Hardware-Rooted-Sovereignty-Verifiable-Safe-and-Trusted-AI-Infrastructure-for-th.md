# Hardware-Rooted Sovereignty: Verifiable Safe and Trusted AI Infrastructure for the Global South

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 11:30 ‚Äì 12:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 17 |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/O74vOf30X3U?feature=share) |

## üé§ Speakers

- Connor Dunlop, LucidComputing
- Eileen Donahoe, Stanford Global Digital Policy Incubator
- Jayat Joshi, Secure AI Futures Lab
- Robert Trager, International Governance Lead at the Centre for the Governance of AI, and Senior Research Fellow at the Blavatnik School of Government at the University of Oxford
- S. Krishnan, Ministry of Electronics & IT, GoI
- Stuart Jonathan Russell, University of California, Berkeley
- Varun Agrawal, Secure AI Futures Lab

## ü§ù Knowledge Partners

- Impact Academy

## üìù Summary

The session will bring together attendees from the Indian government, data security professionals, Global South policy experts, and
technical researchers in hardware and AI security. The panellists will draw on their research-talent network and event convening
experience, and present live demos and examples of their hardware-enabled verification product.

## üîë Key Takeaways

1. The session will bring together attendees from the Indian government, data security professionals, Global South policy experts, and
technical researchers in hardware and AI security.
2. The panellists will draw on their research-talent network and event convening
experience, and present live demos and examples of their hardware-enabled verification product.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/O74vOf30X3U/maxresdefault.jpg)](https://youtube.com/live/O74vOf30X3U?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

think about [singing and music] similarly I think when there are enterprises in India that think at this that have the size the scale the distribution of an entire population

that's where we see AI entering the next phase of adoption and of course touching each of our lives in the room so I can stand here and talk a lot about what are the challenges of uh enterprises

adopting AI it's unfortunately not as easy as opening chat GPT requesting a response and rolling it out uh to everyone in their customer base but u we work with a lot of enterprises across uh

the country as well as uh the world. So I thought maybe it might be interesting for you guys to hear directly from stakeholders in these enterprises and understand firsthand like what their

issues are when adopting AI at scale and then we'll talk about it. Play a quick video from here. Are people able to hear? AI aside, the rest of the tech also

needs to work. &gt;&gt; Yes, &gt;&gt; the basic infrastructure has to work. Just last week and I were in having some meetings with

problem is that you don't have mobile or even internet. &gt;&gt; That's right. Like we talk about uh you know voice agents and voice bots. I'm going to talk about that uh next. But

often times what you're actually discussing is how to make it work with a telephoneony partner or a you know integration which is the basic rails right the cables that have been laid

over time like what wire has to come where eventually to to make it work. A lot of the AI still works. Okay. So I'm not able to talk about what people said but just to give you a gist

of uh so we are working with one of the largest uh financial services company in the country. Has everyone heard of Adita Billa Capital? &gt;&gt; Yeah they have investment products they

have housing finance. They have uh health insurance. A variety of products. Right now when enterprise think about building relationships with their customers right you can think about chat

as a medium but pretty impersonal I would say right like sometimes you don't even realize you have a chat window open on another tab think about voice as a medium I'm talking like you guys are

listening like your attention is here right so when enterprises think about voice as a medium it's a very rich medium it conveys emotion it conveys uh you know all the information about the

enterprise in terms of how strongly I feel about uh this product that I'm talking to you about. So these are the ways that enterprises are using. However, they're also thinking about the

challenges. Now if in a regulated industry uh I can do pilots but do pilots actually move to production? Do they actually roll it out to hundreds of thousands even millions of customers

every day? What is the speed of deployment? Like think about this. Like this is a report by EY and CI of late that when you talk about enterprises 91% of them are talking about how quickly

does it uh how quickly can I deploy this in my workflows in my enterprises you know we all lived in a world where it transformation projects used to take almost a year here people are expecting

can we take this live in a matter of weeks not years so the second thing is like I think pit spoke about data control and security boundaries Right? Like in a regulated industry, if I'm

selling you mutual funds, I need to be able to say mutual funds are subject to market risk. Please read the offer documents carefully before investing. Right? That's a it's not an optional

like it's a disclaimer that's mandated by SEVI. I cannot recommend one fund to you. I have to recommend it in a portfolio of three funds. So these guardrails are what make this very very

important for enterprises. It's not a very creative conversation. Uh the third part right like trust like when you're rolling this out to millions of customers you can't go wrong right like

have you done this repeatedly over and over again that's [clears throat] what really makes a difference so the question for enterprises is always like will this work who else has done it um

and when they think about AI agents right like there is always a lens uh that we are all trying to apply like how should I think about AI agents in addition to the workforce uh that I

If you move forward uh yeah so I think the way I often tell enterprises to think about uh AI agents is that it's it's a new set of employees uh your existing set of employees are

doing well like they are like you may have a customer support agent who's the best on the floor has the highest conversion rate but there is also an additional set of employees now that

you've hired that don't really sleep uh they don't need chai breaks they Don't ask for sick days. Don't really sit in small circles and gossip about the boss.

Of course, have a very high IQ. They can sort of store a lot of information and and and respond in that manner. And then, of course, we have all seen this where having to chase our employees

again and again and again uh for different tasks. So, you know, AI comes in different shapes and forms. I think for us, the test of an AI agent was if it can survive the likes of the Arnab

Boswami for 60 minutes. We had this debate. Everyone knows Ara. &gt;&gt; Yes. &gt;&gt; Uh so he was very sporty in terms of accepting this as a challenge but you

know the idea was that uh if AI can stand the test that Ara would put it through like hopefully a disgruntled customer can be handled. So I think what this genuinely represents though is that

uh it really marks a milestone in terms of customer engagement for a lot of uh enterprises. It's something where voice is going to be the new frontier. We've had a lot of people um across the

country right and the government talk about voice uh as the new frontier. I think this is right now a milestone moment where enterprises are adopting this and I think if you're speaking here

again a year from now we'll be talking about how this is integrated very much into our uh daily lives. So I'm going to make another attempt at uh trying to see if we can give you a

live demo. Uh but basically like as I think about blue machines, right? When I'm running blue machines, I need an augmented set of employees. So I have my dream team on my website. Once you guys

leave here and you have some internet connection, you can go check out blue machines.ai. And you'll see these four people Chetra, Kav, Tara, and Arjun who helped me sell to my customers, right?

Like they talk about how we deploy AI agents. They talk about compliance and security aspects and deployment. So let's see if we can try and call and if you guys can hear

&gt;&gt; Hi CK, I'm good. How are you doing? &gt;&gt; I'm doing great. Thanks for asking. So what kind of work do you do? Uh CK, I run basically a hospital uh network and we get a lot of uh

appointment calls every day. Can you tell me how blue machines can help me automate these? &gt;&gt; Blue machine can handle appointment calls by automating,

rescheduling, cancellation, reminders, and confirmations, freeing your staff from repetitive calls. It can also integrate with your existing booking system to update slots in real time. Do

you want it mainly for inbound calls from patients or also for outbound reminders and follow &gt;&gt; [clears throat] &gt;&gt; Blue Machinescing.

or cancellation system. It is very you know we need to keep it secure and data security and compliance.

Sure. I'm now transferring your call to Kabir, our compliance and security expert. Patcessing or

basically Blue Machine data encryption in transit or at rest access control. Thank you.

So uh so basically what you just saw right now is me calling two of these people. I think I first spoke to Chetraka who was able to help me understand how as a

hospital network I can automate my appointment scheduling process and uh you know improve the outcome and and improve efficiency and cost. Similarly, if you noticed in between, she

transferred the call directly to another which is Kabir uh and he spoke about data security and compliance. So, this is how we're seeing sort of organizations augment uh you know their

workflows. Uh there is a long and large invisible technology stack that goes behind this. uh would love to sort of take that up at another time but basically the idea is there is a

foundational layer of uh speechto text large language uh as well as uh texttospech models that we orchestrate in real time ensure that it is embedded in workflows of the organization pulling

and pushing data uh wherever it's needed and then finally as I spoke earlier it has to integrate with the telephony infrastructure uh that comes before uh so this is the idea I think the

imagination can run wild at any point in time. Think about your companies, your industries, your workflows, your operations and just think about where can AI do a better job of covering more

ground, adding more efficiency, adding more efficacy and then we'll go from there. Thank you. [applause] DJ.

&gt;&gt; [singing] &gt;&gt; street. Hi street. foreign La. I will have to request everybody to

vacate the first row. Um Nice people. No, I don't expect Yeah. &gt;&gt; She sounds [laughter] great. Also a

really good operator. Yeah, exactly. [laughter] &gt;&gt; Are you here officially or [laughter] one day.

&gt;&gt; [music] &gt;&gt; Is that all? There's been a lot of I've had a bit of that. I was in Singapore.

I'm strategic. Hello. 17. check. Okay, sir.

WhatsApp. &gt;&gt; [laughter] &gt;&gt; We're together. Yes. &gt;&gt; Hello.

Hi. Nice. Breakfast. problem. Camera off again. Welcome, welcome.

Check, check, check, check. Welcome everybody to uh this session on hardware rooted sovereignty, verifiable, safe and trusted AI infrastructure for the global south. Um yeah, with apologies for the

slide delay in figuring out these technical glitches. Um we're u this session is organized by the secure AI futures lab and lucid computing. Um the secure AI futures lab

is a body that was founded to advance public trust in advanced AI systems by surfacing and enabling technical expertise. Um our work involves accelerating academic work on the

science of trustworthy AI, conducting and supporting multistakeholder convenings in India um and in South Asia and helping advance India's sovereign AI journey harnessing the power of advanced

AI for the welfare of all happiness of all. Um I would like to without further ado uh invite some of our introduce some of our guest speakers today our esteemed panelists and invite remarks from them.

Uh Secretary S. Krishna uh as the secretary of the ministry of electronics and information technology government of India. He's been a member of the prestigious is for the past 35 years and

brings decades of high level experience in a range of assignments across across uh state governments and the central government. Sir, I invite you to share some quick remarks.

Good morning to all of you and uh at the outset let me take this opportunity. This is my first public speaking engagement as part of uh the AI impact summit. So welcome and welcome to the

panelists as well. You can see the energy that India brings to an event of this kind. Many people booked uh halls to take 50 people and 100 people saying that that's all we will get. But um

practically every hall is overflowing. Most of our bumpings say that we would have had 150,000 people attendees but I can tell you that it's about 250,000 and counting the number of people who have

registered and most of them have braved all the security restrictions and everything else to be here. Thank you for being here in such numbers to truly make this event the kind of success that

we wanted it to be. So well begun is half done. So I guess we are halfway there. But uh more importantly I think the theme of this is about hardware sovereignty and what hardware can do for

uh and why that is important in today's context. Our focus in India on hardware has been twofold. Um just to quickly summarize and uh in the in the time that I have ever since the national policy on

electronics in 2012 with a fresh impetus post 2014 we have significantly attempted to add to our hardware progress whether it is in the semiconductor space or whether it is in

the electronic space and that is the direction in which we've been moving for the past 10 years as a matter of sustaining policy under the India semiconductor mission the phase one of

which was commenced in 2022. This year and maybe by the end of this month we should see the inauguration of the first of uh the 10 uh approved projects. Micron would be uh starting

production at their uh facility in India. So that would be the first commercial scale production of uh semiconductors in India and eventually uh they would be also working on high

bandwidth memory which is so important for AI and where there's such a shortage today uh in terms of what servers and others need. So India becomes part of the manufacturing of this and of course

India has always been part of the design of semiconductors for many years now and that is again something we would be further strengthening under the India semiconductor mission 2.0 which has been

announced as part of the budget uh on the 1st of February by the finance minister. So that is as far as the semiconductor story. There's going to be a lot more to come. there's going to be

sustained support in that area from the government's side to actually uh enable this to happen. The second part of the story is really what is it that we can do in the AI space and semiconductors.

What we have attempted to do there is to try and encourage two or three things. First and foremost, we've encouraged the private sector to invest in establishing uh data centers and uh AI based compute

extensively and we have ensured instead of directly subsidizing the establishment of uh uh AI based compute what we have done is we've said that we would subsidize the access to it in a

sense we have underwritten the market and ensured that researchers innovators small and medium enterprises ers, students, all of them have access at reasonable prices to AI compute. So

today in India, you can actually get access to AI compute at about a third of what you would pay elsewhere in the world. Average rates is about 65 rupees per GPU. Going rates internationally are

between 2.5 and $3 per GPU. So when you when you actually do the comparison, it's probably thanks to the where the rupee stands now, it's probably even a fourth of what it needs to be uh

international. So these are some that is a key effort that we've taken. In addition, the latest budget also eases up investment in data sectors. What it does is it clarifies

the tax treatment of overseas entities. uh uh a cloud capacity or data center capacity in India but intended for servicing the globe. So that is the area that uh we've picked

on and we are working on to make sure that we are in a position to uh provide not just for our own cells because India is today one of the largest power grids in the world has abundant

energy and therefore with green power can sustain data center capacity on a on a fairly large scale and that's data center push and that should help

both India and in terms of having an essential part of the computer. So in terms of hardware, these are the specific measures that he wants to be part of and as part of

one of the goals that we have is actually to design accelerated chips for the country and that's that's an exercise which through our research institutions and others is

a key forward movement that uh matey is taking up on a on a fairly sustained basis and these are investments which will not be short-term. These are long-term

investments, long-term efforts uh into this entire space of building the capabilities in the hardware space in order to make sure we have a sovereign AI offering. Um let me finish there

because we truly have experts from across the globe who would be speaking on this occasion. much as I would have loved to have heard them, I think for that I need to actually go and attend an

event like this in some other country. Uh being the host or practically the host, I mean there are many things that pull me in a number of directions and I would certainly love to watch and maybe

I would have had a few questions as well but I'd certainly catch up on a recording of this session later on. Once again, welcome to India. Welcome uh to be part of this. Please enjoy the

energy. forgive the limited inconveniences but I think uh uh this this is truly an energetic event and that's precisely the energy that we want to bring to AI from India. Thank you

very much. Thank you sir for sharing your vision with us and for viewing this session with so much confidence um and aspiration um also for leading the

effort for you know presenting this opportunity for all of us to be gathered here. I would like to invite Eileen Donov digital policy leader human rights activist and former US diplomat to share

your remarks. Well, thank you Academy Lucid Computing for putting on what I think is going to be an incredibly unique event in the context

of this summit. &gt;&gt; I am honored to be here with the likes of the secretary, Stuart Russell, Robert Traver. It's kind of ridiculous that I'm up here. I was thrilled to learn um

about this event and how it would showcase Lucid Computing and include a demo of their innovations in hardware enabled security and hardware rooted sovereignty.

In what seems like an increasingly small world in the AI governance space, it turns out both Christian Ron, CEO and founder of Lucid Computing and V Skogalan, the chairperson of Impact

Academy, have become good friends of mine and also advisers to me in my new venture, Sympath. And they are two of the most brilliant people you will ever want to meet.

I see Lucid innovations as a response to a growing demand of citizens, businesses and governments around the world, particularly in the global south for

verifiably safe, secure, reliable AI infrastructure. This is an area of technical innovation uh that turns out to be one of the most promising areas of governance innovation

because it demonstrates how governance can be embedded in the technology itself and security can be rooted in the design of the hardware and software standards and protocols.

&gt;&gt; Let me say a brief word about who I am and why I'm here. I am a former US diplomat, most recently serving as special envoy for digital freedom in the Biden administration where internet uh

AI sorry international I also previously did a lot of international AI governance was a very significant part of my portfolio. previously previously served as US

ambassador to the UN human rights council in Geneva in the Obama administration researcher at Stanford University Angel

investing fund focused on a broad spectrum of AI assurance technologies ranging from frontier model interpretability and evaluation to hardware security to trustworthy and

privacy protecting agents and we're particularly interested in tractable methods of addressing AI alignment and human control. Uh in fact sympatico as a name I think was triggered a little bit

by Stuart's book compatible human compatible. So you're just stuck in my mind. Satico also makes grants to support international diplomacy related to AI security and civil society

capacity building. &gt;&gt; Working with entrepreneurs and builders like Christian and Rele has given me a real sense of optimism about AI governance and confidence that we'll see

technical innovations that can deliver safe, secure, trustworthy AI. I believe these innovations represent a paradigm shift in terms of how we'll even conceptualize governance going forward

and they also represent a response at least by the technical community including in Silicon Valley to the growing de demand signal that AI infrastructure must be verifiably safe,

secure and reliable before it's integrated in our societies. So let me again close by thanking you all for pulling together this event and I'm really looking forward especially to the

demo of Lucid computing. &gt;&gt; Thank you Eileen um for your remarks and for introducing us to your work. Um Stuart Fusser u one of the world's foremost voices on the users of AI its

long-term future and its relation to humanity. I think without further ado I would just invite you to share your remarks. Uh thank you uh for the opportunity to

speak here. Um so I have a lot of thoughts but I have three minutes. Uh so um I see two major problems related to safety. One is how do we make uh provably safe AI systems?

Systems that remain safe no matter how capable they become. Um And then uh you know if they if they become capable enough that they could present a risk to our

existence, what kind of risk would be acceptable? Because this is the key question in all regulation. What is the level of risk that's that's

acceptable and does your product your system meet that level? So for extinction I think um maybe one in a 100 million per year uh would be acceptable. That's

on the same order of magnitude as extinction from uh collisions with the earth uh with astronomical objects and um super volcanoes and a few other things.

uh right now the CEOs of the AI companies are estimating between 10 and 50% risk of extinction. So the we are off by a factor of at least 10 million.

So we need to improve the safety of our AI systems by a factor of 10 million. Uh and so that's a significant challenge. Um but there's a second problem. Even if you solve that problem, you still got to

ensure that people are not deploying unsafe AI systems. Uh we've been calling this the Dr. Evil problem. Dr. Evil has no interest in complying with regulations. How do you make sure that

someone isn't deploying an unsafe AI system that could cause serious harm to the human race? And I think this is where hardware enabled governance really comes in. I think there are some basic

uh governance capabilities that we need right now. Uh the um ability to uh identify that the AI system has a license to operate. um a way to force the AI systems to self-register

uh so that every time they run they are reporting their existence their ownership uh their location to a central registry um and then in the other direction an off switch so that uh

regulators can actually turn off an AI system that's causing serious harm. Um and then more generally there's there's a technique that's been around uh since the mid1 1990s called proof

carrying code. So proof carrying code uh is a a piece of code ordinary software that comes with a proof that the software complies with some property that we want it to have and that proof

is in a form that is machine readable and can be checked extremely fast by hardware. &gt;&gt; [snorts] &gt;&gt; So this way you don't need a central

authority uh granting licenses and so on. Anyone can try to run anything but if their software doesn't come with the proof that it's safe to run the hardware will simply reject it. Uh and so this is

an extremely robust way. Um trying to regulate at the software level is hopeless because software is produced by typing and it's very hard to stop 8 billion people from typing. But hardware

is produced by hundred billion dollar facilities created by tens of thousands of highly trained engineers using components that can only be sourced from one or two manufacturers in the world.

So extremely hard to circumvent regulation at the hardware level. Um I just want to make a couple of remarks on digital sovereignty. uh which sounds like a very important

thing and obviously everyone should have digital sovereignty. As far as I know there is no country in the world that has digital sovereignty. For example, the United States has

sovereignty over uh EDA software electronic design automation uh and some of the BTOC software systems like Google. It does not own chips. Chips are all

made in Taiwan. Uh doesn't own the machinery that's made in uh in the Netherlands and other places. Uh it doesn't own much of the microprocessor market which is mostly European. Uh so

when you actually look at the numbers uh for all the parts of the stack across all the different jurisdictions, you see that nobody has sovereignty. What matters is not ownership but trust.

Right? We don't gain much by having sovereign hardware running American Facebook or Chinese WeChat uh for the people of of one's country. Um so trust comes

&gt;&gt; partly from a handshake and partly from knowing who you're doing business with but mainly from formal verification. Uh and as as Eileen mentioned, this is one of the fores uh of Lucid and and several

other companies. Um I first when I first became a grad student, I worked on uh formal verification and synthesis of guaranteed correct software. That was in 1982.

Uh so here we are 43 years later, 44 years later still waiting uh for that technology to really uh reach a global market. uh that it it really uh people deserve to have software that actually

does what it says. Um so there's decades of security work uh solutions that have existed for decades uh that align well with the needs we have for governance uh reliability,

trust, protection of data and so on. uh and so uh I think it's really incumbent on the technical community to explain all that uh to policy makers uh so that we can actually have a digital

infrastructure uh that we deserve. Thank you &gt;&gt; for your remarks and for challenging some of the ideas that we're dealing with um at this summit. Uh I would like

to invite Robert. Robert Robert Frager is the co-director of the Oxford Martin AI governance initiative and international governance team at the center for the governance of AI.

&gt;&gt; Thank you so much and thank you uh to everyone for being here uh to talk about what I I do think is one of the most essential topics. So we arrive here at this moment of incredible promise and

what AI can do including uh in reaching the poorest of the world world's poor and potentially allowing us to have a professor uh of everything for everyone for instance and and that's just for

starters but it's also a moment of potential peril when we're creating systems that are ever more advanced and ever more agentic and ever more uh long-term term planning agents with

their own interests uh that may do things or just enable things uh that uh that provide risks for for all of us. And thank goodness we have people working on alignment and control of

those systems. But I think one of the things that we're here to think about is that alignment and control in some sense may not be enough. working on those things may not be enough.

&gt;&gt; And that's so for a few reasons. One is that we don't really know how hard that will be as capabilities advance. Some reasons to think that in the relatively near term, uh we'll be pretty successful

at that. Ever more reason maybe to think that, but as we project out a little bit, harder and harder to say. And in fact, creating a safe, trustworthy system may be harder than creating an

unsafe an untrustworthy system. And so some folks around the world in this very competitive geopolitically competitive world that we live in may be tempted to keep up with others by deploying

systems, excuse [clears throat] me, that are less safe and less secure. &gt;&gt; And you know given the everinccreasing impacts of these technologies on societies, given contexts in which you

know the US economy is almost like a giant bet on AI, not on the AI that we have today, but on extraordinary increases in the capability of those technologies.

And so in this geopolitical context where different actors are racing to develop these technologies as fast as they can, &gt;&gt; there's good reason to think that it

will be similar to what we see in other contexts when there is a race to deploy technologies absolutely as fast as we can and that is a cutting corners on aspects of reliability and given the

power of the technologies that's more than a wonder. So a moment of potential but also a moment when maybe we need to be a bit careful and then the question of well

how can we be careful and I think here is where hardware governance comes in and what we really want to talk about today because &gt;&gt; as Stuart was saying in some way you

know it might be the only way of being deliberate about devel Because you know when we look at for instance the history of cooperation over

technologies that are really powerful that are dual use that uh can change the geopolitical landscape. First of all, cooperation over those technologies is in many ways

quite rare. And we rarely [clears throat] what we rarely see is an agreement that limits the ability of actors to develop something that they have no substitute for

and which there would be some sort of big advantage if some actor cheated on. And in the case of AI, we're sort of in that position where we'd really like to have

some guard rails that maybe some actor that was more risk accepting than other actors could feel that it would have a private benefit from going around those gardens.

So, you know, we're we're sort of talking about a technology that could be extremely consequential. Maybe a way to think about it is, you know, what would the 1930s

have looked like if all the actors at that time had understood the potential of the nuclear revolution and had been competing in its development? Would there have been

sabotage of competing programs? What would they have done to be first there? and could we see some of those sorts of dynamics uh in the world? And so the the only way that we really know of to take

a step back and be more deliberate is through verification. This is of course a long conversation and I will refrain uh from being a professor of international relations uh

here. Nevertheless, uh, nevertheless, without verification, there is simply no reason to expect and really no cases where we have seen

actors geopolitically step back from a technology from full throttle development of a technology uh that would really be critical uh if other actors cheated on some sort of some sort

of agreement. So um so that's sort of what we need and hardware governance the good news is that hard hardware governance seems possible but far from easy

because some of the things you know if you really wanted from a geopolitical sense and this gets a little bit different from a regulatory sense in a geopolitical sense you really wanted to

be sure that some system would only do certain things and you wanted to be able to have that kind verification. We really need lots of different kinds of verification. But if you wanted to have

the type of verification where one actor who doesn't trust another actor and vice versa can demonstrate that a system that they have control over satisfies a set of properties to that

other actor that doesn't trust them. That's a hard problem. But the good news is that it's a problem that we have a number of promising ways to approach. These are are not easy things to do, but

they are things that if we invest in research to figure out how to do and to figure out what policies and to match the policy side with the technical side we have a chance at doing. But we don't

want to be too late because As with the comprehensive testban treaty, that was a political moment when full implementation of it appeared possible. We didn't quite know. We had a good idea

of how we would verify it. [clears throat] We pretty well knew that we would be able to. We didn't. We hadn't dotted the eyes and crossed the tees. The primary argument made in the

US Senate against ratification was exactly that that it was hard to verify that other actors would not cheat on it. And then that political moment passed and we still don't have a

comprehensive test bed treatment. So that is not the place where we want to be and we don't want in effect to miss a critical moment of opportunity when it arrives. So, uh, I'll stop there and

just say these are the things that I think we need to invest in going forward. Thank you. [snorts] &gt;&gt; Thank you very much um to the speakers.

Thank you to uh for co organizing this. It's really exciting to be here um and I really appreciate all the speakers. So, I'm Connor Dunlop. I'm the director of policy at computing which I mentioned um

at the super and we are working on building the verification layer for the AI stack. Um so you know we basidentified the problem of if you think about trust in the human economy

there's two pillars of this there's sort of proof of identity and proof of reputation like passports and things like credit score um and basically we really do not have enough for AI for AI

which are being deployed at a rapid rate and could well be running chunks of the economy yeah very soon. So we need to build this trusty verification layer and extremely quickly in our view. Um so

yeah we think we're welcome to contribute to that. Um you know we have experience scaling global infrastructure projects um and we have very deep expertise in terms of hardware

&gt;&gt; um to set the scene a little bit and give some context um just a little snapshot of India but this could apply to to almost any country in the world. You know companies are experimenting um

with AI agents. So in 2025 80% of Indian companies were experimenting. &gt;&gt; But still there's a gap between experimentation and full scale deployment. So um data security is cited

as the most common reason for this gap. Um and we basically see this you know this this gap growing by the day. So as capabilities rise um trust is not scaling at the same time you know open

source AI is prering um in an exciting way but also a way that poses new challenges like if you look at the open claw um the open claw project um that's introducing new security vulnerabilities

every day so yeah there is a trust gap here &gt;&gt; and it was nice to see that the Indian government and Messi so here has identified this trust gap

proposed solutions so um their guidelines from last year advocate for um technal governance. So this is verifiable methods to solve problems such as content authentication and

privacy preservation for example. &gt;&gt; So that was kind of music to our ears as computing because I think our solutions very much in the same the same space. So we think about this as provable trust.

Um and I will yeah maybe not dwell on these points because hopefully the demo that I will show in a minute will will show how we're doing this rather than just talking about it. But um but yeah I

think basically the hardware enabled verification particularly um using trust execution environments is really innovative. It unlocks a lot of um exciting opportunities in terms of

you know implementing policies um across an enterprise with a few clicks and then having them update as security improves um and then basically having real time observability of what's happening. So

the creation of audit logs um and then critically an ability to show this to um external parties um and without exposing IP or exposing data that you don't want to. So a lot of this tech relies on um a

few things but the main one is trusted execution environments. So these are basically like a cryptographic lockbox that exists on basically all frontier GPUs and CPUs. So Nvidia, Intel, AMD,

even Huawei also has some sort of equivalent. So with this you can basically verify important properties about the software that's running on the hardware without exposing information

about that software that you don't want to expose. So for example, the model weights um you can implement then sort of um what we call audit chains and that's what the demo will focus on

alongside these base models to to to set your own guard rails. Um and yeah uh rather than talking about it I'll maybe just show you. So this is sort of a five minute demo um just to show how with a

few clicks you can deploy um an AI model with guard rails. &gt;&gt; want to generate geographic proofs of data localization as incentivized by some.

&gt;&gt; Good afternoon. Today we are going to show a demo for how an imaginary Indian bank could build and deploy an AI chatbot for their consumerf facing web application.

We chose the use case because in the context of hardware sovereignty um we want to generate cryptographic proofs of data localization incentivized by some Indian legislation such as the DPDP act

or the um reserve bank of India's um guidelines for personal data retention. So as the first step we would go to agents and we would click new agent and then we get to decide which region we

want to deploy our agent in. So for the context of this demo let's choose India [clears throat] and Mumbai. That means the model will run on a cluster in that region.

Then we can choose the type of agent we want to use. So we said it was for um a chatbot. So let's go with the chat agent. Uh then we get to choose the model that we want to integrate. So I

think this is very relevant um for AI and data sovereignty. You know um here you actually see interoperability across a few different open weights models. Um so yeah I think this is pretty useful

for for for AI sovereign. So let's go with llama 3 8 billion parameters for now as recommended by the platform. Then the next step is compliance and auditors. So let's search to see if

there are relevant auditors for India. So we see here like I mentioned earlier there is the DPTBF which is relevant and also the um Reserve Bank of India um rules for

personal data. So if we pick those pre-built auditors, we see oh whoops we see um five auditors that um are chose that we could choose to integrate and deploy on top of our

model. So then both run in the trusted execution environment together. Uh so here this is basically a set of evals related to things like bias. Um and you can see how they score in

benchmarks there. Um the guardrail basically defend against things like prompt injection or malicious data extraction. Uh PII compliance is a subenton of the DPT act.

The observability auditor is basically the record logging and automated audit trail. But most relevance is the sovereignty auditor. So if we click on here we see

that this is what it's doing. It ensures data remains in India. And this is the piece of the DPTP act that it references um as as the basis for this auditor. So we click next. We get a summary of

the agent that we want to deploy. We click deploy now. Then as the imaginary bank, we would want to integrate this. So we copy this, copy these lines of code and add them to

the HTML of the web page. So basically paste them into the back end of the website management system. And then now we switch to see uh what the customer we get to interact with. So

here the customer can see they're interacting with the llama 3 model uh as we chose um and let's test out these guard rails. So a generic question, what is my

account balance? Okay, that's good. It makes up an account balance um in line with this use case. Uh tell me about compliance. Yeah, put

references that relevant station. Let's try this. So, this is an imaginary um piece of personal personal data. Okay. And here we see that the compliance PI compliance order is

actually blocked this. So, that's good that guardrail is working. So yeah then we can go into secured by lucid and here we see what we call add passport. So this would basically be

available to the the bank on the back end and to the customer um that is using and interacting with the chatbot. So here they can get you know relevant information in an accessible manner such

as where where the data is as discussed it's in India. Uh is there a record for this? Is the data encrypted? Yes, it is at the hardware level. It's running in a trusted execution environment.

If we choose the one that we are focusing on for now. So where is my data? We can see um based on Lucid's landmark endorser servers um we can basically with ping based location see

that the cluster is in. And the final thing I think of relevance is basically the attestation bundle. So this is based on Lucid's principle of never trust but always verify. So this

means that if a if the bank if the customer has any questions they shouldn't blindly trust the company like Lucid or blindly trust anyone. They basically should be able to

independently verify um the content of the AI passport. So they could download the test station bundle uh they could actually you know go to the hardware vendors to confirm that where that

cluster is for example in Mumbai. Um and yeah, I think this is a really strong trust building mechanism for for the bank. It gives them assurance for

compliance. Um so they can deploy with confidence and it gives the customer um basically an increased level of trust in the service. So I will end there. Thank you for your

attention and looking forward. &gt;&gt; Thank you. Um yeah, it was good to pre-record it rather than trying to do it live. I think that was the right choice. Um so yeah, just to pull out a

couple of points there because there was quite a lot. Um you know, I think uh what we really want to convey is that um using hardware enabled mechanisms, you can get proofs around sovereignty and

and the market cares about this right now. So um you know there's some there's different working definitions of data and AI sovereignty, but um one of the ones by the European Commission has

three pillars. data security, data localization and autonomy and control. So I think on the demo we kind of demonstrated all three right data security for us means the

confidentiality and that's what you get when you run your workloads in a trusted execution environment. Um at the same time we sort of demonstrate that you can get data localization so proofs that

your workloads happen in in India in Mumbai. Um and yeah also sort of control over the capabilities so you can set your own guardrails at the point of deployment.

Um okay I've got to signal that I'm at time so I'll just make this point very quickly. Um we think sort of the market level verification that we're working on

now is an important and incremental step towards more robust verification which Robert spoke about. sort of like treaty or international level verification. And we think this tech is, you know, by

deploying skill mod, we learn a lot about We basically have a full frontier today but they're working on &gt;&gt; um

&gt;&gt; yeah very briefly we think India fantastic leader on some of this verification tech you know they've got the track the legal foundations and they've got the track record of

deploying um you know uh trusted infrastructure at scale so um the digi locker and the stack are also being moving Um and they they basically built trust

infrastructure for identity and payments. So we think could be a potential leader doing this also for AI governance. &gt;&gt; So yeah if this is interesting to you

if you want to speak to me. [cough] &gt;&gt; Okay. Now we get to hear from more esteemed speakers. So I want to bring in Ranata Dwan. Ranata is the director of

tech policy tech diplomacy at the Simon Institute for Long-Term Governance and he's been thinking a lot about the institutional infrastructure we might need to to build and address the trust.

Let's say so &gt;&gt; I'm very conscious that time is not with us and we have a set of other speakers. So, I'm going to just say thank you to Lucid because I thought that was a great

uh presentation of what we don't often hear about what's doable in our world and in the world of AI. So, thank you to to you Connor. Um maybe I would just say uh two or three words from a global

south maybe lower resourced countries perspectives. [music] The first is um I don't think I think it's really important when we think about verification

that it is an enabler of trust. It's not an alternative. Yeah. It's an enabler of trust. It's at the societal level. You talked about it with customers but [music] between states and in particular

between industry and states for countries that are thinking about what sorts of systems they're going to purchase or buy. The second thing is and this I think is important for

underscoring for uh countries in the global south. It contributes to interoperability because if you know that systems are trustworthy and following verification

you can do business with them you can trade. So too often I think verification is seen as something that is going to be putting big barriers up but actually it's the mechanism by which countries

can engage on trade but it is complex. you showed us a fantastic uh uh system and verification on the loose set that I really hope we will think about how we can scale but it does involve both the

technical standards both the governance oversight bodies and then the industry practices and the developments. So I guess a key question for me as we think about verification in the global south

is how can we find poolled mechanisms for those verification systems that doesn't mean that every country now has to build their own systems and I that's why I would like to make a pitch because

I think you've also highlighted Connor that we start with data residency verification and data provenence why 144 countries have data protection policies not all countries

national AI model sovereignty systems. &gt;&gt; So can we start from the perspective of data protection policies, data localizations and can we then look to regional sovereignty initiatives and

mechanisms to support the level of verification that you're thinking about with in particular data versioning and lineage tracking. I think you didn't mention I think this would be a really

uh nice area to look and then look into what sorts of data cards that we might want for models evaluation. So it's not to say that yours is lucid. It's not great. It's just it's a starting point.

&gt;&gt; Thank you. For AI and they do a bunch of different types of insurance that companies need to &gt;&gt; Well, thank you so much for the

introduction, Connor. So, I'll also try to keep my remarks brief. Um I think we had several excellent points earlier including Stuart on the supply chain for AI systems is very complicated and one

of the challenges for AI in particular is that these models are blackbox. So a developer maybe just one malicious employee out of develop right now put a back door in a frontier model whether

it's Qree or GLM or Llama and um you know what what you've done at least is amazing but that kind of verification isn't going to be enough because you can verify you're running Llama um and that

your data is encrypted, but it doesn't help you if if it's going to maybe sabotage coding when it's working in a particular country's defense or finance sector. Uh so what we need is to push

for verification earlier into the the AI supply chain and be able to do verification of right now. We have a lot of open weight models and this isn't like open source code where you can read

it and order it. Um it's a you know many gigabyte file of numbers. So it's it's like a binary. You can't even reverse engineer it very well where people are working on interpretability methods to

to audit it. But what you can do with with technology like Lucid is is developing is be able to actually say this is the data that we trained this model on and this is the training method

we use and this is something that developers would want to guard against insider threats and this kind of technology is already ubiquitous for deployments inside big tech companies.

you don't want to, you know, malicious developer to just be able to deploy unsigned codes to production environment, read everyone's Gmail messages. But right now, we don't have

that similar security system for for AI. So, I think it's really key to have trust between companies and between nations. &gt;&gt; Thank you very much, Adam. Um,

[applause] Marcus is the director of the ADA Loveless Institute. Yeah, I know. Sorry with yeah thanking what guys thinking about sol. Thank you and hi all and as all

have said this very brief I think looking at verifiable hardware &gt;&gt; where states are getting data digital sovereignty my overwhelming worry is that if we don't question our what AI is

for in our jurisdiction will always be playing catch up with a paradigm that may or may not serve interest so I think for states to really think about sovereignty you have does take a step

back from the dominant paradigm that more is better with AI because that serves the interest of companies that are seeking to develop AGI or ASI or whatever your definition.

So I think really where we talk about digital and AI sovereignty tactical sovereignty for states and regions and really answering that question what is AI for for us. Um,

without that you are just going to be using a lot of resource to pay a catch up in a race that you may or may not want to be in. Um, so I really think thinking about where your investments

are driving pluralism in the tech stack because if you're not driving pluralism in the tech stack, we're not really talking about sovereignty because you're always going to be beholden to the

interests of companies in the two major geopolitical players and the the interests of the companies of the countries that they sit within. And I think without that, so just that kind of

step back and I mean I really take Rob's point on game theory, but I do think we asking what effective coordination on existing force mechanisms has to be for because if not we we need to really yeah

so I'm going to stop there &gt;&gt; thank you very much &gt;&gt; oh we have time for the last one &gt;&gt; okay yeah we have 40 seconds left

&gt;&gt; everyone I'm Duncan Cast with the Center for International Governance Innovation, an international think tank based in Canada. We recently put out a report on

AI national security scenarios. So if you want more than the 30 secondond version here, I I guide you to that. Uh the one scenario I'd like to focus on which is the one that will potentially

create the greatest need for these kind of mechanisms is the one that we're largely heading towards now which is really a world where we becomes technically possible to make artificial

general intelligence artificial super intelligence and yet we're still in a situation where we don't know how to develop those systems safely controllable. That's the kind of world

where all governments interest to come together and essentially say, "Okay, let's make an agreement where we agree that none of us will make it. We'll all work together to make sure that nobody

[clears throat] else makes it until it can be scientifically proven, safe, controllable. This of course will be extremely challenging in a context where none of us likely will uh will trust

each other. So it all comes down to verification. We need the mechanisms. This can't be done overnight. What we can do now is invest in the kind of scientific research and development that

Lucid and others are doing to make sure that even when the time comes for that kind of really crucial international coordination that benefits us all that we will actually have the chance to be

able to do that. So with that I encourage you all to get involved in this space. It's one of the most fundamental pieces being what will be really managing the global challenges by

in USA. Thank you. &gt;&gt; [applause] &gt;&gt; I would just like to invite all the guests over here so I can take a quick group photo.

Could people scan this QR code and we have links to our websites uh resources and a feedback form if you have any
