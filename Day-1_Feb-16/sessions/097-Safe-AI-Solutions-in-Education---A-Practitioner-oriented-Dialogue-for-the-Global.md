# Safe AI Solutions in Education - A Practitioner-oriented Dialogue for the Global South Perspective

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 15:30 ‚Äì 16:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 19 |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/LMVu3aTcjTY?feature=share) |

## üé§ Speakers

- Anil Ananthaswamy, IIT Madras
- Krishnan Narayanan, itihaasa Research & Digital
- Shaveta Sharma-Kukreja, Central Square Foundation
- Srinivasan Parthasarathy, The Ohio State University
- Sunil Wadhwani, Wadhwani AI
- Swati Vasudevan, Khan Academy India

## ü§ù Knowledge Partners

- Centre for Responsible AI (CeRAI), IIT Madras

## üìù Summary

AI's rapid adoption in education offers remarkable opportunities alongside complex challenges. Adaptive learning tools and automated assessments are transforming student learning and teaching methods. Yet, heavy reliance on these technologies sparks concerns over data protection, bias, accountability, and their long-term effects on learning ecosystems. As India pushes digital education initiatives, responsible design, deployment, and governance of AI systems are essential.

## üîë Key Takeaways

1. AI's rapid adoption in education offers remarkable opportunities alongside complex challenges.
2. Adaptive learning tools and automated assessments are transforming student learning and teaching methods.
3. Yet, heavy reliance on these technologies sparks concerns over data protection, bias, accountability, and their long-term effects on learning ecosystems.
4. As India pushes digital education initiatives, responsible design, deployment, and governance of AI systems are essential.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/LMVu3aTcjTY/maxresdefault.jpg)](https://youtube.com/live/LMVu3aTcjTY?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

Yes, &gt;&gt; good to go. Uh ladies and gentlemen, a very warm welcome to you all on behalf of the Vadwani School of Data Science and AI and the Center for Responsible AI

at IIT Madras. Uh it's an honor to be representing IIT Madras at the impact summit 2026. Uh before we begin, um I I'd like to give a quick introduction about the center for responsible AI and

the work that we do. In case uh some of you might not be aware of what we do, the center for responsible AI at IIT Metas is a multi-disiplinary nonprofit research center positioned in the global

majority. We are among the a few global institutions uh that specializes in both technical and policy research to ensure and enable responsible development and deployment of AI systems. The center

aims to pursue research in ethical and responsible AI and become the standard body in the country to recommend guidelines and policies to make deployable AI models and systems more

accountable, explainable and responsible. Through its work, CI that which is center for responsible AI brings together researchers, policy makers, industry leaders and civil

society to build AI systems that are not only innovative but also trustworthy and aligned with societal needs. Uh today's panel discussion is a reflection of that and it's my pleasure to introduce our

moderator Mr. Krishnan Narayan who's the co-founder and president of Ethihasa research and uh dist over to you sir. &gt;&gt; Thank you Shatsson. Uh and of course I'm I'm very delighted

that we have a a sort of a an outstanding panel panel with us today. Um in fact you know I was thinking so so for example Swati Swati is the country director for Khan Academy India. uh she

brings the uh the school you know the the the school perspective in in this conversation today. Uh Shaveta Sharma Kukra uh you know is the managing director of uh CSF Central Square

Foundation and she brings in a a perspective of taking these solutions to the to the market and uh you know how how that that implementation happens. uh Shini Patasarati

uh he uh is a professor at Ohio State University and he brings in the the higher education perspective into the uh into the mix. uh Anil Anat Swami is a is a professor of practice at uh at IIT

Madras uh you know he's authored multiple books one of which is uh why machines learn and so he will bring in the the perspective of uh you know the

technology behind uh safe AI and so on and finally we have Sunil Vadwani founder of Vadwani AI uh businessman investor philanthrop philanthropist So he's going to bring in

that perspective into this uh uh into this conversation. Yeah. So so we have a fantastic panel. Uh it is going to be like a thali for you. Yeah. It's nice some five things. Hopefully

you know it'll I have requested them to keep the answers short and sweet. Uh we also have lots of engagement with uh you know with the panel. Yes, about SA is done. Oh, this is the

right. So before we get started, I just wanted to set a context. Now this is a over the last four years uh I mean this is my perspective of uh on uh safe AI. Yeah. So uh over the last four years

I've been researching this this topic called co-intelligence. We can keep the mobile phones on silent please. So we've been researching co-intelligence along with professor Wanka Traaswami from the

raw school of business and Karen Jaman Ozan who's a philosopher and we've been looking at we have identified sort of uh six layers of a co-intelligent systems architecture. Yeah. So on the on the

left hand side the first three layers are more infrastructural if you will. So things like data uh you know the the the models what we call as machineachining cognition uh the

agentic systems yeah they all kind of the infrastructural aspect cover the the first three layers but that alone is not sufficient is what we argue that in co intelligence systems you need to have

the next three layers which are what I call as phenomenological layers. Yeah. So they the the layer four is where the humans and AI interact with one another. Layer five is life

experiences. So in the case of education systems, it has to take into account life experiences of the students, the teachers, the administrators and so on. And finally the sort of the ecosystem

view that what what learns in in one school how can the entire state sort of understand from it and so on. So so my my simple statement is when I whenever I think of safety uh I think of safety

across all six layers of this education coin system. Yeah. That's how I'm I'm going to be framing and uh thinking about it as as our panelists uh speak. Uh

okay so so I'll first start off with Swati um [clears throat] you know Khan Migo for K Migo which is which is now a learning assistant used by by millions

of students uh what does safe by design look like? Thank you for that question and thank you so much for having us and what a great group of uh diverse set of

perspectives in this panel. So really excited to have this chat on such an important topic. You know you're exactly right. Um Khan Migo is today being used at a very large

scale with about 44 lakh or four more than four million uh teachers uh sorry students and 14 uh lakh students. Um sorry for no I said that wrong that's 40 um lakh students and uh 14 lakh teachers

across the world that are using kanigo and out of which two lakh students and two lakh teachers are in India alone. uh so very large scale deployment already and we do believe very strongly that uh

any AI tool has to be very thoughtfully designed with the learner um at the center of it um and responsibly tested to ensure safety, security and privacy of the learner and everybody else that's

involved in interacting with the tool. Uh and KMIGO like you said is built with a lot of those safeguards uh in it. uh for example first uh Khanigo sits on top of Khan Academy a tech platform which um

has worldclass content in it which has been uh properly quality checked ratified and aligned to state curriculum uh board. So um it's pulling content from high you know highquality

well-known set of library and not just from anywhere from any unratified unqualified um you know content u source. That's number one. Number two it's also K because the Khan Academy

platform does not capture any personally identifiable information about the student. So Khan Migo does not have access to that information either. So that's another very important safeguard

that's built in from the get-go. [snorts] And third, Khan Migo does have the ability to uh it in fact logs all the interaction that the student has with Kan Migo and uses that information

to identify any potential risk risky situations or potential risky um interactions for the student and unsafe uh potential situations and flags it to the teacher. uh so that's kind of how

the tool has been designed with all of these things in in uh in mind but more importantly I would say we are deploying kanigo today at a large scale in India uh in a teacherdirected manner

&gt;&gt; so it's um it's got human control and supervision that's at the you know at the not just at the center of it but it's leading the deployment of kan migo and um so I think that adds another uh

and the moment you bring in some a tool AI tool in an integrated fashion with existing systems and not as a separate thing that's sitting on the side unsupervised uncontrolled then that jury

that's out on um AI being safe or risky I think that starts to um tilt very heavily towards the benefit side of AI. Uh so to me in a nutshell what I'm trying to say is that uh it's not say

you know design for safety is not just about designing the de the the tool the development of the tool but it's in in fact I would argue more about the deployment of the tool and the

implementation um making it human directed making sure that uh it's supervised and controlled by human beings um and um and and we're doing that we we're scaling it in a very very

thoughtful manner step by step uh capturing learnings uh at a small pilot uh level what's working what's not working what makes it safe and then and investing in that capacity development

and only then taking it to scale &gt;&gt; right right no in fact I I have read about the socratic way in which the things are taught to the the students and so on so it's fantastic uh in the

India pilots Swati were were there any u safety constraints which were different than than in the in the in the U or were they similar in India? &gt;&gt; Yeah. So I think see students are

similar everywhere in terms of the risks attached to um education learning process. uh we have to be mindful of similar things across the board whether and I'll give you an example in one of

the Latin American countries where Khanigo is being used Khanigo was able to identify a particular specific situation which uh where the student was at potential for harm self harm and it

it got identified by Kigo it got flagged to the teacher and the potentially very harmful situation was avoided so that is a very good example and in which is a developing country as well in India

similar ly we are not having to uh do anything that different from how it's being done in the US. The guard rails work equally well everywhere and they must just because you're a developed

nation or a developing nation it doesn't matter. Children need asthma safety everywhere. That's kind of our philosophy. &gt;&gt; Uh and you're right socratic method is

used everywhere. Um which is why even for B2C where the children are using it on their own and it's not teacher directed how do you ensure safety for those children not just in terms of

social emotional wellness and safety but also learning safety so you're not messing with their minds and making them dumber how do you ensure that and that's where socratic nature comes in so it

doesn't give you the answer it leads you to the answer &gt;&gt; and that's across the board. Okay, good. Okay, so now we will we will go to the audience question.

Uh I'm going to reset this slide. Okay, I'm going to request you to now access this. It should have come up on your screen. So what is the biggest safety risk of AI in education right

now? There are five choices. Don't be shy. I encourage you more of you to participate. In fact, we uh you know we we did discuss as a as a panel and we we really

want to understand what uh you know what the audience is thinking here. So the more of you participate that'll be wonderful. So don't shut the uh your this thing

just keep it on because the next question when it comes will automatically get refreshed on your on your phones. Yes. &gt;&gt; I can only select one

&gt;&gt; right. Right now I have forced you to choose one. Your topmost one. &gt;&gt; All of them. All of the above. See, that's why we didn't we didn't choose

all of the above or none of the above. Okay, great. So, uh about 67 70% seem to think student dependency reduced thinking

seems to be the the uh the biggest risk right now. Okay. Uh so, Shini, I I come to you next. So you know this can remain here or

maybe you'll get tempted to keep filling in the answers. So this is done now we are just moving forward but this is going to be the next question. Okay let it be there. Okay. So, Shini I

I'm I'm I'm coming to you. So, so that we get a you know, Swati gave a school perspective. We'll get a a higher education perspective from you. So, in higher education sort of AI touches many

aspects, writing, grading, admission support, research workflows, etc. So, what does safe AI mean mean to you? &gt;&gt; I think it's a very important I think it's a very important question.

Um, I come from Ohio State University where we just announced what I think is one of the first AI fluency programs, which basically by the time a student graduates from the university, they

would have touched AI in some part of their curriculum, regardless of which part or discipline they come from. So we're talking, you know, 60,000 students across uh over a 100 disciplines that

the university covers. And so safety is a paramount in terms of the implementation. Safety is a paramount question in this context. Um let me give you a couple of examples where you know

where these things sort of provide major challenges. I mean think about the current role in admissions. How do you admit a student into the program? Um, right now, so students are increasingly

using AI to write down their statements of purposes, their CVs. Professors who write recommendations for them increasingly using LLM technologies. So, it's hard to sort of really understand,

you know, what the individual is is doing. On the other side of the boat, counselors are using AI to filter candidates. Um, and so we are really what we're moving towards and and this

scares me no end. What we're moving towards is essentially using AI to evaluate someone else's use of AI as opposed to the individual. I mean this this is as you can imagine

this is a problem. Now, of course, when we talk about in the classroom safe use of AI, we we absolutely have to embrace it because AI can democratize access and I think that's really important. There's

a real win here, but we have to do this with careful guardrails. Now, what does that mean? As a professor, I used to love open book exams because I didn't want to disadvantage people who did not

have the best memorization skills. they could actually understand the concept, work with things and do that. But now the danger is if I have to set an openbook exam, I have to run these

questions through five different frontier models, hope none of them capture it and only then can I vet it and actually provide it and even then there's no guarantee because what I set

you know last semester frontier models are advancing so fast on some of these things that you know by next semester they would have solved that. So I have to come up with new clues. But okay,

that is something I'm willing to do. But the real concern and this is related to this question. My position on this question is the biggest harm that I worry about with you know introducing

too much AI into the classroom. When I was a student, I learned not only from my teachers, not only from my books, but from peer interactions. Today's peer interactions is ask Claude, ask Geminy.

And I'm worried about the cognitive decline and you know the mental health aspects of that and we've seen this we've had like more than two decades of social media where everybody is on I

this Instagram that Facebook whatever okay and so you are seeing and in we are now at a point where we can measure some of these things and so that is a critical harm you touched on research

just one point on research there was a recent paper in nature two days ago I believe which pointed out that the use of LLM technologies seems to be helping people at least in terms of getting NIH

grants. NIH is the National Institutes of Health. Um it's one of the largest funding bodies in the United States for health related applications. But one of the comments that came out of this which

concerned me and it's kind of ironic in a way is that a lot of the ideas that are coming through are kind of safe bets. They're not the high-risk high reward which you want in research and

that's a challenge and a potential harm as well &gt;&gt; right I mean I I can I can uh I relate to this I conduct a a college level hackathon in India for global learning

council uh last year when we did it I was essentially we were all judging uh various LLM answers so this year we said absolutely no responses in English from the students

just give us the model that you built and we'll evaluate you. Yeah. But u Okay. Now Shini you you've developed tool for uh for or online fairness auditing of AI systems. Yeah. A war is a

tool that you have built. Uh what what what does a war in the context of uh of an AI system in education? What will it mean? &gt;&gt; Yeah. Again, when you when you think

about auditing or evaluating, we're trying to monitor things that are, as I think uh Schwa pointed out, things that are deployed. Okay. And so uh the um the deployment it depends on who you're

deploying it to. So for example, I I said, you know, OSU is going through this AI fluency effort where, you know, you're training the next generation of a medical doctor. You're also training the

next generation welding engineer. You're also training the next generation historian or literature student. When you say auditing, it depends on context. Another point that she made in in the

context of Khan Academy. So context matters here. Who you are auditing. And again the other point and very related to her answer as well is there's a human in the loop the learner the teacher. So

there are humans in the loop. So when you audit a system um or when you audit learning uh outcomes and things like that you are auditing the entire process. It's not just you know the

machine learning model that's being used or the AI model that's being used. This is a hard hard question. Now for some things we have reasonable approaches regulatory approaches or you know part

of the fiber of a of a university or a college or an institute you know we have you know data must be protected okay there must be privacy one must ensure that data is not released through this

interaction with an LLM system so we have reasonable definitions on that but when it and even for fairness as it relates to regul regulatory fairness. But where things become really

challenging and amorphous is that safety is you know when you look at something yeah that's safe that's not safe this is unsafe borderline safe you can when you look at something you can say it but

it's very hard to determine these specifications a priority for safety and that's what makes this problem really challenging so to speak in the education but we're trying to figure it out as I

said you know we we announced this this is going into sort of our curriculum Um, and pretty much every student who graduates by 2030 will have sort of this AI fluency effort as part of their

degree at Ohio State. And so we've got to figure this out fast. &gt;&gt; Yes, you've got to figure it out. Okay, so time for the next uh audience question.

Um, you know, if if forced to choose, what should educational institutions prior prioritize first? So, I'm going to start this I mean the I panelists too can

participate. [laughter] &gt;&gt; Okay. [laughter] Huh? This seems sort of balanced or

&gt;&gt; you know this is this is the other thing I I use this in in in a class that I teach at IT Madras. I always have hundreds of people who who show this sad face emoji

and then I usually have some friends who take over and and then and then put that 100 100 plus happy face emojis. So okay so I think we are it's sort of settling in now. Good good good

responses. Uh learner learning gains is is integity. &gt;&gt; Sorry, how are we

&gt;&gt; examin? &gt;&gt; How are we ensuring [laughter] &gt;&gt; it's in the poll? Only one person can uh answer. I mean only one answer per per person. Yeah.

&gt;&gt; Huh? Per phone. Per phone. Yes. Of course. If you have many phones, you know, pro professor Ravi, he he knows all the tricks of the the trade here. [laughter]

Okay. But so learning gain seems to be the top most, but student safety and well-being also is like Yeah. So the it seems like a good uh uh Okay, good. So next we will move to

Anil. uh Anil uh you know in why machines learn you sort of explain the math behind the AI systems. So put on the that math math cap. If a if a school system now claims

an AI tutor is safe, [snorts] uh what evidence would you would you want to see before accepting it before it scales? &gt;&gt; Oh, um I'm not sure there's a math

answer to that. Um I think u for me if I'm thinking about these systems being deployed um in schools uh I would want to see some evidence of a framework that addresses

multiple issues. uh one obviously would be uh whether these systems are fair and unbiased right uh for instance if we are using AI systems to tailor the educational content that is being

delivered to students and um you know maybe these systems are trained on native English speakers and you certainly have especially in the context of India we have people who might be

speaking heavily accented English and somehow these systems uh you know determine that uh someone speaking with an accented uh English is probably uh less capable uh in terms of

their educational qualifications and and ends up being fed material that is probably you know less intense than somebody else who might be speaking fluent English. So those kinds of lot of

issues that need to be worked out as to how content is being delivered and how students are being guided in the trajectories they take uh through their educational journey. So a framework that

you know so in terms of seeing uh in terms of wanting evidence about how this is being deployed some framework that is addressing this u also uh potentially uh looking for whether uh these systems

have uh humans in the loop. I really would like to see teachers having the ability to override uh certain decisions that the eyes might make. M &gt;&gt; so again you know are these systems in

place uh also before going &gt;&gt; but can I can I just push back a little bit on that? &gt;&gt; Sure.

&gt;&gt; So one of the arguments for human the loop has been hey there must be a teacher. &gt;&gt; Mhm. &gt;&gt; But but then there are others who argue

look hey my the child wants to understand you know at 10:00 in the night the teacher is not available so you must make this system available to the to the student on their own. How

would you you know the the teacher in the loop is not available now. &gt;&gt; Um fair point and but I think there there must be some way for the educational institute

uh you know which might be an individual teacher a bunch of teachers doesn't matter some humans who are able to audit the trajectory itself that is being pursued by a student you know it doesn't

have to be at the granularity of looking at it every night but somewhere along the line you need a human in the loop. Sure. &gt;&gt; Um the other thing before scaling would

be that before you deploy it let's say if you're taking a Indian state with you know uh tens of thousands of schools instead of doing it across the board &gt;&gt; actually testing this out in a few local

schools somewhere &gt;&gt; making sure that these things work. So a framework that addresses these issues uh I would like to see those things in place.

&gt;&gt; Sure. Sure. Okay. So here is the the tough question for you. uh if if if an AI tool causes harm in a school, harm in a school as in it could be in the form of uh some misguided uh misguides

learning or or undermines the agency of students and things like that. Uh who should be accountable, held accountable. So um we all had this discussion offline before and uh you know my answer to this

was that I think responsibility in terms of safety of AI systems uh cuts across the board. It begins with people who are curating the data for training AI systems because you know we know well

enough by now that a lot of issues of bias and fairness arise from the quality of data. Then there are you know responsibilities that lie with the algorithms you know the people who are

choosing the algorithm there's algorithmic fairness and bias. So out there too there's responsibility to do with uh the model developers and uh mean again you know when we think of AI

these days most of us just think of LLMs but that's not it. There's a whole bunch of other AI models. So you know understanding what are the inherent biases and capabilities of individual

models and being able to uh pinpoint where these problems might be arising from. So there's you know responsibility at that level. The deployers of these uh systems are again you know uh have uh

some onus upon them to make sure that what they're deploying [snorts] uh uh is safe. Uh end users too. So I really think it cuts across the board and yes the ar you know it's it's a kind of a

argument that goes nowhere when we point fingers at everybody but the other way to think about it is these are such complex systems that we are developing that I think by making sure that

everybody feels the owners of responsibility we at least start from the ground up by building things that are fair and responsible. I mean on a lighter note Anil like you is all of the

above guy but but it's a serious point that you say I I just want each one of the other panelists yeah just one uh just one word not a long answer but what do you think what's what's your answer

to this who should be held accountable if an AI tool causes harm in a school who should be held accountable &gt;&gt; so I kind of agree with with what Anel said it it is a shared responsibility

but I think what you mean is Who should be liable &gt;&gt; li okay &gt;&gt; okay so accountable I think all all definitely true

&gt;&gt; but I I think liability it it it is a function of individual cases it's not a one-izefits-all solution &gt;&gt; it depends okay &gt;&gt; great

&gt;&gt; good point &gt;&gt; um so I'll be more direct I think the system that approves its deployment at scale will have to be held liable and accountable

&gt;&gt; which which is &gt;&gt; I mean typically if you're scaling up &gt;&gt; is that school is that a school or &gt;&gt; uh so typically at least in the government system an individual school

will not make the decision the state will typically make the decision. If I am a independent school then the school owner or the leader is making the decision but uh while everyone is

accountable the liability is with the person who approves the deployment at scheme at that unit. &gt;&gt; Sure. and Swati &gt;&gt; I'll do a plus one to what Shabata just

said. uh because to me uh the accountability yes responsibility is for everybody but the accountability really lies on the ones who are strategizing and designing the deployment and for a

simple reason that and I'm I'm not absolving the ones who are developing the tool completely of it because yes they are they have to exercise this responsibility but we also have to

recognize that you cannot make a 100% safe and accurate um AI tool you just cannot so what you have to do is ensure there are checks and balances is while you're deploying it. So the deployment

person it's the onus is on them to ensure that there are checks and balances um taken into account while designing the deployment process. &gt;&gt; Right? Okay. So audience question

uh would you accept an AI tutor that is 5% wrong? Okay. That is wrong 5% of the time but it helps reach 10 10x more children. So access it's doing great work but it's

5% of the time wrong. Would you accept? &gt;&gt; Huh? &gt;&gt; Yeah it it could be could hallucinate. You are absolutely right. you know, when you when you question it, it will it

will give the the exact opposite answer and it'll say, "You're absolutely right." [laughter] By the way, uh I love this crowd. Netnet, we are positive on the

the positive things here. Thank you. &gt;&gt; No, no, no. I'm saying the &gt;&gt; smiles. Smiles. Smiles. &gt;&gt; See, I am I am looking at the at the real metrics that matter to me.

But only with the teacher. So human in the loop. The human in the loop seems to be the the binning. Yes. Okay. Great. &gt;&gt; Okay. Fantastic. So now we move to Shaveeta.

Um you know what does safe mean for the for the global south for the for the students the teacher systems from the from the low-income communities? What

does it mean? &gt;&gt; No. Uh before I answer uh I was introduced as I bring the perspective of the system uh when professor Shini was talking I also bring the perspective of

a parent to two students in high school who understand AI definitely better than the mother does in most cases better than the teachers and the counselors does I'm hoping not better than the

admission officers but uh it's uh it's a lived reality uh and also conscious of saying it in the AI summit it. But uh to keep things simple, the more things change, the more

they remain the same. And I want to go back to the basic of safety in the global south for AI. And by just global south, I would say also for the global north goes back to how any education

intervention will be considered safe when all design and governance choices do not lose sight of what the pedagogy is, what the curriculum expects to be covered, what are good instructional

practices. So at the center of AI adoption has to be these criteria on human role and judgment, instructional p practices, pedagogy and curriculum. For a student, it means it has to be age

appropriate. It has to be curriculum appropriate, content appropriate. A lot of AI engagement is happening for students without them realizing it. I think it took us a long while for many

of us as adults to realize when we are engaging with AI chat bots versus a live customer service agent. Imagine what it's doing to our children. So it has to be age appropriate and centered around

sound instructional practices. That's what safe looks in for a student perspective. From a teacher perspective, it has to be transparent and assistive. Anytime if we if we dare to imagine a

world where AI is being looked as a substitute is where we run into it being unsafe both from a design perspective, governance perspective, uh adoption perspective. You know to your prior

question on who who is accountable, who's responsible, who's liable. At the end of the day, any solution needs to be integrated into how the education system runs its schools, teaches the students,

what's the role of the teachers, how is their capacity being built, how are they being supported, how are they also being held accountable. So a system which gets anchored on pedagogy on relevant

instructional practices with human judgment and adoption at the center age and grade appropriate for students assistive and transparent for teachers. &gt;&gt; Fantastic. But I want to just push back

a little bit. I mean uh both you and Swati sort of seem to indicate maybe I'm or maybe I'm getting this wrong which is like hey safety is more or less the same whether it's global north or global

south uh it's the same yeah but but the the the context seems to be very different in global south the the access issues and so on. So I'm just I'm just asking

can it be the same or uh you know whatever I there's no right wrong answer but I just want to &gt;&gt; I'll tell you from my perspective right access is a different dimension that

we're talking of how many children access AI in global north versus global south the quantum of students can change &gt;&gt; now whether in the global north 100 students are accessing and in the global

south five are accessing why should safety norms be different &gt;&gt; to me the axis of safety doesn't override the access of access and vice versa. At least that's how I look at it.

But Swati, you want to add? &gt;&gt; Yeah, I actually completely agree with what Shvata said and I think uh what you might be looking for uh which again is is more on the access side and less on

the safety side is the ability of the student to be able to actually benefit from the AI and interact with it comfortably by making it in their own language, making it uh easier to

interact with uh because the the content that it's pulling is also in vernacular. the interaction is in vernacular. I think those are some of the things that you might be thinking about but to me

that's again more access side. Yeah. &gt;&gt; I I'll I'll tell you one perspective like uh in my company uh we we have run some you know sort of digital society surveys. Uh now one of the one of the

things that we find different in in India is is a trust in the government systems and the and the reason one of the hypothesis we gave was because the

literacy levels in digital is lesser in India. They they they generally depend on a on an external system to for uh you know approval and and so on. And so given

that the literacy levels among children here are lower perhaps even among the teachers are lower I'm once again I'm just I'm just pushing this out saying so in the in the global north you might say

hey teacher can take responsibility but I might I might go around say even even in India even teachers may not be au greater thing in terms of putting it in the system itself some guard rails uh

for example which may be different &gt;&gt; very fair point and in fact I have an example to illustrate exactly your point and add to that &gt;&gt; uh you're right in the overall learning

journey there are more stakeholders involved in India &gt;&gt; um that need more awareness building and capacity building including parents because and the process of doing that is

very different because in the US if you don't do that capacity building and awareness building for the parents you can still get away with a little bit of that uh and I know I'm going on a limb

here but to to to say that out loud but in India you cannot afford to do that for example um the devices are shared in India in the government school system. So the students do not get adequate time

on a device to be able to access an AI tool and work on it. So what we need is a seamless integration of classroom and at home and the moment you talk about at home again to make it safe it needs to

be supervised and how do you and who's going to supervise it a parent and most of these parents for the underserved students government school students in India are semi literate to illiterate so

how do you ensure safety over there when they you know get past that school boundary and get into the homes so I think that's where you're exactly right that point does come in and we have had

to take extra precaution of doing B to G to C utilizing the teachers to and equipping them with all the tools and um communication um um collaterals etc for them to be able to have a commun that

conversation with the parents and the parent teacher meeting to make the parents aware so that they have some mechanism to apply that supervision even though they're illiterate. For example,

is okay, right? We've literally had to do that. &gt;&gt; In fact, uh Shavita, that's a great segue for for the question uh to you next. You know, through AI summer

initiative that uh you and I Madras and others are are driving. I mean you we are working on AI literacy for for school children and teachers. Yeah. So tell us a little bit about that and how

why this is important. I mean we already set the context but &gt;&gt; hit it for a six now. [laughter] I think it goes back to um you know we are taking the initial steps to the vision

of again I'm going back to when professor Shini was talking about AI fluency again I'm repeating myself but we live in a digital country we are in a country

that's proud to be a digital powerhouse you know we're talking about okay teachers and students on an average are more literate in the global north versus global south but we are a digital first

country in many ways stays ahead of the so-called global north. Right now, in this context, whether or not we realize it, AI is governing my search history, even for a student, even for a teacher.

Uh it's showing up recommendations. It's giving up unsolicited answers. Not everything follows the Socritic method. Uh it is of course not sharing with me what percentage of error I am. Is there

a human loop or is there a tool in the loop in this world? For us, the very first step towards this vision of AI for all is does everyone, every student, every teacher and parent understand what

is AI? How is it already beginning to engage in my education journey? And based on the two, how can I become an active user of AI? Whether or not we like it, AI is shaping any child who's

spending time on a device for learning. As her 2024 showed, 82% of children in the 14 to 16 years of age group use smartphones for learning. CSF conducted a Bhattech survey in 2023. Upwards of

3/4 children are spending more than 30 minutes on their device. Now the moment we look at these two factual these two figures right we know that that percent of children is interfacing with AI

without realizing it how do we move them from passive users to actually having agency making informed safe and responsible decisions and that is really the attempt with AI Samart uh proud of

the work that's already happening in close collaboration with IIT Midros as of last month already 1 million students and teachers across 10 states in India have started engaging on AI literacy and

the vision is for that to you know build on to the levels of AI fluency and when the human in the loop is actually an empowered aware and active user rather than you know I don't know what's taking

over my life &gt;&gt; right no so kudos on the 1 million one small cheer okay so the next question to the audience

Should schools keep logs of student AI conversations? You know, for safety and audits, of course, but should they keep logs of student AI conversations? What do you

think? I mean Shini does have a good point. He says look we can have logs but but really who has access to the logs? That's the important question.

Good trainy. You're not grading my my questions. &gt;&gt; No, Rabbi is doing great. [laughter] &gt;&gt; Okay, it's &gt;&gt; okay. It's settling in here. Yes, but

anonymized and minimized. You know, we did have a discussion. You see the the thing with logs. uh of course it helps in in making more personalized learning and so on. So that

there is some way for example teachers also have access to it so they they they can they can come back that's on one side but on the other side there are issues about students having a uh a safe

space to to answer questions mental health issues and things like that but really who has access to this that's the the critical question okay Sunil we come to you next the uh put on the

philanthropic funer hat uh how do you decide whether an an AI and education idea is is worth funding scaling. &gt;&gt; Sure. [clears throat] Before I answer that question, let me just respond to

the conversation that's been happening. [clears throat] Forgive me. So just for context, my own background is a little bit tech based. I went to IIT, then I went to the US, went to Carnegie Melon,

got a master's degree, and I've started a few technology companies. Both company failed, other companies did a little better. One thing I learned very early on is

that there is nothing in the world, there is no solution in the world, tech or AI that is perfect, right? Everything involves a trade-off. My first company was a medical device company,

but these things didn't work. Patients got harmed. But did it work 100% of the time? Probably not. But overall, the benefits outweigh the risks. Automobiles are a great example, right?

We all use cars. We don't think about it. They kill a million people a year plus around the world. Yet, society has made a choice that it's worthwhile. The benefits outweigh the risks. [snorts]

When we come to AI, we have to keep that perspective in mind. And my concern in discussions like this is we tilt too far one we tilt too far one way or the other. So let's look at

the social sector, right? We've got over 8 billion people in the world. 3 and a half to four billion people do not have access to decent healthcare, do not have access to decent education.

AI could be transformative today. It is starting to become transformative today. Yet are these systems 100% perfect? No. Do the benefits greatly outweigh the risks? Yes. So I'll give you a practical

example. So I started six seven years ago in India an artificial intelligence foundation and Prime Minister Modi G came inaugurated Lavaka. I've got about 400 full-time people in this foundation

in Bombay, Delhi, Bangalore. We've developed over 25 AI platforms that are now scaling up nationally. Many of them are becoming national platforms. As part of this about 18 months ago, we

got a call from the government of Gujarat. A team went in. They said, "We have a large problem with the very high school dropout rate that we have in early

grades, grades 1 through five, especially among young girls. Do you think AI can help?" We did the analysis and we learned that the primary reason

for this and of course once you drop out of school it affects the rest of your life. It affects the earnings you make. It affects the health that you have. it affects you know your skills and

especially when it happens to young girls it it affects not only them but their families so we did the analysis on why is there this high and by the way this high

dropout rate is not just Gujarat it's around India and it's the entire global the single biggest reason it turns out for this high dropout rate is the inability of these young children to

read proficiently in their mother tongue so in this case this was Gujarat and 50 to 60% of the children in grade five couldn't read in Gujarati effectively even at the second grade

level when you can't read it affects how you do in every subject geography science history literature and if you struggle in every subject you say why am I doing this and in low-income families

your f you know your parents say okay come home start working etc so we've come up with a suite of AI solutions that basically we do an assessment in 20 seconds for each child we figure out

what's their reading proficiency level where exactly are they struggling on what words what are the patterns etc based on that we come up with a diagnostic based on that we come up with

a remediation plan we divi we divide each class into cohorts of people with similar issues and we guide the teacher and the parents on how to address this the results that we started getting in

the pilot were impressive enough that about six seven months ago the Gujarat government made it mandatory. All 3 million children and government schools in Gujarat are getting assessed and

remediated with the tools that we have. Thank you. Then Rajasthan saw this and about 3 months ago we became mandatory in all all the schools in Rajasthan. Now it's spreading into 10 states and if

all goes well by the end of next year 2027 million children in India will be improving their reading proficiency thanks to this and again I'm not

bragging about what we've done. Any other competent AI team could have developed any of this. Here's the point I'm I'm leading up to. Would these systems pass all these AI

audits and frameworks and all? Probably not. I don't know. Should we wait 5 years or 10 years to have the perfect system and in the meantime 30 million or 50 million kids in the global south

don't get educated. So the only point I'm trying to make is it's good. We need safety obviously in AI systems. We need lack of bias obviously but we have to keep in mind

again that we have to make an intelligent choice. And are the benefits strong enough to outweigh the risks? We have to manage the risks but we push ahead. Sorry that was a digression to

come back to your question how do we select what AI issues to work on whether it's healthcare or education or agriculture we work in all these domains we take a look at number one is it an

issue of importance to the government so whether it's the national government or the state government is this one of their top three or four priorities over the next three or four years so as an

example in health we work very closely with the health ministry in Delhi and with about 12 15 states they told last two or three years ago tuberculosis is a huge issue. It's probably the number one

or number two priority in India and in fact much of the global south. Close to 2 million people every year die of TB. So we've developed a whole suite of TB solutions that are now being used by the

central TB division. In education this oral reading fluency is expanding dramatically. So again number one we look at how important is it for government. The reason we do that is the

only way systems scale when you're talking about disadvantaged communities is by working with government. So you have you talk to them right from

the beginning. The second thing we look at is is this actually appropriate for AI. M &gt;&gt; do we have the right kind of data or can we collect the right kind of data so you

can train your models on good data and that is where safety and bias and all of that comes in. Thirdly before we start building any AI system we ask this very important question

how will it scale and we've learned the hard way it's complex when you're talking about giant government systems. So let's say in education, you know, you you're

talking about working at the last mile with teachers. You're talking about working with state governments on execution. You're talking about the central government on policy. How will

this get out to the last mile? It's not easy. And that is the single biggest reason why 98% of all the wonderful AI solutions that have been developed for the social

sector don't scale. They go nowhere. They may be totally safe. They may be unbiased but they don't improve people's lives. &gt;&gt; Right?

&gt;&gt; So right up front we work with government. We'll talk to the joint secretaries in Delhi. We'll talk to the heads of the national health min missions at the state level. How are we

going to train the frontline health workers or teachers or farmers to use the system? Where will that data go? How do we manage it at the state level etc. So we look at that. Absolutely. We look

at safety and bias and then there's a feedback loop as you go through. &gt;&gt; Good. Triny, you had a point to make. &gt;&gt; No, I I was in complete agreement. I I think that's the reason Ohio State's

embarking on this AI fluency. It's a total embracing of the AI and and the fact that it's here to stay and it's really really important for you know democratizing access and your example in

Gujarat which I had read about um is is is you know really a valuable learning experience in terms of the kinds of things AI can bring to the table. I think the the the key challenge for us

in terms of implementation is that we have to work with our state government and our you know federal laws and things like that to make sure that you know whatever we're doing because

like for example the holy grail of you know related to this question that you asked all student logs are kept but access to those are incredibly protected and and they can be really useful for

you know student learning outcomes. and things like that. So I think I mean 100% agree with the point you made. &gt;&gt; Okay. Excellent. We'll go to the uh the next uh

audience question. The last one since we were talking about this See that &gt;&gt; parent? Yes. It's uh you know we'll we'll we'll wait

for a few more minutes for the thing to settle. Time is up. Yes. Yeah. &gt;&gt; Okay. So what what we were essentially

looking at if you see whether people are choosing more decentralized uh mechanisms or centralized mechanisms. Yeah. So the the teacher principle perhaps is more decentralized in that

sense. So that's about uh 50 60% of the the answers are are there. excellent. So, we've we've had a fantastic uh round. I mean, I they've they've told me we are run we have run

out of time. There were a few more rapid fire questions, but we will reserve that for the later. And I just want to uh end this with some closing thoughts. I mean, I I've made tons of notes here. Uh

teacher directed uh peer interactions was something that I that I picked up. uh human in the loop that was something that came out

uh a framework for fairness and unbiased way uh pedagogy rules etc. So number of different points have come out. These are some closing thoughts. Uh like I like I initially said the coin

systems I think it's important to look at trustworthy by design across the six layers uh all the six layers uh harm management for example uh as an ecosystem governance. So uh harm to

students uh harm to institutions and harms to the society. So think at uh multiple levels and how how we can uh prevent them. Uh safety is co- agency. Yeah. So this is a very important uh uh

aspect especially for digitally unsophisticated learners. How do we give that uh that uh agency that this is exactly the human in the loop? Yeah. So routed to a teacher and so on that

becomes important. Uh and the last point you know the scale inclusion governance as an infrastructure layer. Just uh uh two days back we had a a conference here called the Bodhen AI conclave where you

know exactly in Barat Mandapum where where uh the uh government of India's center of excellence for AI and education was uh was announced uh IIT Madras is uh is leading that that effort

and the effort is to create such kind of DPIs uh for uh for education. So that's the thing. Thank you. A warm round of applause TO

[applause] &gt;&gt; we have a small No, there's one. &gt;&gt; Yes. Uh we have a small moment from &gt;&gt; [laughter]

&gt;&gt; Oh, thank you. This one. This one. &gt;&gt; I think they want the logo. &gt;&gt; Thank you. Sir,

&gt;&gt; how are you? &gt;&gt; Hold on. Amazing.
