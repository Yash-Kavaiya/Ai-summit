# Towards Global Cooperation for Equitable AI in Healthcare

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 17:30 ‚Äì 18:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 14 |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/QxHpG6qU4OY?feature=share) |

## üé§ Speakers

- Dr. Krithika Rangaranjan, AIIMS, New Delhi
- Dr. Mahima Kalla, Centre for Digital Transformation of Health, University of Melbourne
- Dr. Mahima Kalla, Centre for Digital Transformation of Health, University of Melbourne
- Mr. Aman Taneja, Ikigai Law
- Mr. Aman Taneja, Ikigai Law
- Mr. Mohit Jain, Microsoft India
- Ms. Kanika Kalra, World Health Organization
- Ms. Tess Buckley, techUK

## ü§ù Knowledge Partners

- Centre for Digital Transformation of Health (CDTH), The University of Melbourne

## üìù Summary

Artificial Intelligence (AI) promises to transform healthcare delivery worldwide, improving diagnostics, access, and system efficiency. Yet biased datasets, exclusionary design choices, and uneven deployment risk reinforcing inequities and disproportionately harming already marginalised communities, including the elderly, persons with disabilities, and rural populations. This interdisciplinary panel will explore inclusive AI strategies, such as participatory design, representative data practices, and context-sensitive governance frameworks, to ensure that AI systems in healthcare are designed, developed, and deployed in ways that mitigate unintended harm. The focus is on building equitable digital health ecosystems that proactively address structural disparities rather than amplify them.

## üîë Key Takeaways

1. Artificial Intelligence (AI) promises to transform healthcare delivery worldwide, improving diagnostics, access, and system efficiency.
2. Yet biased datasets, exclusionary design choices, and uneven deployment risk reinforcing inequities and disproportionately harming already marginalised communities, including the elderly, persons with disabilities, and rural populations.
3. This interdisciplinary panel will explore inclusive AI strategies, such as participatory design, representative data practices, and context-sensitive governance frameworks, to ensure that AI systems in healthcare are designed, developed, and deployed in ways that mitigate unintended harm.
4. The focus is on building equitable digital health ecosystems that proactively address structural disparities rather than amplify them.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/QxHpG6qU4OY/maxresdefault.jpg)](https://youtube.com/live/QxHpG6qU4OY?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

message Uh so I would like to call upon Shambhavi, my colleague, who's going to just give us some context behind why we ended up forming this panel and this context in the conversation. Some

research findings from our past research work and some additional uh findings that we've got so far. I don't know if we're able to share the deck because of our tech issues.

Yeah, okay. So, um at Ikigai Law, over the past, I would say 3 to 4 years, we have been investigating different aspects of AI in building and deploying responsible AI.

Um and through our work, we've sort of ident- through our work, when we applied for our grant originally for this project, we identified issues with um implementation and how do you involve

lived experiences, you know, in the building and design process of AI. Um we noticed there were a lot of policy principles coming out. We noticed there was a lot of literature on the harms and

risks um that AI can pose, especially in sensitive sectors like healthcare. But, what was missing was direct playbooks on how do you build responsible and build inclusive AI such that no patient or no

end user and end beneficiary is left behind. There was a great need at the time for clarity um on how do you, you know, build it out and how do you deploy AI responsibly. So, we applied for a

grant um with the University of Melbourne and uh the NALSAR University of Law to investigate uh co-design and participatory design approaches uh to

build inclusive AI. And through our um 2-year study nearly, we identified certain uh practical implementation steps which we have fondly dubbed tenets um and we've identified eight such

tenets. And these tenets can apply across design, deployment, uh development, monitoring um at any stage of the life cycle of the AI system. And the idea is to stop and sort of ask

questions on who are you building the AI for, how will it be deployed, who are the intermediaries in the process of deploying AI and building AI. And this sort of approach allows for iterative

refinement of the AI system being built for healthcare. It also ensures that the AI is fit for purpose um and that it's culturally res- culturally responsible. Um this These tenets were co-designed

with our experts and cohort members from the Australia and India who traveled to both countries and sort of shared expertise from their academic work, from their civil society advocacy, from their

industry perspectives on building out AI. Um and we were fortunate to sort of bring them all together in in multiple rooms and multiple meetings to co-create these tenets. Um

how we did it was we sort of sought of from the get-go, what do you need to do to build AI responsibly? We realized that a proactive approach um is is is beneficial to that. So, in the

context of CN AI retinal scanning device, how would you um ensure that, you know, you incorporate people with low vision and and vision impairments in the process of building out AI for

retinal scanning? How would you include people of different age groups, different genders, um different disease states? Um how would you incorporate their lived experiences to make sure

that the AI is working in detecting diseases that they live with? Um once you incorporate their perspectives, how who where is the AI being used? Is it being used in a rural setting? Is it

being used in a big private hospital in a big city where there's no latency in, you know, internet connectivity, there's um really good backup generators for for light and sound? And and uh where are

you um building and using this AI? Uh finally, we also realized the value of uh tailoring risk assessments and um qualitative assessments of whether the AI is actually inclusive or not. And

this is where our partnership with Dr. Mahima Kalla's center, uh the center for digital transformation of AI in healthcare. Yeah. Um their work is simulation research. Their work involves

a lot of qualitative assessment of AI technologies uh before they're even deployed. Um so, incorporating such qualitative methods into your process of building and designing AI solutions, how

would you do that as well? And so, these tenets are applied, they're not steps that you follow as like a checkbox. They are tenets that you can go back to at any point of time in the life cycle of

an AI. You could even decide to exclude one of the tenets if it's not fit for the purpose that you're building out the AI for. The idea is to be flexible and open

um and the idea is to believe in continuous improvement of AI and to be iterative in your approach. But, it's also to ensure that no one is designing AI in a way that leaves anyone behind.

So, I'll just stop here. This is the context for the conversation. Um and the panelists that we have here will dive a little bit more deeply into the different aspects of co-design

um and different ways in which these tenets can be implemented across the globe. All right, thanks. Thanks, Shambhavi, for the findings. Um I wanted to just

take the opportunity to also introduce our esteemed panel uh panelists here today. We've got Dr. Kritika Rajan with us who's an associate professor at AIIMS, New Delhi. She's a

radiologist and a data scientist uh and and works at the AIIMS. Uh we've got Kanika Kalra who's a technical officer uh at the AI and Digital Health Division, World Health Organization. Uh

we've got Tess Buckly with us who's a senior program manager who looks at digital ethics and AI safety at Tech UK. For those of you all who may not know what is Tech UK, it's the equivalent of

a NASSCOM in in the United Kingdom. Uh and we've got Mohit Joshi with us who's a principal researcher at Microsoft India. Sorry. Mohit Jain. Sorry. At Microsoft India.

And we've got Aman who'll be moderating the panel. All right, thanks. Yes. We've not managed to solve the We'll do it at the end. Okay, awesome.

Uh thanks. Thanks for that, Rutuja, and thank you all for being to come here and to the wonderful audience which has filled this room. Um Uh I am And thank you to Shambhavi for,

you know, introducing those tenets. I I think that uh you since we are at the Impact Summit, I think one one way to kind of contextualize this conversation is also that for years now there have

been conversations which have felt a little theoretical when it comes to AI and ethics and responsibility. Uh and if you're talking about the impact, let's also talk about the

practical challenges or practical tech- techniques to get us there. And I think those tenets speak to uh speak to really what is it what is it that you need to do as we're I mean,

apart from being a policy organization, we're law firm. We're a law firm. And very often our clients ask us, "Okay, what do we need to do?" Um and I I think real-world insights

into what others have done very often gives you some of those answers, right? Um and I'll before I come to you, Kritika, I'll just maybe contextualize this with something I learned in the

last 6-8 months with some research that we were doing um where we're helping kind of draft uh you know, a set of actionable advice for AI developers. Uh this is a project we

were doing primarily from a data protection lens, but also larger uh responsible AI principles. And we were talking to the folks at Wadhwani AI who you know, building that Cough Against TB

um app. And one of the things they told us was along the way uh they found that the accuracy of the the model was much higher for men than it

was for women. And then they had to actually rework that, lost some overall accuracy, but to kind of harmonize the results so that it was fair to people of both genders. And there are these kind

of practical tradeoffs that one makes as we're thinking about this, right? Um and So, with that framing um uh Kritika, maybe I'll come to you first. Um I I I'd really like us to explore some

of the practical realities of inclusive design, right? Uh of health of AI in healthcare. Could you share some examples of the research and innovation being done at AIIMS

uh and how these projects account for known barriers to accessing healthcare and barriers that we all know of, language, gender, disability, location, digital literacy. There's so many of

these barriers that exist and India's such a great, you know, test bed for all the barriers that exist. I'm sure there's there's a cohort which represents that. So, if you could speak

a little bit from from your experience. So, thank you. Thank putting that uh question across, but also thank you for spotlighting uh you know, this issue of practicality of AI really. Because I

think it's it's very easy for us to get lost in theory. And I think one of the primary problems, not just of AI, but med tech in general, is we tend to begin with the product. We tend to think,

"Okay, this is possible, so let me build out something for this." Only then, you know, 2 years, 3 years down, you're then looking for people who would use it. So, uh you know, one of our grounding

principles has been to invert this question and say, "What is missing? What is it that I struggle with today? And what are the ways of solving it?" And even if you are an AI researcher, I

think, you know, one of our biggest learnings has been, "Keep yourself broad and be open to finding solutions which are outside AI." You know, sometimes the most uh

you know, the the best solution could be something which is totally non-technological. Forget about non- AI. And that might be the cheapest way of doing it. So, if you put an AI there,

you're complicating it. So, I know it's an AI summit, but I think one of the most important learnings for us is, you know, don't don't always default to AI. Also, the questions that you ask, you

know, sometimes are driven by the data that's available. And sometimes the data doesn't often represent some of the most underserved sort of populations, right? Uh it doesn't even represent the most

important questions that you want to ask. It's simply that that data is available. That means a lot of effort is going in a direction which may not be that beneficial to patients at large.

So, again, I think uh if I had to give like a one-word answer, it is begin with patients. Put patients at the center of it. Where is it that they are going to benefit? What what what disease entity

uh do you want to target? Uh but also put providers um pretty much at the center because they're the ones who are going to use it. We've had uh examples of

you know, places where an Asha worker says, "Uh I don't want to use this because it takes away my sense of uh res- respect in the community because suddenly the person in the community

thinks that it's the it's the phone which is answering everything and it's not the Asha worker." We've had Anganwadi workers who say that that I don't want to use this because it gives

me too many alerts when a patient is a high-risk pregnancy. So, things like this come only with your ears to the ground. So, again, just a people-first approach, whether it's a provider or a

patient. Fantastic. It's great insights on having the Asha workers and Anganwadi workers who have effectively going to be boots on the ground

with these deployments. I know. Thanks. Thank you so much for that. Um Mohit, I'll I'll come to you. I'll come

to you next. Um I mean, there's Yeah, thanks. Uh I I I understand that you worked on a fair number of projects building

technology for for people with who are differently abled. Um you know, I I think one of the uh apps that you built is called Sign It, which is specifically for people who are

hearing impaired, right? Um so, I I again love to ask you on what your process has looked like. How do you not just seek feedback, but create that environment for sharing feedback?

There's that's the the the ability to kind of do that. And then, how do you bring that feedback into the into the development process, right? Um so, that I'll I'll start with

that for you and and then maybe add to this. Um yeah. Uh good question. Yes. So, I think uh I just give you some

description of Sign It, the specific project that I was asked about. I I did a project wherein we we tried to create a sort of like a game for deaf and hard of hearing population so that they can

play the game. And while playing the game, they also like end up uh generating data, uh which is basically sign language data uh along with the transcription of that.

related. I think the other thing uh I think you asked about the uh something about like the feedback. So, I think we had a very tight feedback loop wherein they would

give us some feedback. We would iterate and modify the system or add that feature or remove certain thing. And then, we will show it to them and ask them like explicitly like, "Hey, you

suggested this. We made it this way." And so, they also had a skin in the game in that case. So, that also helped a lot. And I think Yeah, it goes uh

I I think the other way to put it is that like I think a lot of times we don't uh take that as their expertise. So, rather than think thinking that we are building

for them, I think building with them and thinking that they are the expert. Like like you go to a like you build a healthcare technology, you want to build with a doctor because they are the

expert. Similarly, if you're building a tech for a particular demography, then you should build with them because they are the expert. So, I think that's the I think mindset that needs to change in

order to build something useful. Uh yeah. Thank you. So, so wonderfully said, Mohit. You

know, rely on that as an expertise in itself. Uh you know, and you know, thank thank you for sharing that experience, Mohit. Um Tess, I'll come to you next. And I think what Mohit just said maybe

resonated quite quite strongly with you. Um I I I think a lot of your work is around, you know, equity and justice, but also on the role that the teams that you build to kind of develop AI

solutions have a huge role to play. And I think you mentioned, you know, reflected some of that in in their choice of how to bring that that feedback in the loop itself.

Uh so, I mean, honestly, I would love to hear from you on what what your work around in this space has been and and your reflections. Yes, first off, thank you for bringing

us together on what I see is such an important topic. Um so, non-negotiable, we know homogeneous teams build homogeneous systems. We know that um diverse teams catch bias early on. But I

actually want to respond to this question and take it beyond diversity as a numbers game. I think we need to be considering this is a about epistemic diversity. And I'm glad that I kind of

get to echo what my other co-panelists said here, right? It's about who is the decision-maker. So, not just looking at team composition, but who holds power within that team.

It's about lived experiences expertise. So, thank you for doing that. I don't need to get into that. Exactly what you said. Um and decreasing tokenization, which unfortunately we do see sometimes.

Disability is so different in you know, we could have the same diagnosis, but that manifests in very different lived experiences. And so, we need to be thinking uh about that. I think when you

look at the literature and and bias across literature, it was great in 2018. We had a lot of focus on on sex and race. Important.

But I specialize in ableism and biotechnologies. And one of the reasons for that is we're still really lacking in socioeconomic inequality and ableism or prejudice against people with

disability. And for folks in the room that work in healthcare, this becomes so critical for you because I don't know about you, but if I go to the doctors, it's because of a I've acquired some

kind of disability or difference or because I'm congenitally born with differences and disabilities. Um and so, it just causes harm at scale. So, if you

have a biotech or a prognostic tool um that is ableist, you will harm the folks that come to your offices to be helped. Um I think if you'd like to read more

about the importance of centering disabled folk in your innovation, I always turn people to something called the curb cut effect. And essentially, this is

where we look at the motto of the disabled community, which is nothing about us without us. And we credit them for a lot of the innovation that is now mainstreamed. A great example of this is

closed captioning. So, that was fought for by the deaf community. And guess what? We get to enjoy it when we're in uh gyms or we're in busy spaces. It's not just, you know, a nice thing to do.

It is a nice thing to do. It's also a really smart thing to do in your innovation. So, I think I would end it by also pointing to the fact that we need to consider the types of datasets

we are trying to work with to mitigate bias. And the Royal Society uh in the UK just released an incredible report which pointed us to functional datasets and small datasets to try and mitigate this

this bias. Um yes, thank you. I'll stop there. Thank thanks for that uh Tess. And uh I hadn't heard of the curb cut effect before today. And I'm it's something I'm

going to remember and reflect on after this as well. Um Kanika, for you, I'm I'm going to frame this uh slightly differently because the perspective you bring from WHO is going

to look at this uh I'm sure slightly differently. Um the fact that we need to be participatory and do co-design is I I think that's not up for debate anymore. But from the WHO's perspective,

how do you incentivize this? How do you create the channels for uh the adoption of some of these practical tenants that Chamber we were just talking about? How do you make sure they're, you know,

something that that feeds into projects that you're perhaps funding or So, how do you kind of define it? How do you create pathways for incentivization? How do you make sure that that's actually

happening in a meaningful way? That's a great question. &gt;&gt; [clears throat] &gt;&gt; So, when we're working with WHO and when we're working with partners, the thought

process that I'm hearing from the panel right now is centered to what we also think of. When we're promoting the creation of a design, it has to be with the people who

are affected by the design and with the community that is going to be impacted by this design. Um we have often come across situations where there is a solution for a problem

that does not yet exist or that was addressed, as Dr. Kritika said, way before with easier technologies. the process that we've been taking um uh forward is using multi-stakeholder

engagements. And especially in AI, a lawyer alone cannot achieve anything. A doctor alone cannot achieve anything. It has to be multi-sector engagement.

Which we're trying to achieve using the global initiative on AI for health where we're bringing the International Telecommunication Union, the WHO of course, and then the World Intellectual

Property Organization together. And we have two round tables which completely focus on this topic. But how is it that we're not looking at IP as something that becomes a barrier or

regulation that becomes a barrier towards innovation, but is rather being used to promote and incentivize innovation but in a very ethical, safe,

and effective manner. And this is something that we're doing also through guidance documents that we build and then implement with member states. And these have practical checklists. So much

like what we have learned from Project Blueprint, the there it is essential for there to be practical checklists that can be implemented on ground. We don't want piles of pages put together and

then just sitting on the bookshelves catching dust. That is something that we've been actively working towards and have very multi-stakeholder environment where we

have representation and inclusivity at all levels possible. Thanks. Thanks for that Kanika. Now going to maybe switch gears a little. I do want to

I mean we're we're hoping that the policy makers are listening to us is one way that I'm thinking about this. So really just on the basis of your

work, what are the lessons like from India, from from your experiences around the world, and especially in collaborating with industry to build inclusive AI in

healthcare, right? How do you embed some of that in the policy making process? And are there policies or practices that that you think that we should be importing in

from maybe some of the work you've done in in other jurisdictions? So I think we've been sort of very lucky in terms of being able to partner with a very wide variety of partners.

So we partner with institutions like ISC, IITs, etc. And I think what's interesting there is the focus is very much on how do you build, you know, at the core very high quality

AI which actually takes, you know, we talk about physics informed neural networks. But can you actually build a health informed neural network, a biology

informed neural network? How you able to take the details of what maybe a radiologist, a medical specialist is actually seeing into the design of the neural network

itself. So I think that's that's one very interesting sort of area of work and we have very sort of fortunate to have in institutions that we partner with over there. And then also, you

know, on the other end, industry. So so startups, I mean I think we're very fortunate again to have a very large variety of startups who are very, you know, quick on their feet. So

I think what what works very well with them is you have a very well-defined problem statement. You want to take that to the ground. I think that, you know, the best way of doing something like

that is to partner with a startup. And then on the other hand, industry, large industry, especially when a lot of hardware is involved, a lot of,

you know, equipment is involved, or you need a lot of compute like if it's a foundational model that you're trying to build, then it's incredibly helpful to build

with with an industry partner. So if that answers some of that, you know, just just just deciding where in the spectrum, who is the right partner to really collaborate with. That's that's

fantastic. And I think you know, the interesting thing is our government is thinking along many of these lines. I think they just need reinforcement on on some and course

correction perhaps on others. So I'm I'm glad there is general general consensus. I'll come to you She because I mean at the end of the day you do work with a very large organization.

Right? Well, it is a very large organization and it's global. Right? And and I mean in the next part of this conversation I

want to talk a little bit about what that means to be global and some of that harmonization. But from your perspective also just on how do teams collaborate in a global

organization where there are different, I mean you do have your own set of inclusive design principles at at Microsoft, right? But I'm sure they're interpreted in different ways in

different parts of the world or not. I mean I'd love to understand a little bit of that perspective on in a large global organization, but how do how do you get see that guidance?

How do you adapt them to the context you're working in? Any any reflections on that or or has it been smooth sailing in which case maybe this question is Definitely not smooth sailing.

I think I think at the core of most of these responsible AI, I think the principle like is that like we should It's it's about preventing any kind of

harm from AI. I think that's sort of like the simplest way to put it. I think the other I think the way I think we have been interpreting it more broadly in India

specifically. Again, it's hard to say because I think every team is I think most of these are written or prescribed in a manner that it is open to interpretation. So I think in India or

at least in the research lab, the way we are doing it is that like apart from responsible AI that like it should definitely don't not do any kind of harm. I think the other way is also to

make it like very inclusive AI. So this sort of like taking it hand in hand like the benefits of the AI should not be limited to a certain

demography, be it like social or economical. Like yeah, it should not be limited to that. And hence there is a like wide variety of project that have been focusing on different domains, but

that also like across the demography chain as well. So like in education or healthcare or accessibility or agriculture, so that it's not only like

certain sector, but across it. So I think so those are I would say the broad interpretation I can point to. Just maybe following up on that, but

there's also across the world there different, let's just take for example data protection as as this legal framework. They differ so widely. And and how do you factor that in? Like

do you is it I I because I can get away with a lot more in India, I do it or No no so so Microsoft policy is very very clear there. Again, so I work a lot with the

so so Microsoft has a has a division called as CELA, CELA, which is the legal entity inside Microsoft. So you need to get their approval for any kind of

critical deployments or anywhere where like human is involved. So any kind of human factors, human evaluation or even I work in healthcare. So a lot of and

even with people with disability. So you have to go through the CELA process and the way they work is that they take the the worst case scenario from the two entities. So they have to like it's US

because the most of these CELA people are living in US. And then they have like a body in India. So based on like the laws in US and in India, they try to take the harshest impossible basically.

Okay. So before let's say like DPDP was not there, then the GDPR equivalent of what was there in US that was being followed. But now like now DPDP is there, so they learn about DPDP and GDPR

and whatever like equivalent there in other markets. They say like okay, out of this combination which is like the common things are taken, but like if there are like two things which are

conflicting, the harshest one is taken and applied so that they are doing justice to it. Thanks. I I was I was curious personally about that.

Glad I got that answered. Tess, for you uh I I know there's a lot of work that the UK has been doing to drive AI assurance. If you could help unpack what that means

for this room and also some insights on how companies are actually approaching building AI tools that are fit for purpose, especially to include vulnerable

communities, right? So if you could reflect on that for us. Happy to. AI assurance is my absolute favorite thing. It is the means or how DCMS really defines it our government is about how

we evaluate, measure, and communicate ways that we mitigate risks in in emerging tech, in this case AI. Another way to understand it is regulation kind of talks about what is

responsible AI. And AI assurance is how we actually do that. So for instance, we might hold the principle of transparency, the ethical principle. And that sounds really nice. But in order to

do that, we do the assurance technique or tool a system model card, which is like a nutrition label, but for the system,

right? And within the UK context, this is a real economic and policy priority for us. In a DCMS report called Reassuring the

Future of Responsible AI, this space actually already employs 12,000 folks. It's been quoted worth 1.6 billion and set to grow six times over the next decade if we address barriers to this

space. So, you know, that then was followed by something called the Trusted Third Party Roadmap. And in this, there was four key areas that our government is focusing on. And I think, you know,

assurance and other areas should as well. It's a challenge for responsible AI practitioners, AI auditors, folks that are trying to stress test these systems,

information and data access, the skills and competency framework for that profession itself, and actually money towards innovation and novel approaches of assurance. And then DCMS is also

getting together something called a consortium. So they're taking together folks that are the responsible AI lead at at Microsoft or other larger companies and the smaller companies as

well and they're kind of bringing them together. In terms of my research itself and the UK is taking a sector specific regulatory approach and that as our AI

assurance ecosystem is maturing is getting echoed. So we're starting to see specifically in in public sector a real appetite for AI assurance. You don't have to talk about buying because for

instance in healthcare you guys are pretty mature on it. So is financial services. So is justice emergency services because you have bioethics, because you have a duty of care, because

you have the Hippocratic oath and I think that's really kind of helpful for cross sector learning as well. So bringing along other spaces. And then and finally key to the AI

assurance ecosystem and something I think we all need to work on a bit more. We talk a lot about skills. Skills are important, you know, let's talk about the talent pipeline. Yes for technical

folk, for for founder folk, but also for the human infrastructure that allows us to bring these ethical principles to life. And what I mean is uh the again AI assurance firms, the

responsible AI lead. And so this report really looked at the fact that these people come from very different backgrounds, very interdisciplinary, but they're both a mix of kind of legal,

technical, philosophy &gt;&gt; [laughter] &gt;&gt; um ethics and they're really good at translating across incentive structures and communicating within teams. So

hopefully that paints a bit of a picture of like the UK's AI assurance uh ecosystem which we are leading in um but also ways that we can kind of build it and and create more momentum for it

which I would love. I I and we'll get to that right on on how to how to keep this momentum going. I I don't want this conversation to end at

at this panel. Uh Kanika again, I mean you're you look at I mean the WHO will look at this from a slightly more global perspective. You heard from Tess on

their work on the AI assurance side, but are there like to your mind examples of policies in other countries that according to you show promise or are effective at driving digital health

equity like any notable examples because you're looking at this from a you know a much more global perspective. I love that question because it also uh gives me an opportunity to reflect on the work

that I'm recently wrapping up. Um as a part of the regulatory considerations working group we have a landscape analysis that we're doing and we have covered about uh a population

percentage of 95%. Um in terms of um country valuations. What we have learned in this process is that there are certain countries such as the US where um

well there's US, there's of course the EU AI Act, there are horizontal approaches, there are vertical approaches. There are sector based approaches um

and then you'll see in some countries where there are state based laws in some countries there are federal laws. What we need is alignment in all of What we're learning is that we've made

mistakes in past where in different sectors um manufacturers have had troubles with entering new markets even though the

protocol was quite similar, the taxonomy wasn't and the compliance mechanisms were quite different. Um this is what the landscape analysis so far has taught us. There has to be harmonization is a

dangerous word. Uh alignment is not. There has to be some sort of an alignment between the different laws and policies that different countries have because by the end of it we have to

remember that AI is cross border by default and we have to make sure that there is fairness and trustworthy AI throughout. We cannot we cannot risk fragmentation another time and we cannot

have inequities rooting out of AI systems at this time especially in the health sector where opportunities are endless. Thank thank you so much for that and I

mean the next theme I wanted to talk about was on this conversation around harmonization, but since you've said it's a dangerous word I'll reframe it as as an alignment uh issue. Uh and I mean

it is important that we have this conversation sitting here in India, but we are trying to also you know see what we can do for the the world at large. This this summit is not meant to

be a conversation within India for India or or any of that. It is a larger conversation on what the globe needs to do as as we step forward and it's it's just a it it just so happens that this

conversation is physically located here. Um so I mean uh Kritika if I could come to you, I mean do you have ideas on what institutions like AIMS and the and the CEO you you've

been uh working with how how would you participate in you know this this call for alignment or knowledge sharing to achieve some of this harmonization whether it's on policies or even on the

principles of inclusive AI in healthcare? Absolutely. So uh I I think the project build that we started off with I think is an excellent example to allude to.

&gt;&gt; I I I need to prompt this answer. &gt;&gt; Yeah, I will attest to that. So so I think I think that's just a great example of how we do need to come together. You know, healthcare um by

nature and I think I I think uh you know, you put it very well uh is cross border. So so it's it's you know, every region has its own eccentricity if I might call it that, but broadly you

know, people across the world you human beings at the end of the day and everyone faces the same set of um difficulties, same set of diseases um and it has to you know, AI built in

built in one place typically broadly works in most other places while you need to sort of customize it to that particular place. So some of the some of the examples of what we've been trying

to do is try to get a federated learning sort of system where uh while you can't share data, the model can go. So the model kind of travels across the globe uh to different centers

and gets trained on that particular data. So that it kind of has seen the you know, the variety overall uh but also sort of is specialized to deal with that particular problem. Um

similarly I think uh you know, language is another sort of uh area where uh where where it really helps to understand how everybody has dealt with uh with a certain thing because uh you

know, broadly again um while conversations might allude to the same thing uh the method of expression of diseases changes very much. Like you know, you would hardly ever see a

patient walk into AIMS and say I have right upper quadrant pain whereas you know, you know, that's how textbooks really describe it. The person would it it's very culturally rooted the way the

person expresses symptoms um you know, it's probably going to be gabrahat ho rahi hai and then you have to figure out what that really means. So so while your model per se is actually

capturing um essentially medical textbook sort of language which is essentially but then it's also essential to sort of capture the geographical variation, the

cultural variation um and yeah, I mean I think I think that's how learning and and also learning right? Just just just simply learning uh from each other's experience can can

can move you months ahead uh because you don't want to start repeating the same mistakes that somebody else did. So I think that's um that's that's in short um you know, what we've learned across

the globe. I mean absolutely fantastic examples. I is does everyone agree though with uh Kritika? Okay. Just just just checking. I just want to make sure. Mohit I mean I'll come to you again

uh and again this has to do with the fact that big company across the world uh how can research teams like yours also I mean I I also want to understand

how this kind of dynamic plays out in a large organization um on how do you share insights based on you know, the design and evaluation process that how do you share that with your global

colleagues and I mean what is the role that a shared language could perhaps play or is there one already or is there more to be done? I I want to know a little bit from

your experience as well. So So how do we share our learning or our finding? I think as a researcher I think the I think uh

the easiest way to do that is by writing research papers. Whom not a lot of people read. So So but but at the end of it uh to your to my surprise as well

sometimes some papers do get noticed by the right community. Okay. And um and they do wonders. Okay, I'll just give you an example. Um I'm I'm actually very very proud of this

work. So uh and uh so I think and this also I like a doctor is here on the panel so maybe I think she can appreciate it slightly, but uh So we we uh

um so before this LM boom I used to work mainly on smartphone based diagnostic solution and basically like add some kind of hardware on the smartphone to make it a medical device in short. Okay.

And uh we work very closely with this hospital uh in Bangalore called as Sankara Eye Hospital. And we worked with them to build a smartphone based corneal topographer.

Basically it takes a photograph of your cornea basically your eye and gives you a topography of it. and the topography is being used to diagnose a disease called as

keratoconus. Okay. A lot of technical jargon, but it is a corneal disease that leads to majority of blindness in the among the teenagers in India. So so we built that and like as a

researcher we built a prototype, we did some clinical evaluation in the hospital, we published a paper in a very computer science journal. And we thought we thought that that the end of it.

Somehow somewhere somebody some ophthalmologist ophthalmologists came across that and they wrote us an email that like oh this is excellent we want this

and this is what we are looking for like a handle device because and because they wrote they were in Morocco so they wrote to us but then like over

the next few months like we kept on getting such emails from not only like I would say global south like not only from Afghanistan Pakistan and like other countries but even like a lot of

emails from even pros from Stanford Duke like UCSF like even more recently even last week I got an email from Hopkins. So so we we landed into something which were I think the need of the hour for a

lot of these ophthalmologists and how it was it was completely like we didn't intended to make a big splash of it we just and and because Microsoft is not into

medical devices we open source everything like the hardware software all the code all the image processing and the AI pipelines everything is open source.

Long story short is that like yes and means I think very roundabout way of saying it but but I think even research papers can do wonders at times

again like take it with a pinch of salt. But but like in my whatever 10 15 years of research experience that was the only paper wherein and the fun fact is that the citation count of that paper is

abysmal like it's not even 20 citations. But but somehow the impact of that is humongous like so again like just addition to that story is that

because everything was open source so a bunch of organizations start to use that hardware software to build a medical device and finally last year there is a device by launch called as

Instakc which is built on our platform that is now a medical device launched in India and US which has gone through the regulatory approval by organization called as Remidio so we are not directly

working with them but they took our code they learned from it and then they adapted it and now we have came to know there are like two three more such organizations who are actually trying to

do the same. thank you. It was not a roundabout way I think that was probably the most delightful answer we could have gotten to that question.

One research papers please it's I think it's so for this audience just to know that you know you might not be cited but someone's reading and and there

you never know what it could be. And the fact that you know that something that you open source and put out in the world is now being built on by others I think and truly globally I

could not have asked for a better example so thank thank you for that Mohit. Tess I mean you do represent at the end of the day Tech UK which we know is a

you know industry association of sorts I know I'm sure there people in the room here who are curious about what opportunities exist for them in the within the UK

so what what according to you who are you know that we know that there is collaboration happening in these in these markets a few years ago Rutuja and I led a you know a cohort of experts to

go to the UK to talk about regulatory approaches to AI and health care and that resulted in AcureAI you know working more closely with the NHS and we

we saw that happen as a result of some of the work we did and incredibly proud of that but for for the audience in the room what are some of you know are there bilateral

partnerships happening in this space and especially from this perspective of enabling equity right could you speak a little bit about that opportunity between the two nations

and maybe more globally as well. Yeah happy to I think I would just affirm that we are always really excited to work with India you guys are 39th in the global innovation index you're sixth

when it comes to IP and you're third when it comes to startups and so our members are always looking for that kind of conversation we actually have a dedicated forum to UK

and India and our health care team is one of the largest in the organization I think there's like four folks at least just on that we usually ride solo and so but I think we also need to be

frank or I'll be frank in the fact that when it comes to collaboration it's a bit patchy and I think a reason why for our members is around the data localization laws

and just justifying kind of how that might be a bit more costly in in the kind of contracts that they are trying to work through. And so &gt;&gt; think she I think she gave us the answer

Yes yes exactly please put it in place because I think that friction is real and it is dampening the the really dynamic bilateral relation that we have so I think that's the biggest feedback

that I hear. We have a lot of great building blocks and for that I'm excited cuz it's my first time in India and I've absolutely loved it so I want to keep coming back obviously

we have the TSI and when I was reading through that I was super excited that you know number six of the seven pillars was dedicated to biotech I've been pulled into a lot of conversations with

organoid intelligence as of late as well the frontier. When I look back at some of the things that have worked in the UK's kind of talking about doing more of is

accelerator programs so having folks come over and taking them across the nations and regions so five kind of key cities and connecting them directly with the NHS.

I think that multi-stakeholder and great engagement has proven really useful and D side is talking about just continuing with that market access.

Also still doing a deeper read of the India UK visions for 2035 it was nice that both of our prime ministers endorsed that I think as it was welcomed we still need to kind of

keep it up pace and make sure it's delivered and so still work to be done there. When it comes to kind of equitability and the importance of that kind of

programming the the best one and apologies it's just closed but depending on the success it's the first time they're running it they might do it again

it was focused on taking women leadership in biotechnologies from Bengaluru and Chennai over to the UK and again doing that kind

of cross region relationship building and I think that's something that I'm personally going to be pushing for more of on the ground because in that programming I guess what I see is it

moves beyond the market access and it looks a lot more about who gets to benefit from this existing bilateral relationship. I think the the final thing I'd flag that perhaps we

could be doing a bit more of I work a lot with the government digital services and ICO which is our kind of information regulator and I've seen a lot of good work coming out around privacy enhancing

technologies and that for anonymizing data perhaps that's something that we can discuss after the session. And I think I just want to echo what you were saying is is

just because we could doesn't mean we should when it comes to AI and so I love also having conversations here and identifying what the actual problem is and is AI the right solution so also

going back from that. Good way to kind of call back to that where we started this conversation from. And just on that note Kanika I think you were the one who kind of flagged some of

the challenges your research has identified on the harmonization question right but if if the solutioning were left up to you how would you approach it or if

you were calling the shots at WHO which you are certainly influencing so how how would you go about I love the way you proposed that question. A number of countries have actually come to us

saying that hey we do not have the right kind of regulatory mechanism or the right kind of laws to really govern this space. And we don't want to be left in a disadvantageous position just because we

don't have the frameworks that are required to have these excellent technologies on ground. Based on which we came out with legal considerations

that are essential that sort of set out the baseline for what is the bare minimum that we need to have in terms of alignment. One of which was having a proper taxonomy that is harmonized in

ways across countries this could include how we define risk based classification how do we benchmark technologies. What is it that we mean

when we say registry of technologies what is the post market surveillance how are we doing the evaluations are we being honest when we're doing the evaluations and not restricting it just

to clinical evaluations but understanding are they sustainable are they scalable are they implementable do they have a future are we reinventing the wheel and these are all the things

that we are trying to I completely resonate with all my colleagues and my panelists here. I think we mutually agree the idea is not to create and recreate technologies

but to have to to start with what we have at the moment and build over it there is no point reinventing and that is what WHO really vouches for and that is what we're

trying to achieve but yeah coming back to what you said there has to be alignment in terms of taxonomy there has to be a mutual understanding of what that is risk based classification.

Lovely thanks thank you for that Kanika. Rutuja how are we doing for time? We have time so I mean I'd love to actually throw this open to the audience here

if you could just introduce yourself and who the the question is for or if it's for everyone here. I have a question for Mohit my name is Uday Mehta and I run a foundation called

Sit the Divyang Foundation who happened to be my special child son who's no more and we work with the public health department government of Maharashtra UNICEF and the

K M Hospital in Pune. Mohit you mentioned something about the hearing app in the early part of your presentation because we are finding a lot of issues with the young

newborn babies or the babies up to the age of two or three years having some kind of a hearing impairment could you a little bit tell more about it or can you possible to know more later on?

Okay. I think there was some miscommunication. So, I'm not working on any hearing app for Yeah, this was an app to

sort of like a game for for deaf and hard of hearing populations. It was a game built for them. And the idea of that was twofold. One is that like they will have some

game some fun which is only for that community. Okay, so that was one and the other was that like that game will also help to collect data for the community for building like sign language models

that can be used again to uplift the community further. So, that was the broad idea. It has nothing to do with diagnostic part of deafness or hard of hearing.

No, yeah, yeah, nothing on that side. Yes. Yeah. Okay. My name is Priya. I'm a founder health tech startup founder. My question

is for Dr. Kritika. So, my question is you talked about models traveling boundaries without the patient data. Now, let's say the models coming up from

cloud to build open air that are HIPAA compliant. Now, to train or to fine-tune even these models in Indian data sets, what we require is

first of all, hospitals like AIIMS need to come up and then they need to make some data sets at least publicly available. And then collaboration

with respect to government hospitals, private hospitals. And it's very difficult as a founder to, you know, reach out and then Actually, it's like it's like a

practical problem that collaboration is completely missing at least when you turn the tables here. So, what what we can do better in terms of like how how should we

uh collaborate? No, absolutely hear you and I think I'll invert that to what we can do better. Uh so so, you know, I'll just tell you

some examples of what we've been doing. Um so, a lot of our data, I won't say a lot, but then we've made a start in in in releasing some data publicly. So, if you go to the DBT website called IBIA,

we're building out something like a Kaggle for medical data sets. Um so so so, this is the imaging biobank basically. So, there is a genomic biobank as well as an imaging biobank.

And AIIMS has already released some data including some of my own work on mammography on IBI. Another uh site that you can visit is is an ICMR uh led effort called MyDis.

So, this is by ICMR and IISc. Artpark, IISc. So, um you know, there also there are, you know, gold standard data sets is what they call it because uh you know, we're also trying to

benchmark data sets. So, not only just put up the data sets, but also benchmark them. And also define uh you know, what is a good taxonomy for it basically because, you know, every disease has a

very complex taxonomy. So, we we break that down and give you some more information and insight into what is it that you should be detecting. Um so, that's the effort from ICMR. And then

there is also uh the AI Kosh. So, we are releasing data onto all three of these. Uh very early when you compare it with the effort that's going on in several other countries, but we're trying to get

there. It's just that there are a lot of more layers uh to unpack uh and we're trying to unpack those layers. Um and and you know, hopefully because we've we've done the initial sort of ground

groundwork, hopefully we should be faster when we get uh you know, further into it. Similarly, we're trying to organize some sort of Kaggle type of contest again. MeitY is really

supporting some of those. ANRF is supporting some of those and you'll see some of those coming up uh in the very near future. Um and we're trying to also incubate.

So, the CMIE is an incubator. It's a bioincubator located within AIIMS. So, please do reach out if you want to incubate within AIIMS. It makes it easy from uh you know, an ethical perspective

to share data with you know, to to even if it is a startup within AIIMS uh because even on our side, you know, the overheads of sharing data outside uh are much higher. You know, you have

to take care of patient privacy, patient security of of just annotating this data which is a huge effort. So, this you know, just just some some effort in that in that Sure, I'll reach out to you.

Specifically after this conversation. Then I I think there's a one more question at the back if we have the time for it. Kritika, you're keeping me honest on

time. Uh hi, good evening, ma'am. Uh sorry, my question is also to Dr. Kritika. Uh Sorry, I'll have to give a bit of a background why this question is coming

through. Your point about what you said about that Asha worker really hit home for me because in '18-'19 we had developed a high-risk pregnancy app where we had put in machine learning. It

was in Himachal Pradesh and this actually kind of hit that is it me who's doing it or is it the machine that's doing it? They really hated that concept. And that kind of carries

forward to doctors as well because that thing is running at the back of the head that AI is going to take over my job. So, I'm coming from a place where I would want to learn because clearly you

have a lot of experience and that insight. How do you think we mitigate this challenge because the answer is not no AI here. As we realized when we deployed it, we did have an amazing

result in infant mortality and maternal mortality. A gross reduction. And there's only so much we can upskill our Asha and ANM workers or even the doctors with all due respect. So, from a

doctor to doctor, how do we tell a doctor that look, you got to use AI and it's not going to replace you? I'll say, you know, how do you enter an airport? I think I I I see more and more people

actually adopting DigiYatra, right? And the reason we do that is because it's just so easy to get in. So, I think the answer is also there. I think we have to make it very easy to adopt so that so

much so easy that it is stupid to do it without that. And then and I think till we get there, you can't force anybody to do anything especially in a system which is so overburdened like the Indian

healthcare system. So, if you had to tell me that today I have, you know, 200 patients to see in my OPD, but I have to learn this skill and do it this certain way, I will not do it. So, you have to

show me why it makes my life easier. So, I think, you know, I think that's that's where we'll find our answer. Can I ask something to the question? We're working on that particular thing

with one project Maharashtra with the government of Maharashtra The real problem is the health of the child

and it's not the Asha worker or the Anganwadi worried about it. It's the mother who's really worried about it. Mother is with the baby for 24/7. So, we can use AI to

reach to the mother with a voice-activated app with a backend database and that that breaks down the barrier because mother is with the baby all the time.

Absolutely. I think that's where the answer lies. You must reach to the mother. Keep the person, you know, you're targeting in mind.

&gt;&gt; we wouldn't be able to take that last question. I think we're at time. Thank you all for such a wonderful conversation. I think everyone brought such such a unique perspective to the

conversation here and I've had a My job was very easy. I had a great time listening to you. Uh Yes. So, if you could just stay in your seats, there's an important partner

who's kind of made this project possible who unfortunately haven't been able to join us. So, we're presenting a short video to include them in this conversation. Just a message

from them and the kind of work they do just to give you some context of some real-life practical labs in the university. Hi, everyone. My name is Dr. Mahima

Challa and I'm a digital health equity and participatory design researcher at the Centre for Digital Transformation of Health within the University of Melbourne.

I want to extend a very warm welcome on behalf of our center to all the panelists and attendees at this very timely discussion on global cooperation for equitable access to healthcare AI.

I would like to start by sharing a little bit about our digital health lab called the Villageton. The Villageton was established to address a persistent global challenge.

While interest in digital health technologies including clinical AI is growing rapidly around the world, very few solutions actually successfully translate into routine care.

And even when they do, they can sometimes unintentionally harm or exclude certain populations, some of whom may already be marginalized. A key aspect of our approach is the use

of immersive simulation-based research to identify and address potential issues which may cause clinical AI solutions to fail during real-world implementation. Simulation is enabled through our

clinical simulation facility which has a suite of themed home, primary, and secondary care spaces where patients and clinicians come together to co-design and test tools in a realistic but

controlled environment. On top of that, we also have a sandbox where we can use digital tools to build a replica of a health information ecosystem based on interoperability

standards. Through simulation-based research, we are able to identify and quickly address in a preclinical setting technical, workflow, and equity risks. For example,

barriers related to digital literacy, usability, cultural context, and language, thereby generating evidence which supports more inclusive and responsible implementation of clinical

AI. I'm also very pleased to say that the Villageton's collaboration with Ikigai Law has grown from strength to strength over these past 2 years. It's very much

a testament to our shared commitment to advancing inclusive and equitable AI in healthcare. I'm very much looking forward to continuing this journey with you and

fostering multilateral collaboration on equitable digital futures for all of us. I'm sorry that I couldn't be present in person today, but I look forward to hearing this insightful panel discussion

via live stream. I thank you again for your time. I think we're at the end of the panel and thank you for coming here today and have a happy day one to all of you all,

I guess.
