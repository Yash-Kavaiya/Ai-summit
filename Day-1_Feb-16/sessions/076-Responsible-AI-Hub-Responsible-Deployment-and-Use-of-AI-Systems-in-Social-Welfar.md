# Responsible AI Hub: Responsible Deployment and Use of AI Systems in Social Welfare Delivery

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 14:30 ‚Äì 15:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 9 |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/Wr8eqpD9bMY?feature=share) |

## üé§ Speakers

- Abhishek Jain, Straive
- Amba Kak, AI Now Institute
- Amit Kumar, Fujitsu Consulting India Pvt Ltd
- Gabi Leibowitz, ElevenLabs
- Gaurav Godhwani, CivicDataLab
- Isabel Elbert, United Nations Human Rights B-Tech Project
- Jennifer Mulveny, Adobe
- Kishore Balaji Desikachari, IBM
- Kumar Sambhav Srivastava, NutGraph Social Data Lab
- Maya Sherman, Embassy of Israel in India
- Prateek Waghre, Tech Global Institute, India
- Prof. Balaraman Ravindran, CeRAI
- Ryan Carrier, ForHumanity
- Sundaraparipurnan Narayanan, AI Tech Ethics

## ü§ù Knowledge Partners

- Nasscom - Responsible AI Hub

## üìù Summary

This panel will examine the use of artificial intelligence in social welfare delivery and its implications for accuracy, fairness, and accountability. It will explore risks linked to automated decision making, the importance of human oversight, and ways to reduce unintended negative outcomes to beneficiaries. The discussion will focus on monitoring and evaluation approaches that help governments assess impact, costs, and outcomes, and identify safeguards to ensure transparent, equitable, and effective welfare programmes.

## üîë Key Takeaways

1. This panel will examine the use of artificial intelligence in social welfare delivery and its implications for accuracy, fairness, and accountability.
2. It will explore risks linked to automated decision making, the importance of human oversight, and ways to reduce unintended negative outcomes to beneficiaries.
3. The discussion will focus on monitoring and evaluation approaches that help governments assess impact, costs, and outcomes, and identify safeguards to ensure transparent, equitable, and effective welfare programmes.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/Wr8eqpD9bMY/maxresdefault.jpg)](https://youtube.com/live/Wr8eqpD9bMY?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

a brief context about this session. So, India operates uh one of the largest social welfare system in the world whether it is in food security, livelihood or you know maybe in

healthcare as well. uh also uh when we talk about responsible AI or responsible AI deployment specifically in the production side we talk about how it will behave in the real world uh how it

will scale if anything goes if anything goes wrong who should held accountable and etc. Also uh in social welfare system errors are not abstract actually they translate on ground uh it could

lead to exclusion lead to hardship and at the same time loss of dignity. So this is what we are going to talk about uh in today's session. Uh because this is a short session so we are going to

have one question for each panel members. We'll try to have you know uh finish within that time. Uh so my first question to you Maya uh from a policy perspective what safeguard should be in

place before we even think to deploy AI in production and specifically for social welfare systems. So hello everyone first of all wonderful to be and thank you so much for the

organizers for creating this session definitely timely and important session and wonderful to be alongside the co- panelists um I think there are few safeguard that we're currently thinking

on and you know I think that the beauty that in many ways the answer is to this question is evolving over time um I feel that in the past we were we were focusing a lot on diversity by

design, meaning how we can make sure that more people are able to access technologies. Um, we were speaking a lot about biases, uh, trying to see how we're making sure that the technology

that we're creating is not causing more harm than good. I feel that now we're shifting a bit of discussion on the scalability. We're trying to discuss a bit more in terms of safeguard and how

we're enabling the infrastructure to to to be more scalable to walk over time. So I think that in a ways it's also perhaps a reflection of how AI has evolved over time. Um that if in the

beginning we were looking just AI purely meaning AI tools the way that they are being perceived how we can tackle diversity biases we do understand that today AI needs to operate at scale. So

the safeguard the thing that we're trying to think of are also becoming much more infrastructural and I think it's an interesting shift that I've been seeing in the scholarly community around

it meaning it's still the broad ethical guide that we've been speaking about we are aware that we need to make sure that biases we have the right counters we know how to deal with linguistic

diversity in a way but we're advancing and I feel that today the discussion has shifted a bit on how we're thinking in infrastructural level meaning we have foundational models coming into case. So

the discussions around safeguard have shifted a bit I think as a result of it. That's my perspective at least to the question. Thank you. &gt;&gt; Thank you so much. I'm just building on

that and Pratik will come to you now more from a citizen impact and fairness perspective. So when AI excludes somebody from these social scheme and we see that most of these systems actually

ask citizen to prove the identity or eligibility. So in this case in the responsible AI deployment uh if you know if you think from this perspective like whether this is a design failure or this

is something to do with policy and if you can just bring some insight from an Indian perspective that would also be interesting. Yeah &gt;&gt; thank you. Uh I think you know very

important question and uh very quickly right back back to basics I think this is something that that we think of you know the cranber first law for technology says technology is neither

good nor bad nor the neutral right which is essentially to say that when you're deploying them in in certain scenarios it's going to take the shape of of the system that is and so in the Indian

context it's going to work how things work in India when you're you know when you introduce it in public delivery it's also going to take on the shape of how public delivery already works right uh

so just to take on I guess some examples which will sort of illustrate my point and I'm drawing on work that uh in fact one of our panelists the future Kumar has done uh there's some reporting

that's happened around the you know the special intensive review process uh that that's going on right now and you know other work on on on the public distribution system. So you see cases

where you know for example there was a 102-y old man he was declared dead uh and it took to get the local uh administration's uh attention uh he staged almost like a a wedding ceremony

or you know what we call a barat right uh literally to to to essentially get their uh to to get their attention in another state in the country about if I'm getting my numbers right about 70%

of people who are initially marked ineligible for pension uh for certain for for social welfare of that sort. It was eventually reversed in some cases but there was as you said there's a

process uh you know which becomes uh you know they essentially get caught in a in a bureaucratic trap uh trying to get trying to get themselves cleared right there were cases where uh 64 year old

widow her pension was stopped because uh it it you know the systems in use uh somehow estimated that her 9-year-old granddaughter had an income of six 600,000 rupees or six lakh rupees right

uh she had to eventually I think go to report uh to to sort some of some of these out. There were cases where I think a family was denied welfare because it said they had a car and you

know and they didn't. So you have lot of these cases where you know you a decision is made right uh based on uh you know either incorrect data or inadequate data or you know data sets

not being cleaned properly etc etc etc. Uh and once that happens the the you know the as you pointed out the proof is on or the burden of proof is on is on the citizen right or on someone

receiving uh either that that benefit to do that and that's where you know the property of the existing system comes in where uh you know you are uh you have to prove that you're alive right you have

to prove that you're not earning uh this money but you know the the the system that allowed the fault to take place there isn't there isn't red addresses there right and I think and I think

where where we sort of go wrong is going again back to the basic aspect of it is that we don't have the redressal mechanisms in place right when you're going to deploy systems like this errors

are going to happen the question is even you know even without that I or redressal mechanisms strong enough to be able to to sustain some of that and I think until we're building that which is

you know which is independent of technology deployment right unless we're actually building those systems to be to be responsive to be resilient to actually cover all citizens. We're going

to keep having this conversation again and again. &gt;&gt; Thanks so much. And actually this creates a good foundation for the accountability part and uh I will ask

Ran on that like when welfare a system cause harm like who realistically can stop this and uh what it tells about the gaps and the accountability if you just observe it. Yeah.

&gt;&gt; Yeah. Happy to. So just by uh so that you understand where I'm coming from, for humanity is a charity that's established exclusively to look at the risk from these tools. So I'm only going

to talk about downside. I don't want you to think that we're AI naysayers, not in not in this environment, right? So but but I need you to understand that social welfare has extreme risks, not only

risks to the people. So in the Netherlands, they displaced 2,000 children, took them out of their homes and displaced them because AI told them to.

In addition to that, if you think that there was only risk to those children and those families, which is terrible, the government was turned over because of that. So if there's any politicians

in the room, it's an important message to understand that you want to be building AI. wisely, responsibly, in a trustworthy manner. Now, what does that mean? I'm going to speak in bullet

points. We only have two and a half minutes. I'm going to be outside. If anybody wants to talk more about this stuff, I could talk for days. Okay. So, when we talk about the micro level, we

need transparency of process. We need explanability of these uh choices. We need to understand that there's originally a duty of care that must exist from the provider of the social

benefit and that they're operating with that duty of care. Um, in addition to that, we need to have grievous grievance mechanisms redressal for these problems and they

have to be simplified and they have to be fast. These are people's lives. At a macro level, procurers of these systems, whether they're state, federal, or other types of social welfare, should be doing

so with robust assurance mechanisms. That could involve what what we work towards regularly, independent audit of AI systems, a set of rules laid out through a procurement process that

satisfies the techn techno legal responsibilities of the organization. collectively macro and micro approaches will help the issuance of social welfare schemes achieve their very necessary and

very valuable goals in a responsible way. Thanks. That's well put and you know when uh one recurring issue that we see in the responsible AI that uh in principle it

looks very strong but when you implement something know we start seeing the we start seeing some weaknesses in it as well right and at scale the responsibility get distributed whether

it is tech policy or you know the operations as well. So to handle all of this we need governance in operationalization and because and basically to complete this you know the

entire circle of question I just want to go to Jennifer and Jennifer wanted to understand from an enterprise perspective like how organization uh operationalize AI like responsible AI

deployment uh at scale and at adob at your organization how you are basically handling this critical situation. &gt;&gt; Great thank you. I'm very happy to be here from Adobe. I hope many of you are

um aware if not using our tools. Um look, one of the things that I've been really uh pleased about with a leadership at Adobe, our CEO mentioned actually at an internal meeting just a

couple of years ago when AI was really starting to take off that for Adobe, it's not can we develop and deploy AI, it's should we right where we always have to think about the should. And I

really feel that's a very important value when you're thinking about doing anything. And when you think about a company being very much sort of a microcosm of a government, right, Adobe

thinks about how we're going to serve our customers. The government is thinking always how are we going to serve our citizens and what is the risk and what are we going to do as an

organization, as a government, as a company, um as a nonprofit. How are we going to deploy these things responsibly? Um I was pleased to see a number of the signs around Delhi today.

Uh Prime Minister Mod's quotes and talks a lot about human innovation. And I think one of the most important values that we hold dear at Adobe is keeping the humans at the center of everything

because if we don't do that, we're all going to find ourselves in a lot of trouble. You think about something like social welfare, right? Um you've got to think about serving and protecting the

citizens. And this is this is a of all the different areas where you look at risk and you do risk impact assessments. This is something that does impact real lives. And so it has to be treated quite

differently than say something that's going to deploy um AI to help people find out where they're going to go on holiday. It's very very different. So at Adobe uh well before AI came on the

scene since we've been doing so much uh digital innovation deployment over many many years uh we have an an ethics review board within Adobe right now it is the AI ethics review board where

every new AI tool has to go through that board. The engineers have to uh propose what they want to do. It goes through an impact assessment. There's a diverse board of people that look at things like

bias. So it's not just a boardroom of white men looking at to see if this thing makes sense. That's far from the case. Um and then we have a very um robust sort of system of of training and

testing getting feedback having that diverse input and then um again that feedback is important right what's working not working send that back to Adobe what can we do to improve upon

things so we're making sure we keep people at the center and most importantly we have humans within the company that are making the decisions that do impact lives so I'll I'll stop

there because I know we don't have a lot of time but we can uh continue the conversation &gt;&gt; that's wonderful in fact in at Fujitsu So we are actually working in AI for

last 40 years and we have a separate ethics and governance office that actually takes care of uh safe and secure deployment of AI system. U so uh if I have to conclude this session u it

must be on five foundation. So AI in this uh civil uh AI this so social service must be appropriate, accurate, available and uh it should be accountable and aligned with human

dignity. So if any of these is missing then efficiency is not the responsibility. So with these words I will conclude this session. Thank you so much for joining and thank you all of

you for your wonderful inputs. [applause] Thank you to our speakers for that insightful discussion. Having heard perspectives on

having heard perspectives on responsible deployment of AI systems in social welfare delivery, we now have a multistakeholder panel to discuss on the impact of

responsible AI in social welfare delivery. For this we have Gabby who leads the impact program at 11 labs. Our next speaker is Kishor Balaji the

executive director of for government affairs at IBM India South Asia. Also joining this panel is Abishek Jan the chief financial officer at Strive. The dialogue will be moderated by Kumar Sabh

the founder and CEO of Nutgra social data lab. Over to you Kumar. Thank you so much uh Kitika and um uh hello everyone. Good afternoon. Uh uh really thanks to the organizers and kudos.

data lab uh is an organization that collects great data from the ground to inform policies and insights. So our job is to really look at what's really happening on the ground and uh bring

that to the table to inform policies and decisions of course for the the the development and the progress of uh of the country of the people. Now to set the context uh I think our speakers did

very good job in the previous session. I'll just uh I'll just uh read an excerpt from one of the case studies that we did on the AI in the social welfare on the ground. Uh so I'll read

from that. So Sheila Dvi can't conceive of owning a car. The 67year-old widow and 12 uh and 12 members of of her family live in a cramped three- room house in an urban slum since her

rickshaw husband death over two years ago. ago from mouth cancer. Sushila makes a living by peeling garlic for a local business. But an algorithmic system deployed to assertain the

eligibility of welfare claimments and to catch welfare fraud tagged Sushila's husband as a car owner when he was still alive. This deprived her of the subsidized food that the

government must provide to the pe poor under the law. The algorithmic system took her late hus uh took her late husband Shamlal the rickshaper

for Shamlal Sharma a car owner and the authorities accepted its words. So when the coid9 pandemic was raging and her husband's cancer had peaked, Sila was running between government authorities

to convince them that she indeed was poor. The authorities did not trust her. They believe the algorithm. Quote begins, if we had money to buy a car, why would we live like this? She

asks, if the officials came to my house, perhaps they would also see that, but nobody visited us. So what uh I think the speaker in the previous session did speak about what's the harm that we are

talking about, what's the risk we are talking about in India. uh welfare uh sort of provides dignity and rights and sustainment to almost half of the population. Dollar 256 billion money

goes into welfare delivery and it provides food rights etc. So sometimes exclusion means going hungry and sometimes exclusion also means going without pension being declared dead. So

with that context I would like to uh uh invite my first speaker uh Kishor uh you have uh IBM has worked with the state governments on many of the welfare systems they've deployed technologies

what do you think uh when we talk at scale uh how do you we bring in uh transparency accountability and safety uh at scale in in in algorithmic systems in AI systems when it comes to welfare

just added one more dimension to the troubles this morning. Firstly, thanks a lot for having me here. Uh such a pleasure to be among you all today and uh I for one to begin with believe that

technology is the smartest and the most efficient way of solving some of these issues. But again for the uh for the the the benefit or the goodness to acrewue you got to have checks and balances in

place. Now very topically uh I saw several of the notes that I had made uh covered in the previous uh discussion too. The example that you highlighted uh was perhaps an opening comment I had

that technology for the sake of technology in an ungoverned without a human in the loop model is a very very big risk. So as technology scales it's very

evident that you know the AI is by design not to replace a public servant but to augment a public servant. You cannot have a situation where uh un um managed technology totally automates a

process leaving out the human in the value chain to take value based decision. Therefore, human in the loop is something that is very very important especially for a technology which is so

fast emerging uh as we speak today as we see today. The second is as you build solutions. &gt;&gt; Okay. Closer to the mic. &gt;&gt; Yes, I'll try. Is it better?

&gt;&gt; Perfect. The the second uh aspect is you know uh technology the designers of the solution should look at uh engineering fairness into the system. So fairness is not something that should be assumed. I

mean people should be very mindfully engineering that into the process. that will help people have more you know eyes on the solution to look at bias to look at uh you know all aspects of uh

issues cropping up uh testing them auditing them cataloging those issues and solving for them now as I see it if we don't build for fairness we'll be design we'll be automating for

inequality from that perspective it's very very important apart from human in the loop to look at how we build uh for fairness into the system how we engineered that apart from that uh of

course uh yeah it's important that people trust the AI process for which explanability and transparency remain very very key and lastly uh there is a temptation to use technology

to scale to solve you know issues of this stature and scale right so while doing so fundamentals of uh irresponsible AI like the previous panel discussed too are very very important

and people should be very mindful to bring it all together so that technology can actually acrue the dividend that it promises to acrewue than you know just automating the process in a very

inhumane way. Thanks. &gt;&gt; Yeah, thank you so much. I think two things really came out from Kusho's point is one uh human in the loop protocols which is essential um u

something that should be built in the policy and design and like what he said if you don't design for fairness you are automating inequality uh so hopefully we'll avoid that risk

I'll come to you Gabby um you uh have actually I mean the idea behind your work and the company has been to make uh AI uh useful for the humanity, right? Uh what from your work you think has been

uh uh could be the challenges that could come specifically in the Indian context and uh how the practice of responsible deployment design and policies around AI could uh make uh those risk mitigate

those risk and build for a better better AI. &gt;&gt; Yeah, absolutely. Um is my mic on? Yes. So, I'm Gabby. Uh I work at 11 Labs and I run our impact program. So through the

impact program we partner with nonprofit organizations and then also provide our technology for free to individuals with accessibility needs. So one of the largest areas in which we find that

that's useful is individuals with permanent voice loss are able to create voice clones and then communicate using those voice clones through their accessible technology communication

devices. Now India is extraordinarily linguistically diverse and there's also a stark digital divide. So when we're thinking about um how do we really serve um the population, it requires a lot of

local collaboration. I would say that's a large pillar for us as we expand into India. So a lot of it is about finding um flagship nonprofit partners within India who are able to um ensure that

we're delivering useful technology, representative technology. And then also when we're talking about audio AI um of course it's incredibly linguistically diverse and so you know we have 12

Indian languages available on the platform but that doesn't nearly touch the number of not just languages but also dialects. Um and so it's also been really important for us to partner with

local ecosystem builders who um bring in local creators who record their voices and put them on our platform. Not only does that allow us to have u more variety and diversity in the Indian

languages and dialects that we have available for others um but also those creators receive commission every time their voices are used. So there's also like an economic opportunity there. Um

we work with one ecosystem builder in particular who is trying to bring in more blind individuals to record their voices and put them on the platform um to provide an economic opportunity that

may not otherwise be there. Um, additionally with the digital divide, you know, we have issues with bandwidth. We're of course a cloud-based service. And so again, partnership is key. So we

partner with uh companies like WhatsApp who are able to operate in low bandwidth environments um and on uh lesser like smartphones that don't operate quite as quickly. Uh so that's been really

important for us. I also think audio we've seen has been a very important accessibility layer when we're thinking about low literacy and low vision. Um and so having um social welfare being

delivered multimodally. So having a auditory ways of accessing that information, visual ways of accessing that information that has also um allowed us to be more accessible to more

people is offering multimodal information and democratizing access to information in that way. Thank you uh Gabi. Um I'll come to you Abhishek. Uh you know we we we do did

discuss what are the challenges and what are the uh say self regulation to or voluntary sort of uh practices that the companies can use in designing uh these models or or systems that could be

deployed for welfare. What what do you think from the international experience of policies? Right. uh uh uh we have examples like EU's uh AI act which deals with the risk assessment framework. Uh

there could be uh there are other practices which are about human rights based impact assessment of these systems before they are deployed. Uh there are other practices by international

organizations like UNISCO. uh how some of those could be uh adopted in the Indian context and and you know a a framework for uh before deployment that these risks are covered checks and

balances are played how how can a framework be developed for that &gt;&gt; thanks uh hello everyone I'm Abishek uh I come from a company called strive uh where we strongly believe that AI is no

more a consulting job it is where we operationalize AI right and our previous panels talked about it and maybe through our examples I'll elaborate it as well as to what we

do when we say operationally operationalizing AI right I think we have uh uh as you said a lot of frameworks and policies that has been drafted one theme that come out very

clearly is that you know uh that uh AI governance has to be proportionate to the risk that it carries right it cannot be the same governance level applied to the same features what I mean

by is uh like we talked about a lot of examples of u and an and uh citizen being uh discriminated in terms of the benefits that they can they are eligible from the government offices any

compliances requirements and so forth right so there the act of uh AI has to be much more stronger than any other which is uh so that is driven by EU if you see UNESCO they are more driven

humanentric right which is main saying human in loop I give you One example right we have bradhan mantri uh yoja which is more from an housing and other perspective what we see is that if

you're not eligible because your electricity bill or xyz was higher than what your eligibility are you know we just send a denial note what we have seen government now infusing it is that

we are sending them in a bilingual local languages as well where the real uh people understand that's why the denial was happened right I'm think just giving you one use case of how the AI is being

used by our government itself in a different uh ways as well. Right? Uh on on the other stuff I think what I believe uh from our perspective is that a we should not use the global framework

in a same word weapism. Uh we should see that how human in the loop as our friend said is very very critical to our countries. Our country is very different than any other uh large or large country

as well because of multil language and multiple uh places that we are in very different regions. So human in the loop is very very critical. I think the audit procedures for us becomes very very

critical as well. uh clear accountability and ownership uh would be other theme I would say which would be very critical from our point of view and uh post deployment monitoring would be

the another thing which will be very very critical at least from our country's standpoint of view I would say &gt;&gt; thanks Aishik uh I think our panel has done wonderful job in keeping to the

time limit uh so really uh kudos to them uh just to conclude uh a few things that I thought uh were emerging from the session in the previous one thing exclusion has been talked about majorly

uh and as someone in the previous panel said that it's exclusion is not just the risk for people it is a risk for uh for the scaling of AI as well uh if you want social buying in if you want political

buy in uh we need to see that uh such problems do not emerge on the ground because uh from that example we looked at uh there were hundreds of thousands of people getting excluded they went to

the court and then uh a lot of uh it wasn't a good uh good sort of buying in for those systems that were deployed in in in in those situations. So I think investing in the beginning in in these

processes is even a makes for a good business sense as well because you don't want your systems eventually to be scraped or or get a bad public image right so and I think a few other things

were that apart from exclusion there have been issues about misuse of data in some instances there have been instances of um uh of also opakeness and transparency about how these systems

actually work and the transparenc currency is also important especially in the government deployed systems because that's about taxpayers money which is being used. So I think on that uh point

I would I would close uh for this panel and thank you so much uh for for being wonderful speakers and wonderful panelists. Thank you. [applause] &gt;&gt; Thank you to [cheering] our speakers for

this insightful discussion. We'll now move on to the last session of the day where we'll hear more of technical perspective on the responsible development of AI systems in social

welfare delivery. Our first speaker is professor Balar Raman Raindran who heads the center for responsible AI at IIT Madras. Our next speaker is Gorab Gwani who is the founder and co- director at

civic data lab. Also joining us is Dr. Isabel Elbert the co-lead of UN human rights on BTE project. The dialogue will be moderated by Sund Narayan who is an AI ethicist and adviser.

Over to Sund that's a lot of pressure to finish on time. Okay. Um [clears throat] I want to um the the one difference that

we're doing here is that we are starting the AI system development for social alpha delivery at the last. So that you've already heard the other stories. Let's come to this at the last and let's

see what we are here for. Um we're going to do it as uh rapid fire style as quickly as possible quick answers and then we are going to move on. So any details that you need find all of them

anywhere you get hold of them get to speak with them right um I want to start with uh professor Ravi uh [laughter] &gt;&gt; that's an easy target you know &gt;&gt; sure sure

&gt;&gt; I wanted to start with you um more from a perspective of when somebody is looking at uh uh social welfare delivery and uh investing in AI how should they actually start looking at the problem um

and from your perspective should they start from um what's the impact it's going to create or what's the um technology that they would need. Should they look for uh or they should look at

the sociotechnical kind of facets? You can rank them the way you think is the best priority to look at. &gt;&gt; Okay. So, can you repeat the choices again? Sorry, I'm making it harder for

you to stay on time, but uh [laughter] &gt;&gt; data &gt;&gt; um technology alignment and the third one is the kind of impact social impact &gt;&gt; social impact social technical I I I

think the most important uh aspect should look at is actually going to be the impact on the ground. If everything if all stars align and they are able to deliver what will be the impact on the

ground and if that is going to be sizable then it is worth trying to answer all the other hard questions. If not we shouldn't be. &gt;&gt; Wonderful. Um uh just coming to you

Isabelle on that uh when we are speaking about impact as the most important thing right um how do you see the impact from a deployment environment perspective you need to think about what impact it's

going to cause and accordingly prepare for your development process. So what are the considerations that you would think from that standpoint what would be the priority that you would look at?

So that was a risk that I took the wrong microphone. Um the previous panel really I did a deep dive into the risks. I think um the alignment issue is really something that will hit when the when

when the context of deployment um is then uh specific &gt;&gt; and um the the key issue is really accuracy can have um Those should have the highest value because a small error

can exclude millions of people and if you scale and you have a small error and you don't assess the context properly that can be a key issue. &gt;&gt; Wonderful. U go that comes to your point

in terms of um uh when we're speaking about small error it can exclude people right it's going to be a bias kind of a problem. Uh what is the real issue in bias? Is it the data issue or is it the

socopolitical issue which is the underlying factor for the data or is it the issue of how the technology is built? &gt;&gt; With your permission, I'm going to play

a bit of a poll with the audience so that we keep it more interactive and inclusive. &gt;&gt; So the question was very simple, right? uh is it the data problem where we see

the technology issues soioeconomic inclusion or is it because of how the algorithms are made? What do you think? &gt;&gt; This reminds me of KBC. Huh? You're asking the audience.

&gt;&gt; I'm doing an audience poll to begin with and then I would give my remarks. I personally feel it's a social economic problem. Raise hand if you also believe so. It's because of the soioeconomic

nature of how we are placed and how we collect data, how we co-create these solutions, we end up excluding most of the people and it's not the problem of data biases because while these data

systems were created inherently the bias came into picture. They were not very inclusive in nature and that's where we result into a scenario where we are seeing a soioeconomic bias in place or

the algorithms for that matter because they're trained on those data sets which were having bias to begin with and they were tried in a laboratory which was not well tested in in a participatory

environment and we result in a scenario where we face exclusion. &gt;&gt; Wonderful go um professor Rafi the question then to you is that uh um if the issues ask me how to fix the social.

&gt;&gt; No no no no definitely no that's why we're all here so we all understand. So my question there is that if the issue that causes bias is so political issue how would you evaluate it at the time of

development for that kind of a problem. What would be your top three priorities to look at to evaluate that? Not the technology, not the data, but to focus on sociopolitical

um contributor of bias. [sighs] &gt;&gt; Wow. Okay. &gt;&gt; I'm sure it's a 1 hour lecture worth but I mean I'm I'm not even sure I

completely understand the question. So your question is hey will you be able to evaluate what is the contribution of the socopolitical &gt;&gt; you know system into causing the bias or

I mean what is &gt;&gt; yeah how will you so bias is one of the factors in the in the broader thing now what we are saying is broader risks that can impact uh the social welfare

delivery in that context socopolitical underlying factors the social context or social factors are underlying factors that uh that is a direct contributor to the implications that we are speaking

about how will you evaluate them while you're looking at developing &gt;&gt; I mean the the textbook answer would be to see if I can do some kind of a controlled experiment where some of

these factors have been mitigated and so on so forth but the problem is in real life you cannot do that right so it it becomes a significant challenge so the one kind of a surrogate

that you could potentially actually look at it right saying that um you know you can run you know hypothetical experiments you can't run real experiments but you can run hypothetical

experiments where you actually you know manually change you know I mean change the attributes that could potentially be causing the socopolitical issues right and then see how well your system

performs so that way you can say that it's like a more like a counterfactual analysis if this hadn't happened so is what would my system have done right but for that to really work I need to have a

very good understanding of what should be the counterfactuals I should be setting up and I don't think at least in the context of India I don't know if we have a good understanding of that you

know we don't have a systematic recording of all the things that can that that are possibly you know causing this kind of for example I'll give you give me one one simple thing right so

when people actually tried to be more inclusive of women voices about certain opinion so there were people who were sent to the villages and told to interview women and later on it was

found that most of the women answers was given by the husband, right? So how would you even fix how would you even evaluate? I mean you've made an effort to make sure that you are

being inclusive but still there are factors that so it's it's hard to write it down but the way to do it would be to do some kind of a counterfactual do but then the first step would be to figure

out what is it that you should be controlling for. &gt;&gt; Brilliant. Um one of the things um uh that has come out uh Isabel is that one participatory kind of uh evaluation

participatory kind of development mechanism and using counterfactuals as a mechanism um to test for AI systems as you develop them right uh from your approach and perspective how do you see

them given that there are a lot of context and causalities in the way it is getting applied in a particular environment how do you see that what are the couple top two or top three things

that you would consider from that perspective. How to bring in context of causalities into this whole mix to make it more meaningful. &gt;&gt; Okay. Well, we're diving deep into a

sort of academic panel here, right? Those of you that were tortured with of with statistics and methods in university, you are having a lot of joy. I guess I I think what is clear is that

we need a multi-disiplinary methodology, right? Because the problem that you were describing is is a social science problem. Why is the why is the why is the lady not answering the questions for

the ladies? Right. Um but then also we have the data problem. So I actually think to your poll if I may I wouldn't exclude the two because it cannot be either or. Um so I represent the the B

techch project at the office of the high commissioner for human rights and a lot of the risk that we heard are actually human rights issues about discrimination. It's about um

discrimination of women, discrimination of of of people that are that are very vulnerable. And here it's really important that we understand that we need to test all these applications with

very robust red teaming with pilots with stakeholder engagement. So we need the engineers but we also need the social scientists, we need the political scientists, we need to understand the

social context on the ground and here we hitting a key problem is that I'm not necessarily speaking about the context of India but around the globe there are a lot of offtheshelf solutions that

government agencies use and where they actually know very little about the system properties. So all the questions that you ask many of the government agents wouldn't be able to answer them

because they actually don't know this about the product. they're just sort of deploying it in the context. Um, and we heard the example of the Netherlands. Now, the Netherlands are actually one of

the government that's really preaching a lot about algorithmic impact assessment, transparency, human rights considerations. So, in that sense um I think there's a lot of potential of

getting this um deployments right. But if we if we um really invest in the pre-eployment stage and do a lot of robust testing, red teaming in a multi-disiplinary setup. And then just

last but not least there might still be some use cases where we c can actually not use AI right so also let's not forget that &gt;&gt; understand uh go this goes to uh this

point of let's say if you're speaking about multi-disiplinary setup right um uh one of the things that you bring value in with a lot of your work is how um u social data is so very critical in

these processes right um what are the top two or three challenges that you face to um bring the data to the table with the stakeholders. Right. &gt;&gt; Right. So we take a more of a life cycle

approach throughout the development of uh AI. We need a participatory approach to bring in two specific experts. One is the culture experts who are present from that location from that geography who

know that geography really well who have the soio economic understanding of that geography. Second is the domain expert. uh suppose you're deploying it for agriculture. someone who has worked with

farmer advisory very closely uh need to be present in the whole process and and define what are the variables we need to collect how we have to collect it and how do we ensure we have regular

participatory checks at all the stages of development uh right from the starting of the commissioning to uh collecting your data ensuring your data is standardized then you uh start

piloting your model and then you start scaling the model and finally going to the deployment stage This whole life cycle we take in a more participatory approach and see what kind of existing

data sets and other proxy data sets we can collect to make it more inclusive in nature. Um this comes to another question that I wanted to ask you professor Ravi it's

about let's say if uh wherever we don't have data we are actually looking at proxy kind of data right now that proxy data has um its own kind of challenges that it comes with right um uh how

should people who are working on um uh air development for uh social fat delivery consider those while while taking into account those proxy data, how should they actually take this into

um context while they're working on a solution per se? &gt;&gt; That is there is no simple universal answer to that, right? Yeah, &gt;&gt; it depends on the context and depends on

the problem that you're trying to solve and depends on you know how how much of a quote unquote causal structure that you have about the missing data so that you can use the

data that you are able to measure in order to infer the missing data and if you don't have anything at all about that right if you're literally tossing coins and then trying to fill in the

data then don't then spend the effort that you are doing to build the model to actually go get the data right I mean there are some point but there if there is another way that you know you know

the you know the mechanisms right the social mechanisms that cause the data to be produced and then you can possibly fill in some of the synthetic data but it's very very context dependent I don't

think that's a answer to that &gt;&gt; that's right that's uh that's great so um I have a last point to ask all of you which is about okay given the um India is going to be one of the largest

economies for social welfare delivery um it is it will as well, right? Given that, what are the top three points that you would ask everybody working on uh social welfare delivery with AI to take

into consideration from your lens? &gt;&gt; Okay, I start. Well, I think that the first thing is uh key design principles. I talked about methods before. We can also call it design principles anchored

in uh human rights to diligence with regard to human agency escalation pathways if something goes wrong so that people can actually turn to a human and say I've I've been discriminated against

accessibility in terms of people that may have lower digital literacy rate or no literacy at all bias monitoring and accountability locks so that there isn't we we talked about the burden of proof

earlier in the panel right so accountability locks can ensure that it can be documented for audits what went wrong. &gt;&gt; I think uh three things from my

perspective number one keeps humans in the loop uh by humans ensure the local experts the local indigenous population who get impacted by all of these decision- making need to have a seat on

the table right from the beginning itself. Uh number two, we have to keep it open. Uh not just open source but have openness to critical feedback, openness to collaborate, openness to

also fit uh the solution uh in in a more collaborative manner. And third is make multi-disiplinary work for your work and ensure everyone has uh a right understanding of this problem which

takes a lot of time. So we can't just fast track your development cycle uh if you have to make it inclusive. Okay. So everything both of them said and then uh you know one thing which I always like

to emphasize is that please make sure you're solving the right problem. Right? Don't solve the problem that you can solve. Solve the problem that can have the maximum impact for whatever you're

trying to do. That's one. And second also do think a little bit about second order effects. So because that can have a significant backlog. I mean of course other more important things they have

went so I'm just trying to add variety to this but these are two things and of course my third thing was said wonderful uh I think the panelists have actually um represented a wider variety of

aspects that you one need to take into account in the broader context but I wanted to just broadly summarize it in a in a in a way um I would like to call this as a prism view right uh for

principles, practices or uh purpose. Just look at that to start with. R for risks or rewards. Look at it. Third uh more from a perspective of impact from a downstream impact perspective. Uh S from

social factors perspective and M for market influences. Do not ever uh miss out on market influence because even in social welfare delivery there is economics in it, right? um if you're not

able to take into account uh the market considerations that may have a greater influence on some of these decisions uh it may have further impact. I'd like to thank all the panelists for their

contribution and uh for accommodating this rapid fire style work. Thank you. &gt;&gt; Uh thank you everyone for joining us and sharing your valuable insights. On the behalf of India team, we would like to

present you with some moment of appreciation. I would also like to request all the panelists to come in front and get a group photo.
