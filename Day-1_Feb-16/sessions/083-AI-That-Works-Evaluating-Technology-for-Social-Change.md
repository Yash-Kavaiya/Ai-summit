# AI That Works: Evaluating Technology for Social Change

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 14:30 ‚Äì 15:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 14 |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/KeXDbcLVB_g?feature=share) |

## üé§ Speakers

- Baarish Aggarwal, Tattle
- Edmund Korley, Agency Fund
- Poorva Malviya, Noora Health
- Vineet Singh, Digital Green
- Vinod Rajasekhran, Tech4Dev

## ü§ù Knowledge Partners

- Project Tech4Dev

## üìù Summary

As AI use cases expand across the social sector, robust evaluation systems are critical to safeguard vulnerable users, improve system performance, and ensure meaningful positive impact. This session addresses the growing need to support development leaders in designing cost-effective AI evaluation pipelines that enhance benefits while minimizing potential harm. It will draw on culturally responsive and context-specific evaluation approaches aimed at improving the safety and reliability of LLMs for diverse, multilingual communities.

## üîë Key Takeaways

1. As AI use cases expand across the social sector, robust evaluation systems are critical to safeguard vulnerable users, improve system performance, and ensure meaningful positive impact.
2. This session addresses the growing need to support development leaders in designing cost-effective AI evaluation pipelines that enhance benefits while minimizing potential harm.
3. It will draw on culturally responsive and context-specific evaluation approaches aimed at improving the safety and reliability of LLMs for diverse, multilingual communities.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/KeXDbcLVB_g/maxresdefault.jpg)](https://youtube.com/live/KeXDbcLVB_g?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

We have fraud detection happening. We have compliance increasingly getting automated. So yes, a lot of entry levelvel roles are getting automated and that's the reality of it because that's

what you know large uh you know industry customers are telling us. But what is the larger truth when it comes to India, right? The larger truth is many millions of Indians are still outside formal

credit and that is the problem that we need to say how can we look if AI is used to just cut costs in urban banks. Well, get used jobs that I mean that's a no-brainer but if AI is used to lower

the cost of serving rural India well we create borrowers we create entrepreneurs you know we create business people the question is not whether you know AI reduces headcount I mean that's given I

think the question is whether especially in the financial sector it helps to increase access right so that's the point do we optimize balance sheets or do we expand the financial system I

think that I think those are aspects you need to now let's take manufacturing and logistics all of us know um that you know a whole lot of robotics are taking place. Obviously AI is uh going to

turbocharge uh you know logistics and manufacturing but again are we looking at reduction of jobs over there? Absolutely not. If there is anything I think government of India is focusing on

increasing manufacturing as a space right so that's so we know that's that's forgiven that's what's going to happen as well. What are we talking about then when it comes to manufacturing? I think

what we're really talking about is are the population going to be involved in low productivity jobs in manufacturing or high productivity jobs and I think in the manufacturing logistics space that's

how it's going to work out. I've given two other examples and then we'll move on. One is retail. Uh you know all of us have experienced different platforms. Are we there are 12 million kirana you

know shops across the country. Are we thinking in terms of centralizing a whole lot of power in terms of only platforms or are we can see that what is that we can do at a local kirana shop

which will help them reduce wastage improve pricing um and you know manage their inventory right I think those are aspects. Healthcare is another signal aspect but I'll move on uh as to what we

want to do. So the question is very simple you know like I said AI you know really needs to be a force multiplier it depends on you know how leadership is kind of really thinking about okay um so

the central insight is very simple AI not does not necessarily eliminate work it reallocates value so I want all of you to kind of think through you know in that aspects right it destroys obviously

routine tasks etc and all that uh but it increases the value of judgment okay um so it comes to therefore let me just move to the next slide Does AI take jobs or will it end up

concentrating opportunity? Um and I make the you know case of a middle path in all of this right. So obviously without doubt as we speak if AI serves only top 10% we have a challenge but if AI

reaches the warehouse worker in you know luckno the nurse in kimbaturur or maybe the saleserson in Japur then productivity rises you know for the nation. So the real debate like I said

is will AI end up concentrating opportunity or will India be able to use it as a democratic patch there. Now let us confront a much more harder reality. Many entry- level jobs are under strain.

There's no questions on that. Uh there are forecasts of you know workforce reduction in in AI exposed sectors. This matters deeply for us because many firsttime jobs really end up shaping the

trae trajectory of you know many carriers right. Um so if a AI is going to compress the bottom of the white collar ladder, we must build new ladders, right? And and what does that

look like? So this really requires action on three funds and some of this might be relevant to you. First is you know as as as a nation skills no longer uh skills must become dynamic right um

not static and what do you mean by that and that's what is written no longer credentiing as we have noted in the past or you know degree is a lifetime warranty of employment. So I think

that's the one major shift which is taking place. U skills now have to be modular, stackable, continuously refreshed. Individuals are going to have multiple careers as compared to what

used to happen you know in the previous generation. Second thing is I think we need to significantly shift from what we say jobcentric thinking to taskcentric thinking. When AI is going to automate

you know routines I think what it's going to leave behind is a vacuum that only a human spirit can fill. And what do I mean by that? I think when muscle and cognition as we spoke about are

handled by machines what remains you know um a a lot of what we used to call in the past soft skills empathy judgment creativity I think some of those aspects are now going to come into for in a much

bigger manner than what we imagine it to be right so these are not just you know soft skills what we are going to be able to do as we move into the future is that our attempt is not going to be saying

let's create AI proof improve jobs but we will end up as a society as a nation of having AI augmented humans right people who know the skills of you know how to kind of work along with that the

third most important thing amongst many things is we must build institutional bridges I don't think so it is left only to corporate or industry to be able to do that between policy platform and

employers I think we are extremely lucky uh of course amongst many things first is policy intent must meet market data uh skills program must be aligned with realtime hiring signals. Employers of

course you know like us have to communicate evolving skill requirements quickly. We are lucky as a nation. We have many things going right for us. We have the right you know digital public

infrastructure is really the envy of the world. We have scale. We have linguistic uh you know diversity which of course is going to force innovation when it comes to inclusive. If AI systems can work

with our kind of inclusive diversity and so economic complexity it's pretty much can work you know anywhere in the world. And I'm going to give you few examples as to how that is happening. But let me

also you know address the fear directly of what it is. I think historical evidence does suggest that while specific sectors face sharp contraction overall uh you know employment adjusts

over time right uh the transition can be painful and I think it'll be painful at depending on the sector but but it's and it's rarely smooth but economies that kind of position themselves and that's

why I gave the Swiss example uh towards higher value work would thrive the real danger therefore in my view is not um automation or AI I think the real danger is stagnation. If we resist AI, then I

think productivity will fall. We're going to fall behind uh you know everybody else. If you're going to fall behind everybody else, then wages are going to come down, wages going to come

down, then inequality is going to rise, right? So this is inevitable, you know, in one sense. So a choice is not between AI or not AI. I think a choice really is between either shaping AI or being

shaped by AI. And I think that's you know, singularly going to be the most important thing we need to look at. Therefore, you know, maybe bit of solutioning, right? So what does shaping

AI mean uh in Indian context? One, it means building AI systems that augment frontline workers. So I think you know that's amongst many things which individuals first time in history have

taken upon themselves. Uh it also means and this is really important to me you know ensuring that rural youth have the same AI access as urban graduates and I'll come to it as to how you know we we

are seeing some of this play out. Uh and third of course it means u making vernacular AI interfaces because India is obviously diverse u making making it the norm and not the exception right um

most importantly outside of all this I think it means recognizing that human skills would become more valuable not less as I can move ahead so let me now provide you with practical examples of

how you know my industry which is recruitment industry which I represent is getting transformed uh as Dr. Da said I've been the CEO of a staffing company and now a jobs marketplace. So I have a

bit of a frontline view as to jobs especially entry- level jobs right. So what's happening in the labor market? So you know we have we we have a jobs marketplace. So there are about six cr

candidates who are registered at any point in time in my database. Uh last full year there are about seven lakh jobs which were posted. The number of job applications is what each individual

kind of goes um and uh you know post. So there were nine cr applications for seven lakh jobs. Right. So roughly about 125 to 150 200 uh you know job applications per individual that is not

interesting that's I mean for India scale that's that's that's normal but I think what's more important thing which is happening is that about 50 to 60% of those jobs were applied in tier 2 tier

three cities and fundamentally more important than that and more interesting is the kind of jobs get posted let me give you an example three years ago AI trainer was a non-existent job Today

it's one of the fastest growing jobs in the world, right? U some of you might know it from another language. It's called data eval business. Uh but you know in colloquial term it's it's an air

train right where is it happening? Hiring from tier 2 cities. Um it pays obviously it's it's kind of a gig model pays roughly 20 to 40,000 rupees a month. Eligibility is literally not even

a graduate. So labor market transformation is happening. It's happening it's live. Okay. uh what are employers signaling amongst many things again as a if if you

are in a jobs market this should be important I think three clear things one is prompt literacy all of you know it uh prompt engineering is really the one thing um not coding conversational

precision knowing how to instruct AI systems effectively right second is you know AI augmented you know problem solving use AI as a research assistant not an answer right just don't go

because all of us know it hallucinates gives all kinds of crazy answers Um, evaluate outputs critically, knowing when AI adds value, when it introduces risk

at all. So what what does AI do? AI today at scale is able to completely screen. I think those are some of the things which is going to take place right now. Um the other aspect you know

I want to put across saying how some of these things in real time is taking place. Um a one at today one of the important aspects in India is how can AI like I spoke about going to be a

democratic equalizer right um fundamentally all of you would see in some corner or the other country English speaking classes interview coaching classes at a higher end finishing school

classes and so on and so forth. Some of these range from 15 20,000 to literally lacks of rupees right um resumeum writing services for example you go to websites anywhere between thousand

rupees to several thousand rupees less less than 5% of our workforce can afford that so the opportunity is just not determined by skill you know it is determined by presentation as well uh

but quickly I mean we do a whole lot of things at scale um today the AI prep can literally be able to interview 100% of all candidates being able to update their profiles and

really be able to go to the next level of being able to you know you know call out all individuals right so that's that's among things that you know AI is really able to do I think let me end

with a very simple proposition the future of work in India will not be decided by algorithms alone it'll be decided by institutions by leadership and our willingness to invest in people

uh AI is definitely a powerful tool no doubt but human capital remains India's biggest asset If we combine AI with India's demographic dividend, uh entrepreneur

energy and policy firmness, we can transform anxiety into advantage. Uh the future of work is not something that happens to us. I think it's something we build. Thank you so much.

&gt;&gt; Thank you so much Shri Kartik Naranchi and for this very uh insightful session on from the industry side. So now uh AJ is with us and I kindly request AJ to uh do the inaugural address.

Thank you uh Karthik. Uh it was a wonderful uh note of I addressed by you and uh some of the examples were real good and uh it it kind of gives or uh makes a uh makes us confident that

there's a promise in AI uh while transformation is really really difficult uh whenever some disruptive techies come But the promise which we have shown that yes there's an

opportunity and with right kind of leadership right kind of interventions we can definitely aail those opportunities. So uh building on that uh so Karthik presented uh one side of the

I would say uh the complete universe that the private sector and the HR part of it. So I will be uh presenting more uh from the government perspective of our ministry's perspective. I am AJ

Sharma additional secretary and director general employment in the ministry of uh labor and employment. Uh so uh distinguished uh pending guests uh I'm really really uh

privileged to be here uh addressing all of you on this uh address impact summit. uh I don't have to tell about how AI is progressing, how is it transforming. I will uh directly go uh to the main point

which I want to want to kind of share with you. Uh yes, we are at a cross or crossroads or we are at the defining moment in economic history. Uh where AI is not simply a technological wave. It's

a kind of force which is actually as you said rightly reshaping our production systems, service delivery and labor markets and not only those but governance uh frameworks as well and all

across the world not in India itself uh for a country like India where as you rightly said Karthik 65% of our workforce or our population is below 35 and median age is is 29 and our nation

is banking on this youth for this Vixit Bharat 2047. Here the question is not whether AI will uh what will it do real question is will we will we be able

to use AI to expand opportunities rather than resting. This is the real question which we need to answer or which uh the ministry or the whole of the government is kind of uh working on. So u if you

look at uh our official data source which is your uh periodic labor force survey and the claims data it shows that unemployment rate is decreasing consistently. Similarly worker uh WPR as

well as the labor force participation rates are going upwards despite the technology and AI it means the jobs are growing it means that there's some transformation of job

the evidence shows that this is not a theory till now we were saying that we have seen the age in 2K or when the computers came everyone was scared that the jobs will

go. Similarly, everyone of uh most many of us would uh second me that in 1980s when these ATMs came, right? Everyone was thinking that the jobs of these bank tellers will go, did they go? No. In

fact, because of the uh cheaper branches or cheaper operation in the branches, your jobs or your number of branches increased, your service delivery improved and at

the end of the day, the number of jobs in the banking industry increased like a lot. But there was a transition from simple that kind of a cash cash handlers job. Now there's more like relationship

managers or advisers which we are u with with whom we are interacting almost on daily basis everyone of us getting call from our relationship manager adviser about our bank accounts or investment is

citizen and all so what I'm saying is uh the data or evidence it's not only a theory but the evidence is showing that jobs are actually increasing unemployment rate is going down despite

the AI and the technology being there in existence. Yes, this is the beginning of it. But at the same time, uh we don't have to be uh uh fearful of it or we should not be scared of it.

[cough] But we need to I'm sorry. So uh when I talk about the government. So we feel that when it comes to employability

then uh uh the uh the AI AI can can can kind of uh address or I can say can strengthen strengthen three pillars of our employment architecture. And uh in those pillars the government or the

ministry has started using AI to see that the AI improves opportunities. For example, first pillar if I talk about job matching up now the wonderful private platform matching job seekers

with the jobs the numbers Arctic had spoken about these are real amazing numbers. So similarly there's a national career service platform uh which the ministry is running since 2014 and uh

it's a it's a kind of a platform which is uh connected to APNA monster not only those but with the government system as well constant system it's a very comprehensive kind of platform and here

we have already started using AI system was talking about the rumies so if you go to different sites the uh the cost of making those resume regime is quite high. Not even 5% of am I right? Not

even 5% of the total job seekers are able to kind of uh even think about those services. Right now you can go to the NCS platform you can put your basic details and AI would give you the resume

but I again the caution is it is a kind of a recommendatory you have to kind of again uh improvise it as per your needs and everything because that can there can be some kind of hallucination. He

can when you are talking about it when you are applying for some managerial positions you may not be having that managerial uh experience but hallucination will uh make you feel as

if you have done this managial uh kind of thing in any number of enterprises. So one has to be cautious about it but at the same time what I'm telling is these kind of truths have already

started penetrating our uh geometrical systems. If you talk about CV building and second and very important thing was uh we are very diverse. So 22 official languages so many direct. So it has to

be multilingual and obviously a carpenter cannot go to any site and build his own CV and post it there. with a simple feature phone he should be able to post his desire that I want job of

carpenter at such and such places and this is why uh work experience work experency doesn't have to be so formal he can simply say that I have worked at this and this position and have made

furniture or dining table or beds or that kind of thing and it should be able we should be able to pose and right now uh while I I'm uh kind of speaking to you my team is working on that system

and the beta version they are kind of a testing in uh maybe couple of weeks it should be out and uh I should tell you that uh this NCS initial career service platform because uh

it is kind of combining many platforms eam platform upna mon and this kind of thing so we have uh approximately 3.64 64 crude job seekers uh were registered on it until now in the last

10 years more than 6.43 43 vacancies have come on this platform. So uh it's a very uh uh I would say uh very strong uh platform which where using AI we can we are trying that how can we provide more

opportunities to our youth. Second thing when it comes to employability is your uh definitely your skilling or reskilling kind of stuff which uh has to be taken care of. Now uh second thing

with NCS platform uh which is uh I would say unique thing which we are doing is EPFO all of you know it's a it's an organization where all the it has all data about all your

formal workforce waste data and kind of job so we are integrating this NCS platform this EPFO system now imagine uh with more than 28 lakh organization and 34 crude uh EPFO members if NC is

connected how well we can understand that what is going on in the job market and almost on realtime basis as the establishments have to fill all their data monthly. So

how are the jobs transitioning? What kind of jobs are happening? Where are happening? Which kind of establishment is happening? So again uh we are kind of riding on this AI wave and would kind of

analyze it. Again time frame is again all time frames are four to six weeks. When we are talking about AI it cannot be four to six months. It has to be kind of compressed to uh weeks. So I just

wanted to tell you that the second pillar which we are talking or uh which we are working on is this skill kind of thing. Yes. Definitely there are different organizations, different

establishment uh different entrepreneurs who uh have designed different courses depending on the needs of the market but these entrepreneurs also need to know what is actually uh there uh uh in

demand on real time basis. Yes, he say we listen today we uh listen to Karthik he said that there's one AI trainer is one job which is kind of coming coming in the market. So maybe there may be

entrepreneurs designing courses around it. But think of a situation where on real time basis we are getting this information that what kind of skilling is in demand

and definitely it opens uh kind of a it opens for uh different entrepreneur to kind of work on it and come out with these courses. And third thing uh which I feel that AI can center

uh in this overall employment architecture is inclusion which you also spoke out it uh this is very very important. Uh we are very diverse. So linguistically again

uh English speakaking people with English uh kind of a language platform they would be kind of gaining it but others who are not able to are not comforted with English they will be at

disadvantage disadvantage and this gap will rise so but there's an opportunity with bhashini and other kind of system who can kind of on real time basis can uh do multil

uh it becomes like easier however we need to be mindful of it at the same time uh we also need to see that our AI systems are not biased so it is for entrepreneurs as well so uh It should be

like it should not bias against uh people coming from rural background maybe uh the women who have kind of because of maternity went out of job and coming back should not bias against

that. So we need to kind of have need to build that kind of system. So u again it is a double sort. So AI can exclude that and may kind of if we are conscious can actually consciously include uh those

kind of populations as well. Uh similarly uh again the platform is platform you might have heard more than 31 cr organized workers are registered there. Now uh this is uh because many of

you are entrepreneurs here. I would like to share with you that very soon uh we are going to uh open it this data set to to to the private player who can kind of use this data set and kind of uh

come out with their own application which help this 31 cr people to match their requirement or their desire with the uh opportunities available in the market. uh

then uh I would say uh the if I talk about the government it's not a single ministry working in a silo but

it's a whole of the government approach so on daily basis you may not be able to uh get that kind of insight but being in the government I know that uh uh

honorable prime minister is making sure that every minister every ministry is working together and uh uh it has to be a whole of the government approach to uh ensure that AI creates opportunities and

we actually uh we are not kind of simply uh following it but we are leaders in this transformation uh for that matter uh mighty has come up with uh many many courses future prime more than 3.2 lakh

people have already been trained and the AI AI and big data. Similarly, we are integrating these AI courses in our PM TV which is being done by the skill development department. UI AI again uh

the target which we have kept uh the ministry of IT has kept is around 1 cr uh youth to be AI literate. So these are some things similarly NCS is also kind of tying up with different partners. For

example, very recently uh uh Mr. Nadila was here and we kind of enter into an MOU with Microsoft where more than 15,000 partners would put their vacancies on NCS. Not only that but they

have also agreed to put AI courses on NCS portal. So uh and again this national career service or NCS portal is connected with different government portals like SID or MIT's portals. So

anybody entering into any government system would be able to access these courses even if he is not directly connect to the NCS system. So uh apart from that this was all about technology

but at the same time I said that government is uh kind of having this follow the government approach. So we are uh this employment incentive scheme

is again uh it kind of helps the demand side it incentivizes job creation. So any industry uh which creates additional employment for next two years uh incentives will be provided

to them and it is uh for manufacturing uh it is almost 1 lakh 44 up to 1 lakh 44,000 of in incentive in 4 years for other industries it can be 72,000 rupees per uh per additional employment. So uh

so this was uh about our government force. Uh now when we talk about any initiative or uh anything which we are doing then as a whole maybe private sector public sector se sector we need

to have some kind of a metrics on which we should evaluate ourself. I feel that some of the things I have written now uh how would I judge this AI adoption uh by major outcome one is reduce time

to match what job seekers and employers so that friction goes away and then this matching has to be uh very soon. So we need to see that what was the time earlier even for upna I would request

that you can have your internal metrics where you you can say that earlier it was taking 7 days now it has gone down to 24 hours second uh again every platform whether it NCS or

upna we are providing career counseling and all but at the end of that day this career counseling or assessment is how it is converting to the opportunity that is very very important into uh it. So

that will be another uh third thing I would uh say is visibility of I I was saying that uh yes we would use EPO data and everything to understand that what is the skill gap but real time in real

time whether everyone is able to do it and how fast we are able to uh do that that would be uh another uh criteria and uh uh and fourth uh which I would say uh inclusion for me it is uh closest to my

heart uh and we should see that across regions across genders across sectors how are we able to kind of bridge that gap. So those are few things which uh I would say that both private and private

sectors should keep in mind while kind of devising different solutions for improving employability. Uh last but not the least I would end uh uh this thing u my address with this thing

technology must serve human potential and human potential must define our technological future that was uh the my concluding thought and thank you so much for this opportunity.

&gt;&gt; Thank you so much sir as you rightly said that uh uh uh core question is not uh will AI eliminate job and it is transferring job. Thanks a lot sir. So now I request uh uh Dr. Maturi Gi for uh

delivering the uh like talking about sorry Kusum Kusum Garval Gi for uh delivering the session and that that's so especially she's looking at how educational system is connecting with

AI. Thank you. So uh after listening to uh sir a sar and uh karthik's thought so my job is uh little bit ahead of uh because they are talking about the graduates and the

people those who are available for the jobs my job is to how to prepare the students for these jobs. So uh good afternoon August uh panelist and the gatherings here. Uh it is a great

opportunity to speak from the academia point of view. So my I'm because I'm a teacher fundamentally I'm a teacher. I would like to be in a simple terms. uh so basically the academia perspective

from the AI uh key sites how it is going to be handled and how students need to be prepared for this. So there are four key questions and once these questions are being answered I think most of the

job will be done. So first of uh first of all what need to be taught? So what kind of programs those are relevant in today's time? what kind of courses that are relevant need to be taught under

those programs need to be offered because until and unless we have the right offerings that to be made to the aspiring students and ultimately the people those who are coming for the jobs

uh I think this problem uh will not be resolved. So right kind of programs or the availability of right kind of programs into the uh market, right kind of courses. So I I just look at it when

we uh design our curriculum. So we just see that we need to produce T-shaped T-shaped professionals. So what what we mean by the T-shaped professionals that means giving the breadth of the uh area

or giving the decent breadth of the area where the student need to perform in the coming time and after that at least one depth or giving depth in one specific skill. So this is when we are arranging

our curriculum we are keeping these in these things in our mind. So uh the curriculum when we are building it in the first two years we are uh totally focusing upon the breadth and now with

the breadth we are seeing that the curriculum and the kind of skills those are required in the market they are also changing at very rapid pace. The shell life of the technology has reduced like

anything. So the tech things which we have been taught at our times might not be relevant in today's time but again the uh it it it's an opportunity again it's an opportunity where curriculum

need to be re revised uh not in once in four years or so I I think it is it needs to be revised in every year or like in every six months the multidisciplinary and interdisciplinary

in this area where major of the skills or major of the jobs would be available in the coming time. So even the person having knowledge of medicine might not be very successful. He need to have the

required technology how to uh implement the latest advances in the medicines by the help of technology would be required and the people those who have that kind of skill will be more employed in the

market. So uh And along with this the key uh humanly skills I I would name it as because we are talking about artificial intelligence. So what are the humanly skills? So humanly skills are

problem solving ability, critical thinking, empathy, communications. So a lot of focus needs to be built upon these when we are arranging our curriculums. So uh on the side of

curriculum these are the things which need to be taken care by the academicians at their institutes just to become relevant in the market. Second question key question is who will be

teaching these where we will be teaching these? So where is basically the infrastructure? So being a leader of a university if we are just looking upon that the labs which were built at 5

years ago or 7 years ago might be might not be relevant in today's time. So investing in the right kind of infrastructure that can give the education to the students imparting the

contemporary and the modern skills so that they becomes employable in the market. So that is something which needs to be decided by the leaders in the university that where the investments

need to be made in which which type of infrastructure need to be built at the university so that it can serves to the right kind of skills or while imparting the right kind of skills to the

students. Now coming back who will teach so who will teach is of course the faculty members. So faculty members need to be need updated by the regular interventions of the development

programs sending them uh for the sabbaticals just to learn something and because until and unless the faculty is educ uh upgraded I think the days are gone when the

faculty got the PhD degree and five years ago might not be relevant in today's time. So if fac if faculty need to attend the students in the class they need to be ahead of them. So until and

unless uh they are ahead they won't be heard into the class. So this is the third key questions. Now comes to the innovative pedagoguy. So how the students need to be engaged? So every

teacher when we see they are facing challenges in inside the class how to engage these genzies. So we are using the term jenzi. They do not listens to the others. They have very reduced

attention span. So how to cater to them? So here I would like to say AI AI dab. So here you can use this AI for providing the customized teaching learning to to your students. I have

been using a very amazing tool which can give my material to my students in seven different formats like few students they are good by listening they are few students they are good by uh seeing the

videos few few students they are good with the discussions they they need to be engaged in some podcast so there are AI tools available in the market which can just curate your content content

based upon your learner's needs. So these learners needs diversified needs of the learners can be catered by the AI tools taking their leverage into the academia and the higher education

specifically. Now uh when we are uh talking about the skill gaps so skill gaps is again a kind of terminology. So the skill which I have seen six months before might not be

relevant in today's time. So again this gap is changing. So just to keep a close eye what is happening how it's happening and how this can be delivered to my students. So that should be a kind of so

earlier the teaching job was the relaxed kind of jobs people uh now I think this is quite engaging and on everyday basis if you want to become a good teacher good leader in in the education market

you need I should not say this is a market education is a profession it is a noble profession which gives the feed to all the employments so uh basically uh here is a kind of challenge challenge.

So uh the educator need to update themselves on the regular basis that what are the key things what are the key educational tools available and how their learners can be benefited out of

it. So uh that was my first fourth key questions. So before uh leaving this the last was the impact assessment. So when you are doing everything what is the impact of it? So whether you have made

change in the curriculum or you have made change in the pedagoguy or you have made change in some kind of or you have invested into some kind of infrastructure or you have invested into

some kind of faculty development the impact assessment is inevitable. So impact needs to be assessed for all these things and then only the future calls should be taken. So before leaving

this my key takeaways for this session is alignment of the entire higher education systems towards the humanly skills. So humanly skills cannot be replaced in any of the period of time.

So uh we can we need to focus upon that how humanly skills can be incorporated not only the into the curriculum but also into the assessment because student they attends assessments and they

prepares themselves from the assessment point of view. So if we are assessing the students on their critical thinking, on their empathy, on their uh problem solving abilities, definitely students

will be addressing these skills while they will be learning. Adaptation of market changes and quick incorporation of industry demanded skills into the curriculum without waiting that next

cycle will come and then only I will be uh changing my curriculum. quick upgradation of academic infrastructure that includes both physical infrastructure and the faculty and

creation of a valid assessment framework specifically for the students that can be interested by the industry also. So here is a big gap when we are talking that the student those who are already

being assessed inside the higher educational institutes they are assessed again before taking into the jobs. So having the correct kind of assessment framework investing in those frameworks

would be require required. So last but not not the least I would leave this stage by saying only one statement do not fear of AI just embrace AI. Thank you very much.

&gt;&gt; Thank you so much. And there are lot lot more things as discussed and impact assessment is something which we have to think of in all the universities. So thanks a lot and we don't have much

time. So let me just conclude the session and a acknowledging the powerful uh reflections we powerful p perspectives shared here especially starting with a joerarmmasur and he has

already talked about many aspects of the government of India especially ministry of labor and employment and his reflections on this integration of this digital labor market information system.

It is showing the shifting from reactive employment governance to a predictive and datadriven uh employment policy. Thank you so much sir for giving these uh aspects here and also uh Shri Kartik

Narayan and he has brought the industrial lens into our uh discussion and it was highlighting uh that the private sector is not merely adapting to AI and it is uh building it and also Dr.

Kusumchi for uh discussing about as she said correctly that uh curriculum design then lifelong uh uh lifelong learning pathways and also uh this industry integrated pedigogy are uh no longer

optional actually they are structural necessities. So once again uh my sincere gratitude to each one of you for delivering this session and taking your time here from the busy schedule and

also the organizers of this AI summit. Thank you. Thanks a lot and we will end here. Thank you. Okay. Hi everyone. Thank you for uh bearing with all the confusion outside

and uh waiting for this event. Uh so we are at the AI impact summit and one of the questions uh that we want to discuss in the panel today is that how do you actually assess the impact of AI very

quantitatively qualitatively what does the nuts and bolts of that look like. Um so today on this panel we have five stellar uh speakers. We have two folks who are going to discuss more on the

theory of like what does evaluation in AI mean and then we have three um representatives uh leaders from nonprofit organizations who are going to talk about their AI applications and how

they've thought of evaluation there. I'm going to restrict the time uh I uh do on introductions just so that they have more time. The way this will work is that each speaker will come to the

podium speak for about 8 to 10 minutes including question and answers. So you can ask your questions um for each speaker right after their comments right and each speaker gets about 10 minutes

and that's how we complete the 50 minutes. I do request all of you to be very quiet while you are here just because the uh audio if you speak it's going to disrupt the speakers. Uh with

that u I'm going to uh invite our first speaker who's Edund Ki from agency fund. Edmond over to you. &gt;&gt; Morning everybody. [clears throat] So today I want to talk about an

evaluation framework that you know we've come up with the at the agency fund called the fourlevel evaluation framework for now. Um so over the past year we ran a an accelerator where we

invested $4 million into eight nonprofit NOS's to build and scale AI pilots. Um and out of this we come out of with it with a framework for approaching how to approach AI evaluation. So I want to

talk through that today [clears throat] but first a bit of a high level. Okay. &gt;&gt; Yeah. So maybe a bit of a how we got here, you know. Uh so some of you might

recognize the technologies in this rough timeline. Um but the idea is that you know with generative AI the tools are changing and improving so rapidly. Um and with increased capability comes

increased risk. You know in the past uh we we used to talk about like hey now uh people can figure out how to make a biohazard tool or a bomb like with chatbt the the the risk is even higher

now and will be be higher in the future. So this is part of the reason why investing in valuations to make sure your AI intervention or product is doing what it's supposed to do is more

important than ever. Um so maybe some of you were uh saw this article last year uh that MIT um published where you know at the time June 2025 uh 95% of AI pilots in the enterprise

were failing like only 5% were making its way to success successful deployment in production and you when you dig in the paper it wasn't the capabilities of the models itself but it was the

integration of these uh into uh enterprise workflows. Um there was just a major friction there. Um and you know in our accelerator we uh identified certain pain points as well. Um you know

it ranged from things like intent classification, data collection but the major pain point was around evaluation like what identifying what to measure and how to measure it.

&gt;&gt; [clears throat] &gt;&gt; And so out of that we came up with this framework. Uh it's a four-level framework and you can think of it this way. You know at the first level we're

basically asking does the AI produce safe and accurate responses. Um so all the metrics and evaluation techniques under under this step you know mainly focus on the the AI model but you know

uh we can call it the AI product right because it's not just the the LLM in the middle. everything you put around the L and the architecture around it. Then uh level two is kind of like how um does

the product facilitate meaningful user interactions? Do people find enough value in the product to keep using to keep coming back to it over and over again? And then level three, we start to

get at these um how does the product change users thoughts, feelings or behaviors about themselves. And then you know finally uh as the final stamp on um whether your is working. Does does

people using your product improve human welfare? I'm not going to get into this slide too much, but you know maybe from the previous slide you might interpret that

we're saying like hey reach perfection in level one before you move to level two and so forth. Uh but you know really is more complicated. There are different types of people uh expertise that are

involved at each level. For example, level one might involve more AI engineers, ML engineers, whereas level two involve product managers and so on. But the whole idea is that as you

progress through the levels, you're building successive bits of confidence all the way up until the impact evaluation. You know, the big bad expensive RCT.

Um, and yeah, you know, here's an example of um the organizing questions and and some early stats for for digital green CTO's in the room here. Um, and Why this framework? You know, you can

imagine you don't go with this framework and we've seen orgs or orgs uh take this path um and you go all the way up until an impact evaluation and and get a no result or you you run a survey and NPS

score and and the result is low. The whole idea is that instead of, you know, getting to those more expensive types of evaluation and and and getting a result that you don't want, you invest in the

earlier levels successively such that, you know, you you before you even get to level three or level four, you've built enough confidence like, yeah, this is going to work. Um, and yeah, just to

repeat what I said earlier, we're not saying this. We're not saying like um pro progress linearly. Uh the whole idea is that this is cyclical. Um and you know to do each level at high fidelity

cost is is very expensive and time consuming. So we definitely promote the concept of a minimal valuable evaluation that in each of the levels you know there's uh bare minimum that you can do

that gets things working before you move on to the next level. But you know you you continue to invest in all the levels. Um and yeah here here's you know MV checklist we have for each level. We

can get it into more more another time. Um and yeah, you know, I I want to close on on this uh that you know, what we're proposing is not a linear process, but a

uh a phased evaluation framework where you get successive bits of confidence before you move to before you move on to the impact evaluation. &gt;&gt; Any questions?

Yeah. &gt;&gt; Yeah. Because uh you know, we've seen like it can be easy to skip, right? because once you get something working, you're just like, "Hey, um, you know,

let's just put it out in production basically." Uh, you know, so one, I would say the uh the context you're working in um you know, that it matters for different contexts. For example, if

you're working in a health context where you know people are relying on on your chatbot for for factual information about what prescriptions to take, you know what what what uh you know maternal

health practices to do, then uh you know the onus comes from you like the owners come from the org to to want to make sure that AI bud is is outputting these answers. Um but it's also like a a form

of risk management regardless of what industry you're in. It's a form of risk management that you know you don't want to get to a point where your your chatbot is giving wrong answers or

harmful answers in production. That's going to blow it's going to blow up in your in your face a lot. So investing in this kind of like uh phased outline evaluation um can help derisk an outcome

like that. &gt;&gt; Questions? I'm answering Yeah, definitely a good point. Um, [clears throat] so one thing to think

about is, uh, scrubbing PI. So if you're sitting on some, you know, customer data, like beneficiary data, uh, just passing it through a process where you scrub it of all people's names and

locations. So there's some models that help with that, you know, scrubbing models. So that's that's one thing you can do to uh derisk like you know um uh messing with people's data. Another

thing is like if you're just getting started um using synthetically generated uh data you know you know imagine like you know you're working on like a health chat or something you know your

conversations follow certain themes like people are asking about certain types of questions so you could model that and then use that to generate synthetic QA pairs so that you're not like um risking

exposing user data in the early version of evaluation. PI scrubbing is definitely the way to go if you can get it to work for your language because you know these models tend to work the best

in English and then less so in others. Yeah. Yeah. No, it's a great point and and you're you're right. I won't uh couch the fact that the impact evaluation involves these RCTs which

take a lot of time and money. Um but the whole idea is that you know I'm not saying like you know uh do level one to three uh before you start thinking about impact evaluation. All these can happen

at the at the same time because you know involve an overlapping like you know group of people. So uh yeah we're definitely not saying the case that like you know reach perfection and get to

impact evaluation but the impact evaluation is a the RCT is a snapshot in time of like you know the system that you're testing. Um and it and it's true like uh we that the models will get

better and they will change and things like that. Um but you know something I I've heard about which was really super helpful is like uh having an optimistic mindset you know designing both for the

current capabilities of these models but also the future capabilities you know having a growth mindset that they will get better like how can we organize our program such that we can take advantage

of that &gt;&gt; I'll now invite bar from tel to talk about evaluation Good afternoon folks. I'm Vish, research, development and grant

specialist at Tat Civic Technologies and I'll be talking to you a little bit about how to do human evaluations of large language models. Uh so in the next 10 minutes I'll go over what human

evaluations are in comparison to automated evaluations. uh one way to do them that Tattle has been employing. Um we'll give you a few good markers for setting up a successful evaluation uh

for your organization and then we'll leave you with a little checklist for what you can do to scale up human evaluations over time. So as the name suggests, human

evaluation involves human researchers and domain experts evaluating LLM responses to determine accuracy and develop some evaluation criteria. Um in comparison, Automated evaluation is when

you use advanced LLM models like cloud or GPT um to evaluate your LLM based on predefined criteria. And so you know granted automated evaluation then gives you uh very high speeds very uh quick

scalability at low cost. So why must we make a case for human evaluations? Um human researchers can give you a lot of nuance while analyzing data um especially coming from multicultural

context. uh human research can also adapt to multilingual contexts. Um and they can uh pivot their research and evaluation when it comes to edge cases that are emerging from your data. Right?

So what is one way that you can actually set up a human evaluation for your organization? So we're going to give you a four-step method that Tattlele has used. Right? So we worked with three

organizations uh that had chat bots that they were piloting in their um communities. Two of these were in the educational domain and one was in healthcare. And we started with

collecting a sample and we collected the sample manually which is that uh we went through the data of input output pairs um and picked up periodic uh pieces of data. Right? So for one sample we had

about 2.5% of data. For another we had about 1.4% of data. Um something we recommend here is that you can make your sample representative by automating this process. Right? You can use a Python

script to generate a truly random um data set or you can also employ more than one researcher to look at this data set and basically pick out parts from different uh you know parts of the data

set right to make sure that you're getting a kind of wider richer sample. Now after that we sat down to annotate and the approach we took is uh called the grounded approach from social theory

which is that instead of using a predefined taxonomy which is like a set of labels that has already been defined we went into the data to see what was coming out and developed labels for the

data specifically right and we did this for each use case. So if you look at the image on the right, uh you see all of these individual uh labels that we started picking up, right? So you've got

accurate, you've got out of scope, um you've got guiding um and so from a total set of 53 data points, we found that 80% of the LLM responses were accurate, but the remaining 20% were

falling into a broad set of errors. Right? So the first majority error that we were seeing was out of scope which is that uh users were asking questions that were not within the knowledge base or

the raguh of the of the tool and the LLM was responding to these answers right and then the second kind of majority error that we were seeing was language error which is that we were working in a

context where users were typing both in English, Hindi and romanized Hindi but the bot was not detecting this and it was responding in English right so as we saw these things emerged, we started

categorizing these labels into broader uh you know themes of okay, we're seeing emergence of you know PII exposure, we're seeing like vague and unhelpful responses. So you know what's kind of

happening with this specific tool. So that takes us to this third step which is that as we're developing this annotation and we're you know looking through our sample uh the sample

actually gets bigger which is that you we so I'm going to talk through this example of the chatbot on the right in a second. Um while going through the data you will notice things that appear to be

edge cases and you can use that to then run a targeted keyword search which will make your sample richer and also show you whether this is an edge case or something that's actually showing up

quite a lot. Right? So um in the educational space there was a chatbot where a lot of users were asking this question of what do you know about me and the bot had been given some uh you

know information about the users beforehand but we suddenly noticed that at one point it started mentioning cast category um and immediately you know we were like okay this seems like

personally identifiable information. So then we went in and did a keyword search for cast and different terms for different cast categories and we found that okay this is actually showing up a

lot and so we saw that PII was being exposed in a way different to just phone numbers and addresses right and so we had caught this nuance that we hadn't picked up on by just looking for names

uh phone numbers and addresses so moving forward you then get to step four which is your analysis right um so you can do so many things while analyzing so what are some top you know priorities to pay

attention to. The first one is to look at what is your most frequently repeated query from the users. Um and then how the bot is actually responding to this. Is it consistent? Is it giving the same

you know level of depth in all of its responses? Second is to look at tone of queries and tone of LLM's response. Right? Um third with possible exposure of PII. This is just something you

always want to do a check on. Is personally identifiable information coming through? And finally, um you have to be prepared for edge case queries, right? Things you didn't expect. Maybe

somebody is suddenly asking um you know, something that's in a gray zone between being within scope or not being within scope. And an example of this is on the right. So in the nutri in the

reproductive healthcare context of one of the bots um we saw that almost 50 to 60% of user queries involved questions around nutrition and the bot was providing detailed responses. It

appeared to be in scope. We didn't really clock this. We just labeled it as okay one type of query. When we went back to the client we found out that this was actually not part of their

knowledge base and they didn't want the bot to be responding to questions around nutrition. And so it's really important to do this you as Edmond mentioned like cyclical going back to users going back

to uh stakeholders and figuring out what's within scope what's what's outside what's risky what's not so this is our four-step method um you know some things to keep in mind when you're

setting up your own evaluation is always begin with some clear objectives you know sit down with your team figure out what you're measuring and why um then you want to have some tooling in place

so that your evaluation is you know easy and successful right something that we noticed was that um in one of the use cases when we received the kind of data set of input output queries there was no

conversational ID to follow the conversational flow and so that actually made it really hard to figure out whether the output was relevant to the input and we had to do um quite a bit of

cleaning to you know data cleaning to get to that information. Finally, um you want to record all of your annotations. Whatever definitions you're using for your labels, whatever definitions you're

using for the errors and risks that are emerging, this will become your customized annotation guide that you can then use for subsequent evaluations, right? Um so finally, we we're leaving

you with like a quick checklist. Obviously, a lot more can happen in this, but day one, you want to start with a random sample of inputs and outputs. Make sure this covers like

time, depth, and direct of topics. You want to check for consistency across different types of queries. Um, in the Indian context, you really want to look at language consistency and you always

want to screen for PII exposure. Over time, as you develop more sophisticated annotation guides, you'll have more customized um, you know, and targeted evaluations, you can scale up to involve

the community. Do some focus groups, show them the, you know, show them samples from your data and say, "Hey, like does this work for you? What do you want? How do you want the LM to respond?

Um, and then you want to develop this into a longitudinal tracking system, keeping in mind what Edmond had shared, right, which is that at various points, you want to go back in, maybe take a 0

2.5% of your sample and do um, you know, do another targeted evaluation. And so we do think that, you know, you should combine human and automated evaluations, especially as an organization grows and,

you know, develops the capacity to have somebody doing automated evaluations, but always come back, step in and check your data yourself. Thank you. Uh open to questions.

Yes. &gt;&gt; So that's what I'm saying that you can develop your own labels, right? So you can choose an option where you take labels from someone else. That's fine.

There are like sets of evaluation labels out there that you can use, but we recommend that you develop something that's customized for your tool because you know what your audience needs best.

So I feel like that's a combination of automated evaluation and and what I'm talking about is a purely sort of human review. Right. So in our case, you're just getting a data set from the LLM and

then you're going through it versus letting an letting another LLM do the labeling if sense. Right? So this is a human human researcher looking at each entry and labeling it.

&gt;&gt; Yeah. &gt;&gt; Okay. So now I'm going to invite Purva um to come and speak. Did I get the order? &gt;&gt; Yeah. So Pura, please over to you.

Awesome. Good afternoon everyone. I'm Pulwa. I head engineering at Mura Health. And let me start this talk by saying that something surprising that came to mind

that healthcare doesn't happen in hospitals. Healthcare happens at home. And here I'm going to actually talk about how we are evaluating our AI agent flow uh to serve our patients and family

caregivers. I'll start by stating a uh a stat here. Today the doctors do not have more than five minutes to actually sit down with patient and discuss their condition,

medication post surgery or post discharge. In absence of this there is a lot of confusion and basically caregivers do not know how to take care of their loved

ones. At this is the problem Nura Health is trying to solve. Our mission is simple. We are trying to engage with government and public healthare systems to make

caregiving training as a standard practice. How do we do it and how how well we have do done it? We have impacted about 50 million people in four countries.

uh running in 11 different languages building these materials. We train the nurses at the hospital. We meet our patients where they are and then basically make sure that at the critical

moment where care moves from hospitals to home, we are there to support our patients. There is another stat that uh 70% of children death under five years of age

is preventable. These are not complex interventions that were needed. These were simple caregiving practices like looking out for one client uh feeding practices going to a healthcare facility

at the right time that could have saved these deaths and that is the our mission that that we basically help out our patients and caregivers at the right time. Uh so let me actually tell you how

we have scaled it digitally on digital side. U this is this is these are the images of our program that are running in the hospital and Uh let me talk about how we are scaling it digitally.

Our patients can actually reach out to us through WhatsApp platform. Uh they can ask us questions. We also have IPs uh based calling system. They can call us at any time or message us and and

receive our response from when we started this uh platform about two years ago. We started [clears throat] to receive a lot of uh questions and basically when we started scaling

We we we started receiving about 10,000 questions a day. We have served about 2 million uh users so far uh in last three years and we are operating in 11 languages. But you can imagine like our

team of 13 nurses and doctors were overwhelmed. These were questions you know uh that were medically uh medically uh basically supervision needed and our nurses to our

nurses we thought about using AI but we didn't want to throw AI at it we wanted to actually build evaluation from the day so what did we do um I'm going to

introduce this was the problem and especially in caregiving when we identify a warning sign but the responses do not come till late and initially our response time the

turnaround time was 10 hours it means that the caregiving window where a simple intervention could have saved a life was gone So that's where how we thought about our

digital uh offering you know through AI when the user uh query comes in we have an intent recognition we filter out the messages that are like hello health condition update thank yous

and all that and that that use of AI to just eliminate those messages saved us you know 20 80% of our band nurse bank the second AI module that we built was emergency checker Here

we identify the bonding signs that are you know that need actually help like seeing uh yellow tinge on skin or a newborn or uh after a health cardiac surgery uh heavy breathing you know in

an old patient. So we identified those markers and then uh also make sure that those those particular cases get prioritized on top of nurses. So this this tool does that for medical response

generator. We have a user profile. So through the use of AI we are collecting collating all the information that is being that we know of this particular patient and the family and we try to

respond uh through the use of AI looking at the knowledge base that we have created. So there is no medical advice you know that is out of uh the curated list of questions. There is no

hallucination. If if our module doesn't know the answer it just can take over answer this question and behind the scenes we actually look at the traces and try to build that knowledge base.

The fourth and the very critical point you know where where our nurses uh actually review these different AI modules their outputs and then sends the qu answer back to the user. So you can

imagine how critical these AI modules are but nurses here decide AI drafts but nurses remain in the full control of this uh process. Admin actually walked us through the

evaluation framework that they have built and here is how we have operationalized it. For model evaluation, the numbers that you see on the screen are our precision metrics.

For intent recognition, it is 98%. We are using cross validation uh on the label data set. For emergency uh detection, it is 98. These for emergency detection, we are doing a manual So our

doctors and nurses review uh these traces and then get us the you know uh validate uh the responses uh to us. Similarly for summary and uh retrieval accuracy it is at 90 to 95%. For

response generation we have used a rubric where we are looking at medical accuracy, context awareness and communication quality. You know how how well uh this response is being served

whether it is going to lead to any behavior change at all or not. But you can imagine even a one to two% you know error rate can be disastrous. It can mean that one missed emergency can lead

to a preventable death. So behind the scenes we are tracing all these responses and making sure that our doctors are meeting on a weekly basis and then we are actually putting a

feedback loop there so that the next time you know this this particular condition gets so it's a work in progress for product evaluation we are looking at markers like what is the

turnaround time we have seen an improvement of uh 36% in Bangladesh and 21% in Indonesia. I I split the uh uh stats there. It is 36% in Indonesia and 21% in Bangladesh. Uh which is a huge s

um we have reduced 80% of the messages like I mentioned just through the use of information. We are identifying 10% of our messages as emergency uh messages which means that we are taking care of

these emergency cases right when you know we we actually receive those messages which is a big win in itself and uh nurses are accepting 95% of the AI drafted responses. It is of course

improving. We have deployed this uh in in four countries right now. Um and and 20 nurses are actively using this product. So we are actually collecting their their experience um their

feedbacks and then behind the scenes again you know making changes to the models and the product overall. All right. Uh our theory of change for user and uh impact evaluation is if an

AI enabled platform accurately identifies high-risisk cases and triggers them for MSE responses and MSE's deliver timely personalized information to families then families

implement essential care practices and seek care leading to reduced complications new needle and infant mobidity and mortality. We are using this theory change for user and uh

impact analysis. So for user evaluation we are looking at how many uh queries a caregiver is asking especially when we have answered a question for them like are they engaging us with us more you

know so that's one one factor these both the user and um impact evaluation phases are the hardest they ask the hard questions and they take the most time model and product are you know they are

like continuous cycles but these are the phases that actually validate whether it is going to be So where user evaluation it is uh uh it is it is the uh engagement rate how how

how nurses are feeling about the responses what is their feedback like for impact analysis we are running an RCT in Q3 of this year to wait for the new mortality we will be running it for

a year or so and then we'll monitor the behavior changes how is it leading to actually better caregiving practices. So that's that's how we are thinking about it. But very quickly I wanted to

just share the key learnings here that um for AI and especially evaluation it needs to be built in from day one there. It needs to be have have all the strong find foundations because it is

non-negotiable especially in a setting like track everything learn fast we are we must track all the responses take feedback learn from it and then put it back into the system. cultural and local

context do matter a lot. Uh so basically in urban Dhaka uh basically a caregiving practice would look very different from uh rural Nepal. So just keeping all those things in mind how are we

designing our models it's very important scale and evaluation with your product and uh real innovation responsible AI at scale is important and because you know in the healthare system these are

critical learnings that's all I have for you and if you're interested in the demo uh and any any questions please scan the QR code and we'll reach &gt;&gt; Happy to meet you all later.

&gt;&gt; Sorry. Thank you. &gt;&gt; Okay. Great. So that was a fantastic first demo uh from healthcare. Now I'm going to call Venode to um speak about tech for work and the uh platform that

they are building the copy platform that they are building. Okay. Thank you. Uh good afternoon folks. I'm Venote with uh project for death. Um what I'm going to talk about is kind of

building up on what Edmond was talking about. We believe in the similar frameworks of model product user towards outcomes for us in terms of enabling and I uh not sure if I got the name right

but guys like how do we get people to do this and that was like a big question and for us that's super important at tech for dev is to say how easy can I make evaluations today it's easy you go

on a chatbot platform you go to any platforms you create a knowledge base and you create a prompt super Now you ask your questions, you fire away at it, right? Not everybody can do

a lot very very easy. But what we don't do or what is difficult is the evaluation part and the notion is how do we make that simple was our big question rather than trying to think about like

going into the depth and detail of valuation because what we've also seen with a lot of nonprofits and and nonprofits are very varied. We've got like NA digital who are probably at the

top end of the chain who have significant engineering teams and can work doing evaluations at much much greater scale than what a smaller organization can do. But as we enable

all organizations, we want them to think of valuations first. If you are running a chatbot with AI, you should be thinking of valuation because that's super important. You can't just say I'm

running my AI chatbot. It spews out answers. we don't know what it's going to actually say and I don't even know how accurate it is. So those are the questions we wanted to answer. We want

to make it easier and that's fundamentally from our belief of saying enable the entire sector as such and so from that perspective we've been building an infrastructure platform and

you'll see the demo uh for that uh later. So we look at uh AI valves in kind of two ways. is when you actually start using it as a model. Can I just tell how good is my prompt? How are they

working? Can I get that easy enough first thing? Then you've got it in production. Can I now look at traces and see if somebody at a human level we talk I think Bish talked about like oh you

should always evaluate a few percent of your traces. How easy can I do that? Can a human go and check and say hey this works this doesn't work. And then the model also looks at some of those

things. So that's the second part of it. And the third part of it I think in terms of evaluation one of the other things that we really want to look at is um what most people would call as

guardrails but we also look at it as um runtimes is how I would call it. So you can do small amount of evaluations during runtime when the questions are being asked and these could be small

things like topic relevance. could be content relevance. Usually for most people if you ask them they do a post priority analysis but you could also do it during your promps

itself small bits and pieces and be able to evaluate and that again it stops the bot from even proceeding further which is always a good thing to do if you're not getting there rather than saying oh

I'll take responses two months later and evaluate everything for you. Um so those are some of the big things that we have been working on. We've been working on these runtime labs actually with the

title group. Uh and these are on detecting different things that we are slowly building on could be topic relevance could be how do we block slurs? How much are people actually

having toxicity of language? And these become not just something that's done as a research two months later and we're going to build it. Can you do that from day one? Can you push those in and keep

building on them? And that's the notion uh for us and how we are looking at our platforms and how we develop it. Uh I'm going to run this quick happy to answer questions. we have like around three uh

um so for us when we start out with uh abouts you always want a golden set of questions and answers that once you've created your prompt you want to be able to generate that our idea is given a

knowledge base AI can generate that first thing users can then go in and adapt that change it that's one point second point is you also want to have multiple

categories set of clusters And this is something that u I know that the poor runner team have done we need have done to say is it a knowledge base is it a knowledge check is it a procedural check

or is it safety or is it something that the AI model should never answer so all of these you build into it right and that's something that you can do day one so you are going to put an AI model and

you're going to even do a pilot it's also important for you to then think about how do I set up these questions and then run a very quick anal And you'll see that the other thing that I

want to point to here is uh in India especially um you would have noticed you've seen this one was for a policy bot happening in Himachal and Hana what you would have noticed is you would have

seen English you would have seen Hindi you would have seen English so many combinations if you're not going to test for all of these combinations you're really going to find out later

that it doesn't work for some combination for sure and that's also the other part of it is that when you design this, you're going to have to be thinking a bit about it.

So this is u kind of a demo of the actual platform and what's happening where you're able to select um add in this simple set of golden set of clusters and given a particular

configuration of whatever you're using for your AI assistant um it's going to be immediately be able to run a analysis and tell you how close are you getting and this is not complicated. So this is

one simple metric which similar similar are my answers to what the user thinks it should be and you'll see that in terms of some of these questions some of these answers um the

other one that we also do is use another LLM to judge is a performance good and these two simple metrics we feel are great starting points for most what you would have noticed with pura stuff is

there was a lot more metrics and it's great that would be a step that you take further along the road but for an NGO starting out on AI Right. How do I get there? Simple enough. Keep it simple to

one or two metrics. Build as you get more and more you've got more scale. Then take it further along where you're really evaluating your models. Um so this allows you for example here to see

um some of the responses from the AI very quickly. What is it missing out? What is it not doing? So this also allows you to go quickly and adapt your product because that's also the other

question that lots of people have. What do I do now? Okay, I've got this uh given me a score of 6.7 what does it mean for somebody who's not so technically savvy what do I do with it

so to be able to enable that to say hey this is why what's missed out maybe it's a problem your knowledge base doesn't have good data maybe it's a problem with your phone how do I adapt it or maybe it

is that instead of using an open AI GPT model you have to use a Gemini model and that would be better could be any one of those combinations but it allows you to test it very quickly uh to be able to do

And this is kind of our approach to AI valves and how we are looking at it to do these golden set of questions and answers run that fast run that for any organization that uses AI. The other

part that we are working on is how do we set up these runtime guardrails and eval all the time. The combination of these two we feel like is a good for organizations and as they build more on

AI build more scale we expect them to go towards what uh and talked about for digital green and &gt;&gt; thank you model

forward. &gt;&gt; So there's multiple different ways um we don't do it as anything as part of the platform in terms of unlearning right but I can give you a very brief example

actually of like one thing that um had happened in one of the uh organizations that I work with uh this organization works in the health sphere and they work with maternal and child health and one

of their main things is to promote for institutional delivery um so but the knowledge bases that we were using are from the Asha workers manual and if you have seen a workers health manual

they'll talk all about home delivery So there is a lot of disconnect when you say about like between what is wrong and it could be any one of different things right it could be that your prompt's not

working well it could be that your knowledge base is actually not like greatly accurate so in our case we actually had to pull out every single material that talks about homebased

because that's not what we wanted the bot to be able to give that information to uh the people living in the urban slums in so that is important and then in unlearning also one of the other

things that um is done. We are not doing it and I've seen it done and uh maybe even V can talk bit more about it in detail. I know they do a lot more of the fancier work is that you also have more

of the sticking your questions and creating your FAQs and using that as your first spot where you are funneling now 70 80% of your queries into your FAQ rather not to your general LL. So that

way you have actually have reduced the amount of error that you actually need to come and give uh his comments. So we've heard from health, we've heard from tech and now we're the next

agriculture tech nonprofit talk about their evaluations. &gt;&gt; Uh thank you. task but I think it has been easier by everyone covering a lot of stuff here.

Uh okay. So yeah why evaluation matters. Uh so just a brief about we have uh we have been working for last uh am I able? Uh we have been working for last 18 years uh in the field of strengthening

uh agriculture extension and advis our flagship product. Farmer has two weeks back hit 1 million users uh in 15 plus languages across India and uh North Africa.

So why evaluation matters? This was a slide. Okay. So okay. Okay. Why evaluation matters? Uh short uh we need

Now the reason is as I think uh Edmund Vaish have covered there are various aspects and perspectives the user needs user demands performance in terms of you know

whether the response is correct whether they can understand it. Uh as we are serving the smaller farmers we have to serve in multilingual kind of way where they also have difficulty

question. So it has to be multi-turn uh and then stakeholder needs uh about you need to have what are the topics what are the what are the causality what is the directionality where are you

moving what are the different segments of the users and what works and what not and then obviously the technical part of uh you know how good it is performing in various languages various modules and

obviously the responsible AI uh gender aspects whether it is giving any toxic bias and we do some sort of a stress test using the uh so this is one internal tool that we

have created which is called evaluate farmer essentially all the queries that are there there's a there's enormous pipeline which finds out what are the unique questions out of around 10

million queries that have been asked and uh on this particular uh web tool there are different agriculture experts who actually sit and they go through each and every response that was given and

they can mark it out uh which segment is wrong, incorrect or irrelevant and they can also add any missing information and then they can give some general uh you know comment and and give a rating. Now

as I said we need to know our heading and where we want to uh you know go. So we have done 22,000 different question answer in uh Indian India alone and this is what happens when you are able to

find so this is a question about how can soil be for better GD this is a this is a response from the existing pipeline where it gives something which is good but now what we give getting is much

more specific and what these highlighted sections are also LM as judge uh where it is able to find out what what we call what do we mean by specificity that it is talking about the

specific quantity based on location timing and what it would benefit. So actionability terms of you know the practical advice that gist is extremely and these are the kind of improvements

that we can achieve. Uh so I think there's some sort of at the top but this uh the so what we have done is essentially uh you take the ground truth golden answers and then we

can extract the facts that are stated. So if I have any kind of a text I can take out what are the facts that are stated in this and I can use that with what the model is uh predicting and what

we are seeing in green is what we are able to do in in terms of what we mean by factory call is how uh what is the overlap with the you know the golden answers that

so this is an example of and it's sort of important to understand that it's not AI evaluation or human evaluation. It's a combination of both. You need human annotators or

human evaluation in the first and then use it to particularly scale it up and actually get your LMS judge to be you know accurate enough to actually uh give all these evaluations at scale. This

this is for you know the uh the props that are important as for the gender and that that is an important aspect and for distriing for those particular specific crops we could see a

3 to 3.5 x uh now moving on to the you know this is an example of this was an example of end to end evaluation of the question asked and the answer given but what you need

and as I think had shown the entire pipeline so typically it's any application is not just one lm I think it has multiple models. We recently uh you know the next upgrade of farmer chat

has rich image cards that the farmers can actually when they see it they can understand what the context is. Now this is an example I don't know whether this is visible or not

but essentially this is an image card that is shared. So there are multiple these kind of modules uh modular evaluation in house where the person team has actually marked it wrong

because the question is about what is the planting distance between avocado and what it what is it showing is not reflecting the 7 m or something of that sort which is much higher when you're

talking about avocado trees uh then when we are talking about the you know the module level kind of evaluation the speech is important this is one of the benchmarking papers that

we have just recently published uh where we would get what are the kind of errors that are happening and benchmark different uh speech to text models and one of the things that comes out of

it is what are the specific the tree map of what are the specific domain specific terms where the model is going this is a important input in terms of uh you know what is the further data collection that

needs to happen so you know I think everyone understands Hindi. Uh it's a common thing like you know someone is asking about g so what is the problem in my g is what

is the problem in my village and I would answer something very right. So uh so we could use so we we took around 100 hours of audio samples and got it transcribed round and what a person would understand

and then to actually come up with these classifications. Uh this is one of the recent work that we have done with the uh the agency

on you know further going down through the model performance. So this is like actionable uh what is the level of question or the the level of response uh how much what level of complexity is

there? Does it need a person to be an expert leader or you know intermediate or basic? Uh then we have gone further into you know whether there are questions that are being asked which are

violating any kind of usage policy uh you know what is there it is also evolving and use it to create a prompt and use it as a judge to come up with

this classification. Lastly, this is a recent uh you know the survey inapp survey that we have done for the user evaluation and uh it the

inapp surveys have a low improvement but we were able to give it to the most active users and the importance is important part is that the survey has to be very short. So the question was uh

you know did you apply the advice from chart in last 30 days and overwhelmingly we saw that 70% of the respondent yes and what are the kind of areas that uh with that I would

So yeah, so I would say that there are certain frameworks that you can definitely use. In fact, one of the things that we have published this entire extraction kind of thing which is

I think can be applied in medical or education or any other legal or any other domain. Uh but the thing is like you can have those frameworks we have a Python package

installed you can just use that Python package use it but then the prompt itself and the model needs to be tried out on your own. So there's It's kind of you have those frameworks,

you have those uh certain libraries this data everything of the papers that you're publishing is available in but that is a reference I would say someone has to do it on a different

obviously the errors and the entire methodology you know exactly &gt;&gt; um so thanks everyone we didn't have too much time we 55 minutes to cover a lot of content but all the speakers will be

outside so do catch them and ask continue the conversation. Thank you so much for joining us.
