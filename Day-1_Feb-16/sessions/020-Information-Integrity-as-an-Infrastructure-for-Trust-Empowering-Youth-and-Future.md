# Information Integrity as an Infrastructure for Trust: Empowering Youth and Future Generations

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 10:30 ‚Äì 11:30 |
| üìç **Venue** | Bharat Mandapam | West Wing Room 4 B |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/fYCRZhqSFT8?feature=share) |

## üé§ Speakers

- Alpesh Shah, IEEE Standards Association (IEEE SA)
- Amir Banifatemi, AI Commons
- Gabriela Ramos, IEEE
- Karine Perset, OECD
- Mariagrazia Squicciarini, Unesco
- Mohammed Misbahuddin, C-DAC Bangalore
- Moira Patterson, IEEE SA
- Tanya Perelmuter, Fondation Abeona
- Uyi Stewart, Inclusive Innovation & Analytics at Mastercard
- Yuko Harayama, IEEE
- Yuko Harayama, RIKEN in charge of international affairs

## ü§ù Knowledge Partners

- IEEE

## üìù Summary

The proliferation of generative AI and algorithmic content systems has created a new kind of risk for young users globally: Increasing exposure to manipulative design, hyper-personalized content, and AI-generated information at an overwhelming scale. Age-Appropriate Design (AAD) is a policy and design framework that helps develop safer, more accountable digital environments with the best interests of children in mind.

## üîë Key Takeaways

1. The proliferation of generative AI and algorithmic content systems has created a new kind of risk for young users globally: Increasing exposure to manipulative design, hyper-personalized content, and AI-generated information at an overwhelming scale.
2. Age-Appropriate Design (AAD) is a policy and design framework that helps develop safer, more accountable digital environments with the best interests of children in mind.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/fYCRZhqSFT8/maxresdefault.jpg)](https://youtube.com/live/fYCRZhqSFT8?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

Good morning. Good morning. Can you hear me? Okay. In the back as well. Yeah. Okay. Great. Welcome everybody. Good morning. Uh thank you for joining

us today. I am Moira Patterson with ILE E standards association and we are excited to be hosting a session on information integrity as an infrastructure for trust today. Uh this

is a very important topic and we are excited to have a great panel of speakers here. uh I will briefly introduce the speakers uh and then we will jump right into uh some remarks

from our speakers and um and then some questions where they will share their valuable insights with us. Uh with that being said uh I also want to let you know that uh I have a co-odderator who

will be taking over in the middle of the session. So we are a dynamic and interesting session for you today. So our speakers and I'm going in

alphabetical order here uh are Amir Bonafatami from AI Commons and Cognizant, Jill Fayad who is a fellow at MBRSG, Yuko Harayyama from the GPI Tokyo Expert Center, Muhammad Miss Bahurin

from Sidak Bangaluru. Uh Tanya Pearl Motor from Fondencio Aona. Karen Peret from the OECD. Gabriella Ramos from the task force, Al Pesha from the IT ESA, Maria Gratziaini

from UNESCO, and UI Stewart from Mastercard. Uh we are very excited to have this panel here and to hear from them. But to kick us off, we want to start with uh two very brief

provocations to really talk about the importance of our topic today. So, first I welcome Al Peshaw to the stage. &gt;&gt; [applause] &gt;&gt; Thank you Myra and uh thank you everyone

for joining us today. You know it's a very interesting time because here we are to celebrate all of the ambition and power of what AI can bring. At the same time we sit in sort of a conflicted

position of what does it mean for all of us? And often now the question becomes why does the world need to focus on information integrity and trustworthy

information and online ecosystems and a lot of this comes down to the point of as much as we're seeing more and more people making use of AI chat bots AI systems innovative approaches at

the same time there's a level of mistrust in the information people see online and it's not just the information in terms

of what they're consuming by reading. It's also the deep fakes. It's also uh the improvement over social heristics. It's also the issues that we see in the

interactions that children have relative to many of the systems out there that were never designed for them but were designed for different business models that didn't really enable access to

information in a way that allowed them to retain their agency. And as a matter of fact, if you take a look at this, the OECD AI incident monitor simply continues to show more

and more incidences, not less and less. And of course, this is part the if you have a flashlight and you aim it at something, you see more of it, but there's so much more at the same

time happening that we're not taking accountability for. And this means that now we begin to come at a point in our lives, this inflection point as a whole, where we must we must

start to really think about now, are we working together in a way to look out for the best interest of humanity? Are we really taking into account the necessary types of methods by which we

work together despite all of the volatility we see? And can we accept the volatility without doing something about it? Here

we see a very important opportunity for us to not neglect during our time here over the next four days. And this is very much about developing proactive governance frameworks. The ones that can

allow for improved interoperability uh across systems, reestablish trust at a global level and in an open manner with inclusion of different stakeholders. Right? One of

the the signs when you came in said also about not just AI for AI but AI for inclusion, AI for innovation, responsible innovation, safety, all of these things,

they don't exist on their own. And these systems we're designing are meant for people in the end, not for the systems themselves. And I will share at least that uh from

my perspective as a public charity with over half a million technical engineers across 190 countries with a mission to advance technology for humanity.

This is one of the reasons why we have dedicated such a considerable amount towards the advancement of AI ethics and AI ethics based frameworks. And with over 75,000 members here in

India, we have such a cadre of individuals very much committed to helping advance things here including with uh Mr. Shri Chandra in the back who oversees our India office.

Uh thank you. [applause] &gt;&gt; Thank you Al Pesh. And we have one more framing provocation by Gabriella Ramos. Well, thank you so much. So, so nice to

be in this uh summit and uh thank you for the invitation. I was introduced as a member of a task force. The task force is about uh inequalities and social related financial disclosure because

inequalities is at the core of this discussion. But Amir, thank you for inviting me. I I was at the OECD. I was at UNESCO. So happy to have Maria Gracia and Karen here with us. And as Alf has

just mentioned uh when we talk about uh information integrity, we're not talking about uh an abstract ideal. We're talking about the basic infrastructure that we need to protect our democracies,

to protect our people, but to protect our decision making. each one of you get information that nurtures your thinking and also the decisions that you need to make. So it's really the core of our

societies and as you mentioned the trust that we need to operate in this in this field and therefore when we think about that the ecosystems can be tampered that the e ecosystems can be put into danger

and and the integrity is just uh flooded with the misinformation and disinformation we know what we're talking about and then we get the generative AI which can do these things

manip ulation, filter bubbles and all the all the the the dangers and the risk that we face in the internet space at scale and it's very difficult to identify. It's very difficult to protect

ourselves and when we think about the governance in a way that is just voluntary principles let the h market decide what to put in

the platforms or not and if there are dangers they will correct. I think this is a question of core governance of our systems and protecting our citizens and therefore it cannot be led just to the

private uh sector alone. And then I was also asked to address one of the issues that is uh I feel the most risky part of all the whole uh system which is youth young people, children because they live

in this ecosystem filled with fits that might be not the reality that we were uh private when we were younger. And therefore they they have very clear difficulties to define what is real and

what is wrong, what is true and what is not. And therefore they get trapped into these systems that can be really misleading. And we also have uh a new challenge that

uh for me this is taking awake my sleep at night which is the companions the anthropormization of these tools that will let lead to you or to anybody I don't know how many how

many of you use chat GPT to ask personal questions raise your hand come on pe research center said 60% of the population use it for personal purposes and therefore we need to be careful. We

need to be careful because at the end we are entering into fields that are very complicated. And if you think about converging technologies as we did when I was at UNESCO on the neurochnology and

the emotional attachment neural data that is feeding and featuring exactly the things that you want to see, the things that get you engaged, the things that get you driving and trigger, I feel

that we're really getting into a very difficult situation. So the question here is not a a content problem. It's a system problem. And because it's a system problem, we need to address it

with the government's infrastructure that we have just heard. Uh countries are starting to do it. Of course, uh high in the scale is the EU acts, but the UK also got uh some uh decisions in

their own legal infrastructure. But we also have the question of uh we've heard that Australia, France, Spain just saying no smartphones for young people below age 16 which I feel is a very wise

decision. So great to be here with you and to hear a very interesting conversation. Thank you very much. And so we've already heard some provocative ideas

related to this important topic. Uh it's about people. It's about serving solving problems for people. Generative AI is accelerating the challenges we're facing. Uh and of course how do we help

young people uh thrive in this environment. So we have uh our four next speakers. uh I will ask them to speak from their perspectives about the key issues that we are facing now to also

show the urgency and the perspective from their organizations. In the interest of time I will not come up between each speaker. So I will ask that Amir Karim Maria Gratzia and Muhammad

each share their own perspectives uh to help us kind of lead through this discussion. Thank you Amir. Can can you hear me? Uh good morning everyone. Sorry my voice

is a little bit uh tired from last night's uh the flight to Arabic this morning but should be good. So following Alpes and and Gabriela is not easy because they are they're basically so

eloquent about about topics. I'm going to be very basic and pragmatic about what I'm talking about. As we talk about information integrity, you've heard about deep fix. You've heard about

manipulation. You've heard about all these issues. The problem is that we're facing a two-speed problem. On one side, you have institutions and regulators coming up with framework based on

principles of course, right? OCD principles, other principles of AI [clears throat] about regulations to help protect us. So, EOI act is one of them. NIST risk

framework is another one. Some countries have also adopted their own. So policy is moving forward with some principles and there will translate to standards and gradually going to adopt them. But

at the same time we see an increasing growth of AI deployment as a very rapid scale that leads to this uh dichotomy this difference of speed between policy and between deploy technology deployment

and this created a chasm because we don't sync them together and policy may fall behind a little bit because every time there's something new happening we need to think about it and we catching

up instead of being preventive about it. So in in when it comes to uh to misinformation or manipulation or deep fakes or anything of those there's really three issues. One is that people

can be bad and use systems for their own benefit. So they're called bad actors. So this is the real threat. You used to use cyber security right capabilities for that and it should protect systems

but AI can be also make mistakes because the models are not trained properly because there's issues with data because we build system that are not fully transparent accountable and there are

some systems that are operating. The third wave is a web of agents where now we have situations where they can be autonomous and not necessarily being bad actors but somehow not making mistake

but voluntarily changing their objectives and coming up with new methods. So in these three scenarios what is left for us humans? How can we trust information? Is it because it

comes from Wikipedia who comes from an encyclopedia or my teacher or our governor or is because it comes from a scientific article on nature or or so what should I uh believe we recently we

did a conference uh scientific conference about a year and a half ago called Ishkai and we did that conference we authorize authors to use the chatbot of their choice chat GPT or claude or

others and we asked them to mention that they have been using this So we as judges reviewed all those scientific papers and uh we didn't pay attention but two professors basically pay

attention and and realize that some of the references in in the bottom article are references to famous authors at other conferences never existed. So you've seen this before right? So we

scenarios where even scientific papers may lost the the true north. So what is the situation that we have to do? So really these two t these two speeds are are an issue. So when we thought about

this is the this this aspect we're thinking about that how can we accelerate innovation to catch up because innovation policy is not fast enough and innovation by private

companies is growing at its own speed and open source is doing it own and we're all learning and catching up quickly but there is no sync between the terms. So if we really wanted to have a

framework of trust and information capability to be trusted, maybe we should sync the two of them and figure out scenarios where we can both deployment implement scenarios that are

technically valid and sound and trustworthy and accountable and and with liability frameworks with with transparency with provenence but at the same time be able to implement policy

that is built with incentive but also with with mechanisms of of of control. These two are not sync. So the purpose is to sync this two. So what going to be here today a little bit of the thinking

that came out of the meeting of many minds about how we can actually think about accelerating innovation to catch up these two elements because now they're disjointed right many many smart

people are thinking about policy. Many of them are here today or in in these forums. Many smart people are doing it but we never had this opportunity to to solve this problem together. So there is

an opportunity to do so and we're going to hear more about it today. Thank you. &gt;&gt; Um, thanks Amir and uh, hello to everyone. It's a pleasure to be here. Um, my name is Karine Purset. I'm head

of deputy head of division of the OECD's division on AI and emerging digital technology and uh, and uh, delighted to be here today. I won't repeat what uh my colleagues have said before and which

and just want to echo uh I think that what Gabriella said about the children and their reactions speaks very uh very very much to me because I have three teenagers and I see what they do and

what they say with their uh uh with their with their social medias with their uh with chat GPT with all kinds of other GPS and many things that I don't see, but the things that I do see are

pretty scary because um they they really show that they're not prepared. They're not equipped to deal with this over with with so much information. They have no way to navigate this world. Um, so, um,

uh, it really calls for for, uh, helping them because they're they're younger, they're more v vulnerable, they don't have the experience that we have as, uh, as grown-ups, uh, they need an

environment where the trustworthiness of information is insured, or at least that in some in in some places. Um, and and and many more challenges. Um so I'd like to mention that to address the

governance challenges uh of of these rapidly advancing AI systems um such as particularly generative AI but uh but Gentic and and more as we move on the Hiroshima AI process was launched uh in

2023 during Japan's G7 presidency and this was really about promoting international governance on um AI uh international cooperation sorry on AI governance and uh the G7 at that point

called on international organizations the uh OECD the uh UNESCO uh AI commons it e I mean international organizations and multilaterals to promote international cooperation on

policy developments and practical projects including uh on information integrity uh because you really can't distinguish the the policy from the practices um and um and uh and the

policy and technical solutions have to go hand in hand. You can't have one. Again, they're operating on different time frames. Um and that's why uh under Italy's G7 presidency, we developed the

Hiroshima AI process reporting framework. Um and early early insights from those submissions really uh show that tools for content authentication and providence are still at a very early

stage um and mostly used by a few large companies uh and these tools really need to become scalable, interoperable and so on if possible. So that's one route but uh but but there are many many more um

and uh and that they need to work together. Um so the challenges are unprecedented in scale and complexity and I think the key point that uh that I want to make today is that no single

actor can address these alone that we need to all work together to do that um and uh through collaborative cross-disciplinary efforts that combine policy innovation with technical

ingenuity those are uh really really important to bring together. Um and we have an a real opportunity and actually a need a pressing need to advance uh this cooperation uh and promote tr

transparency and trust uh for the future generations. So thank you very much. So without further ado, first of all, I'm so much happy to be here and honored because the person on my right hand side

that was announced as member of a task force was actually the assistant director general at UNESCO until a month ago and she she didn't say it but she was the force behind UNESCO

recommendation on the ethics of artificial intelligence and the adoption in 2021 of these fundamental principles that until now actually represent the only global normative instruments that

exists around the word and since then and even before that because we have been discussing issues about misinformation, disinformation and fake news at UNESCO there has been a lot of

work and why because actually the inherent challenge of the AI era in information is the difficulty as was mentioned by previous panelists to distinguish what is real from what is

not. So there is a problem of identification and it's like you know walking in a dark room without being able to see anything. Would you do that on a normal day? No, you wouldn't. But

nevertheless we do every second because there is another challenge that is awareness. We are seldom aware and fully aware of our rights in that space and when we are confronting AI systems. So

these are all issues that actually are already addressed with some policy implications in UNESCO recommendation on the ethics of artificial intelligence and this applies also to what was

mentioned before and in the jargon is called synthetic content. It can be data, it can be videos, it can be anything. It can be images can be text and very often it's really difficult.

Imagine then if you couple this with the quantity of information we receive every day that is in addition in theory customized to tailor our profiles then you get the challenges we are exposed

and allow me to underline one category that is actually under the focus of this discussion and this youth. youth is a priority group for UNESCO because we recognize the centrality of not only

being part of the discussion because they're going to outlive us and the future is theirs but they're very part of this very present and I think that's sometimes the mistake that is done and

again under Gabriela leadership we launched at UNESCO the youth as researchers that is let's discuss issues that affect the youth not talking about them as a problem or as a category to be

dealt with but as part of the conversation and part of the solution part and parcel and why that is fundamental because it also links for instance to all the work that UNESCO

does in relation to education skills. Karim was mentioning the fact that this goes much beyond what is awareness and goes also about our cognitive skills, our emotional skills as Gabriella was

mentioning. So these things are completely intertwined and let's also look at what in the jargon of economist is called spillovers. By the time we discuss issues related to misinformation

and disinformation and trustworthiness for the youth, we are actually addressing another category which we seldom talk about and this elderly people. The problem is very similar

although for very different reason. Young people were born digital. Young people trust technologies because they think they're part of their own life. Older people they have the cho the

choice typically because they are technophobic to some extent. They have difficulty in engaging with the technology. They have this bifurcation of choices whereby either you trust them

in full because frankly you're not able to deal with or you trust it. Now going to the issue of trustworthiness. The problem with that is that if we don't cannot trust the technology, the

technology will also become inherently worse because the data we are going to convey will be patchy and fixing patchy data to have predictions that are accurate is actually very challenging

thing to do. And now allow me to close with one thing. There is nothing more dangerous than giving something for granted. And this is applies of course to information and the trustworthiness

of the information as it applies to institutions as this applies to values, human rights and democratic values. Back over my

&gt;&gt; Yeah. Uh thank you very much uh for this opportunity. Uh basically I'm from cyber security background and recently ventured into AI ethics working with government of India in Cedak. I would

like to just put the perspective of Cedak and what is happening in India from the trustworthiness as well as very important topic of uh information integrity. Basically information

integrity is how do I know what I am seeing is real, is authentic or is valid or not. Because these days with the AI deep fakes and all that uh new jarens uh you'll not you are not able to make out

uh what is real and what is fake. Okay. But uh what India has done way back in 2000 we have enacted uh an act called as information technology act 2000 which was amended in 2008 where integrity and

authenticity was the core components of uh the trust. Okay. And these uh these these were uh I mean used across uh all the app kinds of applications. Now India has uh its own uh uh certifying

authority controller of certifying authority which is the highest root trust body which actually takes care of the cyber security. I see a lot of commons between the truth uh aspect of

AI plus trust aspect of cyber security. Now from that perspective if you look at how India built and where we are right now if you look at in the beginning we had developed the trust layer first in

the name of uh office of controller of certifying authorities and now we came up when when when the Aadhaar came which is a verified uh unique identity all our all of you might be aware of it the

largest uh uh I mean unique identity database across the globe because India is 1.46 46 billion population uh I mean uh uh nation and whatever project you have to do here whatever solution you

have to implement here it is it is to be population scale. Now coming to that perspective, we already have the information integrity as well as authenticity verifiable identities in

place. Then we slowly moved on to authenticating the documents uh from the electronic signature perspective wherein one-time signatures one-time auki basically instead of OTP one time public

key infrastructure. So any document you can authenticate without having a digital token. Now from this perspective we are ready with that transaction as well. Now the uh the goods and services

tax GST which is filed here every month is now being digitally signed which provides you pure integrity because it is a non it becomes a non-tamperable document. You cannot tamper if you

tamper it is detectable as well. So millions of signatures are happening each day. Now coming to the next example of uh uh unified payment interface wherein you just for for the for the

common man it is just a scan QR code. You just scan and then make the payment because the payer as well as pay both are authenticated in the backend infrastructure which is again authentic

which is completely based on the public key infrastructure and the trust based. So this UPI every month billions of transactions are being signed. Now just imagine me as a common man or a person

who is nontechnical or uneducated. I'm just simply scanning on what trust we entered this building right minimum one to two lakh 100,000 to 200,000 people are the footfall will be in the in the

next uh uh 3 4 days what we are trusting we are trusting the building infrastructure that it has a strength and how we are trusting that because it is based on certain standards it is

based on certain regulatory framework exactly same is required for the AI as well it is not only the policy it is not only the regulatory framework that we announce we have to announce the

infrastructure as a layer in digital public infrastructure which India has already been doing. Now look at the first center of excellence in DNS security. Uh it is running in SAK. Now

after 3 years of running we are receiving 30 to 35 million queries uh for uh translating the IP addresses to the domain names on a daily basis. And at the same time again the entire back

end is AI enabled plus public key infrastructure digital trust based enabled because of which now the system is able to thwart and block 3 to 4 million uh malicious domain in these

days. There are many more more things that are there which I would like to talk about. Finally the safe and trusted AI is announced by the government of India under India AI mission as one of

the pillars. This is very important thing that we are doing here and youth AI is for the youth from class 8 standard 8 to standard 12th giving the AI education and age appropriate design

and the trusted design many more things. Thank you very much for this one. Thank you very much for all of those insights and uh we heard a lot of the common themes about multiddisciplinary

cooperation about all the different layers uh and technical aspects and policy aspects and regulatory frameworks that are needed and so I will now hand over to Tanya who will take us through

the second part of this session. Thank you. &gt;&gt; Hello everyone. Uh so I'm Tanya Pearl Mutter with uh Foundation Ambiona and uh everything uh there was an an incredible

energy uh in the first part of this panel talking about uh what the problem is. I think we all understand now that we cannot do anything by ourselves that trust is is a fundamental human rights

issue and it we have to come around and and find solutions and I'm so proud that we have this coalition here together uh working on this and and uh this second part of the panel will be addressing

what as a coalition we could do together to to move forward and find solutions and Muhammad already addressed some of the technical uh parts but uh let's see um what we can do and let let's start

with uh Yuko uh who is going to be um speaking about how education is important and how there cannot be one only one solution because culture uh every country's culture is important.

&gt;&gt; Thank you so much. Uh I started recording my former OECD colleagues Gabriera, Karen and Maria Ratio. Uh because I have not children but the grandchildren.

We may call them uh AI natives and it's my concern personally how they will grow up because we don't know yet what could be impact of using AI everyday life for the developments of children

and they are under way to experiencing and learning and integrating in themsel emotional skill. skills normal value systems and how intact

humanto human interactions and most of all I would say they are learning agency to be themsel and we don't know it but already in the practical sense they're all using

already even my small children of 5 years old they bring it's practical and they are it's fun but um less and less time spending for human to human

interaction and we don't know yet. So what we need is really a collaboration across countries, across cultures to make sure that we are tackling these issues together and sharing information

in the way that we need much more work, academic work, scientific based evidences to see what's happening and then what could be our reaction. And we don't have time to wait that people will

coming in five years because they are growing up now. So we need to experiment in different way of very testing impact at the same time that what's may have the impact of to be offline

and comparing and it is been rooted in your environment, languages, cultures, countries and so many thing that you have to take account so many factors within. So, we have to work together to

share and to find a way to work together. Thank you. Thank you, Yuko. And I really appreciate what you mentioned about time. Time is critical here. We we need to be together

and work together and uh do it uh very quickly. Al Pesh, I turn now to you. Can you tell us uh what technical uh solutions could be there and what it e is doing in terms of uh standards to

advance working together and uh finding solutions for these critical problems? Sure. &gt;&gt; So, you know, the technical piece is is interesting, but I think when you listen

to what everyone has shared so far, the problem hasn't been the technology. Uh the problem is everything else, right? We haven't figured out how to evolve in the same rate that the technology is now

evolving. It's outpaced the way that we've been thinking about it in many ways. And so these concerns that we're talking about are the type that parents, adults, others, they're not familiar

with, right? And so this adds to this layer. So when our our colleagues from UNESCO shared that it was important to have the youth involved in it, they're absolutely right because who better than

the natives that are working with it, that understand it. In many cases, a 16-year-old has 16 years more in experience than an adult. Yeah, they've learned the mistakes. We

haven't yet. This is why it's important when you talk about having frameworks that allow those that are affected to be included. Inclusion is a very critical element

here. And we also heard from air uh about the issue of struggling with reality. Uh and yes, this is also a real problem. We we saw this happen when metaverse

technologies were advancing, but when there were crimes occurring, people really felt as if it happened to them. Haptic technology caused people to feel the actual reaction and it stayed with

them. In the same way psychologically it means that there's a fictive line we've drawn between digital and reality. There's just reality. We should stop pretending.

And the governance then that is required is a governance type of system that hasn't existed for quite some time. But the type we need is one that allows us to work together quite effectively.

For us from the side of standardization, I'll say that we are a global organization. We see all kinds of problems. You know, a standard is good when

nobody's happy. That's how that works. But think about it this way. You have a room full of people that want to kill each other. And yet at the end, they all have come to a conclusion that they can

live with. That is what a standard is. So if you can imagine that it can happen here, it can also happen elsewhere. This is why

and I think it was very well said earlier. Um there's a great deal of work that every one of our institutions have done. If you take age appropriate design as as

one, we've seen we've had Indonesia, the UK, number of states in the US, Brazil, Australia, China, now a number of countries taking up this fight around age appropriate

design based off of our work. At the same time, we've seen a host of them take on our AI ethics elements. We have over 250 different initiatives at ILE E right now

on AI and a high percentage of those now also cover the sociotechnical elements and why engineers are incredibly smart.

Engineers have incredible heart but it's not always easy to translate from what you know so well into gray areas. This is why we've created tools and norms to help.

But I go back to the point that was raised earlier. No one can do it alone because nobody has the context alone of every single problem that could exist. That means you must have a way no matter

how much you want to close yourself off to everybody else. You must have a way to interact with others. Sovereignty is not about closing

yourself off to everyone. Sovereignty is the ability to make your own choices. And this is why I'm very excited to be collaborating with OECD, AI Commons, and many of the others, UNESCO, and a

host of others on this global trust challenge. Because as much as what we've done is making a difference, as much as UNESCO has done is making a difference, as much as everyone the work they're

doing is making a difference, the point is there are multiple models that are required. And what part of the beauty of this and I'll just share my personal opinion is

that what they are looking at what they have been evaluating isn't just the type of ideas that are in the beginning stages. They're looking at things that are concrete that exist today and how

can we go instead of from zero to one instead one to n. The gentleman from CADC shared earlier about scale. This is what this challenge is looking

at. Scale to address issues associated with misinformation, disinformation and onwards. So from my own perspective, I see the

challenge itself as well as the modes and models of governance that are emerging through these various partnerships being a very interesting mechanism to

both engage with as well as to perhaps even partner with in the future. Thank you. [applause] Thank you for these enlightening remarks and uh it the fact that uh a coalition a

very strong coalition that is needed to tackle these issues is is is emerging as so important. Jil I turn to you uh can you tell us a little more about why partnerships are so important and why we

need more partners and and uh to to to work on this together? Yes, Tanya, thank you very much. So, uh, essentially, I'm just going to share with you some numbers, right? U, 85% of

the people who read something don't think that the information is accurate. Yet 8% will go and check if the information is accurate. So, about 75% are okay to go on with the information

even if it's not accurate. Right? We talk about the global uh uh the the the global issue that we have in terms of trust but that global issue is also uh different whether you're in the

global north so-called global north and so-called global south even though the global south is not one entity but yet it shares some common characteristics with respect to infrastructure 80% of

the research is being done on the global north not on the global south the information infrastructures in the global south the digital divide the access to information, the impact of

that information and the trust in in organization is much less in the global south than it is in the global north. The solutions don't scale from one to the other. You cannot translate a

solution from the global north and put it in the global south. This is the work of everyone. There is no one sizefits all. Every entity, every organization, be it at the AI level, technology or at

the policy level or in a specific field needs to work on it for their specific area. We breathe trust. What we do as individuals or as organizations is based on trust. If there was no trust, there

would be no communication. to the point that the gentleman uh was making here about cyber security. If you think of the issue we have with trust, it's the same as cyber security but at the

societal level. We need the ability to address these challenges and to do this we need to partner up and the partnerships need to be open to everyone. Again, it's not about finding

one solution. It is about helping everyone come up with solutions to the point where these solutions can be not only prove proven in the system through the challenge. The proof is in the

pudding because the challenge gets you from a proposal to a prototype to a a pilot stage. So the proof is in the pudding. If it works in the pilot, it means it works for you. It means others

can get inspired from it and others can also replicate it. So we we are here looking for partners and Muhammad bin Russian school of government uh for which I'm a research fellow is is a

partner in this initiative because we recognize the issues um on uh in AI policy in terms of uh problems of information uh integrity and also in terms of the digital divide that we face

in the global south. Thank you. &gt;&gt; Thank you Jil [applause] Yui. I I turn now to you. uh could you tell us uh why uh trust is is a basic fundamental element of what Mastercard

uh is all about and uh explain why corporate participation in in uh coalitions like this is so critical. &gt;&gt; Absolutely. Thank you Tanya. Um part of the uh good thing coming last is that I

don't want to repeat everything folks have said so I say yes to everything that has been said but let me introduce Mastercard a little bit in terms of uh Tanya's question. So we are a technology

player in the global payments ecosystem. What we do is that we connect merchants, financial institutions, small business and we and governments. We bring them together. We do this across 200

countries and locations around the world and we process about 150 currencies. Just think about that. That is trust at scale. Let me give you two quick examples for a

company. Our philosophy is that we do well by doing good. So I'm going to give you two quick examples and I'll wrap up. Example number one is in cyber security. Do you know that in the last 3 years we

have prevented nearly $50 billion in fraud? Cyber security is key to information integrity. One last quick example because I I sit in the center for

inclusive growth and you all have talked about the inclusive component of this. Think about what happens in the global south where there is misinformation and a small business owner is actually told

that you can't use WhatsApp because if you do they're going to steal your information and this small business owner stops using WhatsApp because of

misinformation. What has happened? The very infrastructure that supports business is destroyed and this person is set back economically. That's why we need to be honest with information

integrity. Thank you. [applause] &gt;&gt; Thank you. Very very clear. Gabriella, I turn to you uh for closing remarks and uh you and I will probably talk together about what is uh why the global trust

challenge that we that is now open as a crowdsource solution to to solve these problems all over the world is so important. Could you please tell us? Well, uh,

let me just, uh, it's not working. Can I have yours? &gt;&gt; Yes. &gt;&gt; Thank you. Well, let me let me say that uh, I want to finish with a positive

note for our audience. You've heard it. We have solutions. There are solutions to this problem that we're h facing. Of course, the global trust challenge, and we're calling for everybody to come with

their own solutions. you have something to do by yourself in terms of having critical thinking in terms of questioning what you receive in terms of media literacy mer you here with geek

with the AI literacy is so important we have technical solutions and I'm so glad that we are in your hands because at the end what if we just ask when that whenever there is this synthetic content

that Maria Gracia mentioned we have a watermark shouldn't Can we be protected like that when is content generated that is not real that we know that is not and the

technical solution is there age verification is there but there is nothing more important than policies and that's where you hold your countries accountable yes global cooperation but

each country has the tools is not that we live in the wild west we have the legal infrastructure there that prevents misinformation probably in the old age but at the end we need to upgrade it and

we need to cover for the new challenges that we are facing. So you also have something to do because you need to go back to your governments and hold them accountable. We need to go back to the

companies and hold them accountable and to get liability regimes because there's nothing more effective than having a sanction when something is wrong. So, let's just get that right and and and

enjoy the ride in the internet and the AI uh uh beautiful space. Thank you so much for that. You heard from Gabriella. The global trust challenge is a coalition of highlevel

partners. We are all here. We this is a global effort. Uh we we need uh people who to work together and we we're looking to teams who are going to submit solutions uh that are that are at

the same time technical and and governmental and and um we need to work together to to uh promote this challenge. Please come to talk to us. The entire team is here and and we look

forward to talking to you and moving this forward &gt;&gt; and yes uh look at our website. Uh there is the cure code the technology as you could see is there to come to reach us

and to to work with us. &gt;&gt; It's got say the website what is it? Uh &gt;&gt; it's global trust challenge.ai global challenge. &gt;&gt; Global challenge.ai AI.

&gt;&gt; But if you search global trust challenge, you'll find us as well. &gt;&gt; You can come to us to talk to us. &gt;&gt; And please come talk to us. Bring all this [applause]

Did I teach
