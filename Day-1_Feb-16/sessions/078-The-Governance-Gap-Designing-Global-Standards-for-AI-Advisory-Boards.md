# The Governance Gap: Designing Global Standards for AI Advisory Boards

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 14:30 ‚Äì 15:30 |
| üìç **Venue** | Bharat Mandapam | West Wing Room 4 A |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/1eZxDXy6gqg?feature=share) |

## üé§ Speakers

- Julie Owono, The Oversight Board
- Saurabh Karn, Sarvam AI
- Sudhir Krishnaswamy, The Oversight Board

## ü§ù Knowledge Partners

- Oversight Board

## üìù Summary

AI is scaling rapidly across products and borders, yet regulation and transparency remain uncertain‚Äîparticularly for smaller organizations with limited governance capacity. This session examines the minimum requirements for effective AI governance bodies and how ethical decision-making structures can function across organizations of different sizes. Through discussion and reflection, the session aims to surface shared principles, key challenges, and practical next steps to support responsible, transparent, and globally relevant AI governance.

## üîë Key Takeaways

1. AI is scaling rapidly across products and borders, yet regulation and transparency remain uncertain‚Äîparticularly for smaller organizations with limited governance capacity.
2. This session examines the minimum requirements for effective AI governance bodies and how ethical decision-making structures can function across organizations of different sizes.
3. Through discussion and reflection, the session aims to surface shared principles, key challenges, and practical next steps to support responsible, transparent, and globally relevant AI governance.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/1eZxDXy6gqg/maxresdefault.jpg)](https://youtube.com/live/1eZxDXy6gqg?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

AI uh who we have heard we've heard from uh in some of the other panels this morning. So for this afternoon's panel, we're going to focus on AI advisory boards broadly.

And the kinds of questions that we want to focus on is to try and map what a regulatory response to a very uneven AI landscape might look like. It's uneven in several ways, and I'll I'll just go

over a few as a prompt to my panelists to follow. It's uneven in that uh there is a big north south &gt;&gt; u divergence in the development of large

language models and we'll speak about some of that but there's also a large variety of AI companies in the field some very large and and and some very small startups and when we think about

designing regulatory models we We have to imagine this kind of very varied institutional landscape, but we also have a a divergence in the regulatory norms that people might bring to the AI

uh space. So there there are those who might think that what this calls for is strong legislation or strong regulation from governments where whereas there are others who might

persist with a stronger self-regulation model what might bring about self-regulation uh by AI companies uh what kind of self-regulation works is

it self-regulation that is independent that is binding and so on. These are the kinds of questions

that are critical at this point in time. Critical not only in the sense that the technology is at an inflection point. Arguably the political economy of the industry is at an inflection point or

some might say past an inflection point. So this is the context in which we we are having this conversation and we are hoping to spend more time on the question of regulatory governance,

institutional design than on questions of technological governance. I know and I know that many people in this room would would suggest that that the answer is technological

and that if we leave it to the companies to devise default technological protocols and norms that that that will do the work but much of the conversation that we will have in this room this

afternoon is going to be more legal, regulatory and ethical. So that's that's the bias to the regulatory question that we will take into the conversation this afternoon. So the format that we will

follow is broadly that I will give um Julie first the opportunity to to talk about these broader themes and also to advance a description of a model of which both Julie and I are a part of

that we think at least responds to some of the questions that I've raised. then moved to to Sarab to describe what he sees uh a company like Saram AI regulated I mean navigating a regulatory

landscape in India an emerging company in the field how does it see questions u of regulation what kinds of regulatory models might they be um enthusiastic about or or less enthusiastic about and

that's the format that we'll follow roughly about seven minutes a piece and then I will help with a small discussion and then open it out to the audience uh for for a moderated Q&amp;A. So that's the

format. So let me start by inviting Julie to uh get us started. Julie, &gt;&gt; thank you. [clears throat] Thank you very much Sadir. And please bear with me. For some reason my voice just went

out. So I apologize if uh uh if if you do not have the privilege of hearing my very normal and very beautiful voice most of the time. aside um to respond to your to your

initial uh question Sudir about what does a good advisory board look like for um AI companies who want to who care about safety without wanting to sacrifice innovation and profitability.

Uh, I think the model that we have created over the past few years at the oversight board is one that merits way more attention. And what I wanted to do before explaining why, I wanted to ask

if anyone in this room has ever heard about the meta oversight board or the oversight board. Oh, it's pretty nice. Thank you. [laughter]

Um it's a little bit more than what I was expecting but still not the whole uh the whole room. So maybe for those who do not know the oversight board, it's an independent body that was created in

2020 by uh the most powerful social media company in the world which is Facebook at the time and now Meta. And what we do is that we make binding decisions on the content moderation

decisions that Meta has made. So for instance, when Meta decides to take down a piece of content, if you are concerned by that, uh you can appeal to the oversight board and say, "Oh, I'm not

happy my content was taken down, or I'm not happy someone else's content is still available and it's dangerous and it's violating the rules of Facebook." So you can appeal to the board, but the

company itself can also appeal to the board when it is struggling to apply its own rules. And you might not think it, but it's it happens more often than we we might imagine. Um, and we do so uh

through a group of 22 people who are located in on the five continents. Uh so me for myself as my colleague has said I am from Cameroon in West Africa close to Nigeria so maybe more of you will locate

um and I'm also from France uh we have colleagues in Taiwan in Australia in uh in the United States so many places of the world and also we have 20 21 different professional backgrounds

because some of us are law professors some of us are activists civil society organiz organizations. Uh some of us are journalists. We have the former editor and chief of the Jakarta Post in

Indonesia. We also have the former editor-in chief of the Guardian and uh and and and some of us are lawyers. So, it's a very uh diverse uh organization that gathers together to

decide on individual cases. So, individual pieces of content. And what's interesting with the work that we do is that our decisions are binding to meta. So

when we tell meta you should have taken down or you shouldn't have taken down, meta is obliged to apply whatever we tell them to do in the specific case. Now what's interesting is that we're

also able to make recommendations, policy recommendations that the company will apply. Well, it it's not obliged to apply them, but it commits to at least

explore how it can apply those recommendations more broadly on uh all its uh its products and all its platforms. And so

I apologize. These are the two very important elements of a very successful advisory board. Independence. I cannot be fired by Mark Zuckerberg or

Joel Capelan or whoever at the leadership of Meta because of a decision that I've made. Uh we have fixed terms, three fixed terms of uh three years each.

Um and we control our own strategy. We control the members that we recruit. We're not going to be influenced by Meta if Meta says, "Oh, we don't like this member. Don't recruit that person." Uh

on the contrary, that's what's going to make us hire that person. Um but in addition to that our content decisions as I was saying are binding and that is the most important part when

you do an advisory board. What we see usually is advisory board's role are sometimes in the shadow. You don't really know what the company does with whatever recommendations that the board

will give the company. In our case, Meta has publicly committed to apply what we tell them. And even when they're not obliged to do that on in the case of recommendations, they will still uh

explain why they will not apply this recommendation, why it's so challenging, uh maybe it's a you know resourcing problem or maybe it's a priority problem or maybe so many reasons. Uh so it

offers a sort of transparency and transparent dialogue uh on uh the issues that the company is facing. Uh And it's successful because we're not

giving advice that you know are completely &gt;&gt; irrelevant for the industry. 75% of our recommendations have been applied by the company have helped the company be

better, treat better its users, be more transparent and be more accountable. And that's what explains partly why for some of us it's easier to trust some of them the the the products including AI

products proposed by certain companies like Meta versus others. Um another very important aspect that I wanted to touch upon and then I I'll stop there is that our framework is not

solely uh ethical. We don't only look at the ethical angle of the questions that we are that we have to deal with. We look at a very specific framework and that framework is the human rights

framework. That framework gives the right to free expression for instance. That framework gives the right uh to know. That framework gives the right to safety. That framework so many other uh

so many other very important rights, rights to privacy uh which have very specific guidelines uh when you want to apply them. And this is what we help Meta do. We help Meta not only say we

respect human rights, we actually help Meta in the operational aspect respect those human rights on a daily basis. So I'll stop there for now Sudir and we can dive further. Yeah, I mean I Julie I I

do want to come back to uh to hear more about the AI side of the question but but let's let let me bring Saur in first and and and uh prompt Saurab really to speak about for an emerging Indian

company one of the biggest in the in in the uh Indian technological landscape. How how is Saram AI thought about regulation? Do you already have internal mechanisms

that you use in this way or is there some is there something that you find more or less attractive? &gt;&gt; Hello. &gt;&gt; Thank you so much for having me. Um I

think at Serb one of the things that we have done in our journey is to be constantly in touch with the regulators themselves and with the government themselves.

and be very intentional with respect to how we want to build the technology that we are building and how do we want to deploy it with what guard rails and that's why you don't see uh you know

directly consumerf facing application there are like other challenges also around it but I want to pull us back into the reality of you know where we are at right this technology itself is

very very new I mean to one and a half years back if you basically asked a company to gap guarantee that for the same inputs they had the same outputs for a model it would have been very very

difficult to impossible and so while we said that we don't want to touch technology part of it right the technology matured and now there are systems which can guarantee these things

so one we are in a landscape where technology is maturing but I think from a server uh standpoint we already had anchor examples of what safety could look like and so the

Community has done a great job with respect to building you know data sets which tells you that these are the kind of things which are example the negative examples that you want your models to be

aware of and you know techniques to guardrail your models to not give certain kind of responses and things like that. Those are data curation is something that we have worked heavily on

and so we have ensured that whatever data is fed inside the model is of high quality and that so you do not really face the kind of problems that you see in models that were like 2 to three

years old and obviously those models have also uh become much better. Uh but by and large what we have been able to do is we have been able to work very closely with civil society be very very

uh constantly in touch with regulator to understand what are their expectations how do they want to see because ultimately what we are trying to do is we are trying to find out uh you know

what is the best way to deploy this technology for advancement of what and I think as a company we believe that we want India to win and that's why a lot of our effort in building this

technology is is continuing to be for India in that sense Uh so that's a uh that's my response to that. &gt;&gt; Yeah. So if I if I just stay with the

India guidelines AI guidelines for a moment. So the India AI guidelines make it clear that uh that government is going to pursue a regulatory strategy that emphasizes innovation as much as

restraint. So it's the the bias is not towards a a a heavy regulation model. Uh at the same time the the regulatory framework sketched out uh mandates accountability.

Uh the accountability section speaks about accountability at various levels at the entity level uh across the industry, national level and and so on. Um and in a in a competitive business

environment where you you may at this point have a lead in the Indian context with Indian languages and and u large language models in this context but in a competitive landscape

one is one can reasonably expect that the boundaries will be pushed um and you're going to uh you're going to have to deliver both technologically and in a regulatory sense. So is the in the

initial stages uh the approach might be that we emphasize the innovation part but do you see a a future set of step I mean just last week we saw new deep fake um guidelines being sorry notification

being issued they're not guidelines they're binding and uh they seem pretty strong and harsh um by most industry standards around the world so do you anticipate that that regulatory model

will play out okay or do you think there's there's a scope for something else in the Indian context? &gt;&gt; Yeah, I think we have examples of very very strong regulations like uh the

notification that you just mentioned. We also had an incident where uh you know the government did for certain time mandated that you tell us when you are going to be releasing an LLM. But I

think one of the things that we have seen is we have also seen them u you know adjust to what the industry is saying and so it's not one-sided conversation that's happening. I think

you know both sides are reacting to this and it's coming to some level of stability going forward in future I think uh you know companies own governance so for

example we definitely do a lot of like red teaming of models doc fooding of these systems ourselves to be sure of what we are re releasing when you talk about accountability

it's not just at the model level and so in the entire supply chain of you know an AI application You have model, you have hardware providers, you have model providers,

then you have uh people who are building systems and then people who are deploying systems, right? Having an understanding of in what context are you, you know, using these models with

what kind of guard rails is very very important, right? And so you could be using an LLM um you know, in in a high stakes environment where you have no benchmarks or do not have enough depth

of understanding of how it would perform, right? And then the question basically gets back to your liability framework of like uh um um you know how who basically is accountable for these

things right and so each person in the entire value chain has to have certain accountability of their own context and how they are sort of uh uh supporting development of these model with respect

to government. I think uh one companies will uh companies already have mechanisms to dog food these systems internally. I think uh uh one thing I know that these systems are diffusing

faster than social media did and so they will also become very very global soon and so the kind of advisory boards that we would have for internal governance mechanisms

is something that I do not really know just yet what it would look like. There's also an aspect of um um you know like uh these systems are going to get very very complex very very fast right

and so if you look at coding agents or things like that so you have an example of deep fake uh uh but what I also foresee is that a lot of humans are going to delegate their agency to have

agents sort of transact with other agents right and then some of those regulatory frameworks may not really be ready today to respond to some of those situ situations, right? And so

regulators, companies, civil society, everyone will have to have very fast cycles of stimulus and response. And I think I think that's the key part, right? I think all three parties in some

sense are trying to get to benefits for the society. And the only way we will be able to actualize it is by ensuring that the information is not uh is is passed on at at the right time and whatever is

mitigatable gets mitigated fast. &gt;&gt; So so thanks Sarup. So you're you're emphasizing that that regulatory models will have to be nimble. Maybe even regulatory institutional design will

have to be a little different from from what we've seen. Standard departmental regulation uh might might might a take too long and then might end up on the wrong side because of of of the quality

of uh technological understanding behind behind these regulations. Uh Julie just to come back to you to to push on this institutional design question right u so so two elements one I wanted you to just

speak a little more about the AI side of the question the oversight board was initially set up to deal with social media content um but we've ended up dealing with a lot of AI content and AI

inflected content which is a a big part of the issues of the day but thinking beyond that to the question of institutional design. Um Sarab mentioned that if it's not state regulation and if

it's not a just a private uh company regulation then what would it be? What could it be rather and these are both interesting questions which we have some evidence on. I mean nothing is decisive

or conclusive here but at least there's some evidence on what kind of regulation works. Julie. &gt;&gt; Yes. Um I think it's it's as we are, you know, entering or we are in the midst of

the AI revolution, it is important to take lessons from the past, from the recent past. What happened with social media? We were all very excited. Oh, it's amazing. We can connect. And then

very suddenly we started realizing that there were bad things happening also on social media. Um aggressions towards children uh women minorities um hate speech disinformation and so many other

harms that are being discussed. Uh so if there is one thing that we can probably learn from that as we are in this AI era is that in inevitably uh all these great benefits will come also with potential

problems. the advantage and maybe to res and to respond specifically to your questions uh Sudier and to pick on pick up on what uh Sarab was eloquently presenting maybe the solution is in

between because on the one hand indeed regulators and that is one of their most important criticism they are sometimes very slow or they don't understand innovation that's another criticism that

we hear uh and in in that sense it's very reactive Whereas um in the case of the oversight board and we've seen that uh we have had examples in which we have been extremely

proactive even before the regulator even uh saw uh a problem uh or imagined a solution. One example is uh three years ago when Chad GPT came with this big uh big announcements. Um, at the time

everybody was concerned about, oh my god, how are we going to make sure that what we're seeing uh is really is either AI or it's the reality. &gt;&gt; And at the time, every company was

saying, "Oh, no, it's so impossible. You know, we don't have the technology yet to uh decipher to identify what's uh AI and what's not." One of the cases that we received uh was related to uh the

former president of the of the United States, Joe Biden. Uh in this specific case, it was not AI p per se, but it was uh a modification of images of visual images. Uh and in this case, it was an

image that pretended to show him doing something that he was not doing, being accused of, you know, molesting children, etc. It was not at all true. the video had been doctorred to show

that or exaggerately um doctorred to show him doing something illegal. And uh what we saw at the time is that the policy of meta was absolutely not adapted and prepared for AI. It was just

uh at the time the policy was just covering um um certain certain type of modifications. It did not cover um you know modifications to the extent that you would think that something well it

did not cover defects for instance. We're all talking about def fakes now. Three years ago we were all warning about it but there were no real uh rules including within companies to deal with

those. And what the board did is uh we made a series of recommendations to the company in that specific case. So of course we told the company to take to take down the content that was showing

the presence so doing something he didn't do and in addition to that we told the company um we have consulted with stakeholders because that's what we do. We don't make our decisions just on

our own. We rely heavily on collaborations and conversations with u stakeholders such as everyone in this room. And by the way we have a case ongoing and you're all welcome to

participate in that. I'll give more details later, but we made a recommendation based on our interactions with AI experts who told us it is actually possible to allow either people

to label AI themselves when they create AI or the company can do it that they have the means to identify some form of AI generated content. Meta did that and the result was spectacular. They had

billions of interactions with the AI. I don't know if you remember at some point you started seeing on Facebook products, Instagram for instance, you could see uh AI generated or you would be asked

yourself to say whether this whether you're posting a generated content. Um that label was interacted uh with by billions and billions of users. I think it's three billion. I don't have the

it's so huge that I don't even have the figures in in mind, but it was in the billions. And what it showed us is that people are eager to understand. There was a thirst for understanding how can

we identify AI? What is even AI generation? And that label led to more information provided by Facebook to external sources sources or to internal sources within meta. And Meta was the

first company to do that three years ago and now everybody does it. So I think it's a little bit in the middle. Do you want to say something about the institutional side of the question? So U

Sab mentioned AI safety boards and several global companies have set up AI safety boards. Some have set it up and dismantled them. Um there there is a push towards AI safety institutes of

some sort sort of quasi governmental uh AI safety institutes partly policym recommendationoriented kind of institutions and then we have the oversight board structure which is

not a AI safety board in in the way that some of these have been set up not a AI safety institute something quite different so a little bit of attention because my my sense and I I I'll just

say two more sentences about this that you know questions of AI regulation and fate frameworks for example we've been talking about it for the better part of nearly a decade um that we've been

discussing what should the normative principles for AI regulation be and I think the real challenge I mean I know Sar said that it's kind of experimental technology but sort of at the scale of

growth of AI and the scale of impact and market depth really puts it out of the realm of I mean not at you're not at beta stages I mean you you might be and you might be announcing a model shortly

but the industry is really at at scale already so I don't know that we can take that this is an experimental technology approach to the quest and we might have to say yeah the technolog is out there

it operates at scale we'll have to choose some reg make some regulatory choices now um before things go dramatically out of hand and and you know we're picking up the pieces. So

that's my sense of the timing question. But Julie just to prompt on that institutional question before I come back to Sor. &gt;&gt; Yes, if I can maybe share some lessons

learned um if you want to build a successful AI advisory boards um here are the elements you have to keep in mind. &gt;&gt; [clears throat]

&gt;&gt; First of all, boards where companies retain membership control tend to not be as successful. Advisory boarded bodies that do not have uh enforcement mechanisms tend to be not as successful.

um governance structures that can be dissolved at the sole discretion of the company or even the government because they're also uh advisory boards set up by governments is also not a board that

is uh likely to succeed in its mission. Um another very important aspect lack of transparent reporting or on implementation is also not good. You have to be able and this is something

that the board does. We report very frequently, quarterly on where we are at in terms of implementation. Meta, we recommend this. We recommended this to you one year ago. You said you would do

this. Where are we now a year later? Um, so that was for the don'ts. Now for the dos. Um, absolutely critical. a rightsbased human rights based uh decision making framework ethical and

human rights based I really want to insist on that secondly protected independence third make sure the communities being served are represented as I said we

don't make decisions just on our own we work and consult with stakeholders from civil society organization non-governmental organizations as well uh and and many others we have

transparent operations We publish our annual report every year. You can read how many decisions we've made, how many recommendations were applied by the company or accepted by the company, how

many were rejected. Also, this one is a very interesting piece of information. And another very critical and last critical success criteria is making sure they are

meaningful company accountability mechanisms. So when we say something is not working within the company, what happens next? That question is fundamental. So

&gt;&gt; thanks Julie. I'm I'm just going to go back to Sur to um to to go at this question of state regulation uh baselines and then company self-regulation models which is the

direction that the India AI guidelines also point towards and so we'll have to evolve something which may look different but it will have some combination of these two elements. So

clearly the the state will set some default liability rules across the sectors uh maybe topical on some issues uh and then companies will have to do the rest uh and work out some way of

being responsive to your users to the public at large because the nature of these technologies is that you're is pretty much going to be open to anybody to come and use. So you have to account

for your users's actions in in some sophisticated way uh because they're using technologies which are not very deterministic. So your the kinds of outputs that they generate would be u

quite wide. So coming back circling back to the Indian context and the context for this summit because a part of the uh motivations for this summit is to say that if we have uh inclusive and uh you

know sort of an AI for all uh kind of model what would that model look like? It would clearly have a regulatory framework. So just to come back to you I mean I know that and I don't want to

make it a Sarbam question. It's not a Sarbam question. It's just in the Indian landscape. Where do you see this going? &gt;&gt; Yeah. I I I think very broadly you would have regulations, you would have laws

and you would have guidelines and you would have self-regulation, right? I think all of them are going to coexist. I think it's about like striking the balance, right? with the regulation the

uh the real challenge and the opportunity really is like how surgical can you be about issues right and so let me give you an example if you really wanted your platform to not have any AI

generated content uh images right you should ask all hardware manufacturers to sign the images and verify it and only then allow them to upload it on your platform right and then assume that

anything else which is not hardware signed is AI generated Right. And so sometimes it's it's about like you know how practically can you implement some of these things and that's why probably

the regulations of the future will have to be techno legal in that sense. Um that that is that is one aspect of things and there there are there are definitely regulations are going to set

some baselines internal governance mechanisms are going to proactively ensure that the companies do not get in conflict with the laws. I don't think anyone really wants it but I think uh

industry standards could play like a very very important role right like um you could have certifications you could have certain process uh audits uh that ensures that safety has been taken into

account right like similar to how you do your information security um uh checks and those kind of certifications could signal both to the market and create obligations within the organization

to ensure that known mechanisms of ensuring safety are not let go of when you are building some of these systems and the accountability again as I said right like varies across the value chain

between a model developer system developer person who is basically deploying it to communities and I mean I just want to also echo the fact that none of this can happen in the isolation

of a company right and that's why company civil society regulators will all have to have very fast cycles of like you know conversations on how to deal with the current situation.

&gt;&gt; Yeah. I I want to just you you mentioned hardware um hardware manufacturers taking some kind of responsibility here. Just one last question before I open it out to the audience. You you'd spoken

earlier also about uh technological models of reg regulation. I know that that's not been the focus of the panel. We focus more on the legal regulatory structures. But if you were to cast your

eye on the technological models, is it like the red teaming frameworks that you have in mind or is there is there some other kind of more promising uh framework that you think might do the

job? I think I think standards which are uh composite of many things is really where it'll go to right like so for example you would need to do benchmarks you

would need to do red teaming you would need to do sample set this again varies by the size of company right like companies who are um you know in more than 100 countries who have really large

uh deployments right and companies who are very very nent because just to pull you know uh put us back into the context we are trying to do it in the context where innovation doesn't get hindered

right and so setting some of those standards aligning with the maturity of an organization and these standards being compound of many known processes that we can take for uh you know safety

and security is really where uh we would have to go some of those mechanisms are going to be technological right like for example benchmarks right uh if so uh let me give you an example if you go to

ministry of education's website uh you go to Bodhan AI you would see like a bunch of education related benchmarks right and so uh by releasing some of these benchmarks by actively funding

fueling be it civil society organizations or researchers to set up some of these these baselines would allow for developers also to build compliant AI and those are not purely in

the realm of uh regulation as much as in the technology so uh what we we have to keep the goal in mind that we want to build safe and trusted AI Okay. And we have to build these composite systems of

safety where which is both a mixture of like governance mechanisms where you are able to keep people independent who are able to sort of identify issues and report it and get it fixed but you also

need to provide enough tools to for developers to sort of uh you know fix these things. I I'll just give an example uh to put it in more in perspective, right? Let's say the

governance goal is to ensure that all websites are WGCA compliant. That basically means any person with visual disability is able to access these websites, right? Now, pregenai era look

at all the websites and you would not really uh you know find appropriate tags in the HTML and then they are not accessible. Right? And so if you agree with me that the future websites are

going to be built by these models which can write code right they do not by default put out a WGCA compliant code. You either have to prompt it and things like that. But if regulator basically

comes and instead of like passing a notification or a judgment incentivizes the society to build a good benchmark a good data set. Now certainly what you can ensure is that all websites in the

future which are built by AI models are WGCA compliant. Right. So there is both an aspect of exponential inclusion and um you know um alignment with the regulation as well as exclusion in that

sense. And that's why this is a very unique situation where you will have to think both technologically as well as uh you know governance mechanisms that allows people to sort of have control uh

independently. &gt;&gt; Yeah. Thanks SUB and and Julie. So we have time for questions. So I'll open up to I'm going to move them move the mics around. Uh my colleagues have some mics

who uh to get us started. Uh can I start with you first because you had your hand up first. First off the mark please. &gt;&gt; Hi my name is Joti Pande. Um I work with Georgia Institute of Technology. Julie

we've interacted before. I've been involved with many of these multistakeholder content governance mechanisms. Um I've just conducted a study comparing uh the GIFC, the Christ

Church advisory network and the oversight board and uh developing a classification criteria to assess how advisory mod you know advisory bodies function and what are some of the uh key

key criterias that are necessary to make them function um in an effective manner and um so I have three sets of question for each of you. Um uh Julie to you uh you mentioned of all of the benchmarks

that advisory panels need how important is funding because uh you know like you mentioned the oversight board has a lot it's has scale it's uh spread out over continents but it's obviously got there

with Meta's big uh upfront commitment and um how does that um considering that Meta as a company has now shifted and said that we're going to take a hands-off approach content moderation

So you know that also throws up a lot of these funding into a little bit of you know there's no inevitable future and also broadly um Sudi to you and also to you sort of very interesting points

about um the transformation of technology um how are liability frameworks keeping up with this technology framework so for example um I've worked on Manila principles I was

one of the authors of the Manila principles and at the point at that time we were thinking of liability frameworks and approach approaching it from the perspective where platforms are not

really reckoning uh or dealing with content directly. There is a benchmark or established criteria of actual knowledge which is when they're being notified of illegal content or some kind

of you know ill um nonconformity with the law and that is when they have to take action. But we're now moving into technological environments where there has to be a sense of predictability. So

how do we then um approach liability frameworks from you know that very notional broad um shift that is undergoing. Thank you. &gt;&gt; Thank you. U we we'd probably take two

more questions. So no sir. &gt;&gt; Thank you. After listening to the discussions by all of you, my attention was arrested when I heard the human rights you just

mentioned human rights plays a very important role in contextuality to regulatory reformation. In that context, the human rights plays a very important role. So far AI is concerned because I'm

man of English language and English literature. I teach students going to abroad. I have AI students also they come. Therefore my focus is that when we speak about advisory content, advisory

governance and when we speak about strategic regulatory reform as per India AI intelligence supports whatever the panel in our government of India has been given about AI is strict about the

governance internal and external where the AI impact should have allegiance. should have collaboration with human intelligence also in order to have internal governance properly in the

field of AI and artificial intelligence. What I what I want to say with full clarification that assimulation assimulation of intelligence combined with associative intelligence

leads to artificial intelligence for the growth of humanity and that is welfare for all and happiness for all. Uh hi uh my name is Vicasha. I'm currently working with the government on

the summit. Um I actually wanted to ask a question around this because at a previous organization I'd worked on a ethics committee for an AI company at the time. Uh what I wanted to ask is

that what is the so for instance what is serving doing and what is and in the oversight board like in what would be your preferred way for going forward for enforcement of a policy within it right

so suppose you have your policy that okay look we don't want our AI to be used for XY Z purposes we're going to make sure these guard rates are there how do you actually make sure that that

works out within your company itself because you know there's going to be competing interest between the business side between the product side between sales so how do you what's the kind of

mechanism which you guys are thinking is there in place and where does that fit in the institutional structure of of a company to make sure that you're able to actually follow uh your own internal

rules and guidelines around it. &gt;&gt; Yeah, thanks Vasha. So Julie, maybe one round of responses. We have time so I'll come back. &gt;&gt; Uh thank you very much. Maybe I'll start

with uh your question here uh about the funding model. Um so the money that the company put was is in a trust. So it's a trust that um manages all the the money and how it's being

spent and it we directly interact with the trust um at the oversight board. Now that does uh pose a very important question. What's the sustainability of such model? And on that we have started

we are reflecting on whether it would be important for the industry as a whole because what we've se seen with the board is that the decisions that we make are not just applied by meta we've seen

other companies apply them just not saying they do um and what we see is that there is a a shared interest within the industry for bigger companies and for smaller companies um to sort of

create industry standards as you were talking about earlier um because it it it actually it benefits everyone. It it uh it it it helps get rid of the uncertainty that comes with uh sometimes

with innovation. So uh some of the things that we've started thinking about within the board is whether they there could be shared governance um infrastructures between you know

different companies um whether they could whether industry associations could actually have a part of their responsibilities become a sort of governance structure um support for

including small AI startup who want to do the right thing but don't necessarily have the means of a meta or or the like so these are things that we are exploring and um the last question sorry

I will go to the gentleman's question uh how do we ensure the enforcement of policies uh one aspect that I can um that I really want to insist on is the constant dialogue that exists and the

implementation side of the work of the oversight board that we have been doing with meta. There's a constant dialogue. Uh we have frequent meetings. We said this a year ago. Where are you? Why this

is not happening? And this allows the company also to share the difficulties that they might be facing which we might not think of as an oversight body because we're uh you know so impatient

to see our recommendations applied. But uh there are realities that we also um u need to be aware of. So I would say a constant dialogue and close collaboration between the stakeholders

in the oversight body and stakeholders in the company body. So on the question of liability framework at least till last month I thought that liability framework is like

you know uh there is nothing I mean like the technology is unique but if you look at it from purely from the liability framework from a product liability framework I I think it adequately covers

what you have today right but what I'm also seeing now happen is that uh you increasingly have these agents who assume your agency and then act in in a virtual world on your behalf, right? And

so the kind of intents that humans are passing to these agents are very high level, right? Like so I could pass an agent that go and make 100 rupees for me, right? And I will also assign it a

wallet from which it can actually make transactions. So it can send in someone some money and accept some money, right? And uh and when this happens at a sca by the way right like you don't have

virtually any cap you have the compute cap in terms of the number of agents that can exist right and so is it sufficiently are our legal framework sufficiently evolved to carry to you

know govern everything there right like do you attribute all the mistakes to the end user who has delegated agency I don't know so the answer to your question is actually I don't know and I

think it'll be a interesting exploration to see like because This intersects with everything right like it intersects with your finance it intersects with your you know research on chemistry biology

everything right like and so so I don't know is the most honest answer that I can give you and I think that is why it needs to be explored with respect to uh you know how do you ensure that the

companies are actually following these guidelines right I don't think companies are um it's a unique problem uh uh in in the sense that you know companies are not malicious malicious entities who

don't want to comply for the uh for the heck of like you know just not complying. In fact, we do. In fact, that actually benefits our business. Right? To give you an example, if you're not an

ISO 27,0001 SOC2 type 2 certified organization, it'll be an extremely hard sell for you to sell whatever AI you have to BFSI. Period. Right? And so now imagine if there's a company which

basically says, "Oh, by the way, my models basically generate really good marketing content, right? which you could use to sell loans, sell insurance, educate uh your end user about financial

literacy and etc etc etc. And there is a regulator or a academic body which has come up with a very good benchmark to assess the uh veracity and the safety of these images and one company has that

certification and the other one does not. Right? You have to think of it from a very economic standpoint whether the incentives are aligned for the companies to comply or not. Right? So companies do

prioritize um for the business sake actually to comply by these standards and that's why if you have like really good standards built again both technological as well as governance

mechanisms I I don't foresee any challenge in companies not doing this if you have very very difficult governance structures imposed on companies right there is there there needs to be space

for the dialogue to happen and I think eventually everyone wants the economy to grow for the people to benefit and that's that that is how you reach the equilibrium. So my succinct answer to

this is companies do comply by standards build good composite standards we will comply by it and we will report whether we do I mean you can say like you know uh disclose your data and maybe that is

an option you can also say that you know an external auditor can come and you know look at all of these systems right that happens in case of information security quite a bit right but these are

not mature just yet and that's why there is work that needs to happen in defining these standards and companies are willing to comply by them. &gt;&gt; Yeah. No, I like the analogy to

information security kind of regulation which is a sort of you can say analogous field though my sense is the analogy breaks down pretty quickly uh with AI. So I I'll I'll just mention that I don't

want to get into that in greater detail. I just want to speak to the comment from the back about complexity of organizations. Clearly uh external independent regulatory bodies allow for

better communication within large organizations. Most organizations are uh complex uh at least large organizations are complex. They're poly vocal. Different people have different

objectives. They even have different normative frameworks that they operate within. So so in in a sense external independent oversight allows for better conversations. I don't think it's linear

in that it means that only one one kind of line of one view gets to drive the day. It never ends up like that in large organizations. But I suppose that independent oversight

allows for that for better communication all around even within companies. So I'll start at the back. I think we have time for I I'll say two. So I'll just start with

No, no, I'm I I I'm sorry I can't come to you. Yes. at the yeah person in the yellow if Karolina can just get a mic. Uh because you had your hand up right from the start and then you sir and

we'll stop. &gt;&gt; Uh yes. Yes. But I can't have all three people from the first. Yeah. Thank you. &gt;&gt; Ah &gt;&gt; hi. Thank you. Um

&gt;&gt; so I have kind of a two-part question &gt;&gt; and you'll have to be really quick because we have all of two minutes. &gt;&gt; I'm actually uh currently an ISO 420001

auditor. So this is what I've been doing is uh auditing companies with respect to that. Uh my question is at the interface of like content moderation and this kind of auditing because like very recently

in the news we saw that there was uh a social media which was for AI with AI the uh by um openclaw I think was the one that created. So &gt;&gt; let's go on.

&gt;&gt; Yeah. So I um uh my question is in the regulatory and governance framework what are we going to do about I'm sure new ones are going to come up like I think multbook uh

there there's going to be new platforms that are going to come up so in the governance and regulation framework how are we going to go forward with um platforms like moldbook

&gt;&gt; yep thank you and then yeah just the person in front of you we have Karolina Thank you very much sir. And uh time is short but I would be quick. One that I compliment you for trying to regulate

the unregulated. So so you have a tough job and you deserve all praise for that. I have very two quick questions. One is that the very fact that you are an advisory board. I would like to know

that the company is not obliged to reg to implement your advice. They may or they may not. And we see blatant violation taking place. So how many times your advice have been rejected?

That is question number one. Question number two is about regulation building up standards. I have worked I'm SD sax. I worked in telecom for almost 40 years and was involved in telecom revolution

in India. I worked in ITU. I was chairman of a group. Can it be outsourced to to organization like ITU or UN for building up standards where the regulation will have meaning because

the countries will then be obliged to implement it? Thank you very much sir. &gt;&gt; Yeah, thank you. U good questions. So Julie, do you want to go at the international uh institutions regulation

ITU UN question as well as maybe the enforcement question please? &gt;&gt; Um yes. So on the international level, I do know that [clears throat] there have been um convenings about at the

international level about uh regulation of uh um AI and and actually UNESCO was one of the first organizations to adopt an international text on the ethics of artificial intelligence that was in 200

uh 20. So um I think it's very much open um and and and it's important that indeed those international organization governmental organization have a a role to play. We

see uh the the UN working group has done a fantastic job in surfacing so many of the issues that we're dealing with right now. So yes, very much agree with you um on the um uh what was the first question

again? Oh yeah. No, our decisions are binding for the company. The recommendations although they are not binding, the company must respond to those publicly.

They say we cannot implement, we will implement and win. &gt;&gt; So so just to clarify the the distinction that we tried to make was between an advisory board that's purely

recommendation based and an oversight board of which decisions are binding. So uh a part of our decision-m framework is binding and I think that's the point that was being reflected

&gt;&gt; it's only for the meta platforms Facebook and Instagram but sab just a just the last point yeah uh I I wanted to &gt;&gt; book and other

&gt;&gt; yeah yeah no I I I'll just tell you a very interesting anecdote right so what did we see in last one month is a social media for agents right and another very interesting thing that happened is that

on a particular GitHub repository which was an open source code and AI tried to contribute and humans basically shunned it out right said that it's only meant for humans to contribute not for agents

right and now in a hypothetical scenario if this agent basically goes to a social media platform uses API to basically you know bash human a particular human who has basically shed out right uh and the

user has just put this open claw on their MacBook Pro and this agent is completely completely autonomous running there. Would you consider that a user generated content and hold the user

liable right and some of those questions are not very easy to answer? I mean yes from the current liability framework obviously you know it's your MacBook it's your agent you prompted it it went

there it wrote something right but some of these things are not very very clear today and so um it's very hard to predict future and sort of like do those kind of things and that's why I uh I

don't I don't find a good anal analogy in terms of like how do you regulate it of of course you need these you know uh governance boards who can guide these things but I feel humans themselves are

like quite inadequate in terms of like you know predicting exactly what would happen right I don't think anyone expected that you know you would suddenly have these social media

platforms for agents and things like that so you need to continuously uh sort of uh that's exactly why I keep coming back to it right our rate of response to stimulus is really

what we need to optimize for right and uh that's where we will find unique solutions to some of these problems and we can't wait for for all problems to appear and we also can't be in a haste

to predict what problems we want to govern for. &gt;&gt; Thanks Sarab. Can I just invite everyone to thank our panel for this wonderful session [applause]

and thank all of you for your patience. Let's hand over to the next crew.
