# Whose Language, Whose Model? Public-Interest Multilingual LLMs

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 12:30 ‚Äì 13:30 |
| üìç **Venue** | Bharat Mandapam | West Wing Room No. 6 |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/2CQfT7LsTrA?feature=share) |

## üé§ Speakers

- Aliya Bhatia, Center for Democracy & Technology
- Dhanaraj Thakur, Multiracial Democracy Project
- Jhalak Kakkar, National Law University, New Delhi
- Marlena Wisniak, European Centre for Not for Profit Law

## ü§ù Knowledge Partners

- European Center for Not-for-Profit Law (ECNL)

## üìù Summary

LLM development is critical to the public interest in the Global South, yet decisions about their development and use remain concentrated in the Global North. This workshop brings together global civil society, NLP researchers, and technologists to inform responsible LLM development globally. The AI Summit hosted in India this year offers a timely opportunity to convene groups that often work in isolation and to advance questions of language access and global AI governance, including systems.

## üîë Key Takeaways

1. LLM development is critical to the public interest in the Global South, yet decisions about their development and use remain concentrated in the Global North.
2. This workshop brings together global civil society, NLP researchers, and technologists to inform responsible LLM development globally.
3. The AI Summit hosted in India this year offers a timely opportunity to convene groups that often work in isolation and to advance questions of language access and global AI governance, including systems.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/2CQfT7LsTrA/maxresdefault.jpg)](https://youtube.com/live/2CQfT7LsTrA?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

that we are can't be joined by our facilitator, our main organizer, Marina Wiznjak from the European Center for Not for-profit Law. But no worries, we have an incredible session planned. Um, and

so to kick us off, my name is Ali Bhhata. I am the senior policy analyst at the Center for Democracy and Technology. We're a Washingtonbased notfor-profit nonpartisan group. And

today we have an incredible set of speakers who are really going to contend with this question of what does it mean um to build to to to foster more meaningful multistakeholder

participation in the development of AI really across the life cycle of AI when we're thinking about um AI systems from conception and design to development to deployment and post- deployment as well.

Um, and we're going to start us off with a few sort of provocations and thoughts from our esteemed panelists. Um, and then as I mentioned, sort of kick it off to the group to think deeper about

either ways we can foster that participation. Um, but also maybe initiatives um that in the room led by organizations you represent um to sort of learn and enable that sort of cross

sector collaboration. Um and um we'll also be inviting folks to report out after each of you sort of break out into this discussion. So please amongst yourselves choose a few facilitators

will be joining the groups as well to sort of see these questions. Um but before we start you know why is this important? As we know, large language models and other AI systems are already

being procured and used and sort of deployed in multiple settings um in public and private spaces. Um but more often than not these conversations are being led and sort of um centralized in

the global north in industry or G2G government to government spaces and that loses out on a lot of very valuable expertise both from civil society um and sort of policy experts um but also a lot

of experts who are the most experts and people who are the most affected by the deployment of AI technologies. And so today we want to talk and spotlight a couple of initiatives that are bringing

these um these groups into the conversation. What are some of the considerations we need to have in mind when we try to foster this meaningful participation towards what end are we

moving? you know what what is participation for and how do we make ensure that it's not a box checking exercise and then what are policy and regulatory approaches to actually

incentivize or put teeth to these efforts um by either giving uh more standards for companies to adhere to and model developers to adhere to but also um enforce some mechanisms when they um

to en ensure that when they don't we have actual means to um make them do so um so first I'll introduce our panelists and invite them also to give a more in-depth um introduction of themselves.

Um to my right we have Jalak Kakar who is the executive director of the center for communication governance at the n national law university in Delhi. um Jalok has written, co-written and also

steered the development of a lot of resources of uh evaluating policy approaches to the governance of AI systems but also thinking critically about you know sort of who are stress

testing these claims of who these AI systems sort of purport to help and in what contexts. Um and then we next to him next to her we have Danj Ther um who is the sort of inaugural professor

professor um and director of the emerging technology initiative at the George Washington University and I believe the George Washington University Law School as well. Um so Jalik I'm

going to start off with you um to talk us through a few of the sort of policy or regulatory approaches where we are seeing meaningful participation sort of come up um and what these governance

mechanisms either through laws and policy but also soft laws uh voluntary commitments etc. you know what do they incentivize how have they been effective and where are we left wanting more?

Yeah. Um, you know, in in any new space, there's always a trajectory of figuring out what is relative consensus and where can we start to build norms

that then build into sort of voluntary commitments, other forms of soft law which start to then get codified in the shape of standards, in the shape of regulation,

in the shape of other sort of international binding commitments and there's always like a process and trajectory to do that. I think um there's value in having multistakeholder

conversations to start to build that consensus. But if we look at uh past experience with say social media regulation, I think we stayed in the

let innovation happen, we're building consensus phase for too long. Um and I think there needs to be a cut off point where we start moving beyond voluntary commitments to more hard forms of law

whether that's standards uh whether that's you know um sort of um laws and regulations or even which which may not even mandate particular sort of outcomes but may

mandate processes at this stage. Right? What are processes you need to integrate into the design and deployment of these systems so that we are able to at least surface what are the kinds of harms and

challenges uh that we are seeing because if we don't make that shift over the period of the next couple of years from voluntary commitments into harder uh more uh structured forms of regulation

we will sort of may see a repetition of what we have seen with the history of social media where um you know platforms ran free. There was no mechanism to tr hold them truly accountable

uh to these commitments that they were repeatedly making in various forums and and we've seen the sort of mess that we have sort of landed up in because of that. So um yeah I think I'll pause here

and then you know we can come back for more. Yeah, I think that's a really good place to pause because I think um it's a good sort of where we've come from and now when we're trying to build towards a

vision of more people to have a role in the development or in the development of these new technologies but also truly saying when they when it doesn't work for them you know what does it look like

to have the seat at the table I'd like to bring Dhan Raj into that part you know what are some of the effective characteristics ics of meaningful external and multistakeholder

participation. You know, what are we talking about when we're saying we need to have a voice in the conversation and and who who is this we? [clears throat] &gt;&gt; Great. Thank you. Thank you, Alia. So,

uh also thanks to the uh organizers to provide a space for this kind of discussion. Um so when we talk about uh meaningful participation and how we can uh

effectively engage communities in the design, deployment uh and evaluation around AI models, um I think it's important to recognize that this is a an issue a problem that is not new. Um in

the same way Jelik was referring to like lessons from the social media regulation world, there are lessons that we can draw from when we look at other technological interventions and how they

their practitioners and policy makers have sought to uh include uh uh wider communities, targeted communities and so on in these questions. What ultimately emerges though when you look at these

past lessons is a question about power. It's a question about the distribution of power. When we talk about part participation and collaboration and building partnerships with communities

with different uh constituencies, we're in effect talking about how powers are distributed and what kinds of decisions are made made by who. Uh for a long time when you look at the the field of

international development and uh how countries in the global south or the majority world would put in place different kinds of development projects. What emerged from that those kinds of

lessons uh was characterized by a scholar named Chambers back in the based on his work in 1890s is in the supply chain in the thought proc process putting them first starting with their

needs and their concerns. That can ultimately mean for example communities, target groups defining what technology use cases are best for them which is the reverse of what we see now where

frontier model companies uh deploy models with the idea of um let people figure out what the use case is best uh for them um after the fact or evaluations after the fact. Uh another

thing that uh emerged from a lot of that work around effective community participation is recognizing that communities actually have a lot of expertise uh to to analyze their local

context and understand what's really needed. Uh and so when we think about the development of language models for example, how they're developed, what kind of specific contexts, what kinds of

uh linguistic uh uh concerns should be included in development of these models. um that is often where that kind of analysis can can lie. So uh that's kind of a high level but I just wanted to uh

bring bring in this issue that there's a lot of uh debate and research around this kind of uh what are the mechanisms of effective participation that precedes AI but we can learn from that uh without

necessarily needing to start from scratch. No, I think that's really helpful level setting to begin with because I think sometimes in the AI policy conversation there is a lot of

ahistoricizing going on of like acting like the development of these technologies necessitate is you know so complex and like uses this new jargon and

necessitates only technical experts in the room although we've been here before. We've built other sort of incredibly complex privacy-enhancing technologies, for example, with

journalists and human rights defenders and others in the room. And um your framework of like putting those people who are considered last first brings to mind a framework that Wikipedia has used

um where instead of human in the loop which is like a phrase that's often um used in a lot of these contexts where they try to say oh let's have human oversight at every stage of the

development or testing of uh a technology. They say actually we'll have machine in the loop. We're going to go to people who have these jobs where they're editing individual Wikipedia

pages and ask how can we actually make your life easier? You know, like what is one task that we could automate? And that's a totally different paradigm shift from I think what we're seeing

right now in the development of AI technologies where we're sort of retrofitting a problem to solve after the technologies built, especially in the development of sort of general

purpose systems. Um, and so one thing to really think about, you know, I'm I have one last question for this group to tee up the conversation we want to have with the audience. Um, but one thing to think

about is also, you know, in contending with AI technologies, um, you know, what do we mean by AI technologies? there's a lot of um bringing in people after the fact to

test a system but there's a lot of work being done especially in the global south especially in the African continent thinking about actually there's an expansive way of thinking

about the development or the design of AI technologies that don't just overindex on large language model infra uh architectures you know we could have smaller models that you know take into

consideration the compute and the electricity constraints um and build different architectures there are ways to so you know so I think I think what that's an example to suggest that there

are ways to bring in experts at every stage of the development of technology and the testing of technology and beyond just you know hey we have this thing come you know tell us if we did good or

bad um so to to that question of like hey did we do good or bad jalak um you know who should be saying that like who should be accountable in um ensuring that structures to incentivize

meaningful participation and and even beyond just meaningful participation redress concepts of address concepts of right to refusal you know who should be accountable in both shaping that and

then enforcing that &gt;&gt; I think all stakeholder groups have to play a role in this right I think uh you know industry has to be accountable for you know what it does but those norms

have to be shaped by you know civil society, academia, governments, uh multilateral bodies, uh governments have to be held accountable for sort of DPI infrastructure that uh you know they are

creating u for the norms and rules and regulations they are framing to hold industry accountable. Um and I think you know civil society academia needs to

sort of level up and ensure that you know we are engaging with the technical aspects of the design of these systems and not having conversations at a 10,000 ft level but are really getting into uh

the nuance and I think you know communities and grassroot communities and representative organizations have to be involved and I think we need to sort of dig deeper on what we mean by that

community involvement because I hear a lot of conversation about involving communities but you can't just like put a community in a room and tell them you know give us feedback on the system so I

think there has to be thinking uh that is really developed around what does that community engagement look like so that it is not a checkbox activity but it is a meaningful activity right and I

think we have to identify at what points along the AI life cycle you know which of these actors can play a meaningful role because not all actors can play a meaningful role at all points

of the life cycle. My brilliant colleagues at CCG have just put together in a report that we are releasing tomorrow on you know AI safety in the life cycle like um you know at various

points in the life cycle like what tools can you use who should be involved how can different stakeholder groups be integrated and I think it's it's important to recognize that at different

points different stakeholder groups need to play a role and if we're involving communities at different points maybe the way we structure that engagement is a little bit different and it becomes

important to involve communities because we're not actually only talking about multilingual systems. We're talking about multicultural systems and uh you know uh languages are very often

correlated with uh different cultural contexts and we have to build all of that in because if we only look at it from the lens of multilingual systems I think there's a whole bit of like social

context that we're going to um uh really lose out on. I think that's a perfect segue into Dharaj my question to you and one thing I didn't mention in my introduction of you is you know most

recently we work together and you sort of [clears throat] steered um a sort of groundbreaking collection of four reports on how trust and safety systems work in low resource languages and low

resource languages for the folks in the room is um those languages where there isn't a lot of training data um and high quality um resources to train AI models in. Um and so a lot of model developers

sort of use either synthetic media or poor representations of a poor sort of scanned representation of the language to train these systems. And you also um wrote specifically what you know the

only indigenous language we examined which was Ketwa. Um so based on you know that work and all of your other works you know what according to you are the sort of main costs of not um engaging

both language speakers but also contextual experts subject matter experts stakeholders that are most affected by the deployment of these technologies you know how can you know

what are the costs both macro micro individual etc. Yeah, I think this is a important question in the sense that um and and I think Joel started to allude to this that when we think of language,

we're not just talking about a means of communication. We're talking about people's identity, our identities. Uh we're talking about uh means through which we identify

uh and express who we are. uh and one of the issues that came up in this study of Ketwa. Ketwa is an indigenous group of languages uh throughout South America with over approximately 10 million uh

speakers. So this is a large uh significant population. And one of the issues that came up through that empirical work when we engaged catches communities was uh concerns about how as

a medium l uh large language models uh could uh potentially um uh enable or continue oppression of their language compared to other languages that were more dominant in the region. For

example, Spanish. Um, and what would then would that mean for the continuation of the use of their language and their own culture and expression and so on because of the idea

of our large English models being able to parse and essentially mediate what kinds of terms, expressions and so on may be expressed in this case on on social media. So then there's an

important question here. This is not just simply about uh asking chat GP to translate a sentence or a paragraph or asking it uh you know uh uh questions about uh I don't know whatever uh

questions people ask about movies or food or whatever. This is it comes really down to questions about identity particularly for uh cultures and contexts that are not uh within what

people call refer to as high resource languages i.e. European European languages. So the the quest the issues are s very significant and understanding then how to uh preserve these languages

and cultures where there's no increasing use of LLMs um becomes equally important and so these questions are all kinds of mechanisms and tools and so on are very are very important. I what I would add

is in terms of uh engaging these different communities and groups uh it's often at a simple uh not not often not just one mechanism what we are when we in going back to that story what people

pointed out was that they who they wanted involved was not just one community there meant they wanted people like students uh uh society groups community groups um local governments

and so on all in the mix in determining how should what kinds of data should be collected particularly when you have a range of dialects how do you uh own the how do you own and manage and curate

this data what kinds of models should be developed for developed for what purpose and so on and so on the there's a significant coordination cost then when we think about trying to effectively

engage these different groups this is quite different from imagining say I'm going to develop a model I'm going to hold a consultation at some conference center and that's it on a one-off

experience. So, there's a cost of current coordination, but it has been done over time because as as languages evolve, so should the models. Um, so I'll I'll pause there.

&gt;&gt; Yeah. And I think um that those are a lot of the costs, but there's also sort of the intangible costs of things like trust. You know what happens when you convene affected stakeholders and sort

of under the guise of hey we we're listening to you we care we want these systems to work for you but then actually don't follow up with them don't sort of offer systems I recently saw

that anthropic calls this sort of question of context and like language and context as the broccoli problem where um and they gave this example recently in an event in Bangalore where

they said, you know, if you use enthropic systems in um Canada and say, "Oh, you know, can you give me tips on what to eat to remain healthy, it'll recommend things like broccoli and kiwi,

which, you know, are not the most popular, if readily available, um fruits and vegetables to eat." And so, you know, that's sort of a funny example, but you can think of a lot of more sort

of pernicious examples. And then you know even from a strictly competitive innovation argument you know what happens if you're just not serving people's needs right what um in terms of

market offerings but also circumvention. Um so I think that's a good segue into the sort of conversation we want to foster in these breakout groups. Um we're going to put the questions um on

the screen right now. We have three questions which I think lend themselves nicely into asking the questions that we've tried to tee up for you or or give examples towards a broader conversation

of you know what does meaningful engagement look like? What are sort of approaches to foster an included and what does sort of success look like as well? Um so we'll just put the questions

on the stream screen please. Um and um I would love for folks to break out in rows of two. Um yes. And then this question of what specific parts of the AI life cycle you know I think the

panelists up here have given examples from everything from model design to development. Um but also you know evaluation who gets to say when a model actually works. What are the metrics

that are used to um stress test claims um made by model developers? And then deployment, what does deployment, post- deployment, iteration look like? Um, and how can we do this in a participatory

way? Um, so yeah, please break out in groups of two rows and we'll be coming to sort of help um, supervise but also facilitate. And we're going to come back in about 20 in about 15 minutes.

That's a great question. Yeah, put that in the group. &gt;&gt; Please take notes. Um, we'll also be asking each group to report out. So, please someone resume that leadership

role so you can report out what your group discussed. When we are talking about the point three, how can we in incentivize private sector? Are we also focusing on uh app

development as a layer on top of models or specifically for &gt;&gt; Absolutely. these &gt;&gt; life cycle um examples are uh intentionally broad to try to be

inclusive of things like uh application development built on top of models. um you know deployment post- deployment to include both evaluation but iterative testing redress things like that. So yes

we're talking about both models but also applications &gt;&gt; my intervention is with respect to what policy or regulatory measures are needed to improve participatory

&gt;&gt; group okay okay &gt;&gt; thank you yeah please this is one group You guys Um 5 minutes left. Please um ensure you

have a report someone who will report out and have one to two concrete things you will be reporting out. Thank you. Okay, in the interest of time, we're going to end the conversations now, but

we very much want them to continue. So, we invite you to sort of stay in touch with us and each other. But I think let's start with this group and we're going to have each group sort of present

on one to two recurrent themes that came up in your conversation. you can choose one of the questions to answer more directly or just sort of give a quick um survey of the themes you discussed and

in particular if there's any examples folks gave of oh here's an area where you know engaging with this group was successful or sort of um could have been tweaked in the future towards better

ends that would be really helpful as well um okay please go ahead and and keep your remarks to let's say a minute each [snorts] participate I think can you give some use case on that?

minutes &gt;&gt; one minute. &gt;&gt; I'll start and then you give example. &gt;&gt; So my name is Praashal. I lead the AI and agent at persistent systems. The

first topic which we looked at that. So while we all will have a view of the life cycle. So one of the thing which is introduced called ISO 42,0001 a standard only for AI systems that has 13 broad

dimensions. So it addresses many things. So our point one was looking at the points on the screen here the data uh the AI model design and all that depends on the data because that is where the

lot of biases ethical aspect fairness we talk about that get introduced so we believe strongly that multistakeholders should be introduced at that stage what data is being used right whether it is a

right representative set obviously after that model design development deployment and then uh the life cycle follows so Start is very important. Then there may be technology layers in between. We also

believe in evolution uh evaluation as well as towards the end when you are deploying in a customer environment that is where you get the feedback and you do fine-tuning of the models. That is

another uh critical aspect from a use case perspective. I'll let my colleague also add to that. Okay. Um hello everybody. I'm Richard Brown. So I'm from um the UK. I'm from UK foreign

office and we had a fascinating discussion. It was really really interesting. I think one thing we touched on was um you know the use cases that we need to make in terms of getting

that data in the first place. I think another point um which I was um getting a bit exercised about was how do we measure inclusivity? How do we measure how inclusive a model is and can we come

up with international standards to measure that because I don't think we've got them at the moment or we trying to get them and maybe it could be something out of this summit if anything could be

how do we have that standards and also just on the last point as well I think this yes we need to incentivize private sectors to develop inclusive and participatory models but we also need to

incentivize people to use them as well so it's not just about um which for example in the UK government should we be using country specific large language models rather than just the generic ones

that we in today we could be should incentivize to use them because then we get a more inclusion and participative policy experience across across the world as well. So um just joining those

two things together hopefully that was okay for my team. Is that &gt;&gt; sorry we didn't have to move to the other app. &gt;&gt; Thank you so much. Please do have this

conversation after the break. Next group &gt;&gt; who wants the the volunteer. &gt;&gt; Okay. Um so I'll start briefly with the discussion that happened in our group and then of course anyone is free to

add. So all of us agreed that the data set is very important. So that's where uh the participatory uh model should start. So we had the example of how there's cast hate speech and how there's

cast misrepresentation in niler models in India and that originates from discriminatory data sets. Then we also talked about how uh in data sets there can be the question of one first

misrepresentations how indigenous communities are often misrepresented in the data that is collected to train L&amp;M models and then very linked to misrepresentation is also the question

of self self-determination from data. So some communities might want to be represented in a certain way in the data that is collected but there might be some communities who do not want to be

represented in data on AI. So having uh an informed choice uh and looking at participation as a human right is important. And then we also spoke about how there are small language models or

regional language models that can perhaps include a participatory approach in a better way and then have more representative data sets. Uh we also talked about how um data set generation

can uh in data set generation uh the communities uh especially marginalized communities should not just be the providers of data set but the owners of data data set. So they should be the

controllers and owners of data sets. And finally we also talked about the importance of evaluation and the uh importance of having communities in evaluation. The importance of

understanding that uh there are cultural norms behind all languages and hence a participatory model does not mean just having consultations and looking at uh communities as external stakeholders but

looking but giving them uh giving them the space in the system that they are insiders so that they determine how the large language models are made and how data annotation data collection is based

on culture and they are basically insiders in the creation of uh the large language language models if I've missed anything and anyone would like to add. &gt;&gt; Thank you so much. Okay, we'll go to the

next group. &gt;&gt; Uh so should I stand or the question that we kind of talked about was how can we incentivize private sectorled AI

development to be inclusive and participatory. Uh we were looking at uh the default things that we most people think about is civil outcry like making sure that people uh hold these companies

responsible these private sector development if they're not making things accessible and regulation that the governments can put down like accessibility guidelines benchmarks

holding the companies to hold. But beyond that, it also helps to ensure that the money that's flowing in us is aligned like taking raising funds from nonprofits from impactled funds and also

hiring such as the or culture remains value aligned like making sure there's a lot of representation from all stakeholders that the AI might be affecting. Uh one of the interesting

things is also to note that if you only take profit to be your main thing, sometimes some of these diseases affect only a few people in the world and if they had to fund the research into

developing a cure for the research for the disease, you wouldn't be able to find that cure. So it's the same way AI should something something that we all have a right to. So we cannot only lead

as profit as the motive. &gt;&gt; Thank you. &gt;&gt; Thank you so much. And then is there just one last contribution in the back row?

Um we were talking about the first one AI life cycle. Um when we were discussing we are thinking that the entire model design development evaluation

deployment all the steps are important and the every single step the multistakeholder engagement needs to happen because you might have a better data set but the fe feature extraction

on a development might go wrong or when you're doing an evaluation the questions what you ask might go wrong. So until or unless you have an iterative development similar to product, you cannot have

something giving you same kind of results every every time the AI evolves and every time the AI goes from one version to another version. So we would be going into a place where

traditional systems are built by us. But in the next generation we might go to like machine to- machine interaction and a human will be observing the machine to machine interactions and the data what

is collecting will be scrutinized at every single point of time. So that you know what is the interaction between a marginalized person or a well educated person or a different sets of people.

you know what kind of data is coming from a user point of view and how the AI system is learning from a user point of view and how it is evolving over a period of time from a user point of

view. &gt;&gt; Mhm. &gt;&gt; Thank you so much. Um so huge thank you to the audience. Um we're over time but this has been a fascinating start to

hopefully conversations you will continue to have in different sessions and in the corridor and also hopefully with us. I'm just going to put up a slide right now with our email

addresses. So, please do stay in touch with us as we continue to do this work. I'm so sorry we're out of time. Um, but two quick to spotlights I want to say. These are our email addresses. Please do

reach out. We're continuing to think about this. We're starting a project looking at trust and safety. What does it mean to have trust and safety in AI systems in low resource languages and

contexts? Um, and I also want to give a quick shout out to um, Marina Wizniak's work on the framework for meaningful engagement at the European Center for Not for-profit law. Um, reach out to us.

We're happy to connect you with Marina, but please do check out the framework for meaningful engagement, which spells out specific steps to foster this sort of participation and um, inclusivity

that we've been discussing. Thank you again. Um, thank you to our panelists and co-organizers and thank you to you. Bye bye.
