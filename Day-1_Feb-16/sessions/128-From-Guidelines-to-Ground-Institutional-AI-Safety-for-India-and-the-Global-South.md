# From Guidelines to Ground: Institutional AI Safety for India and the Global South

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 17:30 ‚Äì 18:30 |
| üìç **Venue** | Bharat Mandapam | West Wing Room 4 A |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/ECxuXq9Opbc?feature=share) |

## üé§ Speakers

- Abdullah, Zephara AI
- Arundhati Banerjee, Nvidia
- Dr. Arjun Goswami, Cyril Amarchand Mangaldas
- Jameela Sahiba, The Dialogue
- Juan Carlos Rojas Arango, Embassy of Colombia
- Kamesh Shekar, The Dialogue
- Prof. Sidharth Chauhan, The Cyril Shroff Centre for AI Law and Regulation (JGU)
- Ravi Aurora, Mastercard
- Sidharth Deb, The Quantum Hub
- Udit Malik, Government of France

## ü§ù Knowledge Partners

- Zephara AI

## üìù Summary

This roundtable convenes government, industry, legal, and diplomatic leaders to translate responsible AI principles into enforceable institutional guardrails for India and the Global South. Participants will examine real-world deployment failures, identify binding constraints in enforcement, audit bandwidth, and procurement, and develop minimum viable safeguards for high-adoption, low-capacity settings. Outputs include an Institutional Safety Case Standard and Red Teaming Checklist. The session bridges policy intent with ground-level implementation realities.

## üîë Key Takeaways

1. This roundtable convenes government, industry, legal, and diplomatic leaders to translate responsible AI principles into enforceable institutional guardrails for India and the Global South.
2. Participants will examine real-world deployment failures, identify binding constraints in enforcement, audit bandwidth, and procurement, and develop minimum viable safeguards for high-adoption, low-capacity settings.
3. Outputs include an Institutional Safety Case Standard and Red Teaming Checklist.
4. The session bridges policy intent with ground-level implementation realities.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/ECxuXq9Opbc/maxresdefault.jpg)](https://youtube.com/live/ECxuXq9Opbc?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

Thanks Abdullah for having me. Uh good evening everyone. I'm so glad to see a full house till this is the last session I know. Um so well I am Arunati Banerjee. I hit the inception program at

the VC Alliance program for Nvidia for South Asia. So I'll give you a very brief idea about what the program is. I know the topic of this round table is AI safety and as you know that everyone in

this country is trying to do something with AI, right? We are all experimenting either we are trying to build a business or we are trying to use it. So we have more than 200,000 startups in the

country and there's almost 7,500 deep tech startups as we all know which comprises of a huge proportion of AI first startups. So what we do is as part of the inception program, it's a free

and flexible program where we work with startups not just AI ones but anyone who wants to use the accelerated computing platform and support them with infrastructure support, technical

support and guidance raising funds as well as funding in a few startups and then help them with GTM support. So that's what that we typically do with the startups. Unlike other corporate

incubators, we don't run a cohort-based model, but what we rather do is since we understand that deep tech ventures take time, we have to be patient. So, we give them a long runway. So, we work with

them right from their day zero till they're 10 years old and then we graduate them to a different program. And on the contrary, we also have another program which is a sister

program called Nvidia VC Alliance. Um, there might be investors in this room as well. So, we work with a bunch of them because you talk to startups day in day out. So we try to pick up your brain to

understand that what's the next groundbreaking innovation happening so that we can go ahead collaborate with them join hands and make that impact for the country and build it for the world.

So that's what that we do. Abdullah over to you. &gt;&gt; Thank you so much Benji. Dialogue can introduce themselves. Jamila over to you.

&gt;&gt; Thank you. Thanks Abdullah. Thanks for having us. I'm Jamila from the dialogue. I'm a lawyer by training. uh I lead the the dialogues work on artificial intelligence policy and also manage

public affairs strategy for the organization. At the dialogue, we're proud to uh be the secretariat for a coalition on responsible evolution of AI where we do a lot of AI safety related

work uh where we sort of uh make channels for different kind of stakeholders to come together and inform the government's approach and their thinking on AI policy at large more

specifically AI safety. Uh I wouldn't want to take a lot of time because I know we're short on that. Uh so that's about me Kamish. &gt;&gt; Um I think Jame has already introduced

what I also do. I I shadow her and everything. So uh I'm Kamish Shaker. I'm an associate director. Uh I look into the strategy and research at the dialogue and as I mentioned I I

genuinely shadow whatever she just mentioned. Yeah. &gt;&gt; Um Professor Johan. &gt;&gt; Hi everyone. I am Sadhart. I teach at Ginda Global Law School and I'm the

director for the Serv Center for AI law and regulation. uh we are a dedicated research center which has been endowed uh by Mr. Shro and uh we are very grateful for his endowment and our work

as especially focuses on you know cutting edge uh research on AI related to specifically to healthcare that's what we currently are working with and a lot of parliamentary responses where we

are actively involved in deliberating with them. Thank you. &gt;&gt; Perfect. So um we can start now. Thank you so much everyone. So um in global south setting we do see that AI adoption

is rapidly outpacing the institutional readiness um and when audit bandwidth procurement leverage and all of these um things are a bit constrained in these environments

responsible AI might fail in some predictable ways. I would like to welcome Dr. Goswami to to um just start with about um where does the machinery actually break

between when statue is notified and it actually changing institutional behavior and what is the weakest link in India's AI governance framework right now especially considering that um your

experience is a lot around tech policy and even legislation so over to you. Thanks very much and um no nice interesting provocative question right for the last last session of the day. So

I think let's break this uh in the five minutes that I have into three buckets. We'll talk about what the institutional design is talk a little bit about capacity and then we'll talk about some

other factors. So the institutional design if you look at where we are uh in India today it's very fragmented. There's no overarching horizontal AI law at all. Uh and that is a little by

design. Uh because the government has not wanted to go that direction. So um is that in and of itself bad? It's not in and of itself bad because if you look at the spectrum of different countries

in the global south, some have gone for those overarching laws, you see Brazil going in that direction. There are plenty of countries that have gone the other direction.

The issue with that is you have to then have a whole of government coordination because we have regulators on the one side, we have an IT act with rules. Some of you will be familiar with what came

out on the 10th of February on synthetic uh generated synthetically generated uh information uh and you have other things going on right so how do you coordinate that the uh principal scientific adviser

in the prime minister's office says that there's going to be a structure let's hope that works right the second part is if the design is that by design it's very light touch on regular military

enforcement. That is a theme that has been there ever since the country moved forward on the data protection board. That is not a heavy-handed regulator. Now, you're

seeing the same thing emerging on AI. Are we going to have an EU AI office? No. I mean, we're looking at an AI safety institute of some type as part of this structure. Now you may say that

maybe this is a lesson learned right out of failures that we've had in the past under the IT act but there are other problems that arise out of that because if you have heavy compliance burdens on

market players and inadequate capacity on regulatory enforcement you'll have an issue. The last part and I think the most serious on design frankly is we have a top-down regulatory structure. we

do not have core regulation. So what I mean by that is when you've got something as fastmoving as AI, if everything is dependent on a top- down regulation and you don't have industry

standards as the first line of defense, then you're going to have issues around that. So there are definitely challenges in the very design. The second challenge though is on

capacity. Now why is capacity important? Well, if you're going to impose a three hour, 3day, whatever limit you're going to on market players, famously in the SGI rule. Okay. But do you have the

capacity then to verify that? How are you going to do that when do you have the GPU capacity? Do you have the ability to run that kind of verification process? Because if you don't, then

you're depending just on good faith, right? So that's a serious capacity challenge. Last part is other factors. So if we are going to manage both the opportunity and the risk, we have to be

clear as to where liabilities lie. At the moment in the life cycle between developers, deployers and users, that's not clear. That's something that we have to come to. If we do it too much, we're

going to strangle innovation. If we do it too little and not in the right place, we're going to end up with risks and harms. So, we have to settle those things. Last point is this. We say, you

know, global south and all that. Not everything about AI is to do with the technology itself. What about the back end of it? When you create e-waste of the type that we're going to when you

have AI, are do we have a green blind spot? I think we probably do in terms of the way in which we manage waste. So I think there are some significant challenges around this some things which

you know is work in progress uh and I think hopefully if all of you kind of contribute to how this is going to get better as a collective enterprise we're going to do better. Thank you.

&gt;&gt; Thank you so much Dr. Gusami. That was uh very interesting. Um I would like to move to u Mr. Aurora from Mastercard. Um sir in payments and finance um small errors scale fast and in a country like

India when adoption of AI is very high and um of course like and is very digitally integrated uh as a country in terms of uh DPI and all of that. Um I want to ask you like when AI is used in

high volume financial systems like India, what institutional breakdown do you generally tend like generally tends to surface first under the real world stress in a country like India or in the

global south context. Um it's a very broad question. So I would just like to gather your your perspective on this. [clears throat] &gt;&gt; Great.

Thank you very much. Yeah, I think that like look I mean I if I look at you know from a our company's perspective as Mastercard uh you know processing whatever 180 odd billion transactions in

a year right around the world now one of the things underlying every transaction every interaction for all of us right I mean uh uh irrespective of the medium you're using is trust right so trust is

front and foremost uh in everything and uh so I think that as the u you know so for us like for example cyber security investments in cyber security artificial intelligence is something I think m we

at Mastercard for over two decades now it has been core to what we do because protecting fraud and risk is something uh very very critical because otherwise you know people won't use uh digital

payments at all and I think that you know to to to do that uh now you know everything from I mean think about it like there are so many variables that go in into decision making right and now

when let us say whether you are a it's a swipe or a tap or or a tap of a card or you know online and so forth. the milliseconds it takes to go uh for the transaction, you know, to go from a

terminal, right, to the end point to whoever the issuer of the card is, get a validation, a score, come back again to the terminal with an authorization and all of that requires tremendous amount

of technology behind it. Right? And we do you know our engineers and so forth are looking at so you know what is the distance from uh of the trans you know from where the individual resides to

where the transaction took place. How are you holding the phone? How are you typing uh into it? like you know all of these things generate some level of a score right which then gives some kind

of confidence level where is it a genuine or is it a fraudulent transaction right because then that they so I think for us what we have to do like from a and now you take all of that

uh but that and the underlying fundamental has to be it has to you know uh we have to take a very um humanentric approach to uh uh to design, right? You have to minimize bias, you have to be

ethical, you know, all of these responsible practices that are necessary, you know, come into play. Which is why I think for us, you know, we have very concrete design principles

like, you know, it is about you own your data, you should decide how to use it. Our job is to protect it, right? Because privacy is very important. So I think that as models and you know are being u

uh and I think India this way is very u you know it's good it's as long as it's a principles-based approach you know with principles around safety security accountability around transparency

around inclusivity around social impact in terms of benefiting society I think all of these are kind of important when u uh to bring these models into actual use and impact to the end uh consumer.

So that's what I would say that I think you know that uh those are all the aspects for us it is very uh important uh in our uh design philosophy putting the consumer at the center. uh so even

all of the AI models LLM all of all of those you know at the very core need to cater and make sure that um you know uh we are it is responsible it's responsible AI

&gt;&gt; sir I'm curious though like in a in in high deployment situations right um is there any specific sort of institutional failure that you observe like that tends to come up when you deploy your um a IML

products in India or something like that around that. &gt;&gt; Can you re rephrase that again? So sorry. &gt;&gt; I I was asking like in in high adoption

environment like India when capacity is not that much and and not just capacity but literacy and there are other issues as well. &gt;&gt; Do you see any other form of um sort of

failures that you observe that are unique to the global south or unique to these sort of involvements? Yeah, I think you know obviously one has to be very careful of uh uh you know the uh

what's the right word I'm looking for is the uh false positives right because I think the false positives is because you you know it there's nothing much more frustrating to uh an end user or a

consumer uh in terms if if you like you know especially in um you know in our business in payment And you know if the if the rate of false positives is high you know you're just going to get

frustrated and get away. So how do you then make sure that you know that in the design element you minimize that to get you the right aspect of uh uh a you know uh the actual authorization for consumer

satisfaction. Yeah &gt;&gt; that makes sense. That's a very interesting answer. Um coming to U Mr. Carlos. Um um so from a diplomatic lens um AI

governance is um so it is mostly about trust and coordination and capacity as we see and whether the countries can learn from each other as well um you know without um I I don't want to say

the word but importing certain templates of governance or ethics um that may not fit the local realities. So in that way you know um I want to know that when countries talk about

making AI safe and beneficial what do you see as you know a real risk real world barrier that stops these intentions from becoming reality when we talk about international or um um a

global south south sort of coordination as well um from becoming practice and what kind of international cooperation actually will actually help turn these into um

practical sort of solutions. &gt;&gt; Thank you. First of all, thank you for the invitation Abdullah. Initially the idea was to have a member of the Colombian delegation coming but uh

because that was not possible. He was so gracious to insist that I should be here. So uh regarding your question, I think uh there is a big challenge for Latin American countries in all these

aspects of AI safety and not being left behind or been subjected to the imposition of of of uh all the aspects of governance of AI. What I can say is that in the Latin American region there

is uh uh one country that is developing their own AI model is Chile. They are working on that. The other countries uh my country Colombia is kind of a early adopter of technology and uh I think

that uh uh Colombia is a member of OE ECD countries. We are part of this uh grouping of uh GPAI that is having it meeting on on the 29 Colombia have endorsed all of that process of

discussion to make AI uh safe uh and and to be ethical and uh to try to be beneficial. So I would say that uh on the other side there is also a lot of potentiality

in terms of the capabilities that have our country in terms of the energy metrics. Colombia generates 60% of its energy hydro. So the possibility of hosting some data center also there in

the region and also with the impetus that uh my president president Petro is doing for climate change and for the needs of uh less dependency of fossil fuels also could be a contributor. Now

in terms of cooperation uh six uh countries that are part of the GPAI are uh also part of different groupings in Latin America. One it's a Pacific alliance. So Pacific Alliance it's it's

a grouping that uh for sure will integrate the discussions forward. uh also uh India is part of the GPAI and so the possibility of uh through India putting in front all the concerns of uh

of the countries of the Latin American region uh it's uh very important so so I would stop there and thank you for your question &gt;&gt; um so I I would I I have one more

question sorry um I just wanted to know do see any similarity in terms of deployment environments like in terms of the the the capacity uh similarity in terms of deployment environment of AI

when we talk about India and Colombia like is it similar in the way uh you know deployment might might work in India and in Colombia uh I see that uh the like you can see

right now that uh there is presence of uh technology based companies of India in Colombia. Uh Soho it's uh is present in Colombia and has his uh Latin American headquarters there. uh they

have been there and they uh what they pride themselves that they base themselves in Colombia because of the talent pool that they could get there and their association with the high

quality universities and plus that taking advantage of the 19 free trade deals that we have and the uh beta treaties that we have. So uh from Colombia they take care of a lot of

European clients and they says a lot about possibilities and of technologicalbased companies in India to to to move to Colombia. The other thing is that Colombia has a great advantage

in the Latin American region is that it has it's it has been attracting foreign direct investment in an stable manner for the last 15 years. So so that's that I could point that that also as one

pillar that could be attractive. Thank &gt;&gt; Thank you sir. Um so I would like to go to Kamesh and Jamila. I mean you run core AI. Um it's a coalition of uh coalition of

responsible evolution of AI a multistakeholder sort of an organization. Um so Kamesh starting with you. Um what what sort of challenges have you seen uh in the deployment space

when you talk when you talk to like so many diverse sort of um organizations um be it government and academia and industry from an institutional lens when we talk about failures and challenges

and risks from an institutional lens? Um very broad. So do you have any any perspective on that? &gt;&gt; [clears throat] &gt;&gt; Yeah, I think like um when we talk about

like you know institutionalizing safety uh the major two points that I would like to like you know make here is that which which also comes from like you know bank of things that we do with

startups and like other industry things is that is like I think like when we talk about safety um like the standards that we actually are setting for ourselves is like you know has to be

least common factor. So most of the times like when we talk about institutional safety or trust or like anything of those angle it's always like we keep the bigger organization in mind

and then we kind of like go about you know synthesizing how do we operationalize it which might or may not like may or may not like actually like you know let the smaller players out of

this not understanding they work in a different socioeconomic like you know um background and like they come from a different stuff. So I think like one common factor when it comes to this is

that is like you know we need to look from the least common factor within which we operate within um we should be like you know looking into this. Second aspect I think like I keep talking about

this in a lot of forums and like I think like to an extent like matey is also working on this is um when we talk about um institutionalizing safety and um you know trust and everything one piece that

we miss is incentive structure like what is there for the ecosystem to like go about doing this right like you know one way is looking at a hard touch regulation but that may not be the best

way to like you know go about doing given the you know the innovation curve that India's is putting itself at this point of time. So then what are the other ways through which we can actually

achieve is like making value how can we make trust and safety a value proposition. How can that give a competitive advantage to people? So that that is a piece that has to be like you

know uh pondered upon so that like I think like when we were in like you know those days or when we talk about the privacy that we kind of like talk about privacy first culture. How do we talk

about privacy first culture is through making privacy something that is like you know people compete on right so similarly I think within AI when we talk about trust and safety that's where like

we should be like you know aiming to move um where the ecosystem can pick it up themselves so you &gt;&gt; so um Jamila from your experience leading the coalition in a way the

secretariat uh what makes responsible AI commitments from all the stakeholders actually stick in practice. What do you think about that? And how can coalitions create value um in the in accountability

in this in this AI governance sort of space? &gt;&gt; Right. Thanks Abdullah for having me on this panel. I feel uh that we talk about AI safety and this is bases our

conversation with the 55 plus odd members that we have in our coalition is that safety needs to be looked a little differently from what it is currently being looked at. Right? Oftenimes we

look at AI safety as a regulation problem and not so much as a capacity problem. And why I say this is because AI safety should be looked at the way state governments or the central

governments procure AI. It should also be looked at the way it gets deployed because a lot of times safety ris risks acrew because of uh you know uh misfit deployments or like you know deployments

in a way that do not cater to the needs of the society or the issue that they're trying to sort of uh resolve. Sometimes it could also be because of the way state governments or governments at

large procure AI right so in your vendor contracts in your technical capabilities of of you know deploying AI at large I think it needs to be looked at from that standpoint as well another point that

I'd like to bring to light here is that AI right AI safety right now needs to be anchored through constitutional and democratic values in India AI safety broadly should uh also include

procedural justice which means uh who gets to decide on the technology but also substantive justice facets of substantive justice which is to include who who is going to benefit from the

technology and also who's at risk. So when you talk about all of this in our coalition meetings we try to unravel a lot of what this truly means. Who's best benefiting? Who's at risk? Who gets to

decide and call the shots? Which brings me to the next part of your question. Why do we set up coalitions like this? Our aim is to actually break the silos between different kind of stakeholders

and to get them a seat on the table when where these decisions are being made. And I think any kind of coalition building is significantly important right now because when I talk about

democratic values and its inclusion within AI, responsible AI or AI safety at large, a big facet of that is going to be inclusiveness. It's going to be the ability of of a small creator to

influence the governance of it. at the same time also be you know chocked out by the bigger players in the ecosystem. I think it's going to be by creating uh by sort of breaking down all of these

barriers and creating a safe space for everyone to be on the same table and to influence how policy gets created in the country and how AI safety as a notion gets advanced. So I think that is what

we're aiming to do at core AI and and I think coalition building of any kind is very very useful towards that. &gt;&gt; That makes a lot of sense. That's a that's a very interesting answer. Um

coming to Sadhart Dave, there are [laughter] two Sadharts here. So um I know you have written about the AI safety institute and and um is there some issue? Okay. So

&gt;&gt; yeah, &gt;&gt; perfect. So um we know that even strong um intentions fail without a clear institutional design. I want to like get right into it and and ask you that if we

want AI safety to be operational &gt;&gt; and drawing from your experience of um uh working in this space and um uh writing about the AI safety institute for India and all of that um you know I

I just want to know like if if we want it to be operational what should be the governance design inside these institutions that we want it to be um more or less

&gt;&gt; thanks that's actually an excellent question and a little bit I'll draw on my previous work on AI safety institutes but also look at some previous work I've done in the context of cyber security.

So one of the big things when it comes to like strong institutional design in the realm of cyber security is incentives for proactive information sharing. So when there's a vulnerability

somewhere in the ecosystem if a player discovers it, what are the incentives that they'll go to the institute and actually disclose this is a vulnerability and this is how the rest

of the ecosystem should respond to secure their individual nodes. The problem is that a lot of global majority countries tend to situate let's say cyber security institutions like your

certain within the enforcement enforcement architecture but better design is to kind of remove enforcement authorities from cyber security or safety institutions and make them more

like conduits of information sharing. So if you kind of design incentives in a way where safety institutes are more about information gathering and information pushing across the ecosystem

and then you create incentives where businesses if they are good actors in the space and if there is a breach there are liability reductions that will create more forthcoming knowledge

sharing between all corporate actors. So you need to really think about incentives there. The second thing is from an institutional capacity point of view. So like let's say if we're trying

to set up an AI safety institute, I think we need to it's great that right now in the context of this impact summit, there have been conversations of setting up a global south network of AI

safety institutes cuz what we're talking about is there's a recognition that we need to pull resources so that everyone can benefit from their own areas of expertise and the international

community as a whole can benefit. This pro this summit flows from the Bletchley process. So we need to be also mindful that after the soul summit there was the institution of an international network

of AI safety institutes. It's important that if global south countries are also setting up AI safety institutes there's an opportunity for dialogue between global majority AI safety institutes and

first world or developed country AI safety institutes. So what can happen is so for example the UK and US get early access to model frontier models and they're able to audit what are potential

safety risks with models. So model evaluation is happening happening there. If you createus and bilateral partnerships, that information can actually be shared with AI safety

institutions in countries like India. So that instead of companies having to share their frontier model with each individual AI safety institute, information can essentially flow from if

if one let's let's say the UK safety institute finds an issue, it can share it within the network and countries across the world can benefit. What value can global south institutions bring?

They can bring the perspective that safety cannot only be looked at through a technical lens. You need to have a human-centric point of view and they can surface issues like linguistic exclusion

and bias and discrimination. the these networks of AI safety institutes through the Bletchley process can even set up thematic working groups so that then you're actually problem solving and

creating actionable templates on how to mitigate risks but also at the same time and one point that Arjun brought up which was about clarifying responsibility across the value chain

right I think it that's super critical but we also need to think about clarifying to what extent can businesses and the ecosystem collaborate and cooperate with government governments.

So what do I mean by that? Right now the just last week there was reporting that certain military use cases of AI are also coming up. That's a huge problem for businesses because they need to have

a certain level of trusts that the security interventions that they are undertaking are not being compromised by governments themselves. So we actually need to also have the Bletchley process

feed into conversations at the United Nations. So for example, the UN has set up the global digital compact but also the independent scientific panel on AI safety. Right? They need to start

commencing a multilateral dialogue on what is responsible behavior not just in cyerspace but when it comes to responsible state behavior and the use of AI. So at least there is some clarity

amongst businesses, civil society and governments. These are areas we will cooperate with and of course these are areas of contestation. That kind of clarity is really super important and we

have templates for international cooperation at the UN level because we've had a body of since 2004 there have been UN GGEs and open working groups that have been deliberating what

is responsible state behavior in cyber space for international cyber security and we need like parallel conversations on AI safety as well. So I'll pause there

&gt;&gt; sir. Absolutely. Yeah. &gt;&gt; Yes. I I I just wanted to comment that uh Colombia for was the first country to adopt UNESCO guidelines for AUS in judicial systems.

uh even though we still don't have a national AI safety institute but we have a planning uh a a planning authority that's released recently in 2025 a document for how to regulate AI. So it's

a work in progress but uh &gt;&gt; Colombia has been one of the first countries to get into the AI ethical space in fact post LLM uh boom which is quite interesting um

I mean first among the global south at least um I I want to come to uh professor Johan and Dr. Goswami for this next question starting with professor Johan.

So in real world context, accountability depends on what can be shown, reviewed, contested, especially when harm is alleged. I want to know that what should institutions be able to show in a simple

and credible way to demonstrate responsible AI use when decisions are questioned and how do you think legal and governance frameworks could shape that should shape that standard to a

degree. &gt;&gt; Right? uh so you know if you talk from the point of the seven sutras which uh you know we're sort of building and talking about from human capital to safe

AI practices and which of course shows that you know the focus is on uh the global safety in global south but you know AI safety in India it also needs to be a tool um for India and it sort of

begins from procurement which is a lever for AI safety you know the India a AI mission has a budget uh and I think they need to sort of think about how procurements are happening and from a

start startup level that how we are thinking about startups essentially like how we are widening access and also you know what uh you know Mr. Goswami brought in about the SGI rules which

have come in. So especially uh you know if the government is uh trying to have more investments and they are trying to encourage startups uh I think there is a need to sort of have those mandatory

original labels specifically and then how we are sort of clarifying that is of course a challenge um and third party verification comes uh which is important uh I think that's there and graded

liability is one bit which I want to focus on but also the technolal safeguards uh which need to be instilled build right when the technology comes in uh which sort of you know 2026 IT uh

rules uh sort of focus on labels and markers that's a good example but how technology uh technology sort of created right in the beginning those they they do create certain safeguards and

essentially you know when we're talking about the safety measures I think very critical bit is on sovereign AI as well uh where we talk about the AI infrastructure and you know how much of

computation and data sets actually exist versus in India. So I think that's something where India produced 20% of the data and we only have 3% of the world's data sets uh data center spaces.

uh but you know AI coach is on that direction but seems a bit of challenge there and I think and coming linking your question to what we were discussing about institutional failures and the

path forward I think when we are talking about you know multiple layers where data protection authorities also come in at the same time the government intervention is around that we want to

make it easier for uh you know players and there's ease of doing business then I think we need to sort of clarify uh how that is going to uh have a single clearance windows specifically and you

know there's future skills uh program uh which is which needs to then focus on training experts uh on AI safety assessments and essentially grow the uh audit market there needs to be sort of

coordination between the regulatory bodies otherwise it's going to see the same fate when we say competition authority versus say another regulator so I think that clarity uh should come

right in the beginning and of Sadhhat spoke about the role of uh safety institutes. So I think the guidelines need to sort of uh you know be clearer on that sense.

&gt;&gt; It's very interesting and Dr. Kusami I just want to uh know the same thing from you. So um legal and governance framework that could shape that standard um yeah on harms.

&gt;&gt; So you know um what a great panel you have. It's really hard to follow on after them, but let let me just maybe say this, reflecting a little bit on on things they've brought up,

there's always going to be a lag between especially the speed of machine learning and what we're seeing in terms of technological deployment and where law and regulation is. It's it's it's

there's that gap in fact is increasing. So as this deployment takes place there's also risk. What do you do? One answer which India has is kind of only a partial answer is sandboxing. So

you say okay you know don't fully deploy it you just put it in a sandbox but people aren't going to be satisfied with putting things in a sandbox. They'll want to fully deploy.

So actually I you know listening to Sadat I I felt he was really on to by which I mean this Sedat I mean the other Sedat is great too actually on on what he said but we're listening to this

Sedat is on on the procurement side. So you know one lesson which we can learn from Brazil is what they did in Sao Paulo on the metro system. It's it's kind of interesting because what they

did was they built in an algorithmic impact assessment and they made sure that before deployment took place that risk was assessed. So they're not stopping deployment but they're saying

mandatorily you have to have that impact assessment. It's a way in which you can manage the risk. There's another very famous example which is the IBM Watson case. You guys will know this where they

were deploying medical technology using AI but the result was hugely expensive drugs not appropriate in a global south country. Again you need to have procurement tools which will step in and

manage that risk before that deployment takes place. If you're going to wait for an answer from regulation and law to do it, I don't think you've got an adequate answer really. I mean, we can all try.

We all should. I think all the things that people are suggesting are important, but we need to tailor this. And we need to tailor this because global south countries certainly a

country like ours has a huge number of languages, has entities at different levels. So when I was talking about liability, Sedata Sedat, he's right that there's a value chain but there's also

gradations in terms of types of entities. Why should a startup entity be pushed to the same level of compliance as a large entity? You need to have that built in. Right? So if all those things

can come, I think we can manage harms much better. &gt;&gt; I think that's a very comprehensive answer. Um thank you Dr. Gusami on this. Um last but not the least miss Banerjee

I want to ask you um from your ecosystem vantage point because you support startups um in India and other countries what practices should AI builders adopt as non-negotiables

before selling them um into high stake settings. &gt;&gt; That's a great question actually Abdullah. So especially when we work with startups but I think what my answer

I'll keep make it generic because uh there are some non-negotiable factors doesn't really matter whether it's corporate gsis GCC startups anyone for that matter so uh whatever is the size

the first thing is there is no oneizefits-all kind of a solution and that's quite apt for AI safety as well while we are all working on the framework and trying to make some

guidelines so that we are all safe and actually enabling uh AI safety uh it's very difficult because we are all doing it for the first time. There is no precursor. We don't have any

learnings. So things are pretty things should be pretty flexible at this point in time. So that the it can change with with the time as we learn. So that's the first thing. But when it comes to the

enablement the question that Abdullah was putting so I think we still have a tendency where we see that this becomes more of a compliance paperwork right because okay this has been imposed upon

me. So I there was a lot of discussion on the incentivization of it. Um that's the reason that incentivization in the first place is coming into the discussion because otherwise it is more

of an impos the imposition that's done. But what we should actually consider it is especially when it comes to evaluation when it comes to guard drills that's infrastructure and also when it

comes to governance these should actually be first class engineering problems. These are not AI safety problems at all because if we consider that as an engineering problem then we

will deal it in a very different way. So if evaluation always has to be continuous. It's not a one one kind of a deal right? It's not a one-way procedure. It's not a one-time thing. So

that's the first thing. The second thing is when we are talking about guardrails. So guardrails has to understand the infrastructure. They have to be infrastructure aware no matter how big

or how small the infrastructure is. And then the last thing that I'm going to talk about is the governance. So governance has to be operationally tested. So governance cannot be just

dictated by the terms and conditions because what might have worked in some other geography or maybe in the same geography for a different industry might drastically fail for another. So we have

seen that with scale. So that's the reason there's not a one-sizefits-all kind of a solution. And I I saw that there was a lot of discussion on the procurement side as well. So I'll just

put in a last point on that. So we all discuss about that okay is this system accurate what is the latency what is the accuracy what is the throughput so but everything at the end of the day is

talking about quality but no one hardly talks about that is this system auditable can we inspect this system can we suspend this system if we cannot do that if we cannot audit the system we

cannot govern it that's the reality so I think that's something that regardless of whether it's a startup it's a scale up it's an enterprise corporate that's what that we should be doing and before

we are going for adopting AI safety three questions we should know how to articulate not just for the sake of paperwork the first thing is that what is AI really solving who is the person

accountable if it fails and what happens if the model degrades and the data shifts because these are bound to happen so if you don't know the answer for it I don't think so they are mature enough to

actually employ it just for the sake of it &gt;&gt; thank you so much banana and thank you so much everyone. We just have one and a half minute left. So maybe let's just do

questions quickly if there are any questions in the audience. Yes. Let's just collect all of them and then we can go through it. &gt;&gt; Yeah. Yeah. Absolutely.

&gt;&gt; Okay. So good evening everyone. Uh the panel was amazing because uh I think we have people from all over like the panel is very diverse. So it was a lot to learn but I think my question is to you

sir um to go swami sour uh because he said a point which was much to my interest wherein he said that we have a blind spot to um e-waste generation that is going to happen in this process that

we're talking about and a lot of people also talked about how there should be inbuilt um uh things in the software that incentives actually that people follow a certain thing that businesses

follow certain things. So what do you think can be the incentives for businesses that they make green AI? Uh because I don't think so businesses would like to really do that.

&gt;&gt; You know um it's true that businesses will ultimately look to their profitability. But uh I think we've long passed the point where we just talk about shareholder

capitalism. I mean we are we have for a long time been in the age of stakeholder capitalism and corporate social responsibility that's mandatory in India. The question now becomes with

really large investment on the horizon for data centers etc. What requirements need to be put in place for all those collateral things that we should be concerned about right

from the start not as an afterthought. So, you know, the reason I brought up the waste issue is that, you know, a lot of this is not it's not just a matter of toxic leakage. It's a matter of who's

doing it, the gig workers who are doing it. It's the informal economy that's doing it. Now, if this is linked back to where the labor codes are headed in terms of better standards, you know,

higher wages, etc., uh we can I think push hard for a better solution rather than just say well we want the FDI to come in yes we're concerned about our foreign direct

investment levels and AI is going to be a a big promoter of that hopefully uh but see it in the whole social environmental governance all of it it's the ESG G

covering on top. &gt;&gt; Yeah, &gt;&gt; it's a question for for the rest of them, right? &gt;&gt; Thank you so much for the opportunity.

So, my question is with Joan, sir. Yeah. So, I just checked that uh December 2024 UNESCO guideline that Colombia adopted and that's specifically for use of AI systems in courts and tribunals. So I

just wanted to ask you it's a very much open space question. I'm not going to evaluate you that what exactly it's 2 years down the line after they have adopted that in the court systems what

kind of issues are still not able to be addressed by AI because even in in India it's even more complicated. So if you can shed some light on that. Thank you for the question and I don't

have the answer but [laughter] but I promise that I will research it. &gt;&gt; I I was expecting that &gt;&gt; you know [laughter] it's it's a very nice challenge and thank you for the

question. &gt;&gt; Thank you. Thank you so much. [applause] &gt;&gt; Hello. So my question is anybody can answer it. So we've had a discussion

about leading countries Colombia, Brazil and everything but there's one country that we all missing that is China and China also released recently the AI guidelines or principles in its cyber

security act. So what are the benefits and what are the flows of a system like China which treats cyerspace as its territory and as a following question the question of digital exceptionalism

since security is the first condition of freedom. So how far can we uh what is the middle ground between digital exceptionalism where we could sacrifice the or compromise the individual rights

or the digital privacy rights and how do we balance it? How how do we come at a middle stage? So these are this two questions. Anybody going to &gt;&gt; let's just do all the questions

in the &gt;&gt; Thank you. Um I wanted to ask about regulation. Uh given that so much AI is reliant and underpinned by personal data and we've talked I've heard a lot today

around trust. Um I'm really interested in the panel's perspectives on the role of the regulator in enforcing um well in terms of its enforcement posture. Um I don't know if if the panel would like to

comment on that. We we've obviously heard around the need for kind of guidance and sandboxes and more in that proactive space. I think what's missing and what I haven't heard very much of is

beyond kind of audit capability is is enforcement and making sure that people know citizens know that regulators likeelves have their backs. I'd be interested in anyone's views on that.

&gt;&gt; So I'll just try to first start with the China question. So in terms of the way that China has approached AI governance so far is primarily through its cyerspace administrative

uh institution but more so it's actually set up an algorithms register. So that registration institution so they keep on passing peacemeal regulations and more often than not it directs local

companies to make disclosures in terms of how they're using AI and how they're using deploying their algorithms. So it's kind of a peacemeal approach. But one positive lesson from the Chinese

experience is that you know over time incrementally it's investing in institutional capacity to know more about how these systems work before making directives towards businesses.

The downside of course is it leaves itself a lot of discretion when it comes to uh enforcing uh any company behavior that intersects with politics. So of course that has an implication for civil

and political rights. On the question of data audits, I think in countries like India, what we're looking at is more so instead of getting the regulator itself to undertake the audit, we're looking at

requirements where businesses have to get or larger businesses have to get independent audits done and then submit those to the regulator. So that's kind of a uh a tertiary kind of way to

implement our legal systems, but I guess it's a more pragmatic approach. &gt;&gt; You know, the uh the beeper went off and it says times up, so I really can't answer the question anymore. No, no, no.

I just just maybe a very brief one. So you know as far as regulators are concerned um I mean our regulators are looking at applications of AI particularly in the

financial sector and they are concerned about issues of bias for example coming into areas like credit scoring etc. So it's not like that that's not happening. There are advisories coming out and the

financial sector regulators in particular and to some extent the telecom regulator are going into that space. But what we're not seeing is a very heavy-handed overarching regulatory

enforcement. That that's really where we are today. Last point, we have a lot of data in our country. We should seek to leverage that for advantage for ourselves. Right? Companies are going to

come here to want access to that data. I mean, the comment at the back was it's all going to be about personal data. A lot of AI is not going to be personal data. It's going to be it's going to be

aggregate anonymized data which shouldn't be deanonymized and it should be non-personal data. Let me stop. Thank you so much. Thank you so much to the speakers and uh to everyone for coming

and we have to conclude this now. So um yeah, thank you [applause]
