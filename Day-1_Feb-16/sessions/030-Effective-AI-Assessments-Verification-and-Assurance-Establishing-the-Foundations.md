# Effective AI Assessments, Verification and Assurance: Establishing the Foundations for Responsible Confidence in AI

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 10:30 ‚Äì 11:30 |
| üìç **Venue** | Bharat Mandapam | West Wing Room 4 A |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/ZRKQ6b0c8ZI?feature=share) |

## üé§ Speakers

- Anne McCormick, EY
- Dr. Ansgar Koene, EY
- Dr. Jibu Elias, Mozilla
- Dr. Philip Howard, International Panel on the Information Environment (IPIE)
- LEE Wan Sie, Singapore Infocom Media Development Authority, AI Verify Foundation
- Narayanan Vaidyanathan, Association of Chartered Certified Accountants (ACCA)
- Prof. Dr. Balaraman Ravindran, IIT Madras

## ü§ù Knowledge Partners

- EY (Ernst & Young LLP)

## üìù Summary

A structured discussion on the challenges and requirements for building an ecosystem for AI system assessments, governance, and risk management. Building on global efforts such as the UN's AI auditing review, recognized AI standards, and emerging AI assurance skills and testing frameworks, the session will outline a roadmap toward reliable reporting on AI system quality, governance, and monitoring. Expert presentations will cover practices, pilot studies, reporting demands, barriers, and pathways to overcome them globally.

## üîë Key Takeaways

1. A structured discussion on the challenges and requirements for building an ecosystem for AI system assessments, governance, and risk management.
2. Building on global efforts such as the UN's AI auditing review, recognized AI standards, and emerging AI assurance skills and testing frameworks, the session will outline a roadmap toward reliable reporting on AI system quality, governance, and monitoring.
3. Expert presentations will cover practices, pilot studies, reporting demands, barriers, and pathways to overcome them globally.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/ZRKQ6b0c8ZI/maxresdefault.jpg)](https://youtube.com/live/ZRKQ6b0c8ZI?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

How do we make sure that the AI systems will deliver what we want? In this session, we will provide a discussion, a conversation on the requirements for building an ecosystem for the

assessment, governance and risk management for AI. By the end of this session, we hope that you will have gained a greater understanding of the requirement for

reliable assessment and reporting on the quality of AI systems and their governance. We invite you to join us in building a community of engagement for ongoing work

on AI system assessment and reporting. With me today I have uh WC Lee, executive director of the AI verify foundation which is actively engaged in uh enabling the development of a global

AI assurance ecosystem. She's also the cluster director in at at the IMDA responsible for AI governance policies which works on uh marketdriven AI assurance as complement to other

mechanisms to uh enable trustworthy AI. Next we have Philip Howard who is professor at the University of Oxford and president of the international panel on the information environment. The IPIE

is a global science body dedicated to strengthening the information environment through applied research, scientific consensus and real world impact.

Next we have uh Raven Balaraman who is the uh founding head of the center for responsible AI a multiddisiplinary research center that conducts original research on AI evaluation metrics

benchmarks and reporting guidelines. Then we have Anne McCormack who is EY's global leader for public policy and digital technologies. EY is a leading global network of

professional service organizations and uh EY teams work across sectors and provide a spectrum of expert services spanning assurance, AI and technology risk consulting and a range of other

specialist services. Then next to the close to the end uh we have Narinan Vajinathan who is the global head of policy development at the association of chartered certified

accountants. The ACCA is a professional educated members body for accountancy and finance professionals. They work in areas like IT audits, risk assessment and internal uh controls uh

which [clears throat] are well recognized responsibilities that many AI ACC members ACCA members fulfill. The ACCA works across policy maker engagement and member support.

And then last but certainly not least, we have uh Jibo Elas who is working at the Stimson Center and Monzilla's responsible computing challenge where he uh focuses on AI governance and

implementation in emerging ecosystems and on how assessment, auditing and accountability mechanisms can actually function in real world institutional contexts rather than remaining

theoretical frameworks. Just before I I invite my distinguished delegates uh to start their opening remarks, let me provide a brief overview of some of the type of assessments that

we will be discussing. One of these is governance assessments. These are evaluating the eternal internal governance structures surrounding AI systems. How do we make

sure that the AI systems embedded within our organization are being managed in the right kind of way? Next we have conformity assessments. These determine compliance with any applicable laws,

regulations, but also voluntary standards and contractual requirements. And importantly also of course there is performance assessment measuring the AI systems against predefined quality and

performance metrics. Each of these types of assessments look at different dimensions and it is important to understand which dimension has been assessed. What exactly one is uh

reporting on while performing and reporting on any of these assessments. It's important to have clarity on the purpose of the assessment, the subject matter of the

assessment, the methodology criteria against which the assessment is performed as well as importantly the required qualifications and accountability and the absence of

conflict of interest of AI assessment providers. So with this context having been laid uh let me start off with you and ask you if you could perhaps uh share from your perspective on the

importance of AI assurance in supporting responsible AI development and how it complements existing mechanisms such as regulation. &gt;&gt; Thanks Asa. Um very very happy to be

here today. Uh all of you can hear me well from the back of the room as well. No hands so I assume they can't hear me but it's okay. Um but all right let's coming back to the

to the topic. Um yes, how can assurance right and assessment um support um regulations and and what we are also looking at is market driven assurance. So I'd like to talk a little bit about

that. Um the analogy I like to use at least in terms of assurance can support um regulations is um in aviation industry. Um we have regulations today planes must be safe. Um so because it's

important right that uh as uh as more and more people travel um you don't want accidents to happen in aviation. But how do you make sure that planes are safe? And that's where the technical

assessment comes in. That's where the skills and the ability to actually measure and test for safety is important. And that's that's one element of how you

how assessment it's important. And that's why I talk about compliance and so on. But at the same time in in AI, unlike in aviation, you're going to have so many applications and so many

different contexts in which AI is used. It's going to be in healthcare. is going to be in um financial services, in employment and of course in aviation as well, right? And how then do you figure

out how do you test for safety or test for reliability in each of these areas? Do we have laws in all of these areas to actually define very clearly this is the expectations? How many of you think that

our laws today are sufficient to clarify expectations? Nobody, right? Yeah. um we have I mean countries have been imple have defined horizontal loss. I think that's very

important. I think on the product safety side is also very important to start identifying um what's important in each of these sectors. But at the same time I think citizens are saying or consumers

are saying I worry about how I'm interacting with the AI. I worry about whether this AI system is actually handling my data properly. I worry about whether I'm getting the right

information. And on the company's side, they worry about reliability of the contents that being shown um to their customers. Um they worry about security um you know uh their systems could be

more easily infiltrated because of AI and a host of other factors. So we think there's actually an opportunity here besides you know compliance with regulations for marketdriven assurance.

What we mean by that is organizations who want to demonstrate that the AI products and services are actually meeting safety expectations of their customers and their stakeholders will

need to find a way to do that. And that's where this assurance components comes in. Um that's why a lot of the work that we do in Singapore is really figuring out how do we create technical

standards? How do we look at technical capacity and capabilities to do this testing and it's highly dependent on context and highly dependent on use cases at this stage before we can

generalize it. A use a test for financial application, a chatbot that provides financial advice to a user will be very different from a from a test for hospital looking at uh triaging you know

use cases before it sends uh sends the patient to the right um right department. So risk so risk testing in very specific context is important right and that's

why uh we've started work on uh this what we call a global assurance sandbox um that brings together testers um third party testers um um to do technical testing as well as provide some level of

assurance as well as deployers real life deployers who are trying to implement AI today who wants to benefit from the use of AI and are trying to demonstrate to their board uh to their shareholders to

their customers that the implementation is going to be um addressing some some risk that everybody worries about. So the sandbox actually allows to start you know understanding how do you think

about testing and then eventually coalescing this into general policies and guidelines and standards that everybody can benefit from. Um so you can go to the air verify foundation

website look up uh the global assurance sandbox. We have some already about 20 to more than 20 to 30 use cases in flight. Some are uh already completed and we have interesting case studies and

what we've taken from what we'll take from these case studies is to then move this into hopefully um policy guidelines as well as standards. Uh we invite all to participate on the sandbox with us.

So hopefully that gives you a sense of one um regulation is important compliance at the same time I think given where we are in terms of use of AI today and

adoption across many many sectors and the speed in which technology is moving I think firms wanting to show that they are actually implementing AI well it's also just as important and that's where

assurance comes in. &gt;&gt; Thank you very much. um [clears throat] you highlighted that this is a developing space and that there is still a lot of work that needs to be done to

establish really what does a good AI assurance ecosystem look like. Um the IPIE has been doing work in this space. So could I invite you Phil to perhaps uh fill us in on some of the findings that

you've uh had from that work. &gt;&gt; Thank you. Thank you Anskar. Um and thank you all for coming and the invitation to speak to you today. My name is Phil Howard. I'm a professor at

the University of Oxford but um more important I'm on leave as the head of the international panel on the information environment based in Zurich and we're a group of scientists some 500

scientists now from over 75 countries uh who specialize in technology issues we've run a scientific panel on AI and peaceuilding uh we currently have a panel on AI and financial on economic

inclusion right bringing one everyone in to the benefits of AI I one of our first scientific panels to run uh was a panel on global standards for AI auditing. The reason we started with with this as the

a project for the IPIE was to help establish that it is possible to speak of global standards for AI. It is possible to expect it is reasonable to expect firms to share model cards. It

was part of an auditing process. we can develop language around uh data provenence. It's reasonable to expect a firm to explain where it gets its data before it trains models.

Having established all of this with our engineers, our engineering community, um we've moved on to this question of what an audit should look like and we've all had experience with the myriad of laws,

the myriad regulations that are now in play in jurisdictions around the world. The law barely keeps up, right? It rarely keeps ahead of the it's rarely ever ahead of the technology. The

technologies move so quickly. The scientists uh innovate, industry implements. And one of the lessons from our work on AI auditing is that um the best way to build confidence in an

effectively audited AI system is to rely on several different kinds of groups to settle on terms and to conduct the audits. Industry self-regulation rarely ever works. You need community

groups. You need government regulators. And government regulators on their own shouldn't forge ahead. Yeah. Without consulting with industry. So it's this shared conversation across different

types of actors that helps define what good governance looks like and what it would mean in the context of AI. Now working ahead there are a number of uh social issues including social

inclusion. We want to make sure that AI systems benefit benefit everyone. To get there, I think the next challenge for us is to agree on how often how regularly an AI audit should occur under what

circumstances an AI system is actually high risk. Some of the AI applications are relatively no low risk. Yeah. And require different kinds of oversight. But the message I want to leave with you

with these opening remarks is that this has to be a collective conversation, right? Moving ahead with a particular sector without some sense of oversight and shared language, shared conversation

is very likely to leave the other players out of the loop and very likely to result in less confidence, less public trust in the AI systems we use and the the the business operations that

implement. So, thank you. &gt;&gt; Thank you very much, Phil. um you one of the things that you mentioned is that the diversity of areas where AI is being uh used means we need to have

specialized insights into the challenges that comes in each of these um and this is one of the areas where Ravi you've been looking into the sector specific um challenges so uh please if you could

share some of your findings in that space. &gt;&gt; Sure. So once again I I join the other panelists in thanking all the people who actually found their way to this room.

So [laughter] and and again thanks for the invite to be on this panel and and I kind of agree with what

and Phil have said so far in that you know there is no single you know framework that's going to work for all all sectors right so if you look at the uh India's governance guidelines even

there we have recommended that uh you know each individual sectoral regulator should take a reook at what is it that they are doing right now and and know and figure out how AI is challenging

those regulations and what are the gaps that they should cover up as instead of directly starting a conversation on overarching AI regulation right so I think I think it has to be again like

what Phil said it has to be a much more collaborative effort and to tell you how hard some of this can be I'll give you an example of something that happened in one of the things that we were doing so

you know that um you know there are these uh computational models that people used to estimate the age of a fetus right from scans and many of these computational models have been you know

based on you know western babies right and they underestimate Asian babies by as much as 30 to 40%. Underestimate the age of the Asian babies by 30 to 40%. almost making the uh you know the

inferred age medically useless. Right? So what we did so we actually you know worked with a couple of hospitals and built the model some of my some of my colleagues built this model and they got

this to be very very accurate in predicting the uh age of the fetus and then they go and show this to some doctors and the doctors say hm you are using the head circumference of the

fetus as one of the features that uh you're using for the prediction. All of this is great. I mean it passed all our internal testing. The laboratory testing was good and all the held out said I

mean all the things that the you know the machine learning people would do it performed very well. But the doctor said you were able to measure the head circumference accurately because the

images that you were using were gathered under very strict quality control and that's not what happens in the field. So if I take your model and deploy it in the field, it's probably going to be

more erroneous than the western model that I have been using already because the quality of scans we get, you cannot measure the head circumference, right? So we had to go back and now work

with features which the doctors said I can measure accurately even on low quality scans and even then we managed to you know get a good model now. So right now it's in field testing around

various hospitals in the country. But the point is there is no way you can talk about assurance of your model. No way you can talk about verification of the model unless the people who are

using it right are involved in the evaluation pipeline as well. I mean otherwise you're going to work in a vacuum and come up with these kinds of uh uh metrics uh uh which um you know uh

are great in work but useless in practice. I can keep going on. I mean there are so many different domains in which we have found out that people have been reporting many metrics which are

not useful when I have to take decisions about deployability of the systems. So there's a lot lot of work that needs to be done. So as you said it's very much a

developing field. So thank you very much. Um and yes, you know, with the need for having specialized kind of understanding in order to be able to properly assess

whether this is an AI system that can be valuably used. Um may I perhaps uh turn to you an uh for some insights as to how industry is actually looking at this space and also where the value is for a

lot of industry in getting for instance third party assessments. I hope you can hear me and thank you Anska. No, I mean bringing a business perspective to this um EY works across

uh all sectors and and works global north, global south, very different societies and environments um including environments that are highly regulated and environments that are highly

unregulated. We've really seen a shift over the last I would say two years which is interesting. Um you know more and more uh the conversation is around uh embedding AI less about adoption. Um

so businesses are trying to make sure they get the value out of AI that they know how to run it properly. um but also boards and and uh seuitees so executives and and others are uh still struggling

with the complexity of the systems and the use cases and and a lot um is around the unknowns right what are the unknowns am I sure I know what I don't know of course I don't right so what we've seen

is is the need the growing need to build confidence in the AI systems that are either being bought if you're buying them in and adapting the systems that you're deploying and perhaps systems

that you're then selling to to customers. Um systems that are affecting your operations, your bottom line, but also potentially your ability to recruit and keep the best talent. Um you know,

do these systems reflect the ethics that you are associating with your brand or are they out of step? Um and are these systems trustworthy by your customers? um and beyond your customers, your

stakeholders. So, governments, communities in which your employees and your brands operate. So this question, this challenge of understanding what's in the black box or you know whether

your systems are doing what they should do is really kind of becoming more better defined in the minds of you know boards seuitees but also we're starting to see more questions coming from two

other communities which I think is still um you know anecdotal but interesting one is the investor community you know is my money is my investment being well protected you know, do I understand if

I'm buying into a business? Um, do I understand how this business is governing AI and how they're managing the value of that AI and the risks? And we're starting to see also questions

coming from very big insurers. You know, do I ensure some of these systems? What does it mean about the insurance products and services that I have been offering uh given the widespread use and

embedding of these systems which are still you know phenomenal full of promise but also still not necessarily well used or sometimes surprising. So what we've been seeing really is more

and more interest in the use of assessments, AI assessments, whether they be AI audits, you know, as you said, Ansgar, they're also called AI assurance. Um, a lot of focus has been

around assuring the governance uh and the management systems. So for example with ISO 420,01 this standards ISO you know so the governance if you like around uh the use of the systems there's

also a lot of interest around compliance you know are these is the compliance are the compliance obligations being properly managed and and addressed um there's a lot of talk around assessment

of the systems themselves um really the debate is ongoing as to whether proper assessments of the You know if the system itself of the algorithms is workable is trustworthy

there need to be very clear criteria pre-established criteria whether they be quality operational or other against which these assessments are made and that's very challenging. Um but what

we're seeing is a lot of interest that goes well beyond compliance. We're seeing businesses looking at this as a governance question and some of them are even starting to think about this as a

source of competitive advantage. So yes, you might have some legal and regulatory obligations, but if you're running a good business, if you've got a good brand that you're building, if you're

trying to distinguish yourself from your competitors, how do you have some sort of a marker that your use of AI systems or the systems that you're putting out in the market as services or, you know,

products? How do you show to your customers and other stakeholders that they do what you're expecting them to do, that you understand the risks, that you manage them well, and that you're

able to manage the value around that? So, we're seeing kind of this shift from compliance to governance to actually strategic management of value. And um the conversation is is developing. Some

countries are still very focused on regulation though, you know, we're also seeing, for example, in Europe a bit of a easing of the focus of hard regulation towards a more pragmatic mix of

regulation against the highest risk and perhaps more flexibility and more time to learn around the risks that are still, you know, less well- definfined or that might, you know, um, if

overregulated, slow down necessary area innovation. I think where I'd like to to kind of conclude is to give a couple of practical examples. I mean we're seeing without giving company names we're

seeing at one end of course in financial services a highly regulated industry the use of AI you know the risk assessments whether it's using kind of a NIST type uh risk framework or other frameworks to

really make sure that the financial service um leaders are cognizant of the risks that those the risk management is embedded in corporate governance and and reporting were appropriate. But we're

also seeing for example in very large FMCG consumer good products um you know one of our one leading uh global company not just looking at um voluntary third-party uh AI assessments to

understand how AI systems are used in their organization but they're also starting to use it in a way to um define some KPIs for themselves define make sure that uh Accountability is clearly

defined in their organizations and with a view to start putting themselves out as an example which other players in their value chain their suppliers right um their partners are

asked to emulate. So putting themselves first using these assessments as strong benchmarks and third party you know perspective and objectivity and independence to to make those solid but

also kind of pulling the ecosystem pulling the value chain along them. So uh lots of examples to watch and a developing space. Thank you very much an um very

interesting and to hear how the demand for these types of assessments is coming from multiple directions. Now so far we've heard about the kinds of assessments that companies are looking

for the kinds of assessments and challenges around this from a system or organizations perspective. But one of the big challenges of course is who's doing these assessments and are they

doing them in a way that we others can rely on and also that can compare results from various different assessment providers. This is obviously a space where the ACCA is active. So

Narinan if you could share some of your uh insights from the work that you've been doing. &gt;&gt; Thanks Anskar. Uh so I guess from an ACCA perspective our interest is in

professionalizing of AI assurance as a discipline. So what I mean by that is um really understanding um this is a relatively new space and at

parts of it could seem a bit like the wild west but we think there's both a need and an opportunity uh to understand how we can streamline how AI assurance or contribute to streamlining how AI

assurance is thought about across the ecosystem. That's the conversation we have with policy makers around the world. Um, and that's where we as a global body seek to share best practice

examples from one part of the world to another because the more there's a common understanding as AI scales, if you want to trust AI as it scales, you need to trust the AI assurance

process that goes with it. And if that is very um confused and not very um streamlined, that will introduce um risks to confidence and trust. Now in terms of professionalizing AI assurance

as a discipline, we see broadly two or three key pillars to that. The first pillar is and which has been alluded to before developing a common understanding of what do we mean by AI assurance? For

us, there are probably some some simple things that go with it. One is what specific attributes of the AI system are we looking to test? Secondly, what is the basis for the testing or the

methodology and are we willing to share that with people who are consuming the AI assurance report? And thirdly, are we ending up with a pass fail? So either you pass the test or you don't. The

reason I say that is because you might do a very insightful report about an AI system telling you all about its design factors, its pros, its cons, how it can be improved, where it could be best

utilized. that might still not be an AI assurance report if it doesn't really do those things I just described in terms of what did we test, how did we test it, and did it pass or fail. Um I think

linked to that is a common understanding of terminology. You talk about assurance assessments, AI audits, risk reviews, compliance reviews, I could go on. Um so there needs to be clarity about what

these things mean and also what level of confidence are you looking to give from an AI assurance uh engagement. um you might have something which is quick and dirty for some certain types of AI

systems. You might have certain very high stakes high value uh situations where you need a much higher level of confidence and the AI assurance the way it's communicated of what it is needs to

take that into account. So as I said from an ACCA perspective we're speaking to policy makers around the world on a few things. One is this idea of common and I would invite any others in that

space who want to talk to us please reach out. We're happy to have the conversation but it's about common understanding. It's about sharing best practice globally. It's also about

explaining the value of AI assurance. A government can't just set rules top down and expect everything to happen. AI assurance will give you a bottom-up mechanism where if it's streamlined

properly, providers will create a marketplace where highquality AI becomes something that is almost um part of an industry that's developed in within an ecosystem. Um so that's the key thing. I

think this there's a couple of other small pillars. one well not small but pillars that I would see alongside that two is placing ethics at the core. So I think um you know we were talking a

little earlier about the sandbox and I think those are essentially very very important in India. We talk about a technolal framework um and the idea of embedding compliance within the design

of the AI is very sensible and it's correct but it's also the case that we have to make sure that alongside technical testing we have the thought of the human in the loop. I think Ravi's

example about the circumference of the brain really highlighted that of the head. So um this idea about human in the loop and this idea about transparency. So what is assurable, what isn't and I

think an I think sort of referred to that in in something she was talking about. So if you're not ethical and come across as ethical as an AI assurance uh product and industry, no matter how

clever the technical testing is, people will not trust it. So it's really important to bear that in mind. And I think the final pillar I'd say is skills, education and skills. Um we are

an education body a professional member education body we train half a million students in 180 countries. So the point is people need to understand what AI is first and then to understand what AI

assurance is and then to understand how to get value from an AI assurance output. Um we are embedding it into our own qualification. We are um thinking about um how we include electives like

data science into our qualification for an accounting qualification that is relatively new. So the point being that when we put all of this together, AI assurance is something which is new but

also something which is um very familiar because areas like assurance, risk assessment and analysis can be transferred. There's a lot of transferable learning to be had there.

So I'll pause there and uh I'll pass across then if I may too. &gt;&gt; Thank you very much Norinan. Um I think you very nicely and clearly laid out sort of the core principles that need to

be uh followed in order to be able to get real uh reliability out of these kinds of assessments. Um now so far we've heard a lot of um insights regarding what is needed in order to

have these assessments uh really deliver good value. Um but I think one of the questions that will be on many people's minds is what does this look like in the reality um when we get to do this in

practice where things aren't ideal. This is an area that you've been engaging in uh Jibus. So please uh we all would like to know. &gt;&gt; Yeah thank you Anskar. Um so I work in

the intersection of AI governance, public policy, uh socioeconomic impact of AI and implementation uh on ground especially from a global south perspective and currently my focus is

mostly about how we can build institutional capacity uh in uh for responsible AI in our universities and in our public uh systems as well. So here I'm going to speak from a

practitioner's lens uh from a global south perspective. So looking at what actually happens when these policies these standards how does this look like uh in a real world environment or uh

environment which are much not prepared. So when it comes to assessment um if you want to build real trust right uh the question is not about standards right there or lack of standard but there are

many standards out there um but it's all about how these standards uh actually look in less how do I say less matured uh environment you know it it plays very well in highly mature

corporate environments but that's not the case because often it's I felt that uh the conversation around AI assessment and assurance uh assumes that uh most uh deployers are large companies with uh

compliance uh team and uh you know and assuming that there is a stable regulatory institution and uh importantly access to specialized uh auditors right uh so but if you look at

uh countries like India or global south uh a large level of deployment uh happens in uh very different environments like professor revi mentioned it happens in public hospitals

right uh state education systems or small fintech uh employers or uh local government bodies universities so these uh deployment doesn't have the kind of infrastructure and pipeline to have

efficient AI assurance or you know evaluation uh framework and most importantly there are quite a lot of challenges uh or barriers uh as well uh for example, but they are not technical

in nature. They are mostly in institutional in nature, right? So we have a huge shortage of trained uh evaluators. Uh and uh look at the fragmented standards across various

jurisdiction, right? And importantly, there is an unclear um incentives for companies to uh companies to share information. And finally the lack of affordable tools especially uh for

institutions like once I mentioned so I think if you we can come across you know road map a common goal you know common agreeable road map then we would have a system where uh you know governments uh

you know procuring AI systems which have built-in evaluation requirements or uh universities teaching assurance alongside AI and ML courses right so and open source open source and open tools

available which will bring down the cost of uh testing as well and importantly uh cross jurisdiction calibration is required uh so that these results can be uh you know interoperable across the

globe as well. So that's what I hope for but I will stop here for the rest of the discussion. &gt;&gt; Thank you very much uh Jiu. Uh I think these are very critical elements that

you've raised on how do we translate from the conceptual thinking to the reality. Um and you very nicely open it up to our what we are looking to work towards getting a roadmap towards having

an AI uh assurance ecosystem that large parts uh of the world are at least partially agreeing with so that we can get interoperability between uh assessments. So this next is going to be

an open question to everybody on the panel and actually I would also like people in the audience to think whether or not they have some uh views on this. Uh so in the um later part uh I can

maybe go to you. Um so really when we are looking to create an ecosystem of AI assessments um the idea is that this will be a way towards building trust that people organizations and

governments need in order to gain the benefits that AI can bring. But what do you think are the core ingredients that would make this successful? And this is an open question. Please uh who would

like to start? I actually agree with Jibu's uh few points that he raised and we've tried all of that. Um we we're doing training uh we've put out open source testing

tools um um we tried to see how to make it practical um and what's happening to people on the ground. Um and I think the experience is that you you really need

to do it right. I think it's very hard to talk about it at a conceptual level as you mentioned um and that's why I think this idea of actually testing the systems whether it's a system so we have

a we have one particular example where uh it's a financial services uh system where it's aicing implementation where you have uh an agentic system pulling sources from different parts of um of of

a bank um in terms of databases and so on and making a recommend recommendation to a customer and some kinds of investment suggestions let's say over an email. So that requires multiple steps

and multiple tasks. How do you think about assuring a system like this? Is it that the email must be correct? Is it that the in between steps has to be correct and how do you go about

technically um defining it? Um so that's one right. Um the methodology in terms of assurance is important. Um what the bank cares about is also important. So when we ask so what's your threshold

mean we talk about pass or fail right so what's your threshold for passing or failing something um actually is it 90% generally accurate is good enough or is it 85% I think that's a that's a really

difficult uh decision for banks to make right or for any organization to make so they must be sufficiently comfortable um to release this and clearly none of my fellow regulators are defining

thresholds today uh uh and what we would need is really to see what's coming up from the ground. So this bottom up um conversation that we have been talking about just now is really important

because with data then we can start understanding where the trends are and uh and then be able to then set the water level accordingly and it's also different for different use cases. So

that's why a lot of um experimentation, a lot of trying, a lot of doing it's extremely important and this is a practical way to move it forward. On the one hand, we should define our standards

and our expectations on the other actually get things moving from the ground. Um that's why again I'll just say that um that's why we wanted to do this um um assurance sandbox because

it's really about trying and doing um hopefully then be able to coales some best practices but with with enough of us doing this around the world and hopefully we have sufficient data to be

able to start understanding where we feel about these things specific to use cases and then start setting some standards in the longer term. Ravi

so and I want to go back to a point that Jabu made earlier right so one of the biggest challenges for us especially in the global south right so even if you want to set standards I mean how you

normally set standards mean you have some benchmarks against which you' evaluate it but there are no benchmarks for things that we would like to test in the global south so I'll give you an

example right so you would like to make sure that your language model is sanitized right it is not you know outputting swear words. I mean do we have a dictionary of swar words in I

don't know in Santali right we don't right so we don't even have that kind of resources uh in in uh you know in the global south where we can go back and say okay look here are

the standards and these are the you know tools for you to ensure that your AI is meeting those standards because we don't have that for for a large fraction of the geography so that's something that

we really have to work on right so one of the things that we are hoping thing that will come out as an outcome of the summit is the setting up of a global south focused you know tool repository

that will allow us to do this in a more reliable manner. So that's something that we certainly need to do uh to make sure that uh we have more assurance in the AI tools.

&gt;&gt; Thank you J. I I think to add to uh the road map what Revi and Win just mentioned um one thing I mean just a thought you know we can do is build more capacity beyond regular

auditors you know how we can uh think about um uh assurance as a largely distributed uh capability right so it will be like faculties in universities and colleges will get to evaluate the

models are deployed in the class I mean who's better uh to do that than them or domain experts can do the same with models you know deployed in animal husbandry or uh

agriculture or many other uh fields as well right and interestingly you know um I think uh uh Revy was mentioning before that the users also needs to have a uh a say in how these models are evaluated.

So that brings me to the idea of how we can have how we can we have to measure you know how we can empower civil society groups to also to audit public deployment of AIS as well. So uh and not

to disconnect technical skills here we also need the technical skills for uh uh for a for testing and uh documentation as well. So this can be one of the recipes or uh however you say you know

for the road map you know &gt;&gt; maybe if I can &gt;&gt; thank you I I I like the direction of this conversation because it seems that we all need each other right so that the

research community can produce these lexicons for um uh hate speech misinformation or bias that the scientific community does work on these things and in very important ways we

found that uh governance is a key feature because having an uh having an understanding of what your exciting new tool does uh and showing that you have some oversight is an important um

demonstration to the rest of us that you put in a good faith effort. Um in fact we we already know what independent auditing looks like, what good audit systems look like. The there needs to be

some independence. You need feedback loops from your consumers, the people who are using your um using your toolkit. uh and the assessments need to be done with some arms length

relationships. So in an important way we all we all need each other. &gt;&gt; Yeah, if I can just quickly build on I think absolutely right. I think to what Ju was saying I think you're talking

about a distributed you distributing the capability and within for example the accountancy stand. So firstly I think global harmonization to the extent that's realistic and while taking into

account local needs at least certain principles so everybody's not reinventing the wheel and overfitting um regulatory approaches uh when there are certain common things that people can

learn from each other very quickly but also I think if you look at domains as well like in accountancy there's let's say the the international audit and assurance standards board that sets

audit and assurance standards for 200 countries around the world they have a standard ISA 3000 at the moment which will talk about how to go about um assurance as a general principle without

thinking about AI. So you don't need to start from a standing start. There are obviously lots of other standards I e various others which are all interconnected. They're not

contradicting or duplicating in fact um they might give you criteria for AI while you might something else like this might give you criteria for what assurance steps looks like. So there's

something here around making sure and within that within that standard in fact there is a specific provisor for something called a practitioner's expert. So you could be someone who's

not a technical developer but you could work with a technical developer to conduct an assurance engagement. So these norms exist um and it's about really all all of us across the

ecosystem working together to really maximize things which are out there while building new norms as well. I think you make a really great point. I mean just trying to step back and look

at this in a you know look at the differences across the ecosystem at what at one end you have the frontier kind of models um with either high risks or unknown risks um and there the you know

tools like sandboxes um other uh cooperation with academics and transparency uh is important. There are also are these AI safety institutes that have

started to come up in different countries following the Bletchley summit which was the first of these summits whatever it was 3 four years ago. Um what's key here is sharing sharing

information sharing learning and building a better understanding of how these models work in real life. So that's kind of one end of the spectrum. At the other end of the spectrum is kind

of mainstream, you know, midsize businesses that are looking for investment, that are looking to protect a brand. Um, that are trying to communicate to their employees, their

customers, their partners, their investors whether their use and their embedding of AI is kind of, you know, is it done properly? Are you running your business properly? You happen to have

AI. Well, that's another thing you have to run properly, but you're still having to run your business properly. And what's important there is having kind of comparable assessments. So back to what

are some of the emerging standards not over complicating but identifying some areas where you know there are certain risk categorizations that are comparable across geographies. there are certain um

tools to assess and standards like ISO standards to assess whether the if you like the management system or the governance around the use of higher risk uh AI systems or higher risk use how is

that done it's really important that those assessments are robust it doesn't mean complicated robust does not mean necessarily complicated it means clear it means done by uh you know a qualified

expert it could even be an internal expert. Um, ideally it's an external indep independent third party. Uh, but there is if you like a need in the ecosystem for comparable

basic kind of labels saying this is well-governed, this is safe, you should only use this in these circumstances and you know carry on doing business. Um, so I think there are different dimensions

to this question of assessments. um we're all still learning and certainly you know as a business person myself what I find you know most promising in this space is back to what Phil you were

saying the dialogue with policy makers in Ujiu with policy makers but also with academics what's you know as a European um having been part of the dialogue on the EU digital services act whatever it

was 3 four years ago what was very noticeable is we were no longer looking at laws that were set. Here's the law. Here it is. Go implement it. It was this is what we think you should be doing.

Here are some things we're going to be checking on and let's keep talking because we're going to need to develop guidelines with you as we understand how this works and doesn't work in the years

ahead. And I think that's a new way to kind of look at um you know governance voluntary and regulationdriven approaches to this space. We are learning. We need to be transparent. We

need to be clear about where the greatest risks are and what are the needs of society, investors and communities in this space. &gt;&gt; If I may just go ahead just quickly add

on to what And just absolutely agree. I think there are many dimensions. I think for many of us in this room u we are more worried about application about how we're using AI. uh I mean we clearly

about worry about frontier models but that's a different set of assurance alto together but right now it's about application and use cases and how AI is actually going to be adopted and

deployed by all of us so that we can all be be able to enjoy its benefits and that's why I think the conversation here is really about how then do we build that capability to reliably deploy AI

and be assured that it's actually going to be useful and safe right um I think the point about testing is important and I and um Narin makes a point about you know what are some of the common

baseline common issues around that we need to test for um we've done some work in that space um we actually identified things like reliability hallucination undesirable content security as some

common areas that all sectors and all companies need to worry about um and put out a testing toolkit that uh helps people understand how to go about thinking about testing in that context

it's not one size fits all but at least in these common dimensions how do you think about testing so it's called the starter kit for testing LLM applications not LLMs but LLM's applications. The the

lead author is here is here today. I just want to point out in case you want to find out uh and uh and and hopefully it's a good resource for all of you in the room. Yeah.

&gt;&gt; Great. Um I think one of the common threads that uh you've all brought in is this is a journey that we're on on learning how to really do this. And in order to to do that learning, it means

getting getting into this, doing it, testing it as we're doing it, finding out where the challenges are so that we can then bring that up also into the conversations with policy makers, with

standard setters in order to get those details in there. As somebody who's also engaging in some of this standardization work and having had conversations with policy makers, I've been in the room

where um I've had this observe the pingpong back and forth. We need a criteria of this is good enough. Well, we're not the ones who are qualified because we don't have the expertise. Uh

but we are not the ones who are qualified. We because we don't actually have the mandate for this. Who's going to be able to get to what are the criteria? And it's something that needs

to be explored. As I mentioned, I do want to be able to go to the room if possible. I realize we don't have a microphone, so I um encourage you to shout it out and I'll repeat what you

said uh in order to get it for people online to also hear. &gt;&gt; Um some great points discussed right now. Um my my question is maybe to Dr. Har and

thank you. I need to score things but yesterday the feature reported that anthropics was used in the oporation. &gt;&gt; Yeah. Uh so we we're talking all about

regulation and regulation is great and I think um Dr. Howard you mentioned that uh we should try and make sure that there is a a coordination between government and um and sort of companies.

So how can we ensure that the responsible use of AI upholds things like international law and minimizes harm? I mean like like Mr. Vinatan said, how do we put ethics at the core of AI

when looking at sort of the assessment and and building confident practices? &gt;&gt; Thank you. Um go ahead. &gt;&gt; Um yeah, okay, we'll get we'll take one more question. We don't have that much

time. Um please behind you there. &gt;&gt; All right, good morning. Uh thank you. I think the question that the guy asked pretty much mine is on the same line. Uh so we're it's in yellow fonts. It's

written establishing the foundation of responsible confidence in AI. I mean and you guys are talking about a bottom top approach more than the top top to down approach but we just saw when Grock went

live we should look at the usage everybody had a field day in the name of anonymity right and we know how the moldbook thing was transpiring. Finally MIT came out saying that this was more

on the human side, not really AI agents which were doing it. But it did set the alarm bells ringing, right? I am in education publishing sector. I hate AI because with one prompt a 300page aer

book could be written in a minute. I'm not a fan of that. But we have to implement it because the higher management wants it. But you from a B2C standpoint, you really need to put the

legal framework in place before you give this this thing called AI to the users because they're having a field day without facing any consequences. But I I really feel it needs to be top to down.

It could be very hard. We've not achieved success with internet. We know what all happens on the internet. But now this is a new technology and I I really feel that

unless and until the legal framework is put in place this technology should not be introduced especially to B2C and B2B yes somebody said that the frameworks are there so that the regulation can

happen but when B2C comes in it it becomes really difficult. So I want to know somebody can have an end &gt;&gt; and we have one minute for some answers to these questions. Go ahead. My my

quick response would be that some of the questions are great. Some of it's a little out of scope for what we've been talking about today. Um but I would offer that and I don't know the

particular Wall Street Journal article. I would offer that this is also a good moment for corporate leadership. So we do have international norms around human rights. Yes, there is an international

multi-party signatory statements on what human rights the code of human rights are. There are dozens if not hundreds of philosophers who've given us ethical criteria to reach for. Um this is a

moment where corporations can lead right show that they've done their due diligence and in so doing help regulators understand what reasonable high but reasonable bars can be set.

&gt;&gt; Can I just add one thing? uh the the gentleman mentioned the gro incident and also the reason why we are advocating for assessment and assurance is to prevent this kind of uh things I mean

not just like large scale issues not from happening I think on mentioned about other sectors right the aviation and many other sectors where you have this kind of uh systems in place so I

mean just want to add that &gt;&gt; I'm afraid we have run out of time um as I mentioned at the beginning we do invite you to join us on this journey. Uh the QR code that you see up there

will lead you to a link that shows uh a good to report that we've done on this topic. This is a report between EY and ACCA which also references work by uh Singapore IMDA as well as the IPIE. Um

please uh do join us on this if this is an area that you are interested in. Thank you very much. [applause] We have all of our respect for each one of you.
