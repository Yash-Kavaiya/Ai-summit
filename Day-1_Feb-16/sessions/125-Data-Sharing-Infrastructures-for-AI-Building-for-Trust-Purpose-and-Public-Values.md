# Data Sharing Infrastructures for AI: Building for Trust, Purpose, and Public Values

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 17:30 ‚Äì 18:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 10 |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/5CwyE-MO4XA?feature=share) |

## üé§ Speakers

- Astha Kapoor, Aapti Institute
- Chenai Chair, Masakhane Language Hub
- Rahul Matthan, Trilegal
- Saranya Gopinath, RazorPay
- Vijay Suresh Kumar, Gates Foundation

## ü§ù Knowledge Partners

- Aapti Institute

## üìù Summary

This panel discussion is part of the India-AI Impact Summit 2026 which seeks to highlight the demonstrable impact and tangible progress in global AI cooperation. The objective of the panel is to create common principles and guidance for data as DPI that support openness, accountability, and public value in the age of AI, and work towards concrete impact.

## üîë Key Takeaways

1. This panel discussion is part of the India-AI Impact Summit 2026 which seeks to highlight the demonstrable impact and tangible progress in global AI cooperation.
2. The objective of the panel is to create common principles and guidance for data as DPI that support openness, accountability, and public value in the age of AI, and work towards concrete impact.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/5CwyE-MO4XA/maxresdefault.jpg)](https://youtube.com/live/5CwyE-MO4XA?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

unlock the value of data uh while safeguarding rights. Um and in that context we've been looking at what are the different models as countries become more AI ready that work for different

use cases and what we've been realizing is that it isn't necessarily about the techn technological architecture but maybe about the motivations that countries are trying to solve for. So

we've heard a lot about uh questions around sovereignity. So a lot of countries are thinking about how do we build data exchange systems that are amplifying sovereignity. Uh we also have

been looking at motivations such as delivery of government services which obviously is a big use case for um for governments. We also look at questions like how do we make sure that there's

transparency and accountability in the data exchange ecosystem and what emanate from that and and I have uh an incredible panel to talk about some of these fictions um and you know I'm

joined by Vijay from the Gates Foundation Chennai from Asakane Sarana from Raza and Rahul from trial these are some of the minds who are going to be do you have have written and are doing a

lot of the incredible work around how do build data exchanges. Um, and I just wanted to start with a quick question to the panel and we'll just go this way and then come right back is um, you know, do

you think that there are there are these frictions between unlocking data for innovation on one end uh, but then also maybe safeguarding it for sovereignity. Um, and how do we think about you know

community value from that perspective as well? Well, it's great to be here. Um, you talked about tensions. I think uh there's data generation, there's data

sharing, and then there's the safeguards. They're kind of pulling in different directions, and it's all about incentives at the core. So, there's got to be a a viable sustainability model or

a business model. Um, so it's got to be value exchange. I think that's really the heart of all this uh is there value being exchanged? If so, then you can get all the stakeholders lined up and uh get

the exchange going. &gt;&gt; Thanks, Aster. Um I feel like if we give you the right answer, you will have a million dollars. [laughter] This is a million dollar question. Um

but to put it out there, yes, there are tensions. There are tensions um around who actually has the data and was considered the data steward particularly looking at African languages um the

assumption is there are there's a cultural and social norms to it and they identity markers so it's not everybody who can simply say I'm creating this language without actually following the

appropriate route and then there's also kind of like the point that VJ is talking about the value exchange it also comes back to the value exchange for that particular if you're thinking about

community which is that when looking at African languages they cannot just focus on an indiv individual. It has to be communitydriven because that is the type topology of most different cultures um

and exchanges and relationship building. It's based on community. But the question becomes whose value in that community? Because we still do exist in a context of um gendered inequality,

gendered inequity, um rural inequity based on rural versus urban. So someone who sits in a university in an urban area might say, "Great, let's go collect this language." um from that rural

community so that we can provide some technical solution without consulting that community and they will think that they are the person who can make the decision because they hold the

university qualification and that rural community might actually then say absolutely not because you will begin to learn things about us that we have managed to exist and be under the radar

without this knowledge being public. So then that is the question that needs to be added on into the value and understanding the tensions particularly thinking about community power and

already existing inequalities. &gt;&gt; Thank you. &gt;&gt; Um no I think in its current format yes tensions do exist but my argument it doesn't have to be

that way. Uh I think fundamentally to your point as well, the friction comes about when there is a difference between the entities uh being able to uh extract value and

the data contributors or the actual data creators themselves and the current structures do not allow for those to be the same as having the same person. We look at I mean for a very pardon my very

crude framing but we look Let's say some version of big tech being the entities that extract value and that the entities holding uh are in this constant they find the tension as why they do not see

the value being extracted as coming about to bring value to themselves. They do not see the data extraction process as giving anything to them. And so it is the the balance between sovereignty in

terms of protectionism keeping the data out of those hands because there is no world where they envisioning where that value process benefits them as well. Once and I completely with you that is a

little utopian and I'm sure that's why so many billion minds are in this room. But once we figure out a way to bring back value in the data extraction process to the entities contributing or

creating those data sets then at least the the incentive process doesn't need to be an eitheror. it becomes more streamlined and then we can get to the actual business of extracting the data

because ultimately that is a requirement and in all of this conversation sometimes that gets completely substitute. Um before you jump in, I just wanted to

throw something at you because I know you have this paper on the New Deal for data which also talks about you know what are the different kinds of rights we're trying to safeguard against and

also how do we solve for some of that epistemic injustice that Chennai touched upon and so in are you proposing that this new deal for data will take us closer to the utopia?

&gt;&gt; Short answer yes. Uh, I wouldn't have spent three months writing the paper if I didn't think at least I'd have a chance. But I'm going to now since since when you said we've decided to disagree,

you were looking at me. I'm clearly the disagreeable one. I'm going to start disagreeing, which is first with the fact that tension is a bad thing. &gt;&gt; I think it's actually great that there's

tension. &gt;&gt; When there is tension, we are forced to think hard about the decisions we make. If there's no tension, we stop thinking, right? So I'm glad that there is a

tension and I would be more worried if the world starts to think there is no tension and if we and I think quite frankly a lot of us in today's world don't recognize that there is a tension

and that really is a problem with all the things that we've said so far. So first I would like to say embrace the friction. Uh friction is a good thing. It allows us before we take a decision

to think hard about the balance that we're making. The point that I make in the paper is um essentially that the governance frameworks that we have today and I pick three um uh two really I pick

uh data protection I pick uh copyright and then I speak about data sovereignity uh but that's sort of not an issue that we have I think as Jennai says it's an issue that AI brings to the four that we

really have to think about but if you think about data protection data protection is a framework where we have enforced cost data scarcity. We believe that the way to protect personal

information and then the way to protect privacy is for the people who collect it to have as little of it as possible. Which is why you need consent before you collect. You are obliged to only collect

for a narrow purpose. You are obliged to only collect as much data as is required to achieve the purpose. Data minimization. You're required to delete the data after it has served the

purpose. Retention restrictions. This is the global framework for data protection. It's a framework that we have in our country. I suspect it's a framework we have around the world. This

framework is at odds with what artificial intelligence needs because AI needs as much information as possible for as long as possible. So there is a friction now with the governance

frameworks. We think about copyright. Copyright is a lot. I mean the paper's utterly provocative. It's like you know it's it's shocking that I managed to get away with publishing the paper because

it's such a heretical paper. Essentially as a lawyer I'm going against everything that I've studied for, you know, I'm not going to say how well no I've been writing about this for a while. I mean

I've been moving in this direction for a long time but copyright is essentially uh you have the word copy in copyright and therefore you assume that you can't do any copying. That's not true. When we

read a book we sort of copy it in our head and how different is that from uh the way it's being adjusted in the large language model. And so I say the fundamental basis on which most people

are going against copyright is that you cannot copy a book. Now quite frankly the large language model does not copy a book. It reads the book it adjusts its weight but nowhere in the large language

model is a copy of the book stored just does not exist in the company. The moment I say something they will shut the mic down but but it's it may be in the large in the company but it does

not exist in the model. And so there is a fundamental need for us to rethink the way we have governed data because if we don't rethink the way we've governed data, we're not going to be able to get

the benefit of all that AI has to give us. Having said that, I'm not Rambo in the wild west. I do understand that AI is dangerous. It could cause harm and we've got to be utterly mindful about

how we go down the path. And I'm not going to spoil the surprise because you all have to download the paper and read it for yourself. uh there is a solution that I prescribe for all of these things

and perhaps we can discuss that in more detail. &gt;&gt; Yeah, thank you so much for that. uh and uh yes very very provocative paper because it is walking back a lot of the

rights frameworks that uh we've all attached a lot of importance to over the last few years because you know we need some shred of hope in all of this to negotiate but do you uh you know you

also have a paper uh please also read that uh and and and in your paper you talk about a decentralized marketplace and I think the question again is that that you know if we are just allowing

data to flow and people to train their models then there will be some accumulation capital that we have been talking about. So if everything is a marketplace then people who have deeper

pockets will just buy more things right. So how does the proposal that you have like solve for some of that? &gt;&gt; Um yeah the way that current market places do

&gt;&gt; um so it's not an either or. And thank you for the chance to plug my paper. Um but here's the thing right in I to the earlier point in the ongoing conversation

we have to first all come on the same page that there is a need to make sure that data is extracted let's take a step back for a lot of the emerging economies we are nowhere where we want to be in

terms of our data for our own selves being adequately published cleaned transformed even just core digitization we We are not in a place where let's say rural clinics which have a wealth of

understanding and data of rural communities of local diseases of local um conditions for let's say waterborn diseases that wealth of data needs to be digitized for ourselves to begin with

and which brings me to the very important reference of capital. All of this requires resources. It is a capital intensive activity and in the in our quest to make sure that this data

doesn't go into the wrong hands, we are not getting this data out in our hands. This is very much cutting off our nose to spite our face. And that's not really where we want to be. If we want to,

let's say, have healthcare solutions that work for us. We want to have AI healthcare solutions that work at scale to serve our own needs. It's not going to happen unless we have the capacity to

have our data published to be consumed by whoever is the service provider. I am actually of a slightly differing view and I want to I think subtly put I'm at a point where I want to

understand the value of data sovereignty and in what form. Data sovereignty is a term we use fairly widely across a variety of use cases depending in certain cases it's about establishing

jurisdiction in certain cases it's about establishing ownership in certain cases it's about establishing our capacity to negotiate the ownership of the data all of which have their own place my only

argument is if we are establishing data sovereignty we should have a good perspective of to what end and why to answer your point it is not an accumulation of resources. So the way

the marketplace construct is put across in the paper is that in pricing or valuing the data set there is no need to let's say have the same cost point of a data set for a big tech company that you

would otherwise want to give for academic institution. There is capacity to have varying terms whether it's in pricing whether it's in licensing terms. having that elbow room occurs. However,

this is with a keen understanding that this entire construct works only when we as data contributors start having the capacity and willingness to start extracting data and publishing it and

working across mechanisms where it is essential that the value extracted of the data is coming back to the contributing ecosystem or entities to keep that virtue cycle moving. And again

what is the point of it and what is what what are we trying to protect is something we need to know but in the context of but how do we get there ourselves?

Chennai any thoughts on this like question around you know the is is the sovereignity versus value question are they again like I agree the friction is good but are there models that provide

like a through line where you can create sovereign infrastructures that also give value back to communities or the data providers um and again I think the question is again like sovereignity is

coded very much as the state acting on behalf of its people so there's a paternalism to it as well. But is there an assertion of agency via sovereignity as well?

&gt;&gt; Yeah. So I will then give the example of Masaka where we exercise sovereignity because the state was not interested [laughter] um and and in their defense

they were AI wasn't considered a bread and butter issue for a very long time. So Masak is an example of a community that start of communitydriven and in this instance a community of researchers

who went into spaces international spaces and they were trying to present work on African languages and they were laughed out of rooms because it was what do you mean Africans can actually do NLP

and machine learning and they are not in a university in America or something like that. And so then COVID happened and people basically started making use of slack again doing things by the

bootstraps. There have been a number of papers that have been published because it was that knowledge of for us to be considered valuable we also need to be producing knowledge at certain levels

and so then those who had the um capacity and skills or some sort of university tenure would bring along young authors as well. So you find that their masaka papers were there like 20

people on a paper because that was the only way they could get the knowledge out together collectively and I think that was an exercise of agency because no one was paying attention and then for

those who did pay attention they were resourcing the community and this is where I think tension becomes good because for Masak we were being resourced by Google and in that in the

digital rights community no one wants to talk about Google. [laughter] So we existing as this entity of young people who are interested in building up technology. Our governments are not

giving us the resourcing but the tech giant woof is actually saying what credits do you need? What cloud computing do you need? We've got this research center that we've opened. Send

your team here. And so then and so then that is kind of like that indication of when we think about sovereignity again it's coming back to the value of like I know what I need and you can provide it

but then we're also acknowledging the reality that like we when we say community doesn't mean it's you know frictionless and everybody's happy in doing the work but it is definitely

about the understanding what the end goal is and the end goal for Masak was seeing African languages represented on a global platform because they got tired of being laughed out of rooms and so it

became important for them to take back that power and then work with other people to ensure that those communities exist. So now we've got deep learning in DABA, we've got Masakan as a community,

we've got equalize AI, we've got all of these um entities that have emerged from just a tenacity of young people wanting to see their language represented. &gt;&gt; Yes. Sorry. Uh can people in the the the

thing please be quiet. Um the technical team please. Thank you. &gt;&gt; Yes. &gt;&gt; Um uh I want to jump in on what you said about what's the motivation to share

data. So one question is what if the I as a patient for example when it comes to data I have the rights to my data not the gatekeepers because that's the core problem like in the US is the R is not

really I'm not in charge I don't have access to it across hospital system. So that's one one solution that will move us forward and there's a lot of work happening here uh verifiable foundation

technology I'm interested in API. So naturally I think about uh solutions that already exist for doing that. Um the other one is in research I've seen attributions a big one like publications

and then There's also some talking about million dollar idea. There's some startups that are looking at blockchain based solutions where for the data that was

used to build a model you attribute and you actually give them monetary value some value but yes &gt;&gt; u you're absolutely right the substrate

of this assumes that there are rights over your own data for the data contributor but um I the framework framework I like to use not in the reference that we're making but is like

copyright 101 right is that if you publish it you claim ownership of it that it is your data in being the capac in the very act of publishing is how you claim ownership of it now here's the

point right now yes uh I did the paper starts off from a point of privilege and saying that there is adequate legal frameworks that assures uh you know the ownership of

data but even if you take non-personal data right even if you don't go down uh but the non-personal data framework isn't very different. A farmer should have the capacity to he knows his form,

he knows that data. So that is his contribution of non-personal data as well. However, all of this uh we eventually need that structure to transform it from the theory to the

practical to say that I have the ownership. Great. As an individual patient, I have my ownership. Yes, absolutely. Affordability of EHR is a fundamental issue. But in terms of this

construct of being able to derive value in whatever way you use the word value whether it is attribution value. Uh the construct required for everybody from an individual to a farmer to a community to

a small group of motivated individuals being able to claim that ownership and being able to then say we here we are we declare our ownership over this we now want to start the process of deriving

value from this. I think that's the kind of market construct. I know you have a mic if you want. So &gt;&gt; yeah, I mean I don't I so look this whole thing about deriving value and

attributing some monetary amount to it is fraught because because it's actually I mean the way AI systems work is it's not one is to one. So if I've contributed my data, I will

get some value now. But two things happen, right? One is your data is not as valuable on on its own as it is when combined with vast amounts of data. And then to try and figure out who gets what

value aortionment is next to impossible, right? So why did a particular image generation model generate a particular image really well is not attributed to this training data set or that training

data set. it is attributed to the engineering by which this whole thing was put together. So there's a fundamental challenge in valuing this. The second problem with this and that's

really the point of disagreement with with our two papers is that the uh uh once the data is consumed by the model the model never needs the data again. Now what when that happens what is the

benefit to the communities that contribute to the data and so the point of disagreement so I completely agree with this idea of you know we've got to have a market I like the market solution

because that's the best way to sort of uh attribute value but I want to go one step further which is to say that if you are a sovereign AI if you're a large AI company frontier AI company you have

trained with my data that you know has some value I want you to commit to localizing the weights of the model that has been enriched by my data for the benefit of the community that has

enriched it. See what's the value to us of providing data to a large language model. It's such that the cultural and the contextual nuances that are currently missing from western centric

AI models can become more relevant for you know African Indian communities. And if that happens, they just need our data once because once they've got our data, the model is already better. It already

has the cultural context. They can keep that and then release the next version. And who knows what geopolitics is going to be like today. They may just say, "I'm not going to serve India anymore

because, you know, I don't like the way you disagree with me." Now, if that happens, I have contributed to making the model better. Africa has contributed to making the model better. And the

people who have contributed are not going to be entitled to this. And so my claim which is the one step further that that I go what Sarinia has done is to claim that I want to use my leverage.

India is a large country got a lot of data that's going to enrich the large models. I want to use that leverage. Africa each individual country may be small but the entire African you know

maybe parts of Africa is powerful and it has a lot of data that's not written which is why it's missing from the models. We add that data we will enrich their data. If that is the case and if I

have done that I want you to commit to giving me a the the current sort of freeze frame of the model such that I can use it the next iteration of the model you know you want to use my data

you don't want to use you want I really can't say anything but if you use my data to improve your model I will allow so in mine my my paper says uh visit don't move so essentially the models

come to the data the data doesn't go to the model which means I will give you the chance to iterate one model you make that model better and once you make that model better, you leave those weights

down. You don't take my data. You want to make your model even better by using my data again. Come back to me again. But when you come back to me again, you keep that new iteration of the model in

in India or in Africa again. That's the model. I think we have a small window where the models need to be refined by this additional cultural data. I think if we don't use our collective ability

to negotiate, uh, we're not going to be able to achieve this. That sort of is the data soarity answer. for the copyright and the data protection you still got to read the paper.

&gt;&gt; No, I mean for I mean I agree with that part of the paper. So u [laughter] because I also think that I I think that there is this is the answer to that extraction and that accumulation of

value without any ability to negotiate and though at the same time we've seen like in a lot of African countries like this notion of data localization is seen as like the regressive thing to do. It

is the anti-inovation thing to do. Um and therefore you know it has always been said that if you're not going to if you're going to do localized parts of your ecosystem then there's not going to

be any way to come out of it. Um one is just on the ecosystem to you Vijay I know that like the gates foundation has been doing a lot of work on like building an ecosystem and also you know

data is third DPI so how do we you know we've seen the ID and ecosystems being built data seems to be much I mean already on the panel there are you know so many different bits and bobs how do

you build an ecosystem about something that is so amorphous and full of disagreements &gt;&gt; that's a tough question But uh I'll lay a framework. This is

something uh in my own experience working in healthcare um with cancerous disease and I think that model kind of expands to other areas too. Um one is creating

uh repositories of data and making them discoverable knowing that there's data sets out there that could help you build the model. So I mean let's talk about AI readiness. Is that the focus here?

Then comes uh governance of the data um making sure you're fully in charge of who you share the data with. There's audit trails and so on. And then sometimes there's also clean rooms you

know like what you said federated access is a good one where the model moves to the data but the data is always secure. Um and then one thing we've also done is build tools um to make the data usable

because a lot of times data is shared for the sake of sharing especially in research you see that a lot I've checked the box but it really doesn't serve any other researcher so making the data

usable which means harmonization alignment standards ontologies and healthcare and so on um and then attribution we already talked about that I think uh this this this would set you

up AI ready data sets and us as an engage foundation as a as a convener we're creating ecosystems back to your point I think what we did is uh is creating

benchmarks so big tech if they are consuming data sets that we build data sets training data sets for voice um we have our own benchmarks that serve the needs in Africa and we ensure that

they meet those benchmarks and uh the standard ones are not sufficient. They don't address the MMLU benchmarks are not good enough. They're more factual and then the ones that that Sahara

linguistic. So like at Masak how do you safeguard like like do you do the localization parts of it like how do you safeguard do you have the model for how who is able

to use it and how &gt;&gt; yeah so I think this then speaks to our young in the ecosystem it's open for everyone so we've so maybe let me balance it out because again I really

love this idea of good intention um it's this existence of we've built with values of auntu And that is an African um value of practice where it's basically my humanity is centered on

your humanity as well. Where I end is where you begin. Um and that idea of course I mean like I'm saying this it's it's kind of it feels so fluffy and warm but how to actually get it done in

reality is again that approach of getting everybody into the room and the attribution and the contributions. So we've at least I think um at stage one is that collective action of the

researchers and our community everybody on slack everybody creating it is open and people working on it. So we haven't actually and then most of our data sets are hosted on GitHub so it's quite

accessible everybody has access to them but then that's a question we actually are now confronting which and specifically looking at it from a perspective of licensing because I

realize um it hasn't been mentioned here. So for us um and I really would love to hear your thoughts on this. If a question of licensing in terms of like do we have we want for it to be open

source but open to what extent and what aspect is open. So there have been two license frameworks that have been created called Esetu and Noodle. Um and they are talking about the value point I

think that you're mentioning where they're basically saying we will create this data set and it'll be open but it'll be open for Africans for at least two years first and we innovate first

and then other people can then if they want to use the data set then they can pay for it. They're still trying to figure out how it all works. So I think the model moving to the data actually is

a critical point in thinking about that licensing framework and then what's actually met as a challenge is people's awareness of licensing issues. So we do have cases where people have um you know

I think the thing is oh because I'm African and you're African you going to just give me your data for free but we do have state entities who basically will sue us for actually making use of

public information that is out there in the domain because it is their government mandate to provide information in all recognized legal languages of that state. But when we try

to make use of that and when I say we I mean the technical community try to make use of that data we do get flagged for actually going against copyright but also the fact that we are not paying

the state broadcaster but the state broadcaster is getting all of that information from the people. So that is kind of like that re that realization in terms of dealing with the

localization of data and also really surfacing the friction that exists um amongst the different stakeholders who may be from the same region but in understanding what benefit I can get.

They are still gatekeepers of that knowledge who are like we've done this work to make sure that TV broadcasting has 11 of the languages and you just will now want to come and build an AI

system on that. But we are arguing that there's value on both ways. So that is um I think the approach that's taken of kind of like making it communitycentric and then also really trying to nuance

the awareness around licensing and ownership um of this data in that context of that it is community centered um but then again sometimes community is such a misused word and that's the

reality of the politics that we try to navigate. S I know you also talk about tiering of access of various kinds. Uh how do you decide the tiers?

&gt;&gt; That's actually the thing one doesn't the contributors do. And I think that's where the in the construct of I think what you're mentioning Rahul when you're talking about the localization of

models. &gt;&gt; My one point and again if localization of the model is what a data contributor wants as the value attributed so be it. If they want it to be monetary, so be

it. The where I differ is in that quite often the data contributor loses all negotiation power and it becomes subject of the state and that's part of how data sovereignty is interpreted that all of

the contribution goes towards the ability of the state to have this negotiation and say you company who's accessing it does this. uh and then they say across multiple companies how you

want to do it and so on and so forth. Uh where and the the point of this is not just merely in having agency. The point of it is how do we keep the data extraction cycle moving because in any

other activity if it's a one-time resource or one time funded activity and then the state gets to then negotiate it. We we've often seen how that's dead on arrival. It happens for that one

year. The data extraction process happens but then there is no way to keep it moving. There is no way to keep the digitization efforts going on the maintenance of it and the ongoing

process. It just doesn't happen until and unless you have value going back to that community or ecosystem or actor or actor on an ongoing basis unless you have some framework on which that is

possible. These are all onetime funded activities and I don't think that's relevant to where we want to be. I can just talk about let's say in India to even get to

the base level of digitization or data contribution we want to be there is historic data that we need to digitize. There are I don't want to even go as far as millennia but there is decades and

decades of local data that needs to be digitized and we have not even come to square one for that process to happen. Uh this is not to undermine philanthropic efforts. This is not to

undermine you know foundations such as yourselves kickstarting that cycle but again how do we keep it moving and that has to happen with some sort of value attribution. Now if it happens by virtue

of the state that's great but unless the to your completely to your point why does a company come back why would a company come back to get the data only if the data value or the data quality is

better than what they can scrape off the web unless and until we have some process and that's a resource inensive process. Unless and until we're able to say that don't scrape it off the web,

take it from me. It's at least better in quality, it's better in accuracy, it's better in just uh the timeline in which it has been given. All of that is the fundamental bedrock for all of our

conversation. When we are saying that there's capacity for anybody to negotiate data access, it only happens if your data that you're contributing is better than what's out there or at least

relevant. And for that, we need to get some value mechanism in place. I am arguing that let the contributors, however you may wish to define them, be the people who

decide what is valuable enough to them to keep that cycle going. &gt;&gt; I mean, look, I'm a lawyer. Ultimately, I'm just looking in a negotiation for leverage. And I don't think a single

contributor of data has leverage. I don't think a community has leverage. I don't think a country which is smaller than India has leverage. So if essentially we want to negotiate a deal,

we're going to need to make a critical mass of people in order to be able to bring the other side to the table. So I'm, you know, I really like the

marketplace model, but I'm concerned that we will not have enough leverage to do what it is. It's a mountain we have to climb. I I I'm concerned that we will not be able to do it. I seriously think

for Africa, countries in Africa need to band together in order to actually because they will they will divide and conquer. I know that there are um many data deals

that are being done even in Africa right now where medical data is being extracted for in exchange for something for 25 years and that data is gone. The only way you can defeat that is to

get together. And so the my only point behind this is not that I disagree with you. I think we must go to the market to that we must enter the market with a winning hand. And that's only going to

happen if we have a large amount of data. The second point I will make and this is to Chennai's point on you know licensing. I'm a lawyer. have been a intellectual property lawyer for 30

years and if there's one thing I know uh laws are great and it makes lawyers a lot of money but uh if someone infringes or violates your license it's gone you know you can sue them you

can hopefully get some money from there but you know when uh I'm not sure certain unnamed uh AI companies scan books by the millions they will come to a $1.5 billion dollar setlement because

you know they have 300 billion like in cash. But how's that helping? Because each one of those books gets something like $3,000. Is that I mean is a huge for

like an author who's never seen $3,000 book royalty. That's a that's a reasonably good. But is that the compensation you want? Which is why my model is embed the legal

principle into the design of the technical architecture. And so make sure that you don't have to rely on laws. You build clean rooms where you keep the data and then you can only train your

model when you come to that clean room and train the model on the data and then you're gone and then you cannot infringe my data rights because you don't have the data. And so my I we have a short

window India and Africa to do this because these deals are being done right now. And you know many people are funding this uh and then once they fund the the collection of the data the

digitization of the data they will own it. And so as countries we need to come up with a philosophy and I'm a huge believer in actually allowing people to do this but we've got to do it smartly.

We've got to realize that the long-term incentive of all of this is really uh you know to to make better AI systems. But I I would hate for us to do some kind of a a new age digital colonization

where data is taken to benefit the western countries that have the capital and the GPUs to build this and that benefit does not come back to the people who have contributed. That's the reason

for the model. I agree with that and I and I appreciate the sort of call to collective action in this whether it is countries but then also communities because you also talk about that in your

paper because I think it's very clear that we are not going to be able to negotiate uh in this fragmented perspective. We have 14 minutes on the clock and happy to take some questions

from the audience. But yeah, &gt;&gt; thank you very much from Google partnership on AI. We cooperated with API before. Um I I just wanted to to ask about the the kind of business models

you are mentioning because I I almost only heard the the word marketplace but what about comments for instance because there is the economy of comments and can be different. I mean some some

organization can have common interests and try to develop some products together and maybe have some ways so that the the way each organization has uh something back to it can be different

that what happened on a marketplace. So did you investigate these kind of options? Absolutely. And thank you so much for that question because that's exactly the

point. When we're talking about a marketplace, the architecture contemplates variable terms and variable access capacities of the marketplace. So starting from uh whether you want to

give it for commercial terms, whether you want to give it for free and the architecture contemplates that I can choose to give the exact same data set as a data contributor to a big tech at a

certain cost and to a university for free. I can also have varying licensing terms of access. So the access uh capacities are also varied whether it is you even have like an API download or uh

data to model multiple versions. Uh but fundamentally it contemplates this that there is that capacity of choice and in making it decentralized it enables discoverability

and fundamentally tries to be is a tide that raises all votes because in all of this we all know that none of these are net new contemplations right there are several data efforts already in place

but without that connective fabric is where it's not really reached the critical that we want to do. So the idea is with the marketplace construct but as well as being decentralized you're able

to create access for all you do not have siloed access and then you have better opportunity for marketplace. There's one challenge with a commons problem, right? So commons, if you think

of the classic commons is you've got a field for grazing where the whole village grazes their cows. It's common. It's for everyone's benefit. A commons works where everyone actually

needs it, uses it, and continues to need it. AI is currently a bipolar word where there are a few companies that are going to generate the models because it's just fundamentally capital intensive and we

just do not have the capital for every farmer to be able to build a model for I mean just take model instead of car. So there's a fundamentally extractive uh characteristic of the AI that AI world

that we're in now which goes against the idea of commerce. If they graze their AI model on our data field once, they never need to graze it again. And how do we deal with that in the context of

comments? &gt;&gt; I I agree with that and I'm actually trying see because the the theory now is that these foundational models are going to get to AGI. Once they get to AGI,

we're We're never going to need uh we're never going to need a foundation model again because AGI is going to be so brilliant that it will solve all the problems for us. We're seeing already

seeing signs of it now because the agents are becoming so good that you really don't need to work anymore. Claude's calling it co-work. Um and so you don't need to work anymore. Now when

this becomes so good, you are going to go to the models that give you this sort of magical experience. Those are the people who you are going to do your make data deals with. So I I'm look I I'm a

huge believer in in you know all the commons we created commons open source I've been doing this I did the world's first open source hardware license agreement in the year 2000. So, I've

been doing this for a very, very long time, but I am very concerned about the fundamentally extractive and it's like one time they feed on this grass, they're never going to need the grass

again kind of world that we're in and we're just sleepwalking into this all of us. &gt;&gt; Sorry to to the other point, these are all we're talking about the general

purpose situations when we start getting into the purpose specific implementations whether it is let's say medicine and things like that. Not to say that the large language models will

be like useless in those situations but will we not isn't that the hope that we will have multiple entities availing of these for specific purposes that I would disagree with that because

already why do you have country implementations of the largest language model in the in the world for this purpose why do we even have more than one if that is the argument because

you're trying to say that there is additional value. There is an additional specific purpose that when you're trying to create sovereign implementations, there is a

maybe it's a delta, maybe it's a small delta, but there is an additional use case that you're trying to serve, which is why there are countrywide implementations of this thing to begin

with, or else it is here's the box and get going. &gt;&gt; This is a much longer conversation. &gt;&gt; I was going to say, does anyone else in the Yes, please go ahead.

Uh hi I'm Barat from the Takshila Institution. So my question was uh to Rahul about what you said about data in exchange for models. Um so you already see that there's a large market

concentration in in the model area and wouldn't this mean uh further concentration I mean you're is there no way out of that then and you don't see that happening and you're just making

the best with what what that situation is. I I personally don't think see it's just the capital capital intensive nature of this industry. I do not think that we can let a thousand flowers bloom

as far as building foundation models are concerned at best where I mean we see uh many people build foundation models see India is building foundation models in that are state-of-the-art. If you look

at what uh Son is put out putting out and you look gener it's state-of-the-art in a narrow vertical that the big air companies are not so interested in you know English Indian language uh

simultaneous dubbing uh uh OCR for old England Indian languages it's very good at that quite frankly it's because Claude and I mean you know Dario Amado and Sam Ortman are not lifting their

little finger to put some attention into this. Dario said let's build something which is good for coding because I don't want my engineers to actually write any lines of code anymore and they've built

the best uh command line interface for coding by AI so much so that all of uh cloud is being written by them they will completely destroy us if they really turn their attention to this the

economics of the fun the the current way in which AI is being built which is large language models the scaling laws and what Daradio has just called the end of the exponential

fundamentally makes Is it impossible for more than two maybe three and if you look at what we've got right now we've got Claude we've got Chacht and we've got Gemini there's no one else in the

ballpark everyone else is nice to have but as someone who uses all three every day there is no one who is close Kimmy 2.5 from China is sort of okay but that's good if you want to do like

poetry or something like so you know I'm saying for for the straight up across the board there are only three that's the fundamental economics of the way this this uh AIA world is being built.

No, I so when I so when I say so of these three they're going to of these three they're going to want to essentially uh get better and I think the last frontier is the the data that's

not written that is not in the model and that's our last bargaining chip and if there is value of that perhaps we can enter into a deal but at some point in time if the if the scaling laws persist

in the same way that Mo's law has persisted long before we thought it would. Uh they will get such a good general understanding of the world that they will probably start to understand

the things that we because of our ancient civilization have in our ancient texts because our forefathers got all of this. But they will understand it. They will get it. I mean if they could if

they could beat uh the world's master at go uh without w with moves that have never been played in go, I can't see why they will not do that for traditional knowledge. I can't see why they will not

do that for health. I can't see why they will not start to understand languages they have never seen before. There must be some underlying semantic rules and logic to language. Who knows? They'll

just find it. So, I'm look I'm deeply bullish on where this is going because I feel that claude code is a superpower. You want me to plug but I've just vcoded an app which will be on the app store

tomorrow. Al, I'm a lawyer. I've never written a line of code. I've vcoded an app which will probably be on the app store tomorrow if it's not rejected tonight. Yeah,

&gt;&gt; it's called anagram stack. Uh, and it's just to make like it's a wordle for I'm just plugging it too much. But but essentially the point is that if a lawyer who has never written a line of

code can now actually have an app on the app store. I don't know. I mean lawyers could perhaps do medical stuff and you know ordinary people could do legal stuff. I don't know. We all will have

superpowers and if that happens then all bets are off. So we have a small window to negotiate and we just have to sort of use it. &gt;&gt; Uh time for what did you do R?

&gt;&gt; Uh time for one more question. I will go back to people who are not my colleagues at API. Huh? &gt;&gt; The app. Yeah, it's going to Yes. You since you don't work it out.

Oh, hi. I'm a moch. I'm a policy researcher and like I've done most of my work with data systems with governments. But I'm looking to turn my attention into like how digital labor shape like

how AI is going to shape labor relations. And I think what we can summarize from this talk is that it is going to probably replace most work as we know it. So if the only bargaining

chip we have is data, why don't we make it very like very monetarily expensive or like make frameworks in a way that you can use that as a bargaining chip to get

money to the people who have created those. So why don't we restrict the use of data heavily right now make very strong laws against like using any sort of data by AI models before like you

create a strong law about let's say a baring with collective data through data cooperations of fiduciaries. Why don't we push for like stronger protections against using any kind of data to begin

with before like we get to AGI and sort of start having those &gt;&gt; just one one comment. The point is just that as a negotiator I know that when your price is too high

&gt;&gt; they will not come to the table. So you've got to get the sweet spot. &gt;&gt; Uh would you roll up your data at Masaki? Yeah, I I was smiling as you were asking

that question and I'm like then the reality is oftentimes in these conversations we talk about those with power and resourcing and what not have you but then there's also kind of like

the inverse of limited innovation for the small people who just want to get access to that information. So and I really like the point you ra on licensing that that that's not where we

should begin. It's about in the design. So what design principles are we putting forward in thinking about that data that's going to be created? Because I can immediately tell you that if we well

that comes back to the example that I pointed out of like a national entity that is mandated to collect um language resourcing basically then will charge a smaller uh tech entrepreneur millions of

brands uh for my equivalent because they're like oh no no we created but a national resource and so but then if a that bigger because it's rans that bigger entity will call it $100,000

so they can afford to buy that so then that where is the power dynamic that exists of like do we put money to it or do we figure out that collective action where we bring everybody to the table

because I can definitely tell you the technologists have just been doing this because we've been ignored for a while but we definitely need the legal minds in the room we definitely need the

philanthropy funing and it has to be south to south collaboration. I think that is where the sweet spot is because as Masak there are certain things that we don't need to do because our

colleagues from India have done it. We don't need to fix and then I think another key issue is also actually coming back to the point of labor. I think I'm quite curious when you say

data is all we have the data labeling association the people who actually do clean the data are mobilizing. So people are going back to old school trade unions because they've realized that the

value that we are not having conversations about is in their physical labor that cleaning of the data sets the you know and and and and you know certain tech companies are very clever

thinking that if we go to Africa no one's going to care and they actually got dragged into courts. They're being there currently three court cases on the continent involving um tech companies

from Silicon Valley who were like we were never going to go into court in Africa. They're in court right now. And it may be something that they're like we can pay because we've got 300 billion

and it's just a small amount. But it is also signaling to the people on the continent in the different countries that hey hang on we've actually got some power here and we have to find which big

um you know big colleague state pushes first and then the rest of us started. So it started with South Africa then went to Kenya and then Uganda and Uganda has its own politics but the fact that

they're actually having these conversations with these entities in court is a strong signal to the rest of the continent. So I think that is the point of of defining how do we actually

um allow still allow for innovation and designing processes that don't result in us then limiting our own innovation as the people who don't have the resources. &gt;&gt; Uh our our board says oops sorry uh so

before we are kicked out of the room very quickly uh we might forget but this is still day one of the summit. It has felt like a very long day but uh just from you vijay like where do you see

like the data conversation showing up in the discourse and like if you had a line to add to the leader summit basis this conversation what would you do &gt;&gt; look at DPI and AI how they come

together creative light it's very important &gt;&gt; one sentence in the leader summit &gt;&gt; I would say resource not just money but even the learnings of exchange and also

something that is sustainable in the long run. &gt;&gt; Um one thing that I definitely do agree with Rahul on which is that the window is closing. So I think just starting now

after all of this if we can start ASAP that would be nice. &gt;&gt; Not at all. I'm very far away from it. But look, I mean, see, I'm I may have sounded pessimistic, but I'm deeply

optimistic about all of this. I would just um say, look, embrace the superpower. It's AI gives you a superpower. Uh embrace it and uh we will just sort of make beautiful magic with

it. &gt;&gt; Uh thank you so much to my wonderful panelists. This was lots of fun. Uh and um we'll be here, you know, talking about some of these issues. read the

papers, play the apps, uh, you know, and thank you so much for for being here.
