# Trust as a Global Imperative: How to Operationalise Safe AI for Al

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 15:30 ‚Äì 16:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 17 |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/5V2aNMHOnMc?feature=share) |

## üé§ Speakers

- Dr Chinmay Pandya, Dev Sanskriti Vishwavidyalaya
- Ms Gabriela Ramos, UNESCO
- Ms Marine Collins Ragnet, NYU's Peace
- Paola Galvez, Globethics

## ü§ù Knowledge Partners

- Globethics

## üìù Summary

AI systems increasingly shape social, economic, and political life, raising concerns about safety, transparency, and trust. Global efforts recognise that trustworthy AI requires more than technical safeguards; it demands ethical foundations, strong governance, and awareness of societal risks. This session brings together leaders from ethics, government, multilateral institutions, and peace research to examine how Safe and Trusted AI can be operationalised through inclusive, evidence-based approaches that advance responsible, globally aligned AI governance and policy practice.

## üîë Key Takeaways

1. AI systems increasingly shape social, economic, and political life, raising concerns about safety, transparency, and trust.
2. Global efforts recognise that trustworthy AI requires more than technical safeguards; it demands ethical foundations, strong governance, and awareness of societal risks.
3. This session brings together leaders from ethics, government, multilateral institutions, and peace research to examine how Safe and Trusted AI can be operationalised through inclusive, evidence-based approaches that advance responsible, globally aligned AI governance and policy practice.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/5V2aNMHOnMc/maxresdefault.jpg)](https://youtube.com/live/5V2aNMHOnMc?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

Bringing concrete practical recommendations to operationalize a safety is critical. That's why Globetics has been working on responsible and ethical AI for the past five years and

being involved in ethics of higher education and technology for the past 20 years. It gives us a big and immense honor and privilege to be sitting here with such wonderful speakers bringing

diverse experience and expertise to this conversation that we aim to be as as much practical as possible. So you can get out of this room with concrete steps and ideas on how to really

operationalize safety. um for this today uh is joining me Miss Gaba Vmos who is co-chair of the task force on inequalities financial disclosure and former um assistant director general at

UNESCO you know she is the one behind the UNESCO recommendation on the ethics of AI the one single framework that has been adeared by 193 nations so thank you so much for joining us um also is with

us Dr. in my panda pro chancellor of de sensitivity visha vida and I'm sorry if I mispronounced it [laughter] and miss coins who the head of innovation at NYU's peace research and

education program it's really an honor to be having this discussion with you and you know uh very recently the international a safety report was launched many of you may have already

seen it and key findings include growing evidence on you know the risk associated with AI regarding malicious use and unintended failures. So

to start we will be doing a round of interventions uh from our speakers and after that it will be a round of Q&amp;A. So first of all let me give the floor and please a round of applause for Miss

Gabina. Thank you so much Pablo. So so nice to be here with you and I feel that um amazing amazing India and amazing uh uh summit. Uh but the fact is that through

these summits we are broadening the understanding of what is at stake and what are the possibilities of artificial intelligence. Uh I went to the park it was really about safety because of

existential risk. We went to the I action summit in uh France and now it's the impact and I like that framing because at the end what happen in these discussions is that we go too deep into

the technologies the generative oh my god the reasoning and the agentic and and we are always chasing the capacities because in each new addition the technologies do more things that mirror

the cognitive capacities that were only humans before and therefore this is what get us mindboggling and so attracted to these technologies because we use them every day and we know that they are able

now to code and we are now know that they can produce very complex uh health diagnostics etc. But that's not the point. The point is what the narrative that we want to have as humanity in

using these technologies and getting back the sense of agency to humans because yes the technologies will continue flying high and more with the geopolitical competition and the and the

race forget general artificial intelligence or super intelligence but at the end is how the countries and pa you did the readiness assessment for Peru one of the tools that we use at

UNESCO to know where countries were h how do we bring it back to the realities of the people and and and how do we use the technologies to enhance the strategic vision of each country in

their own development and that's why with the work we did at UNESCO and before I was at the OECD also looking at the uh principles uh we needed to have a a common narrative and the common

narrative was human rights based these technologies cannot derail build the framework that we have to protect human rights, freedom of expression, privacy, freedom of not being harassed, etc.,

etc. Uh the question of fairness, thousands of decisions are being taken with these technologies. Maybe you didn't even know, but your bank is using algorithms to decide whether you have a

loan or not. Uh and many times these things are not as transparent as they should. Therefore, transparency another principle, inclusiveness, another principle, but all of these things need

to be translated into policies and this is very important. Governments shape the policies and then there needs to go into the depth of what countries need to have for these technologies to be really

useful. And I feel that this is what we managed to do with the with the support of our colleagues here um and of course with the universities that are so important because we need more research

to turn the conversation and try to think how do we really ensure that this technology is delivered for good because yes we know all the safety issues we know the question of mis misinformation

disinformation we know deep fakes uh we know how much women because we're also part of the women for ethical AI are under reppresented in the data under represented in the sector but over

represented in cyber security threats and and and harassment and therefore all these things we know but the other part is how do we equip ourselves with the laws with the institutions with

investments to shape these technologies because we can shape them just one and I finish with that one very concrete example of how countries in the global south can

shape the technologies if you are going to use these technologies is for example to improve the educational systems you go out with your procurement processes 15% of the budget of countries

are procurement it's a very powerful tool and then you put some conditions I need this to be triple check I need to you need to show me what was the assessment of the system you need to

show me how you protected the treatings all of these things and then you shape the market because market actors want to participate and therefore they will fulfill whatever you put in those

conditionings and that's public procurement. These are the kind of tools that we have at our hands all countries and they should use it. Thank you. &gt;&gt; [music]

&gt;&gt; Thank you very much Gabriella and and one of the reasons why we gathered this panel and and why is it called trust as a global imperative right and being this the first AI global sum in the global

south meaning that we can really bring impact to what we're already discussing the component of trust in AI safety right and it bring us immediately to to the human component and and that's why

this the perfect seg Can't wait to listen to Dr. Chinme Panda who's working especially on that ethical and human part. Please the floor is yours Chinme. &gt;&gt; Well thanks a lot Dunad to you Paula. Uh

first of all you all are here in India. So being an India and being a barter my warmest welcome to everyone who has traveled from different parts of the world to be here in India today. uh and

not only on behalf of India we extend this welcome but also on behalf of the organization that I represent. We have 150 million members 5,000 centers all across the globe. So a warmest welcome

to not only the panelists but also to the audience and I thank especially to Dr. DAO for choosing this probably the most important theme of the AI summit trust and the reason that is important

because you know we are meeting in a very fragile geopolitical environment and in these times of uncertainty chaos uh the humanity when it seems to be like you know uncertain and suffering our

deliberations on the future of humanity becomes more important than ever before. There is a beautiful saying in Sanskrit and it says that when entire humanity comes under the threat then humanity

alone has the power to rise above it and it's not me who is saying as a faith leader that humanity is under the threat rather we should listen to Jeffrey Hinton godfather of AI during a Nobel

Prize and he had to resign from his position to speak about the dangers of the AI and he called it an existential threat to humanity and there are many reasons for

him to say that. I'm not going to name all of them but I bring two because they are important to this session. One [clears throat] 2023 and listen to it carefully. A person father of two in

Belgium committed suicide because he fell in love with the AI chatbot named Eliza &gt;&gt; and Eliza said not only prompted him to commit suicide also said to him that

let's meet in the other dimension and live there forever. So imagine that and then we have got the another situation where AI technologist they asked like you know AI to break the capture and

capture cannot be broken by the AI. So we tried to recruit a person from task liabit.com and asked him to to break it and when person got suspicious and he thought

that maybe the AI is asking me to do so. So he asked that are you an AI asking me to break the code and what it said actually should alarm everyone. It said no I'm a person with visual impairment I

cannot read it. So trust is gone, [clears throat] manipulation is in action and in this time like I don't have to be you don't have to be prophet to understand that you know we are the

AI is not a speculative technology of future it's a shaping force of our present from healthcare diagnostics to education governance to warfare AI is critically embedded in the

infrastructure of human civilization and the power of the AI is growing with every passing day and as the power grows so does our collective responsibility to ensure that this power is aligned with

the human value of social stability and planetary wellbeing and in that system of humanity &gt;&gt; the biggest like you know foundation or the most important human value is the

trust relationships are built on what trust friendships are built on what trust any contract trust governance trust international relations trust without trust you would have nothing

like you know if there is no trust Basically the everything without trust the most powerful technologies of the world they would face resistance outreach rejection and without safety

trust would become frail and unsustain. So I going to say much there is a saying in in Hindi and we say that it takes years to develop trust seconds to break it and forever to

repair it. So like you know you you have got in AI the trust is not going to come automatic it's not going to come in a day it cannot be declared by the policy document alone like you know it has

going to take its time you have to trust it you have to pass through consistent visible and demonstrable practices so I end just by saying like you know in the current age of algorithm where

I think everything is uncertain trust is the most valuable currency that we And in this time when data is fueling the machine, confidence is what moves the world. So our confidence is

completely reliant on the trust. And if that has been taken away, I think AI which I doubt like you know but it would fail one day and hopefully not so soon. &gt;&gt; Thank you Chin.

Indeed and in fact I would say artificial intelligence let's not antraformize it if it's the word like putting um qualities of humans to a technology because AI is the tool

that could be extremely wonderful for a nation and a society. I remember launching the Italian garus Peru while I was working in the government and a old eight-year-old told me I am going to

learn to code to create a problem for my mom to crop and that you listen to this girl she's trusting the technology but then we in every organization and position we are we cannot let this

change as as you said in seconds this trust could become um a disastrous right so this tool can also be very negative for us for a session and for that It's it's also a perfect segway to listen to

Miss Maren Collins who's who's leading this peace research and education program at NYU. So Marina is yours. &gt;&gt; Thank you. Uh and I used to live in India in Delhi. So it's really a

pleasure to be back. Um I think a lot of what I'm going to say is going to echo what Gabriella um and doctor have said already. Uh we're at the fourth AI summit in three years basically. Uh

Bletchley was safely se was about innovation. Uh Paris was about action. Um and this is the first time that we're looking at impact. Um and this this really matters because impact is

measurable. Um and it can't hide behind principles. Um this summit is also very different because it's the first hosted in the global south. Um and India isn't just hosting, it's really trying to

position itself as a proof point. Um you know, India has ADAR, India stack, um they're launching an indigenous model. Um that this is very strategic. Uh but I think one of the honest questions that

remains is that after three summits and and hundreds of commitments, um the basic architecture that benefits AI hasn't really changed. uh for example Africa holds 18% of the population but

less than 1% of data center capacity. Um as we know most AI models emanate from the US and Europe. Um and um I think the gap isn't necessarily in the commitments, it's in who's at the table.

Um and we found in our research that often the people who come up with the principles are not necessarily the ones who um experience the consequence the consequences on the ground. So a lot of

the work that we've been doing at New York University um entails um including communities in the design and governance of some of these AI tools. and we found that this is the best way to make them

sustainable trusted safer. Um so yeah very excited for these conversations for the outcomes &gt;&gt; University of Camry. Thank you so much

Marine and to all of you for your first interventions and I think we have a common component from what you just guys said &gt;&gt; human center at the right the humans

sorry human rights at the center um trust and and something that you just said having the population participating in the design um for that Gabriela you are you listen to that the UNESCO

recommendation on the ethics of the eye is is a framework that was really made in such such a participatory way. Um and that's why for instance this was adeared by the whole member nations and

[clears throat] now we're talking operationalization. Why do you think um governance tools or regulatory mechanisms have proven effective in translating AI safety

principles into enforcable policy? It's a it's a it's a big challenge and let me just um choose some examples. The the recommendation as I said was just a framing in trying to uh

steer the development uh avoiding the infringement of human rights and human dignity uh and the human determination. Human determination. Everybody listens

to that. Human determination. human. It's it's basic. It's basic because at the end what happened and what you have heard so many times is [clears throat] that these machines

are by their own and have their life on their own because uh first when there was machine learning uh the developers would say I didn't know there was a black box and then the computer came out

with something and I'm not I don't know how they did it. with generative AI this got worse uh large foundation models who understand them no and therefore you can hide behind that and say it was a

machine no yes sorry it discriminated the people that doesn't speak English sorry but this was a machine that cannot be and therefore the core element of human determination needs to be

practical it cannot be just a nice statement in general how do you do that well you do it with accountability mechanisms that will held somebody accountable

other than the algorithm. And therefore, this needs to be translated into law. And and and I'm I'm I'm more positive now probably. Yes. When Jeff Hinton came out, we were all

like, my god, [laughter] if this guy is saying that and this existential risk, this must and it's true because if you can code for good, you can code for bad. And therefore, yes, that could be

amazing existential risk. But the fact is that when you go into these very concrete definitions, what do we mean by human determination? How do we put it into law? Who is accountable? And you

know what accompanied that decision that was very important not to grant legal personality to AI. And you might think that that looks very strange. Well, it took hours during the negotiation to

arrive to such conclusion because if you are going to be granting legal personality as we do with corporations in many places in the world, then you are cutting short the accountability

mechanism, the transparency mechanism that lead the development to the human that needs to be behind. And therefore when you think about liability regimes and I think this is very important

because one thing is to create the incentives and investments to have more AI that will solve the problems of agriculture of health instead of uh raising uh military guns or all of those

things. you can create incentives. But the fact is that if you don't have the liability regimes when something goes wrong when the young people committed suicide, get your legal book of it and

see what's how do you determine the wrongdoing and how you pursue it. And this bring us back and and you know it because you have worked on these issues and you and you train so many people on

this bring us to how fit for purpose are we for this world? How fit for purpose are each one of you? How fit for purpose is the government, the business sector that are not the developers but the

adopters and it's a call of awareness that we need to invest in ourselves and the government needs to invest in themselves and it's not a empty issue. The reality that is happening every day

I was in San Francisco talking to one of the biggest brains that is looking at property rights. We have the laws. The only thing is how do you expand it, translate it, create the prudence so

that it covers the events that happen in the in the in the cyber space. &gt;&gt; Thank you. Gia, does anyone of you like to compliment or respond to this question?

Fine. Then [clears throat] if I may um chime um now we're going to to this um what Gaba perfectly mentioned as you know societies and having the the regulations in place in

your opinion given your background what are the most pressing risks AI is posing to social co cohesition and peace &gt;&gt; I think that's a great question for that and Gita one of The most important

scriptures of the Indian Muslim traditions comes Lord Krishna says wherever there is fire there is so of course like you know there are so many

things which are great it has improved the connection accessibility everything I give my personal example 1987 I'm old so like you know 1987 we were

looking for one poem of Swami Vivean and he had written it in English and it was translated by three great poets of India. Surikata sing but to find one poem one translated

poem. We had to go to 22 different libraries because it was not like current times that you can access it on the internet. Now you just put it on the air you get it immediately. So like you

know of course the with the advancement in technology many things have become nicer and as as Dr. I was saying and she was absolutely right. Many things are happening very great like you know we I

I come from a medical background there is the application for zebra vision you can basically scan anyone's limb any time to check for fractures or any kind of problems but it's our duty both moral

legal compassionate ethical spiritual duty to ensure that something that's been developed is also going to the right and what is considered the most addressing this in my opinion is the

illusion of our shared reality because you the AI is the only technology which has a power to amplify the misinformation. You have got the classical like you know

example of the Myanmar crisis which was in part driven by the Facebook algorithm. So you have got a wonderful example of that. It has got a capacity to deepen the polarization

like it can only show a part that would wish you to be seen. And we had the 2023 incident where the bombing of the Pentagon like AI mediated image was like you know shown everywhere on the

internet and and you can argue whether it influenced the elections did not influence the election. But you have got absolute classic example of Romania where the constitutional court had to

cancel the presidential elections last year because of the AI me into the election. So just imagine like you know this is not something that we are thinking and arguing about something

that would happen in future. This is happening right here. So my pressing concern is that if we don't make the decision at the right time then it would be probably too late. In India, we have

got a thing that you are training the horse but you are not training the one who going to ride it. So we are working too much on the technology but probably not so much

that who is going to use it and that's equally important to to consider and that's where like you know the theme that you have chosen trust does not remain a social value anymore. It

becomes a global imperative because unless everything that we are working the operationalizing the safe AI is built on the concept of the trust and and and mutual understanding it will be

very hard to create something that is equally applicable to everyone and India has a very unique position as Dr. Mar was saying because India is in between the global north and global south and

most of the countries of the Asia, Latin America, Africa they look at India because India has the same linguistic diversity, [clears throat] same kind of governance strength. So if you create AI

model which is only for the like you know high income and low population countries it's going to fail like you know it has to have something that comes from from

uh uh equal amount of trust from the all possible populations. Can I just follow after this question maybe to you because um it's a reflection that just came when you were

speaking about what you made um trust right and let's not be too late to start regulating this and and in fact a while ago let's say 2020 the question was should we regulate AI but in 2022 I

think we started saying how there was not anymore the question of should we there must be guard rails and and in fact um Nobel Prize Maria Resa signed with along with different organizations

including robetics they called for red lines right and safeguards um but we have many regulations now after the EUA act Japan approved their law South Korea um with different approaches even Peru

has approved an AL law but they are not as comprehensive let's say or or going as you were saying before on accountability right what happens if these principles are not taken into

consideration what are the consequences so you don't know many many parts of the world and different um points of organizations what do you think is the best you know if there is one best

approach so that we can translate these principles into concrete action so that we can have safe reliable trustworthy AI systems what do you think &gt;&gt; I think I think it's a very complex

question because of course different countries have very different constitutional traditions and legal traditions. uh but the fact is that you really need

to translate it and this is the beauty of the work that we did at UNESCO because we did not get into the one size fit all that would be responsible I don't think that we can even try that of

course there are some uh solutions that can be shared u explained but we need to take into account the context the cultural setting the societal the the aspirations the vision of the country,

the strategy of the country and therefore it makes it completely tailored and that's why we have a wonderful Peruvian doing the Peruvian readiness assessment and we have

wonderful and in the in the uh room next door there was a presentation of the Indian Ram who was made by an Indian because when we started and I was working at the UCD and also working with

the World Bank with many and I said no we cannot just get into the countries pretending that we know the police we you need to have the experts that know about the technology but that also speak

the language and understand the thing. So I think it's not a one-sizefits. I think it's more how do you really uh equip the the judicial system because at the end this is the beauty. We can think

that there are four big techs big tech no that is producing everything and it's true. There are two countries that are producing 90% of all the foundational models which is the US and China and and

the US is doing nine times what China is doing and is doing 19 times what the UK is doing. So the the proportions are just huge and and therefore you can feel defenseless to think that oops if open

eye is doing this or if what can I do you can because you have the policies you have the laws you have the judicial system so let's not worry about what they do worry about having your system

in place so whenever somebody breaks those rules you have the means to enforce it and I just kind of want to answer that I completely agree that um what you said about one size does not

fit all and I think there are some countries as well where the institutional capacity is quite low or the trust in these institutions is quite low which is why I think in our work

we've also found it very important to kind of include the users the communities the civil societies in the design and government's decisions of how these tools are um developed and

governed do you eliminate you can elaborate a bit more on you know what role should research on these in particular and see society play

lots of mics choose from [laughter] um well I think um I I think I can build a bit from what Kevin said I think a lot of the discourse on AI safety is around the technical risks so we talk about

alignment we talk about hallucinations, bias benchmarks um which is important work but it's kind of missing some of the categories of risks that can be um that can impact um the communities and

the people when uh they interact with social fractures. So when AI for example is deployed in in in fragile contexts um we hear a lot about fronter models we hear a lot about the geopolitical race

between China, the EU and the US. Um but around the world AI is already impacting communities in lots of different ways. Um and we don't hear about that as much I think um in the global discourse. Um

and that's where kind of peace research comes in. Um we uh bring conflict sensitivity analysis into the work that we're doing. So um my entry point is that I I used to work in the the Central

African Republic uh where the information environment is is quite complex. Um, and since then, you know, with um with the um adding AI to that environment has just multiplied um some

of the some of the violence can that can erupt from um some of um the social media and disinformation that's being spread out there. Um and so yeah, I think the importance of adding that

peace component, that conflict sensit conflict sensitivity, that context analysis of what is going on in countries is really important before you implement um AI technologies um in this

in in the Yeah, &gt;&gt; indeed. And in fact, this also reminds um the ROM call right where you participated at Chinme and in 2024 there was this first in in history meeting

where multi-religious leaders gathered. Um and this this marked I think a very important point given that we are experiencing fragmentation in our societies not only political right but

in different kind of fragmentations and I would like um if you could share your thoughts uh chinte on how how this multi-spiritual gathering uh could could come either

into an agreement or what do you think were the best or the main outcomes from those discussions and what is impacting now in our society. &gt;&gt; Well, we have been working on that for

quite some time now. Um that's what led to meet honorable Dr. Dao here. So we had uh this first session in Abu Dhabi where uh people from variety of religious backgrounds we all gathered

together to think about that how if the ethical guidelines have to be shaped and how the different religions faith systems schools of belief they can play their role in it because if you create

just like I was saying about the economic disparity you have got equal amount of uh the the voices disparity if it is only created like you know from a very chosen and selected point of view

it would be very hard to have the representation of all faiths there. So honorable shik Nayan was there and we started the dialogue there that led to and unfortunately we are missing father

Paulo Benati. The father Paulo Bentati was there in Rome and it was initiated by his holiness pope and then uh the representatives of the different religions all gathered in Hiroshima and

there was a reason to gather in Hiroshima because the the biggest technology of the last century was the the atomic power and what it led to Hiroshima unfortunately had seen

that. So we wanted to meet again in Hiroshima just to say like you know if this technology goes out of the hand then what would be the outcome. So all religious leaders from different parts

ranging from like you know the leaders from Jewish traditions, the Islam, from Christianity, Hinduism I was there and uh we gathered together to create like you know a single voice for it

[laughter] and I think it's a very important issue because I'm not a pessimistic person like you know if somebody is hearing only me for these two times they would think that I'm only

criticizing AI and I'm not I'm just saying like you know that it needs to be controlled. But it needs to be controlled in such a manner that the people who are controlling it they have

got humanity in their heart. Uh Gurdev who founded our organization he 1987 he wrote a book called uh great moments of change and it started with one of the most beautiful ever written. He said

that current times may look dark and gloomy but they should not bring fear or despair to us because this is a sign that we are born at a very special time where everyone is called to accomplish

that was never accomplished before which is to fight together like a single family. We have got a saying in in India and India just had the G20 summit and this was the motive statement of Indian

G20 summit. This is mine that is yours. That kind of sentiment belongs to small people. For those who have got big heart, whole world is one single family. So AI is not

a problem of one area or one geography or one race or one religion. It's a problem for everyone. So solution also has to emerge from every possible falter of the society.

That's what I would say. I I think most of us would agree and and that's why international cooperation I would say is one of the main needs that I think our societies is screaming for

right and for that Gabriela what would be your thoughts and what what kinds of international cooperation do you think are most needed now by our society? Well, I I feel that international

cooperation is flourishing here at the end uh India called when so many countries come and share their views. Uh just a changing the views you have seen many times. One comes with one great

idea and then starts talking to people and I I mean I remember when we started drafting the recommendation there were 24 experts from all over the world and then suddenly multid-disiplinary super

important because geeks are fantastic but they're not the ones that are going to be thinking about the downsides or about humanity or about let's bring the philosophers because probably they have

the right questions. Let's bring the engineers. Let's bring the legal profession. So but what was impressive is that when we were discussing that suddenly one lady from Africa she raised

her hand and she said shouldn't we put in the recommendation that we should also have the right to decide whether we want AI or not and I was like I was like but we are

discussing how to use it why would you put there that you don't want it well because maybe we don't need it and then we decided and translated that into the principle of proportionality.

We don't solve everything with AI. Many times we don't even need AI. &gt;&gt; What we need is to think what is the problem that we're trying to solve and whether the technology can help us more

accelerate the solutions. This is the kind of of things that come from there. There was this other uh lady and it was interesting because uh we had some Latin Americans that were super strong on

gender. Constanara Gomez Mont. But then the the the the other lady that was there from Namibia, she said, "Yes, but I also want to put there because the the content of this recommendation sounds

very individualistic. It's about the person and the humanentric and the me me." And in my culture in Ubuntu is this African philosophy. I am because you are. I

exist because nature exists. And therefore I'm part of this energy and I don't want this recommendation to be sounded so individualistic because it represents only certain cultures. This

is the kind of things that you come out just by bringing everybody together and then we did not only came with these experts that were amazing multi-disiplinary we came out to the

world to receive comments. we received like 5 55,000 comments that then we used AI ethically to incorporate them in the in the draft. Uh but the beauty is that also all around the world people have

very different takes and and and this is what matters because the problem is when you think you have all the answers and technologies think they have all the

answers. Beware of that. I think that we should really trust more people that have all the doubts and know how to express them in a way that is useful for us to find the solutions.

And and at the very beginning I said I hope this session brings you one or two key actions or steps and I think we already have many but if I can highlight one and it's not only on international

corporation but it can be translated into any kind of organization right if we have here do we have entrepreneurs developing the technology here can you raise your hand oh I love to see women

raising your hand wonderful great so guys for you in your teams as well while you're developing the technology make as diverse as possible as as Gabriella just said, right? If if we are policy makers,

do we have policy makers here? &gt;&gt; Yeah, [laughter] we do. Um, so I think you already know, but it's not it's never too much to reinforce that we need different voices, society, academia,

um, of course, technical community, spiritual leaders, etc. Because different points of view is what bring this that this go one step back and think ah we think about it and actually

it's a good point and artificial intelligence is just &gt;&gt; first thought first word that came to my mind is disturbing but I would not I would not say that is impacting all kind

of sectors of our lives that we really need to start putting this and and and let's not break the trust that our child and our society is having into this these technologies, right? Um so we're

going into the last 10 minutes. So let let's go to the last question for all of you and then if if time allows we can maybe get one or two questions for the audience. Um the last question is what

is this one practical action that government and multilateral bodies or institutions can take in the next 12 months to operationalize AI safety? just one one concrete action the same as

&gt;&gt; well said a beautiful thing and he said with that purpose is the defining factor of the human beings without purpose our life is like a pendulum moving a lot reaching nowhere and I think that's what

is happening to the AI that we are talking a lot but reaching nowhere and I uh strongly think that what is needed right now is the safety embedded devices that you have to ensure from the very

beginning that AI is built safely and there is always someone accountable and responsible to ensure if anything fails then who should be held accountable for it like imagine the two scenarios that I

just mentioned who is held accountable for it if a person commits suicide who is held accountable for it so like you know the things like that if there is no one to take accountability and

responsibility then things would fail so my strong emphasis is on safety embedded devices I I I really think that uh we need to understand better how things work. We

need to do more research and research uh the majority of the research is being done uh by the global north. Uh fine because at the end we learn a lot from all these uh analyses that are uh

produced by very solid institutions. But we need to produce more knowledge. What does it mean these technologies for your own society? Uh and by the way the the research the

majority is also being produced by um private sector and therefore let's just make that bet because we're learning day by day. I am sure that all of you are learning day by day when you interact

with the technology when you read the headlines when you try to apply it for your own uh purpose or businesses or schools or uh we're learning through it but we need to invest in knowing better

how things work to improve our capacities to take good decisions and if we do that we will be documenting better use cases we will documenting better the downsides But we will also be

documenting what you said PA. This is not a question of what what's the problem? What's the problem? Let's discover the No, it's like how do we address it? And and in that sense, I

think that is also great that we have for example, one of the outcomes of the Plesley Park summit was to create the safety institutes and now you have that network and they're exchanging and

they're looking into issues and then you learn from each other. I feel this is a very important point learning with each other and from each other uh to to address whatever is a downside but also

to magnify the positive impact. &gt;&gt; Thank yeah I think we all agree that there needs to be more you humanity in AI and I think we all agree that there are a lot of principles but that

implementing them is the difficulty. Um and uh I think as as I said before it's often that the people who are designing these AI systems and the people who are living with the consequences don't

overlap and so these perspectives are not included and how these systems are designed. Um so I think my one recommendation would be to invest in governance infrastructure where it's

weakest. Um and it's not one model fits all. Uh regular regulatory capacity uh may be an issue for one country but not another. maybe technical sovereignty is an issue for one country and not the

other. Um I think what our research has shown is that a lot of the power concentration and some of the extraction that exists is a pattern that's replicated in a lot of countries but the

responses have to be different and so that means uh funding universities funding civil society funding the organisms who can kind of counterbalance some of that.

&gt;&gt; Thank you. Thank you very much. Um, we do have five minutes. Do you have any questions from the audience? &gt;&gt; The first answer is from the lady at the So, if may I ask you to come? I'm sorry.

We don't have &gt;&gt; Okay. She's going to give you one mic [music] to the &gt;&gt; Hi everyone. So, I work for AI

governance company and uh this company is usually working with government of Canada. Um we are helping to make sure that AI is safe uh their technology is safe. So I just wanted to learn about

from you guys like how do you know that this AI technology being used in whatsoever banks, institutions, government agencies can be trusted like what do you look for in that?

It can be an artifact or anything like how do you know that it can be trusted? &gt;&gt; Well, I think it's very different from context to context and from use case to use case. Um what we do is that we

co-create the technologies with the people who will be using the technology. So for example in Malawi we're building an early warning system with uh rural communities. So we're implicating them

in the design process of that technology from the start. Um and they're also going to be governing the data and the technology once it's been developed. So that's the way that we found um to

create a tool that is trustworthy in that specific context. But again, I don't think there's one sizefits-all and that, you know, um there's one response. &gt;&gt; Yeah. But at the end, if if you are the

one that is going to go to the market to buy a service from a AI company, I think it's very important that you make sure that that company fulfills or comply with the highest

standards in the sense that they should be uh asked to provide with how many how much they assessed the systems that they are putting in the market.

&gt;&gt; How much they relied on non-biased algorithms. How much they uh introduce the famous red teamings which means that whenever these things are developed uh you can

do adversarial kind of problems create problems so that you see how you solve them. uh and more than anything I would say it's not about how safe is the technology because in

that sense you with your processes you might have a better answer but for example what Paola said the the teams developing these technologies were diverse

it's not it's not something out of the blues is that again you are open it seems that this company is more open to look for different voices and and ready to

challenge &gt;&gt; I believed like you know there are first of all you need four types of governance. You need institutional governance to ensure that laws,

regulatory bodies, public institutions. They are equipped to understand the AI and then only they can oversee that. Second thing that you need is the technological governance that who is

designing the AI, who is deploying the AI and whose values are imported in them. Third thing you need is the civic society governance because at the moment the digital literacy is not at part of

the digital power like you know people what AI scientists are probably concerned about is not the chat GPT like you know people think when they are thinking about AI is something that a

search engine would do but what AI like you know technologist they're worried about is something else it's working about the transformer the T in the CH like you know that's working at a speed

of 36 6.8 beta flops twice the speed of the human brain. So that's what we are worried about and fourth thing that you need is global like you know governance because AI would not respect the

national boundaries and borders and and democracies they are largely confined within that. So how a crossborder AI platform would affect the democratic foundation of this country? Imagine a

scenario where something is being developed by one of the neighboring countries of India and that's affecting India and how it's going to impact. So like you know you need governance at

four levels. The important thing is that how do you test them what you are asking and you do the same manner like you do in the any kind of medicine. So I come from a medical background. If you are

launching a new medicine you have to go through phase one 2 three four trials before you launch it to the market. And the concern right now is that something that's been assessible and available to

the common man, nobody's voting for that to happen. Like you know if you if you for example select your member of parliament as member of legislative assembly at least you're voting you have

got a little say in it but you have got a say in it but if something comes on the internet who decided I think the question whether there is enough say in deciding something that is

Let us let me &gt;&gt; uh actually the problem here is that the people are uh the uh people for the next session are lined up. They're waiting outside.

&gt;&gt; How much time? &gt;&gt; Yeah, last two minutes. Yeah, please go ahead with last few questions. &gt;&gt; Okay, but I am the mother. &gt;&gt; Hi, small question.

&gt;&gt; Thank you very much after listening to Maharaj very well. &gt;&gt; It's a very important issue that which you mentioned. If we really want to safeguard and protect the society, we

need to have assistance of human intelligence with respect
