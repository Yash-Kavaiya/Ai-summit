# AI and the Future of Work: Employability, Skills, and Labour Market Transformation

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 13:30 ‚Äì 14:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 14 |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/M3M6E1UUv8o?feature=share) |

## üé§ Speakers

- Dr Dhanya M. B., V. V. Giri National Labour Institute
- Shri Ajoy Sharma, Ministry of Labour & Employment, GoI
- Shri Kartik Narayan, Apna
- Shri Ritesh Hada, Karnavati University

## ü§ù Knowledge Partners

- Apna.co

## üìù Summary

This panel examines how Artificial Intelligence is reshaping labour markets, from job creation and skill transitions to education and policy responses. Bringing together government, research, industry, and academia, the discussion explores AI-enabled employment platforms, emerging skill demands, workforce strategies, and how education systems can evolve to prepare workers for an AI-driven economy.

## üîë Key Takeaways

1. This panel examines how Artificial Intelligence is reshaping labour markets, from job creation and skill transitions to education and policy responses.
2. Bringing together government, research, industry, and academia, the discussion explores AI-enabled employment platforms, emerging skill demands, workforce strategies, and how education systems can evolve to prepare workers for an AI-driven economy.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/M3M6E1UUv8o/maxresdefault.jpg)](https://youtube.com/live/M3M6E1UUv8o?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

Vive also spoke about so the way we connect to anything digital. So uh I think 15 years ago Steve Jobs uh keypad he said that the keypad is useless and he said I want touchscreen and now I

think 15 years later we thinking of why do we even need touchcreen we just need your own voice when we look at Iron Man we think of wow Javas can do everything that is only possible through voice

because you you whatever you are feeling you speak it you don't type it I mean it's not that relatable when you type it you just speak it your problems, your grievances, the the kind of support you

want uh everything is uh something that you speak. So uh that is where the whole idea of voice AI comes into the picture where basically the entire vision is to make AI accessible. So when we talk

about India, I think Vivik already mentioned that we have 1.4 billion people. So within this 1.4 billion people, we have 22 officially recognized languages, 18,000 dialects. So 18,000

dialects is the the the main number and me personally I'm not from here. So when I came to Uttar Pradesh I realized whatever Vive was saying Muzafuragar Hindi is different. Gurukar Hindi is

different. Well while I'm from Karnataka so I've heard Uttar Canada Duhan Canada everything is different. So and these people don't really understand each other. So why do we impose that our

agent the the voice AI agent is to be understood by the people in that local dialect? we don't we shouldn't be imposing any of that. So that is where we've built a platform that is a 24/7

support thing for distress because when we talk about customer support the the mantra of customer support success is resolution and timely resolution. So uh I' I've heard of resolution being fine

and so there's a it's a it's basically a chicken and egg problem where if you want to solve this uh issue where you want to provide customer the best support let's talk about Swiggy for

example now when Swiggy wants to provide their customer some support if you if you uh click click on the refund button at night there are no AI agents available so they actually just refunded

on spot they don't check your problem so the the basically Ally Swiggy is facing a loss here. If there was AI integrated in this where you know you could actually call Swiggy and explain that

you know actually my food was rotten and our AI was able to understand counter to it not just not just you know understand it and do the thing basically counter to it have an entire conversation with the

bot that is where you're understanding the customer that is where you're serving the customer well now this is a basically a corporate thing that I spoke about now let's talk about the

government right I'll just move to the next slide. We have multiple use cases where I can actually talk about how we can integrate these AI systems on a scale because we right now in here we're

here to talk about underserved populations and how multilingual voice AI is going to going to solve their problems. Now let's talk about flood relief, right? So uh we've we've heard

in in the entire country there's a flood in Chennai, there's a flood in Visag. So when [clears throat] we talk about the customer support in all of these flooding areas that is all in Hindi by

the way that is something that I read about recently rarely it's a situation where there's a local team supporting these uh helpline numbers because the the entire uh team for the national the

the team for helping out people is basically centralized. So there is not much of a localization that comes into the picture. So when you want your flood, when you're in a situation where

there's flood, you want support, you want rescue, whatever it is, as as uh Vive already said, we are a country where we have so many people and we have very less in terms of services. I think

there's a number that goes like we have one doctor for every one lakh people. So that is not just doctors. When we talk about general services as well, we in terms of call center, I mean I I was a

victim a few days ago when I booked my Indigo flight and the whole Indo Indigo fiasco happened. I waited for 1 hour 30 minutes just to get my flight rescheduled. Now that kind of wait I

mean I have waited at the airport already. So imagine my state I mean as a customer. So that is what we want to avoid. So when we go at scale when there's a flood going on nobody wants to

wait. I mean imagine you you're calling that rescue me and you're hearing the elevator music playing while you're waiting for the waiting on the call. That is not good. I mean in terms of

when you when you talk about a big scale situation now national schemes is another problem that we could solve. There are so many schemes you know recently I opened NSWS

national single window system and I realized we at Indis are eligible for so many schemes. We didn't know it. Well through these calling through help lines we will definitely be able to understand

because as I think Rajiv Gandhi was I don't know he said that when the government is releasing 100 rupees so there are so many schemes available for us we just don't know about them so

that is the situation and maybe these help lines when we integrate them with all of these schemes and all of the bodies who provide schemes we'll be able to understand it better so in

Maharashtra they're running a scheme for 2,000 rupees for all the females in the house. So I'm sure if we are able to inform everybody that you're going to get 2,000, everybody will be able to

access this scheme. Now juvenile support and helpline. This is another very important aspect because I mean we have se seen so many movies about suicide prevention. We have seen so many things.

But then when it actually comes to dialing the number, I am not sure there's anybody sitting on the other side of the line. Now this is an extreme case. Even general day-to-day support

that juvenile people require. I think that is very very inaccessible in India. Nobody talks about it. That is a problem. Well, we when we don't talk about it, I don't know how human are

going to be comfortable in India. Senior citizen another aspect of the society where they are they are very underserved. Um so I mean you could think of a situation that um maybe a 65

year old man is stuck while he was at Barat Mandabam maybe and he was trying to cross the main road. So imagine we have a senior citizen helpline that basically

takes these calls alerts the local team. So this entire case is very much solvable through this. I mean when we are able to take the input of India only then we can give the right output to

India. So this is a medium where we can take the inputs from India. We can understand people's problem and we can tell them that we have we actually have a solution for you. So we can inform

them that and then another very ma massively underserved uh community would be the the farmers in India. I mean I've heard of so many of these apps that inform farmers of the schemes. Well, not

every farmer would be good enough to have a have a have an app, right? That's what when we talk about scale, we got to talk about everyone. We got to consider everyone in the society. So for that,

the best thing is a telephone call. You just dial a number. You call, hello sir, next. That's that's how simple it should be. That's how simple it should be. So but

that's what we have everything available in terms of data but then how are we providing this data to the people the current medium is websites apps I don't think that is accessible BPL yna

helpline eration cards this is a thing that happens at a very massive scale so when again when we talk about scale the best thing we talk about is accessibility when you talk about

accessibility the best thing you talk about is voice calling so nasha mukti well I go to uh every movie I I watch, I see the ad, two ads actually, I see the phone number on it, I am so damn sure if

I ring it right now, nobody's going to pick it up. Well, then the whole ad is pointless, right? So, we have to have someone to pick up these calls and we have to have someone empathetic who

understands you, who does not judge you uh for especially for this use case. So, that is where AI can play a very beautiful part. So that is what I was saying you know

immediate triage zero forms felt the direct call and then the entire aid whatever it may be in any form is directly dispatched. We were we were also proposing to Zeppto, you know, like

we call you the order the customer just calls to order. Hello, can you send me a Monster Energy to my address? That's it. The order is done. So, we were proposing this. So, this this is the kind of you

know use cases that we see from uh the voice AI. No forms, no downloads, just talk. And uh while as we talk about scale uh one thing is very very uh important when we talk about scale and

that is how our tech is scaling because when we want to serve India we want to be of the scale of India. So for that I would like to invite Sachin to carry on with our presentation.

Good afternoon everyone. I'm Sachin Gisha. So as V and Sh rightly mentioned the use cases uh which all the voice agents can serve right. So of course what we are building everything they

have told the next question is how do we build it because when crisis happens so systems don't fail gradually they simply just sudden disaster it's over there and that's where the infra comes in. So we

at Industs have built a in such a way that in such situations for example flood prices over there right so the system will scale not gradually but you know at exponential level where uh let's

say if it's an Orisa region or any other region our uh voice agents our speech to text recognition models which we already mentioned which we are building homegrown they understand the dialect

and not just the dialect. They'll recognize the emotions around it. They'll recognize what's the urgency around this particular uh let's say a particular call. It will direct that

call to the required authorities or the required help. Right? So that's the type of scale which we are building. Okay? So of course it's not just a once again

I'll emphasize in this point it's not just a simple voice agent it's a complete mechanism where people can go to the right authorities and end time again the next thing comes during the

crisis that's the latencies and every other terms that you must have heard but the actual ground level problems let's say I'm dialing my phone number. It's

already a crisis situation. It's not going to be you know a very good network condition or something different. So what we have done is we have trained our models in such a way. So you have

jittery voice, you have lag of voices. The model understands it, the agent understands it and it provides the you know the accurate responses according. So yeah again focusing on the point like

we have 22 dialects uh sorry 22 languages with different dialects the way we have trained model you know uh based on the speech based on the emotions and everything uh they will

redirect it to the right authorities and that's how we are working so yeah I'll invite Dr. A very good a very good afternoon to all of you present here myself Dr. Lakshmi

Gupta from school of management planet university times of India group going to present you right now how the design matters in this world so when we call it a humanentric design. So what do you

mean by humanentric design? So when we talk about technology, technology we always focus in technology should be more faster, it should be more smarter. But we often forget to ask the three

major questions. The first is is it easy? Is it comfortable or it is s is it safe for people? So human centric design it is the technology is not only for user it's for humans. So when we

Integrate these three things either the comfort uh your safety and easy level uh complex reduction in complexity reduces the efforts in humanentric design. So the design should be like the first it

should be empathy. What is empathy? Then normally technology speaks in commands but humans talks in emotions. So emotions emotions is empathy is when the people when the AI agent itself

understand the feelings of others that is empathy and our tone the work inside in the indust uh tone that how you can give empathy to your users to your humans. Second is

inclusivity. As in India, we know millions of peoples are there who are not too digitally fluent, who are not too comfortable in understanding the commands

nor they are usually impaired. They can't most of them they can't read. So for them if the system is not for everyone then it is not inclusive. So we want that system should be more

inclusive and the designing for illiterate and visually impaired people the system is called as humanentric design continuous feedback. Whenever the technology is a straight

away when it's give a continue direction to the particular some output it is a continuous it is not it is direct but when we want continuous then we have to give a feedback that whether the users

are comfortable users are aligned with the technology users are ready to adopt this situation so continuous feedback is necessary while doing this designing The next is building trust through

culture. When we talk about trust, the technology doesn't bring trust only. Technology doesn't bring trust. The culture is very important which plays a important role in playing our technology

innovative. So when in India normally when we call namaste, what namaste means? It gives a respect, it gives a dignity, it gives a feeling of personal to the people. So when the AI assistant

in the in this lab says namaste G how can I assist you today? It feels you a personal feeling it gives you the respect. So this feeling the whenever the AI assist gives namaste G. So this

small cultural difference the small cultural difference make a huge changes in the system. So our app uh the system will give you a respectful address which give a dignity and pride to the users

and the humans. Second all of you all of us says that local relevance that our voice will work on different languages. So the users are friendly to every languages. Whenever

the users are comfortable with their own language then the system will give them trust and the When the trust build your stress goes out the trust the technology will bring the trust and trust will

reduces your fear and the technology booms. The second is transparency and the clearly whenever the AI agents uh the whenever the AI call has been

come it will clearly indicate that I'm uh it's a AI is a system any it's the the system is currently identification is in transparent and clear resistance the next is governance and ethical

adoption which is very important for any new era so when we call as The technology the technology is a powerful only when it is responsible. So governance gives us rules but ethical

gives us the direction to move through. So the core three tenants are privacy by design. So whenever the we work on this the designing part uh the it should be private private means the data whatever

the data the personal data we are using it should be minimum usage and the anonymous verbal consent whenever we ask the system uh to users the verbal consent is

being taken care with the users. Second is third is accountability that is human in oversight for critical decision. It means not for every decisions we are rely for the AI agent. Some the critical

decisions can be overlooked through human support also. The case study that is civic engagement. C engagement means how all the citizens all the participants are being closely

attached with the system or being closely attached so that the system and the power will become increase in the participation. So how your local community updates with this technology

and as this is uh we say that the trust will when the trust will come then obviously the participants of uh citizens will go inside the system and it will become the output as a more

resilient and faster response to the users. The next is crisis response. Crisis at the crisis a single single second matters a lot because small delays can

impact to a human life, human threats. So this system will bring out reduction in the waiting time. It can prioritize the highest task. It can prioritize the highest task the and which can increase

in successful incident reports and our goal is to save the lives because the human life and the human I think uh life is very important during the crisis. So our mission at last we say that to

poster a positive dynamic ecosystem that empowers every citizen we strive to facilitate growth by encouraging innovation and strong work ethic. So ethics,

human support, government, ethical adoption, it is very essential for our work. Thank you. The next part the new launch is being done by CEO.

&gt;&gt; Thank you ma'am. So today uh we are at India AI impact summit right and we are celebrating this festival five year sorry 5 days long uh festival right. So today uh at this event we are taking

this opportunity to launch our new STD model the speech to text model which can understand all the emotions. Right? Until now we were working in a generic STD which just understand the text just

understand the voice. Now we name this STD model Switra. So this switra model is going to understand all the emotions right if you are angry over the call it can detect

that you are angry if you are happy if you are laughing over the call it can understand that uh you know you are laughing let's say somebody is telling a joke over the call and it is mixed with

emotions earlier STDs were not able to I mean this is the first model in India where I'm speaking at the moment I'm saying this is the first model in India that we are launching indigenous model

which is mitra can understand all the emotions Based on the emotions AI can generate better response, right? If you are angry, AI cannot laugh. AI needs to understand your emotions. That's what we

are launching for Mitras. So, we have launched our developer document as well. You can access all the APIs how you can integrate in your systems, right? How you can build in your your voice agent.

So, we as a company provide two kind of models where you can build your own AI agents as well on our platform. Also if you have your AI voice agents you can use our SPD API or TPS API or LLM API.

So you can check out on our website that this developer document is already out there and this is going to be a magic soon. So thank you all of you present here and I would like to know that you

know this is a new era of innovation the market is not just stretched till now it's going to be deeper in coming days in coming years. So yes when adoptability will increase trust will

also increase I think I'll just add more context in terms of what we mean by swarma in terms of the em emotional aware right so when we do make the call uh not every time we

need information sometimes we just need that little bit of reassurance sometimes we just that everything's going to be okay. I mean that is only understood through emotions. So that part of AI was

never existing to this day where it basically the entire pipeline works in a way that we have a speechto text model. We have an LLM. We have a texttospech model. The the duty of a speech to text

model is only and only to convert whatever I speak to text. So there is no emotional understanding that comes with it. Only if I say to only if I say that I'm angry only then I am angry is

written there but well if I'm saying what is this the simple text is what is this it could be what is this or it could be what is this so that is the the the key part that we were missing in

terms of uh the entire voice AI pipeline because every time it just used to be the text that we were processing but with swaritra we we're going to be able to detect stress we're going to be able

to detect panic confusion hesitation. So when for example if you're working for a bank and the customer we're asking him sir you have actually forgot to pay your EMI within

that if you're able to understand his emotion is he hesitant is he in denial is he assuring that I'll pay tomorrow that gives us an edge that that is unbelievable because a customer might

just you pick up the phone you you hear that it's your banka that is very different from so that is where emot question basically you know that is how we make AI more

human that is a very humanentric approach towards voice AI multilingual agents because we'll be able to go beyond these words because you know what I feel is with this we'll be able

to empathize with the user very well so that was about the swarmra now another takeaway that I think Vive has already given and I'd like to reassure you guys so we We are building at a very fast

pace and we have a platform where you can actually go and uh launch your AI agent live in 20 minutes within 20 minutes. If you're a real estate builder, if you're a uh you could have

any use case honestly we have a very agnostic use case agnostic platform. You could be a real real estate builder, you could be a healthcare company. Well, we have had a demand to build your own

receptionist. So there was a person who was like I get 200 calls per day I can't deal with them can you build me a receptionist in my own voice that was a very peculiar case but then hey it was

useful because the person now gets like 20 calls per day so we also do customer car customer care we do car service centers so in our platform you can sign up you get free credits I think we have

in increased the credits in this these five days so you get free credits you Just go on there, build, buy a phone number, and you start making calls. That's how simple we've made it. And the

CRM to CRM connection, we also allow you to whatever call outcomes you get, we allow you to post them straight to your CRM. So, it almost feels like a call center. I mean, in the end of the day,

you the the closer it is to to a call center, the better it is, but obviously, we have to be better than that. So, I think there are aspects that we're better at at at the from the call

center. So I think that was it. Um I would love you guys to explore our platform, build on it and I mean honestly we are planning a hackathon in the next five days. Hell the best the

person who builds the best AI agent with us with some good So yeah I think uh that is what we were trying to convey today and I I hope that soon enough you get a call from any customer care or a

health plan and it's going to be our voice. We hope that and we're looking forward to that. So that's it. Uh thank you everybody for patiently listening. Uh and yeah the floor is open. So

Okay. So hello hello. So just to answer this specifically answer is yes we have our own indigenous LLM. So we have different techniques. So

if I go deeper into the tech uh right so for individual use cases for example loan uh related cases for example reception hotel related cases so big LMS are not required our technology is such

that we have individual small models if you want to know more into technical perspective knowledge distillation is what we have applied right so yes we have our own models we don't use any

other specific APIs from you know different model providers. So yes, that's the answer. I hope it answers the question. I think you hit the nail right.

[laughter] So, uh I I think you know you you asked it rightly and this is the this world's problem. This is not only our problem. The whole world is facing this crisis

GPU crisis, right? But again, right, we are into the industry since last 1.5 years. We have different partnership models with GPU service providers. Some GPU providers are physically GPU service

providers, some are like on cloud, right? So we have seven to eight service providers those are on cloud available and our system is so much integrated with them. So whenever let's say you

talked about H00 so whenever the load of 500 is done so whenever it's 450 it start spinning the new GPU on a cloud. So as soon as it reaches 50 a new GPU is already available.

So we have you know parked our GPUs into different different service providers uh clouded and we have also partnership with very big giant and they are we are having data center in itself and as soon

as data centers are emerging in our country a lot of foreign players are interested in building data centers in our country whenever they will rise right as soon as they will rise this

problem will get eventually resolved down the line but as of today our system can handle more than 25,000 calls at this moment we have tested we We have tried 25,000 calls we can handle minimum

more than that it comes then we can also handle that because we have partnered with different service providers. &gt;&gt; I would like to add to that. So Vive obviously mentioned how we scale up but

then there is one more thing that Sachin and is obviously our team has done very well. So when you mentioned that 800 can take 500 calls. So there is certain technologies I I don't really recall

them but that is where we are trying to add more calls to this to the single H00 rather than you know getting more H00. So we are actually exploring many other models to train on many other backbones

to train on so that we reduce the model size so that we can you know increase or scale up very faster in terms of and take more handle more calls and we're also looking forward to accessibility in

terms of B200. We're looking forward to it as soon as that is there. Obviously, Blackwell is going to be very very much faster. So, yeah, that I think is the way we do it. And yeah,

&gt;&gt; this industry is too much volatile, right? As soon as innovations will come, I think that's true right so they have a portal right uh in that portal you will find lot of service providers GPU

service provider they are listed uh and they are mentioned their prices on subsidy so as you mentioned 56 but I haven't uh seen anybody in 56 again right you can find a good service

provider and you can again negotiate with them &gt;&gt; we had uh an an interaction with RDRA you must know Yas Shaki cloud RDRA right so actually they told us they're going

to give us a lot of credits because we are a part of Nvidia inception and everything &gt;&gt; so RDRA is managed by a company called CC organization called Ced under MIT so

uh we connected with Cedak guys and they gave us assurance that you can access at that time they had A10 machines couple of A10 So we access them but eventually we have moved to H00 now. So that is a

mismatch but again government provides supports. I mean you can find on the portal and you will find lot of service providers out there.

&gt;&gt; Thank you. Thank you. I think you know &gt;&gt; so uh when it's uh come that in the governance uh I had mentioned one point then the waiting time is been like which

parameter is it depends on your priority Mostly all the things will not taken up care by AI. AI is the thing but the human support is necessary in that because the critical task is being

assessed by the verification by your humans. So if any disaster which is a very particular the priority is very high then obviously the AI agent can give the transfer these calls to the

exact human. how the things can be transformed. Can you add on? &gt;&gt; I I think I think so you you ask it rightly. The problem here is right you

know AI agent is just a facilitator here. Government owns it or some department owns it right and it is just a L1 layer which listens you which hear you where somebody was not able to hear

you now an agent is there to at least hear you and uh thinking about your problem and thinking about some solution it would report to the control center of NDR. Right. So the role here is to

connect you with the NDRF control center which you were not able to make. It is kind of a medium to help you and uh we also have very much strict guard railing. So I would consider it

very rare that uh you know in a in a situation that our AI is suggesting you something that is not shouldn't be done. We we minimize the risk through very strict guard railing. So our AI I mean

you can play around with it. It'll never tell you what it shouldn't. It'll never say what it shouldn't. So, yeah. Yes, please. Okay, I'll answer that question. So

yeah, you have rightly pointed us to the intricacies of it like the uh end of prints or the VA modules, right? So yes, our models we have built individual models for those as well. that uh with

respect to the just talking with respect to the terms you mentioned. So the st model itself uh detects what's the end of the turn when the user or in kn language when the user is supposed to

when the user has ended his speech uh with respect to the noise and everything right so we have these adaptive filters so in noisy uh in noisy environment these filters would have us different

settings in a very clear environment like this it would have a different setting but these are uh intelligent ently adaptive so based on whoever is calling uh it will respond accordingly.

Yeah. And your next part of the question like 2G and 3G network issues right so again uh as I me as we mentioned earlier as well the models like how the traditional models are trained it's like

uh audio is given uh transcript is given and that's how the traditional way of training is but what we have done intelligently is provide some additional jitters out of it provide some you know

uh noisy background so just to st s st s st s st s st s st s st s st s st s st s stimulate the actual environment and with respect to all the dialects we have trained accordingly and that's how we

are ensuring that this type of cases are minimized. I hope it answers. &gt;&gt; Thank you. Thank you so much. &gt;&gt; Thank you.
