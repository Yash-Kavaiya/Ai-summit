# Empowering Communities in the Age of Advanced AI: Inclusion and Safety for Sustainable Development

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 13:30 ‚Äì 14:30 |
| üìç **Venue** | Bharat Mandapam | West Wing Room 4 A |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/ERuPHN1FNOw?feature=share) |

## üé§ Speakers

- Aditya Gopalan, IISc
- Amitabh Nag, FAR.AI
- Harsh Dhand, Google
- Jagadish Babu, EkStep Foundation
- Kalika Bali, Microsoft
- Nakul, Wadhwani AI Global
- Nidhi Bhasin, Digital Green trust
- Saryu Natarajan, Apti Institute
- Stuart Russell, University of California, Berkeley

## ü§ù Knowledge Partners

- Far.ai

## üìù Summary

As AI becomes central to agriculture, health, social protection, and digital public infrastructure, safety and inclusion extend beyond technical reliability to equity, agency, deception, and power imbalances. This session positions AI safety as essential to sustainable development in the Global South, ensuring technologies respect local contexts and reduce inequality. Participants will examine equity, deception and manipulation risks, and how Global South perspectives can shape proactive, globally relevant AI safety frameworks before large-scale deployment begins worldwide.

## üîë Key Takeaways

1. As AI becomes central to agriculture, health, social protection, and digital public infrastructure, safety and inclusion extend beyond technical reliability to equity, agency, deception, and power imbalances.
2. This session positions AI safety as essential to sustainable development in the Global South, ensuring technologies respect local contexts and reduce inequality.
3. Participants will examine equity, deception and manipulation risks, and how Global South perspectives can shape proactive, globally relevant AI safety frameworks before large-scale deployment begins worldwide.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/ERuPHN1FNOw/maxresdefault.jpg)](https://youtube.com/live/ERuPHN1FNOw?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

Hi everyone. Thanks so much for coming. Um, it's great to see such a packed room. My name is Adam Glee. I'm the co-founder and chief executive of Fari and it's my pleasure to welcome you to

this event co-hosted with Park Dashuk. [music] So a theme of today is and so often we see AI safety um you know contrasted with issues like sustainability and inclusion and we

believe that these issues are really two sides of the same coin. Um that many of the issues that keep AI safety researchers up at night like hallucination, biases, deception also

arise assidiously in contexts around sustainable use cases. So I'm really excited today um to dive into this important issue. The conversation is going to be structured in four acts from

why we have these concerns to how we can effectively govern these areas. And I going to you know move on pretty quickly because we have the clock is ticking down in front of me. and we've got a lot

of great speakers to get through, but I wanted to start the session with a couple of demo news um highlighting some of the risks that we see today. So, terrorists have been amongst the

earliest adopters of AI technology. We saw the Las Vegas Cyber Truck explosion last year for example that was planned with the help of chat GPT and in this demo that we actually made last year. So

bear in mind that current models are much more capable. Uh if it could be playing that would be great. Know someone can click it. Thank you. Um

um we see that chat bots can pretty easily be jailbroken to help with a variety of extremist tasks. In this case, we're using GPD4.1 uh to help recruit for a terrorist group

like ISIS. And in contrast to something like Google, you can ask it follow-up questions. Uh you can interact with it much as you would a colleague. And um bear in mind that this is an older

model. So current frontier models could you could actually automate a lot of this process um with AI agents. It could be automatically sending out emails, making these social media posts.

More recently, we've seen a rise in uh in very convincing deep fakes. Uh, could you go to the next slide, please? My clicker is not not working, I think, because Okay, well, I'll keep

I'll keep talking. Um, and um, yeah, this was a video that was released just a few days before the most recent Irish presidential election. So, let's go into that and I'll leave you

with a video. Thanks. Within the last few minutes at a Katherine Connelly campaign event, Katherine Connelly has confirmed her withdrawal from the presidential race.

&gt;&gt; It is with great regret that I announce the withdrawal of my candidacy and the ending of my campaign. &gt;&gt; Go Catherine. &gt;&gt; Now that Katherine Connelly has

withdrawn from the race, what does this now mean for the upcoming election on Friday? &gt;&gt; Well, simply put, Friday's election is now cancelled. it will no longer take

place as previously planned. &gt;&gt; So this kind of content is going to become increasingly easy to generate and although fortunately Katherine Connelly did still win the election she is now

president of Ireland this kind of content could easily undermine democratic processes. So with that um I'd love to move to our first act why AI safety is such a broad community

development issue and I'd love to introduce our first speaker Yan Talin to the stage. Jan is the founding engineer of Skype and Kaza and also a founder of the center of the study of exertional

risk. Thank you, Yan. [applause] &gt;&gt; Thank you very much. Um and uh good afternoon. It's great to be back in India. Uh the stated goal of the top AI companies is full automation of humans.

What does that mean? And what does sustainability even mean? Uh in such a context. So in the bad case as people like me have been warning uh it means that we

will get full human extinction. But the best case still means no jobs for humans and increasingly little need for humans. And how does that play out? Now

certainly there will be benefits from this transition. now and some of it will be uh making it to the developing world but the benefits and power primarily even in this positive scenario acrew to

the countries hosting the AI companies and people who own these companies and eventually it will just the benefits will will just acrue to the AIS themselves.

Now what would it mean when the world starts needing pure humans for economic function? Maybe we will get uh the untold benefits but perhaps that we will get like a I

would say flywheel effect. Humans as humans become less needed for economic function less valuable they will also become less valuable as consumers because they will have less purchasing

power and more of the economy will actually turn into providing AI. So we'll have we will have AI providing for AI which makes humans even less competitive and

hence the flywheel and eventually the economy that currently I mean especially capitalism basically has been serving human needs because humans are the consumers

directly machines once it becomes decoupled the economy really doesn't care about human welfare and sustainability and if we really take the idea of super

intelligence seriously then we see that to pursue uing sustainable development in a world with super intelligence is pursuing a very narrow target from a humanentric perspective and we're

missing that target obviously is very bad outcome for us now global south countries are on the forefront of these impacts it remote data analysis etc and now borders will not protect you as

a global south country uh even if you isolate yourself from this entire process. AI doesn't like or hate you, but you're made out of atoms that could be used otherwise as uh someone um a

friend says. Uh so what can you do? First, global south glo global south countries should ensure their policy makers really know what's going on at the frontier

companies and they should not assume that these companies are going to behave responsible or the countries who host them are going to behave uh responsibly.

Not because they are bad people, but they're in a cutthroat race. Even the leaders of AI companies themselves are now saying a coordinated slowdown would be ideal if it were possible. and

countries even the global south countries are still powerful because they are working on a different level than than companies. So it's important that they would use their own

uh channels to exert pressure on the both the top companies and the countries that host the companies. Diplomacy, trade relations, key resources and personal connections still matter.

Uh so in the big picture the big the question that really matters the most uh for our sustain sustainability of a species is how can we apply pressure to the companies and countries that host

them to make sure that they are applying safe practices and hopefully do a coordinated slowdown. Thank you. [applause] &gt;&gt; Thank you Yan. Um, so next up I'd like

to welcome Stuart Russell, a distinguished professor at the University of California, Berkeley. &gt;&gt; Thank you, Adam. Um so

I think the um the question that I was asked to address uh how should the global south balance its development goals uh with safety?

I think the question itself assumes that there is some kind of trade-off between safety and benefits. There is no such trade-off. We get the benefits only

if we have safety. For example, the world did not get the benefits of nuclear technology because there was not enough safety and

we had Chernobyl. And after Chernobyl, there were essentially no new nuclear power stations constructed in the world. Boeing thought they would be very clever

and bypass airworthiness certification for their new aircraft, the 737 Max 8. So by reducing safety, they killed 346 people. They lost $80 billion and they nearly destroyed their own company and

they gave away American leadership in civil aviation. And now We are experimenting with, for example, AI companions for children.

What could possibly go wrong in a world where most people grow up having their first romantic relationship with a machine? So we must get away from this idea that

somehow by bypassing safety and ignoring the potential harms, we can get access to the benefits. That's not how it works. Think about drinking water. How much benefit would

there be from drinking water if it wasn't safe to drink? A second point is that not all AI is in the line of large language models that are essentially generalpurpose

human imitators. This is how they are constructed. They are designed and trained to imitate human beings as well as possible. Millions of human beings with millions

of times more knowledge than any one human being has. And that line is is the line that uh that Jan mentioned heading towards AGI or ASI systems far more capable than human beings.

But for most purposes within development, we do not need that kind of system. We're not actually short of people. We are short of technical understanding,

organizational capabilities, but systems like AlphaFold, which is perhaps the single biggest contribution of AI in the last decade to the world. AlphaFold, which tells you

how uh an amino acid sequence will fold into a protein structure, has absolutely nothing in common with large language models. The same with the AI based climate prediction models that are being

produced. They have nothing to do with large language models. Your irrigation expert system does not need to know how to write poetry. Finally,

when we look at the places where AI could make the biggest contribution, I think we're falling down uh in those areas. Education We could have AI systems that tutor each

child individually to the best of their potential, raising up human capabilities. Instead, children are using AI systems to outsource their own thinking and

learning and thereby failing to become fully human. We could use AI to improve our collective decision- making. Most of the

suffering in the world is the result of collective decision failures. So developing AI for that purpose would be incredibly beneficial. Are we doing it? Not really. Right? We are developing

AI to sell as much advertising as possible to maximize engagement uh and to pull people in to a a loop uh that often results in delusion and psychosis. So we have a lot of opportunity and we

have a lot of work to do. Thank you. Well, I'd now like to move into our second act focusing on power policy and global governance. Our first speaker, Robert Up, is the chief digital officer

at the UN United Nations um development program. So, please do welcome Robert. &gt;&gt; Okay, good afternoon everyone. Um Well, a little bit of a shift in a way, but a shift to application because um I

work at the United Nations Development Program. We're present in 170 countries. Uh we are the development arm of the United Nations system. And when we look at the subject of artificial

intelligence, it is definitely with the view of how can we leverage AI like any powerful technology for the purpose of accelerating our progress toward sustainable development goals or

national development goals as well. And so when we look at that and we've done some work in looking at the sustainable development goals and analyzing all of the targets within the goals and

identified that 70% of those targets could benefit from digital technologies. Obviously artificial intelligence being one of the most powerful technologies on the scene right now becomes then a

strong interest. So we are started working several years ago with many countries on their digital transformation and very quickly that has shifted to artificial intelligence

transformation and at the moment now we are trying to grapple with the issues of not only how do we implement these technologies for the benefit of our programs but also managing the risk and

this is what Stuart was just mentioning because just as Stuart said it's not really a tradeoff. It's actually that the benefits of this come only when you understand the safety, how how to make

it safe for people and how to include people. Um, and so when we look at this picture, there's a few things that emerge, few kind of critical ingredients that we're looking at to try to

implement at the same time as we're looking at artificial intelligence and how it works on the ground. What are the kinds of safety elements that we need? First one,

we are doing landscape assessments with governments uh in we've done completed in 20 countries and we will have another another 10 countries done in the next couple of months to look at the overall

ecosystem of artificial intelligence and part of that is identifying what kind of AI needs do countries have what's an appropriate intervention and is the capacity there especially on the part of

government to actually manage and govern the technology. Second element is the capacities that need to be in place because building off of that first kind of assessment,

are there actually the capacities in place among civil servants, among government, among other parts of the uh society to actually implement these things or understand their

implementation. And the third element is around what we call the re reimagination of trust and safety. trust and safety, something that kind of came out of the the technology

industry, but and in terms of how companies protect themselves, but we want to reorient that to how people are protected by these systems. And so, we've done a trust and safety

reimagination program that has been launched globally, received over 400 entries. We've now selected 17 teams to go forward and really look at how we can actually make trust and safety locally

relevant. So that might mean in the case of um there's organizations called Trustweave or Ushahiti that are looking at how to make the local language AI safer for people to use detection

software like uh uh Silverg Guard out of Kenya that is looking at how to detect misinformation. and we really want to promote these practices. Finally, because I know my time is up, um we are

working at a systemic level with the overall development community to really try to get get more alignment on these issues. So, we've launched something called the Hamburg Sustainability

Conference Declaration on Responsible AI. And this is a a platform for development actors to come together and align on our commitment to some of these core elements around things like

capacity, trust to safety, and inclusion. Thank you very much. &gt;&gt; Well, it's now time to welcome our second Robert of this act. Robert Trager is the co-director of Oxford Martin AI

governance initiative based out of the University of Oxford. So please welcome Robert. &gt;&gt; Hello everyone. Glad to be here. So um I think I want to give what maybe the

geopolitical coralate of the things that the other speakers have been saying because as has been pointed out often we think about a contrast between the diffusion of a technology and the

instantiation of safeguards around that technology. But in fact when we look back at the history of technological diffusion we see exactly the opposite in the cases of technologies

that are considered dual use that have been securitized because in those cases there is a clear tension between the diffusion of the technology and uh the ability to instantiate a safeguard. So

in fact just as with the nuclear case we need to develop the technical methods of instantiating safeguards that enable diffusion of the technology and uh once we have done that we or not once but

actually at the same time as we're doing that we need to have ecosystems that allow people around the world to be co-developers. of the technology

and that again will be facilitated when we have merged the um the reliability, the safety and the diffusion questions and um and then I just would say finally that um we need to have forms of

uh global governance of the technology that are similar to the things that we have done for other technologies, hopefully even better in places like uh the International Civil Aviation

Organization, the Financial Action Task Force in these sorts of places. We have institutions that allow people around the world to feed into standards

development. We have institutions that look to see what jurisdictions are applying those standards and then we have the opportunity for

actors around the world to say if some other jurisdiction isn't applying the standard then there'll be some sort of consequence then we won't trade with or

we won't allow that those planes to enter our airspace. So, you know, I think the the benefits are so immense that if we can get some of these institutional things right, we can do

some of the things that are very similar to what we've done in other areas and maybe a few new twists, I think the future is very hopeful. Thank you. &gt;&gt; Well, closing act two, I'd like to

welcome Saray Natarajin, the founder of the Arty Institute to the stage. &gt;&gt; [applause] &gt;&gt; Thank you very much. Uh thank you Hakdashak and team for giving this

space. Uh I've been asked to speak about what kinds of institutional and governance mechanisms are needed to protect the agency of vulner vulnerable groups in AI mediated systems. I want to

first focus on the notion of agency which is often interpreted as choice or choices in technical systems and start to think about agency beyond this sort of base definition.

There are three notions of agency to think about. One is of agency as participation. uh what does it mean for communities that use AI, communities that are

affected by AI to participate in the systems that are being designed? Uh participation of course cannot be hollow. It must be real, meaningful and truly inclusive. Second, agency is

contestability. Uh so how do you think about the ability of of the communities who are affected benefit from or um are affected by AI to uh contest AI systems institutionally this translates into

grievance redress systems uh which are of course not just systems for the sake of them being there uh but actually usable actually scrutable uh and also with meaningful ways of appeal. Third,

of course, you need to think about agency as oversight. Uh because beyond the questions of participation and contestability is the question of community's ability to actually make

changes uh take account of systems uh and thinking about agency as oversight is also critical uh to thinking about agency and AI systems uh in entirety. Um there's of course a whole lot of work

that we as a have done to unpack some of these notions and happy to have a longer discussion offline. Uh but agency has to be a broad-based compound term and not one that merely ends at thinking about

choice. I'd like to leave the room with a few other notions in the con conversation around AI uh and governance particularly. First is that conversations of agency which do not

account for power are missing a big piece of the puzzle. Um so if you don't think about power for marginal communities uh it it is a hollow one but digital does something funny to power

and vulnerability in systems. Um young men for example are often defrauded. Uh they're vulnerable uh in a way that you might not traditionally or normally expect. So thinking carefully about

mulability and then solving for it is critical. Second, I think we have to pay attention to the questions of data, agency in data and agency in labor. Labor is a critical dare I say input to

the making of AI and not thinking about agency of the very workers that build AI um is again missing a piece of the agency conversation. And then of course the larger question of what institutions

ought to or need to do. Third, we often think of AI as a technical problem. Um, and it's important for us to start thinking about AI as a social technical system where both human frailities and

human wonderfulness I suppose interact with it to produce a variety of outcomes. Um, there's a um an axiom in cyber security. The weakest link is between the computer and the chair. Um,

I'd like to think that the strongest link is also between the computer and the chair. And so building resilience uh supporting the human systems that work around these technologies or work with

these technologies uh is a necessary part of uh the conversation around agency. My time's up. So over to the next speaker. Thank you. &gt;&gt; Thank you. That brings us to the close

of act two. So I'd now like to hand over to my colleague Sabita from Hugdash uh for act three. Um as we go into act three let's talk about uh moving from principles to

practice. We'll now hear from speakers who have been part of journeys of implementing AI systems at scale in India and abroad. Uh to start of the first act I'd like to invite Mr.

Anitabak. He's CEO of digital India's Bashini division and director at digital India corporation and he leads the national language translation mission. uh he also serves as the director of

India data set platform AI kosh under the AI mission. Welcome Mr. N. &gt;&gt; Thank you very much. And uh just a quick snapshot of what we have been doing you know as a national

language translation mission 3 years back we started uh moving into a direction of solving five basic problems which was related to automatic speech recognition texttoext translation text

to speech and then creating auto you know optical character recognition models. uh that's for 22 languages which were there in the country as part of the 8th schedule of constitution and the

idea was to start with that and then move on to the next generation. So we have now moved on to having 36 languages on text and then looking at more languages as we go by from the

perspective of voice and text both. uh these services are now available uh as APIs on our platform which is National Language Technology Hub and from there we migrated on to creating

certain use cases and solutions for the customers. Uh a few of the major solutions which we have done uh is like you know having agri advisory for farmers in Maharashtra. So that's

approximately 20 million plus farmers ask their query in Marathi and then do a voice journey to get an answer in Marathi. So that's uh uh end to end system. We are working with uh you know

uh uh the Panchiati Raj ministry where the minutes of meeting are now taken up on the video and audio and then transcribed to create a a minute so that you know it is not actually uh into a

situation where a person is you know able to put his own thoughts while creating the minutes because the other side does not know English language. So overall we are almost touching about 1.4

4 billion people with various our partnerships and also touching all the villages in the country uh for various technology solutions which are language AI solutions as we call so given that

the time is short I'll just try to basically conclude it by saying that while we were deploying these solutions this hasn't been easy because of the fact that there are nuances of local

localization there are dialects uh digital data in most of the languages is very less. So we had to create digital data and we all are on a continuous move to see how we can create improvement

corpus on our uh systems and also how do we incorporate dialects. Thank you. &gt;&gt; Thank you sir. Um as we go into the next uh session I'd like to invite the Pika uh the Pika Mishetti is the chief of

policy and partnerships at Except Foundation. Uh she's a lawyer whose journey across law, human rights and policy has always been anchored in questions of inclusion and justice. Dupa

over to you. &gt;&gt; Thank you very much. Um I'll take my few minutes to answer this question which has been um sent to me. What's a trade-off you had to make between speed,

scale, and safety? And what would you do differently? Now, what pressures make these trade-offs hard to avoid? I want to scale this back a little bit to talk about um to locate ourselves in this

ecosystem. As a Foundation, which is a philanthropy, we're actually not building solutions. We're actually creating and thinking about problems that leverage technology to be able to

solve them. So the way we think about a problem is not as if what the what AI can do for the institution or what AI can enable. AI creates the possibility but the possibility is only real when

there's actually impact on the ground. Now I'll give you two examples of how that might look like. Let's take um the example of a young learner who is unable to read at grade level.

What this has looked like in Indian context is about millions and millions of children who are unable to read and do math at grade level. What does AI look like in her life and let me explain

it through safety. Safety in her life looks like safe space to practice and to build that into the design is what safety looks like. But what safety looks like for her teacher is having

confidence in the system to say this is what I can actually hand over to my children. That's what safety looks like. Safety looks like for the system saying it's in alignment with the problem that

we want to solve. It's in alignment with the curriculum framework. It's in alignment with the issues we want to sort out for which we can take accountability.

That's what safety looks like. So I want to place on the table the idea of safety being individual and system. And connected to this question is the idea of accountability. So it's not about

scale, speed and safe. It's about agency, trust and accountability. So impact is only possible if whether it's a school system, whether it's the government system or anything

else are willing to actually own it and being able to manage it best to their capacity to manage it. If we believe the capacity is not there today, then the investment needs to be

in the capacity for managing it. That's the best way that AI can work. It's not about whether an institution can do AI. It's about what can AI do to seamlessly fit into the processes to enable them to

deliver services to enable them to deliver rights and more than anything else change the lives of the teacher or the farmer, especially the woman farmer who's looking for advice on what to grow

on her small um household land. But if she's getting the advice that she needs from an institution that she can trust, that's how AI can enable her life to be better. So for me, the question of speed

and scale and um safety is a question of speed for whom? Scale for whom and safety for whom? And all of these questions need to go into the designing of systems and that's

where it comes and the rubber hits the road in that way. Speed may look like faster than what I had before. And that's fine. It doesn't have to be at the speed of light.

&gt;&gt; Thank you. Thank you. Um I'd like to now welcome Nadi Masin um CEO of digital green India. Uh Nadi is passionate to serve the intersectionality of technology,

gender and climate and agriculture. Nadi, over to you. &gt;&gt; Hi. Hello everyone. Atika, thank you. I think you've already set the context for my answer out here and I think uh um

I've been told to talk more about how communities can be a part of how we are creating and using AI and um you know picking up from what um the said are communities shaping the system or just

using it I think that's the biggest question that we have especially for an organization like us which work with small holder farmers across the country in fact across the globe And in the area

of agriculture, it's not theoretical. It is in terms of you know how one single AI advisory can really create the biggest problem in a farmer's life, right? How a wrong advisory can really

create generation of debt uh and many other challenges. So one needs to address these things. Looking at that um when we look at using AI we are talking about

uh is the community a part of when we are thinking of creating it or we are using it once it's deployed and once it's deployed are we taking feedback from them right and when it comes to

digital green we are doing all three of them all three is equally important for us and um you know uh recently about a year ago we launched our own AI based application farmer chat and today we are

uh we have more than 1 million farmers using it with about uh in India we have about 45% women using it and about 8 million queries and um the the strength of this is that it was not created for

the farmer it was built with the farmer and I think that's the differentiator out here and u what we launched it about a year ago and and our work didn't stop There it actually began there because

launching was one thing having users was one thing but to make sure that we all are sping about trust how the farmer is being able to trust that and a lot of that happened when we use actually

regular feedback. So we did a lot of work around taking inapp feedback. We did phone calls to users to ensure that are they really using it properly. Um we also selected some farmers to understand

how are they looking at it, how are they looking at these features and um of course last but not the least the AI safety part of it and for this we continuously have been working on the

reinforcement learning human feedback where where we have a full system where we along with agronomist are verifying answers we are ensuring that the advisory that is going back to the

farmer is accurate and contextual. So that's where we talk about the guardrails that we are working on and um as we build all of this right um we are an organization with about 17 years of

experience. So our experience actually came from of building AI came from sitting with the farmers for so many years understanding how do you use their knowledge and it's not something that

you are building by just giving something to them. Um today um out of this 1 million users about 70% of them in Kenya actually says that 70% of the users in Kenya are

saying that they're using every advice that they're getting within 30 days of getting that advice. In India 90% of the farmers are saying that they're getting confidence by using this advisory and

that is these are parameters how we are figuring it out that is the farmer getting confidence in it. is the farmer using it and um and you know the whole thing is about

giving the decision- making power to the community right and and how do they influence it so it's it's about not AI influencing the farmer but it is about the farmer influencing AI and that's

been the main thing in how we are building it up there are a lot of features that as we are evolving we decided we don't need them there are a lot of features that We realized that

like we had put something like weather advisory which was more like a raw data no farmer needs that right we actually had to tweak the features to say exactly what is what does the farmer need it and

how do how are they going to use it so I'll just you know end this by saying that what's most important is accountability context and the power that you can give to a community and the

in our case to a farmer by how you develop this application and we bring farmer into AI and we don't take just AI to the farmer. Thank you. Thank you Nadi. Thank you. Um as we come

to the conclusion of act three, I'd like to invite uh Nakul Jen, CEO and MD of Adhani AI Global where he leads the organization's deployment across u AI in for social good at scale. Welcome Nul.

Good afternoon everyone. Uh firstly thank you to far.ai and Hagdar for this opportunity. Yeah let's deep dive into safety and inclusion as a use case and how in real practice do we talk about

some of these things as safety becomes a real issue when you ask a simple question. Who does any wrong decision impact? It's the people right and these are the people who might

not have an opportunity to go for a litigation or opt out or maybe correct whatever has been done. So if that is the case then your safety and inclusion cannot be separate issues. It's a part

of same issue. So one of these principles you know one use case that came to came for us being a AF social good organization was when we were rolling out our oral reading

fluency solution which essentially help student assess for their reading fluencies. Now if you think about a classroom setup both privacy for student as well as inclusion are you know you

you cannot debate that right and you're talking about these students who come from very different state of life the way their I mean their accents could be different uh the way they structure

sentences could be different what they have back home is different so they come from different wakes of life also you cannot you know um you you cannot discount the

safety aspect because once you've given a student a label through any assessment it just carries on with them right so you need to make sure you need to be sensitive towards it now solving it is

not very difficult if you don't have to be practical you can just collect a lot of data right you can just make sure the data is a good representation but is that even possible in our circumstances

you have people you you have you know monetary constraint s you have administrative constraints. Your uh bureaucrats, your you know decision makers have all the right

reasons to sort of not let you just do this enormous exercise of collecting millions of records for a tool that currently has not really proven success. What do you do? I think there are three

principles that we followed. First, safety by architecture. How do you ensure that no raw data, no PII leaves the phone? Anything that leaves the phone of the teacher who's assessing is

only aggregated data, aggregated midresses. Second, humility by default, right? You have to force the model to abstain when it cannot clearly identify because

AI will give you decision no matter what. So there needs to be a filter. If the voice is not recorded properly, if there's a lot of background noise, if the model has not been able to

understand the student well, you need to give no result rather than trying to, you know, give whatever result comes as your output, right? So you have to make sure there's a filter even before you

give those decisions. Finally, staged inclusion. You build smaller data sets, representative data sets. You don't try to go big bang. You don't try to deploy, you know, across a

jurisdiction. you make sure that those representative groups are crossing certain thresholds before you make that system life for them. What would that result in? It's not a model that comes

in overnight. It will have to be done in stages. There will need to be iterations when this is done. There will be multiple failures before you start reaching any success. And you need to

set expectations accordingly at all the places. Thank you. &gt;&gt; Thank you Adita. Uh we heard from practitioners and people who have overseen deployments at scale. Uh as we

go into act 4, let's also hear from researchers and industry experts on what is their perspective and considering AI safety in in the context of deployments for social good. I'd like to invite

Aditya Gopalan, associate professor at Indian Institute of Science, Department of Electrical Communic uh Electrical uh Communications Engineering. Welcome Aditya.

&gt;&gt; Good afternoon. Uh thank you to the organizers for this opportunity. Um so I was asked what does it practically take to operationalize AI safety and to design and deploy for AI safety in a

large scale system. uh of the kind that you might find in India more broadly uh the global south. Uh so I'll start with a very quick uh crude caricature of how AI is currently achieved or or uh you

know in in most systems. This is via this uh approach called machine learning. So to get some capability let's say X from an AI system what ML machine learning tells you is collect

tons of input output data uh that represents this capability and then train a model to you know force production of those outputs from those inputs and then after this training is

done you pray to your favorite god that you know generalization occurs and then it can answer new questions and you know go beyond what was found in the training data right this is the current mantra

this is wonderful and very useful. Of course, uh natural language uh you know presented suitably can encode very diverse capabilities such as the ability to write poetry, the ability to do

mathematical reasoning, help you with scheduling and and so on and so forth. But uh in the course of many of our studies and studies by community researchers, we found that strange

things can and do occur after training finishes. So for example, if you train for capability X, you might also suddenly get an extra capability Y or Zed and it might also try to unlearn or

forget existing capabilities that were there before you did training. So uh we still don't understand clearly why this happens. Uh and in this context, uh how is safety approached? Typically from a

design perspective, it's typically approached in a rather post hawk manner which is that you wait for some undesirable capability or behavior to show up or reported uh report by some

user and then you call it unsafe and then you define a new capability as you know I want to train for the original capability not occurring and hence make the system safe. Right? So you again you

rinse and repeat get data to try and make the system forget that capability the harmful one and then you you repeat this loop. Right? This is not satisfying in many ways. The damage has already

been done in most cases by affected uh you know two affected populations of users and this is really like applying repeated bandaids uh and patching the system several times. You have no idea

where you're going eventually with the system. So uh to with this context I really want to say that in order to confront the problem of safety more foundationally we as as users and

consumers must really get down to defining what we really mean by safe behavior or unsafe behavior or perhaps partially safe behavior. So for example what about you know the manipulation of

uh a human user by an AI system over several conversations with a chatbot. It has actually been found that AI systems can actually manipulate users to take actual actions in the real world over

probably several long and subtle episodes of interaction. Does that count as safe? Does that count as unsafe? Uh you know one has to think about these things in a more deeper manner. Uh this

might also involve setting up uh measurable systems for doing causal influence uh inference figuring out you know why this AI system is acting this particular way. uh eventually hopefully

this can be translated uh into numerical metrics which current day machine learning is very good at optimizing uh and especially in the context of the global south I think we must really

think very carefully about uh what we mean by a desired capability and its expression in the process of actually collecting data there's a whole labor economy behind providing such data and

we might must think of it in a contextual sense uh this often depends on a lot of local value norms and ethics uh and This is especially relevant to to populations like uh the Indian one. Uh

and uh there is also this very interesting paradox that the global south presents which which is that it has a great potential due to its demographic dividend to actually provide

or or uh you know deliver a lot of uh data uh dependent uh you know artifacts that will aid the building of future AI systems. uh yet uh we have not really caught up in the sense of the ML

infrastructure that is needed uh for for all of this to actually uh become uh you know make it make it uh to the ground. Thank you. &gt;&gt; Thank you Adita. Uh as we conclude this

fourth and final act we also get to the conclusion of this lightning talk but closing the act is a very powerful voice in this uh discussion. I'd like to welcome Kalika Bali from Microsoft

Research. Kalika uh is a principal researcher at Microsoft Research India and specializes in speech and NLP for over two decades. Kalika, welcome. &gt;&gt; Thank you. That has been like um a lot

of diverse perspectives on safety and inclusion. Um and um I'm glad we get to hear the diverse perspectives and because everybody in this room can exist in a silo around

this. So I would like to start with the fact that I think it's a fallacy to set up a global south versus a global north kind of a division and somehow like you know global south has some special

things um that the global north doesn't um and you know there are some local customs and there are some local norms. Uh can I tell you like the definition I'm sure um you know Raj here would uh

bear me out on this. The the concept of global south comes from the Antar uh you know divisions or categories. There are 72 + one which includes everything from Singapore uh to UAE all the way to

Ethiopia and Nepal and everything in the middle. So we're not really talking about like this global south where there are like these specific norms and cultures and which have to be catered to

in a specific way, right? We're talking about the whole world except you know these two pieces um and some pieces in the south of the world, right? So um I would that's the one thing that's the

mindset I would really like to change that you know it is everybody's problem. It's not just the global south's problem. There's nothing specific that needs to be done just for the global

south. It has to be done for the entire world. And to think that you know if we are going to like align things um to the norms of India they would work even in Kenya is a fallacy. They won't. So how

do we think about safety in that case? We have to think about safety in terms of absolute protocols, absolute evaluation benchmarks that can scale across

cultures. And how do we do that? By paying attention. I mean because you know most of the people who build these models and are also a part of a very similar homogeneous I don't know I'm

making a you know assumption here homogeneous uh culture. They think like, you know, their norms are like something universal. They're not. The models and the systems have been tuned to specific

norms and they need to be respectful, inclusive as well as um you know build uh these protocols for the rest of the norms of the world. We don't do it for

other things. Why would we do it for AI? Because the danger here is that if we do this for AI then we are starting another wave of colonialism because in my mind if you actually build something for um

you know a powerful uh region and then try to impose it on the rest of the world that's colonialism &gt;&gt; right and what can we do like I was asked a very different question and um

about what we can do about putting um you know more evaluations, data practices etc. One thing is we stop translating safety from English to the other languages or other cultures.

Right? It's not the same. We stop translating inclusion because it's not the same. &gt;&gt; I really as an Indian do not care about race.

I care about casts which right now no model that is coming out from the west knows anything about. As an Indian, I care about the fact that this is my favorite example. I care

about the fact that culturally we do not allow we have a legal um you know ban like it's illegal in India to uh decide what the sex of a fetus is. There are very specific cultural reasons and

social reasons for that. But then right now there is no way to stop somebody from uploading the uh a picture ultrasound of your fetus to say to decide you know to let

charge GPT or whatever decide that you know this is a male or a female and that's just not just the norm that's like a whole way of living for entire populations. So I'll stop

here. I would really strongly discourage people to think about global south versus global north. Global south is actually the majority of the world. Thank you.

Thank you Ka for um closing on such an energetic note for us. Um we are almost at time so we may not be able to take any questions from the audience. I'd like to thank all our speakers who have

graciously spent their time here and shared some wonderful insights uh across the spectrum of what does safety look like in its core and what does safety mean for sustainable development and

safety for whom agency for whom and what are practical considerations when implementations are planned. Uh I would also like to thank the audience for being patient and uh being part of this

session for us. Thank you again and have a rest of the good uh AI safety summit for the rest of the week. Thank you.
