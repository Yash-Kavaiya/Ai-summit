# Enterprise Adoption of Responsible AI: Challenges, Frameworks and Solutions

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 11:30 ‚Äì 12:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 8 |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/EHLHi1siJj8?feature=share) |

## üé§ Speakers

- Abilash Soundararajan, PrivaSapien
- Ashish Tewari, Infosys
- Dr.¬† Subodh Sharma, IIT Delhi
- Dr. Geetha Raju, CeRAI - IIT Madras
- Raj Shekhar, iSPIRT
- Shri Avinash Agarwal, Department of Telecommunications, GoI
- Sureshram Venghatachari, PrivaSapien
- Vibhav Mithal, Anand & Anand

## ü§ù Knowledge Partners

- PrivaSapien

## üìù Summary

How can Responsible AI be implemented across the data and AI lifecycle? This seems to be a distant dream across the world, with very limited unified approach to legal, technical, governance and operational requirements. This session builds on top of the unified Techno-legal framework published by the Indian Office of PSA, brings together researchers, policy makers, framework developers, technology platforms and enterprises, to understand the challenges, opportunities and demonstrates a reference implementation of Responsible AI across Data & AI lifecycle.

## üîë Key Takeaways

1. How can Responsible AI be implemented across the data and AI lifecycle? This seems to be a distant dream across the world, with very limited unified approach to legal, technical, governance and operational requirements.
2. This session builds on top of the unified Techno-legal framework published by the Indian Office of PSA, brings together researchers, policy makers, framework developers, technology platforms and enterprises, to understand the challenges, opportunities and demonstrates a reference implementation of Responsible AI across Data & AI lifecycle.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/EHLHi1siJj8/maxresdefault.jpg)](https://youtube.com/live/EHLHi1siJj8?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

Okay, good morning everyone. On behalf of Sapion, I'm happy to welcome you all for the event. Uh so we as a knowledge partner are presenting on enterprise adoption of responsible AI challenges,

frameworks and solutions. So as part of this 55 minute session we have a keynote address for about 12 minutes by Shri Ain Shagaraj GI who's a deputy director general of department of

telecommunications and also a member of the drafting committee of the AI guidelines and uh follow following that we will have a panel discussion and then we will have a

text act presentation responsible text that's so I now request to come forward Stop. [cough]

Thank you. I hope now it's better. So I will be talking on the responsible enable and kindly bear with me because we don't have the screen the front. We

have to turn back and look again and again. Uh check. &gt;&gt; So a very basic background is that now we all know that responsible AI is a necessity. Now uh AI is now just not

just a choice but it is now becoming part of the infrastructure. We have seen AI deployed in all other applications especially if I come from telecom we see it is there in the core decision making

chain. Uh if we think of power grids it is there. So uh it is em embedded in the enterprise decision making and integrated into critical infrastructure and as moves from experimentation to

infrastructure governance should be also moving from architecture. Uh I'm going bit fast because I'm told that time is limited. So considering all these aspects the office

of the principal scientific advisor recently came up with this uh report which is uh strengthening AI governance through techn technolal framework and I'm happy to share that the founder and

CEO of Mojaki contributed in this report. Uh in this report they have outlined the technological approach for AI governance in the country. And the definition they

have given for this approach includes that the AI governance must integrate legal instruments, rule based conditioning, regulatory oversight and technical enforcement all embedded by

design. Uh so that AI governance is not something which is external. It should be intrinsic within the systems itself. And the report also defines the why, what and how. So uh

these are from the report itself which says that the Y includes protecting privacy, safety, security, fairness, constitutional rights. Uh what includes safe and trusted AI across the life

cycle and for how it should be technical safeguards and governance mechanisms ensuring transparency, accountability, explanability, proability. So uh I will be focusing more on the how

part because we have to move from the high for this this is a five layer framework that we had some time back it is a published work and uh uh it gives a

holistic view of how AI governance should be addressed. So if we think that many times we are looking only at [clears throat] one or two or few layers but not at the whole step. So the whole

idea is that while we have the laws, regulations and policies at the top layer what are required sometimes the legal mandate sometimes the Wi-Fi password but

then we need the standards to define what what are the actual requirements like if we talk of fairness then what actually fairness in that context what are the uh conditions

properly and clearly articulated. After that is the standardized assessment processes and sometimes I feel that the standardized assessment processes are more important than the standards

themselves because unless we follow a standardized process to assess some uh compliance we different like if we are saying like the weight is 10 kg. So there has to be some standardized

process to come to the conclusion that it is 10 kg or so also we need standardized tools and metrics because everyone is not an expert. So if we are deploying AI maybe in government in any

other company also in enterprise &gt;&gt; if you have standardized tools then the trust is there anyone can use it and then develop their tools not develop the tools but assess their products or the

products they're inducting in their uh applications or networks and then the last layer is the voluntary compliance ecosystem. So it could be some kind of voluntary certification like if we are

talking of fairness then we could have a voluntary fairness certifications based on some standards and based on some standardized assessment processes. We could have some voluntary disclosures

which are very much helpful in building trust in the applications we are using. So uh this we can consider as the vertical dimension five parts of the

&gt;&gt; and these are the five five this is very well articulated in the report cover the whole life cycle of the AI products from data collection data in

use protection AI training and model assessment safe AI inference cycle and also trusted uh agents which could be for disciplinative or generative or agentive AI. So if

we have the vertical column with all those five things and then the horizontal again five aspects and then if we multiply these things or combine them we get a matrix like this

I'm not going into the detail because the time is less this is just an indicative things. So the whole idea is that each life cycle stage we should look at all the five uh governance

aspects not in silos only then we will be able to have a fully responsible AI actually implemented at the ground otherwise we only talking at the high level principles or there will be gap

between the implementation and the principles. So I will just go into details of these three broad aspects like if we think of the first two uh life cycles they are

related to this data collection and data in use. So uh this we can think as data governance and so I have mentioned the five layers on the left side and what I mentioned on the right side are not the

firm or actual these are just indicative to give an idea like if we talk of policies for data governance. So these could include statutory data minimization or purpose limitation

policies clearly assigned data accountabilities. When we talk of standards, we could have data set documentation standards or security baselines for assess control

for assessment process. So like this we can have all these things at different layers. So these are just indicative like we could have representational balance metrics at data set level. So

all those things can build trust when we talk of data governance. And uh then we come to the third life cycle stage which is model governance. Uh so in model again if we think of what

policies could be there. So just an indicative aspect that we could have mandatory AI model risk categorization policy. We could have policy defined intended use and deployment boundaries.

Uh if we think of standards so standardized model evaluation framework like it could be performance reliability fairness or bias assessments could be there. Uh robustness and resilience

benchmarking requirements should be standardized. What actually robustness like again if we come back to telecom so in telecom robustness could be different but if you think of the medical

application the robustness requirements could be very different. So these have to be very clearly uh articulated and standardized so that the interpretation is not different between two individuals

or two companies. Uh if we talk of assessment processes then we can have assessment fairness testing across protected attributes what the TC standard is also proposing. Uh adversary

test stress testing protocols. Then tools could be like we could have different matrices also which are standardized. So uh we have so many different matrices and which is good

like different matrices for different purposes but also we need some kind of standardization like at the higher level combined matrix which are agnostic of the matrix lo used used at the lower

layer level but still like comparable like some product tested in one organization another product tested in some other organization using different matrices but still the end result is

that we can compare them and say okay they are relatively ranked in this manner uh I I hope I still have good so uh and then for compliance we could have model assessment reports third

party validation or certification so all these would be helpful in building trust also and if we are having some kind of voluntary certification then for startups especially like yours and other

startup companies MSMES they cannot compete with big tech so if they have some kind of voluntary certification that gives authenticity to their products also. So if we talk of say

again coming back to the fairness example and we have a standard in PS and TC standards it also has the assessment product uh procedure tool respect also PI fairness

assessment. So if we have a mandate that our product should be fair now we have the standard assessment process standard complant to that standard. So we have all the four requirements at the

top level and we also have the combined bias index score for combined agnostic of the lower level matrices metric process. So all the four requirements are fulfilled but there should be an

ecosystem like you go somewhere get your product certified not mandatory but voluntary it is in your benefit. No we are not mandating it. So then the whole ecosystem can be there like we have

these fivestar ratings for our AC and so kind of some kind of rating disclosure. So people know that okay it is at least we are confident to use it at least for the freedom.

Uh then we come to the last uh life cycle stage which includes these two infants and agents that we can say runtime governance and this is also very critical because the model is everything

fine perfectly okay when we are launching it but when we use it then also lot of uh governance is required and maybe for policy we can say human oversight requirements could be there

incident reporting frameworks should be there uh defined and liability pathways are required. Then we say of the standards then what are the harm probability thresholds like thresholds

have to be specified for different cases. Uh agent resility standards assessment process could be including standardized drift detection processes standard AI incident classification for

tools. You could have all these metrics like drift detection metrics and all those things, hallucinations, frequency indicators and then we could have compliance like AI incident disclosure

frameworks, examine review mechanisms. So like postdevelopment deployment governance determines whether EI systems remain trusted over period of time.

[clears throat] &gt;&gt; Continue is the last. So uh if we see like where enterprises typically operate. So most enter enterprises focus on policies uh then security compliance

and then tool experimentation uh most most of the time this is not a generalized statement but it is like most of the time people or companies lack standardized AI assessment

processes or comparable fairness metrics or structured AI incident reporting frameworks and all these things. So like this creates some kind of governance fragmentation.

[snorts] &gt;&gt; So a full step responsible enterprise could require like every life cycle stage uh governed across all the five layers and with measurable and auditable

alignment. So if we can in simple way say that we have policy standard standardized assessment uh metrics and then a disclosure. So this is like how to operationalize the technological

governance which is very well articulated in the PSA report also and just last two slides. So we are very well placed. We are fortunate that in India we have already articulated the

techn technological philosophy in our report. We also have the life cycle framing which is there in the report itself and also the rights anchor. So the most

important step should be the implementation architecture. If enterprises align life cycle, India can lead not only in AI deployment but also in AI

engineering. And uh so in the end I would just simply say that responsible AI is not a policy document. It is not a certification badge. It is not a single technical control. It is a structured

alignment across life cycle stream and governance layers. Thank you. [applause] &gt;&gt; [cheering] &gt;&gt; Yeah. AIS

photo. photo 15. So uh thank you again for setting the context as a keynote speaker. Uh it was

a wonderful uh session uh insights uh drawing from the report as well. So now with that we move on to the panel discussion on responsible AI uh enterprise adoption of responsible AI

challenges frameworks and solutions. So for that we have Raj Shaker who's an A regulation fellow from Ice Foundation and we have Vibamethal who's the associate

and we have Dr. Gita Raju who's from senior policy analyst and we have Dr. of CSC [clears throat] &gt;&gt; and myself

a VP of growth and community from APN. So we'll be the final for this particular one. So the discussion will span about 25 minutes and I would hand it over to Raj for moderating the

session. &gt;&gt; Thanks Suresh. Good morning everyone. I'm Raj Shaker. I'm an AI regulation fellow at the iceberg foundation. uh a think and do tank that has long

championed the digital public infrastructure story. Uh welcome to the panel on enterprise adoption of responsible AI. Today's conversation focuses squarely on scope. What does it

take for enterprises to move from rhetoric to real world implementation of responsible AI adoption at scale? We are at an inflection point as we speak. 5 years ago, AI trust and safety were not

central boardroom priorities. As we all know, for most organizations dabbling with artificial intelligence, today we look around that has changed dramatically.

Responsible AI has moved from the market to mainstream so much so that safety and trust are at the epicenter of this impact summit. So let me begin with a common question to

all the speakers here. What do you think has changed? Uh what do you see as the single most decisive incentive for enterprises across the board that is driving them uh

to integrate safety into their technical operations? Uh I think we can start with Suresh. Yeah, thank you Raj. Good morning everyone. Thank you for being here. So the if you look at it from a

responsible a perspective from the organization, &gt;&gt; the single most important thing is to have a trustworthy AI. So when you look at it uh the trustworthy A is something

which helps the organizations to mitigate risk from an enterprise risk perspective. In in case an organization does not adhere to these uh responsibility principles and implement

it in the right way, the organizations are liable to have certain uh regulatory risks related to that. So it could be penalties, it could be some kind of a u u fines or warnings and things like that

and also it could be certain financial risk associated with something and also reputational risk to the organization in terms of uh the organization's uh reputation could be tarnished in terms

of uh the uh principles not being adopted and so on. So it becomes all the more important for an organization to ensure that they implement a responsible AI framework and as Vin G just mentioned

in terms of uh the life cycle of the entire A it's at one stage the entire five stages of the A life cycle uh in fact it's been mentioned in the BSAS report also so I think the controls have

to be there at all five stages so adequate controls to be put in place so that uh you implement it right at the data collection stage and of course during the model training and all those

things. So the different life cycle of AI I think we need to implement those controls. Uh so &gt;&gt; 91180

7860 &gt;&gt; I think um the answer to that question lies um in the keyword responsible AI and as solution G already pointed out and Abin G in his talk earlier that you

require a trustworthy uh AI to be implemented in order to move towards uh providing this responsible AI framework. Now &gt;&gt; for enterprise adoption, I think the the

biggest pain point is the cost of implementing safety and compliance layers and that cost has to be lowered the [clears throat] cost of possible harm that could come out without having

these technology interventions in place. So cost is a big uh and that essentially prioritizes that moving towards trustworthy AI. What is it that as designers as um you know technology

architects you should be thinking about. Now there are a lot of terms that flow into that that are being discussed in this landscape as a part of trustworthy AI. people are talking about explain

and it's a higher order semantic task in the sense that there's a lot of computation and there's a lot of complexity that has to be brought in to essentially enable an AI system to be

exposed to it &gt;&gt; trust is indeed very important [clears throat] but what are those basic building blocks

&gt;&gt; that hard which are cost effective and yet could scale population level and there I think we have my request for everyone's consideration

would be to think about can these mutable evidences be recorded extremely cheap at the end of the day you could pabyt storage system starting

logs which could be publicly auditable or could be um auditable by regulatory agency or not. But these logs have to be in order to engender trust it is very important that these logs are sort of

immutable. &gt;&gt; Why? Because that gives a sense to the user that even though those logs are residing with service providers, these service providers as system

privilege privilege system admutable evidence is the first chief building block building block towards moving uh into this direction of trust. And I think the second low hanging fruit I

would say would be um collecting some sort of a incidence repository because that essentially creates um um enough framework for the entire community to rise up right because everybody is

talking about the incidences that they have come across and it need not be you know out there in the public domain with all the details. It could very well be anonymous. You have a technology that at

scale could an anonymize data and can increase in that sense it essentially becomes a digital topic. Good. But unless these things cheap building blocks are sort of thought of

&gt;&gt; excellent. Now I will uh come back to you uh you know to elucidate uh further uh like uh the integration between DPI and uh AI but from what I understand is that the u enabling ecosystem for

responsible AI adoption is becoming more favorable and it's evolving uh rapidly. Uh Gita uh same uh key question to you as well like what do you think is the single most important decisive incentive

for enterprises uh to do AI responsibly? So when we think there are number of research say for healthare you see lot of advancements

being &gt;&gt; so when we see it from the enterprise adoption perspective uh from each of the sectors right so say for healthare we have lot of uh AI models that are

getting [cough] each and every day but how much of it is being scaled up and adopted so how do we build trust in from the low level right so from the end user level

we need some certain kind of sociote technical assessments to be done so that it can be adopted on ground and be scaled up so what kind of metrics should they want to give

and how do they understand that these are all the risks that can come up when they adopt it in real time right so uh when when I actually start developing a product uh we will actually think about

certain kinds of risk that we can anticipate and then come up with a risk architect &gt;&gt; from the organization perspective but I don't think that will be the exhaustive

list of risk that can happen in real time so uh as said we need to have a cross domain crossplatform incident reporting mechanism which will enable people to

report incidents and might be not a centralized database but in terms of a federated system which can be accessed by multiple people multiple sectors by multiple regulators to understand what

is the impact that all these AI systems has made from uh the human perspective right so usually organizations will always think about uh revenues think about the market stability

and how do they actually come up with some business right but how do we make sure that there is some ethical values integrated into all these adoptions and uh the most prominent incentive that I

feel is that government should come up with some voluntary commitment and self regulation things which will enable organizations to disclose certain things so that uh say if I'm an organization

adopting an AI system in healthcare perspective I will disclose certain risk to public saying that these are all the potential risk which people will face so that people are still aware of certain

risk and stay back when certain things happen in their right so those kind of assessments and awareness should be enabled uh through different stakeholders who are involved in systems

&gt;&gt; sure uh web &gt;&gt; excellent uh discussion I'm going to answer that question Raj uh my name is I'm an IP lawyer who's transitioned governance. So I bring in a

non-technical and sort of legal and governance sense to this conversation. Uh I'm actually going to answer that question by stepping back stepping with and stepping one step ahead of what my

co-panelists have said so far. Step back. &gt;&gt; Your question assumes what's my incentive for responsibility implementation.

This question assumes that we have identified the business use case for which we want to use A. So that that's the foundation pillar of this question, right? Responsible AI can't

happen if I don't know what I want to use it for. The second assumption that flows when we're talking about responsible AI is that we're also clear about which type of AI are we talking

about? Are we talking about fine-tuning generative AI models and building AI applications? Are we in a 2020 2018 2019 era where AI is just doing pattern recognition like healthcare and

financial space or am I building a foundation model? both these two aspects. So there's a context of AI type and there's when we talk about I know which model and which AI use case I've

chosen then we've gone into se separate sectors separate use cases and there's a lot of context now once we've established that context I think I would like to give a different spin so a lot

of my co-panelists have spoken about the incentives right so incentive I can just site a study right the Harvard business review published a study in March 2025 about a financial based lending app

which included responsible AI by design. There were five features. Three features they included in this financial based uh pension based app which was understandability, auditability and

privacy. And they surveyed about I think 3,000 respondents and the minute privacy was incorporated in the AI product adoption went up 60%. So there's the

incentive but I do want to because in our previous discussion before we entered this room was also what are the bottlenecks I kind of want to focus on the bottlenecks for one second here.

&gt;&gt; We will come to that in the next uh line of questions. &gt;&gt; Okay. So then then I think that would be my sort of thing that the it's it helps you approach AI holistically.

&gt;&gt; Absolutely. Uh brilliant. I think when it comes to safe and trusted AI, the business case for safe and trusted AI is increasingly becoming apparent uh to industry stakeholders across the board.

I think that's a major shift uh that we are all witnessing in the ecosystem. Uh moving swiftly on to the next segment of the panel. I'll be uh throwing some specific questions to each of you. As we

know right, I think u sub spoke about it and uh Gita you as well the regulatory landscapes right are actually evolving at a breakneck pace. compliance is increasingly complex, costly and the

line between mandatory and voluntary compliance is blurring right against this backdrop. Uh Surra uh can I bring you in? Uh let's start with crossber reality right so companies today are

building once and deploying everywhere globally. How should organizations think about navigating global regulatory fragmentation in AI safety without slowing innovation or hurting uh

commercial viability? &gt;&gt; Yeah. So in terms of an organization's use cases that we were talking about, right? So of course uh businesses can do better when they use AI and we are able

to reap the benefits or reap the benefits of use of AI but at the same time when an organization is uh building it somewhere and deploying it elsewhere across the globe

&gt;&gt; there are multiple uh regulatory requirements that came in come in for example if you're talking about the EU other thing is when you're talking about A you cannot leave data because data

becomes a fundamental part of the so when you're talking about it Again personal data comes into picture the data protection laws of each geography comes into picture. So you need to be

aware of that and if you look at EU the EU AI act is there in India if you look at it the EPDPA is there so on. So given that kind of a different things the organization first need to look at the

various regulatory requirements that are there in the geographies in which they are operating and deploying these particular AI systems. So you need to have an idea about the regulatory

requirements and the best way to do that is to map the regulatory requirements of each country and try to see what are the differences of requirement between country A to country B or geography A to

geography B and then try to set a common baseline which we can implement across the globe. So the minimum common baseline should be there across the globe and then for each geography you

can look at how that can be managed and that is one way where you kind of mitigate the risk because while you're trying to implement AI and try to go at a breakneck speed you might also end up

not fulfilling some of the regulatory obligations. So to ensure that we we are doing things right. So of course we need to take care of uh things. So when you're looking at it from a model

training perspective or a model inference perspective. So definitely technical controls play an important role and of course I'm not going to squeeze the limelight of Abilash's

presentation that's going to come for next but definitely when you look at it from a model training perspective you use machine learning and other things and try to make sure that you train the

model appropriately and then use it and when you look at it from an input perspective ensure that the model is doing what it is supposed to do whether it's going to be giving out decisions or

whether it's going to be uh adhering to certain processing requirements or whatever. So ultimately we need to ensure that the model is behaving in the right way. So

which means you can do technical things like uh taming and those kind of air retaining and uh you can do uh other assessments AI risk assessments and so on to make sure that you have built the

model the way in which it's supposed to be used and it doesn't create a risk that you're not uh foreseeing. So I think uh I would put it that way. &gt;&gt; Right.

uh interesting insight and uh like extremely important but uh I think it is here where I can perhaps ask uh to chime in how do we optimize uh for this right like let's unpack the legal layer

because you know that liability regimes are evolving rapidly every fortnite there is a new harm that gets recognized in law rightophy what specific legal uncertainties uh do

you see uh are holding back enterprises when they think about responsible AI implementation? &gt;&gt; That's an that's an excellent question. &gt;&gt; The short [clears throat] answer to that

is that responsible AI is actually law plus and I'm going to expand that. When we look at responsible AI, let me tell you what it's not responsible AI. Well, okay, hold on. So, making AI responsible

or holding harm from AI systems and holding people liable from harm from AI systems is a part of responsible AI. &gt;&gt; [clears throat] &gt;&gt; What we have to think of responsible AI

is actually thinking in terms of features that you want in your AI systems or services and that which feature is important for an enterprise and that is tied to which sector you

operate in which customer you interact with. Is privacy an important feature? Is explanability an important feature? Is transparency an important feature? Is safety? And then in safety, is it is it

foundation model, frontier model safety? Or is it accountability? Then when you look at accountability, we come back to laws. &gt;&gt; When we look at the responsible AI

landscape, it actually solution lies in this framing. Accountability, the way we traditionally understand law that there's a there's a rules-based approach. Someone did a

harm, someone held li someone's held liable. Yes, that is part of responsibility. Yeah. You have adverse incidents and you take you use laws to address that. You have intellectual

property laws, you have data protection laws, consumer protection laws, information technology laws in India, all applying to that. But there's another piece to responsibly in the

legal compliance department which is one you're not thinking about features that we spoke about and then when you think about features tying back to what Ain G spoke about the life cycle thinking

right you have to talk and think about risk mitigation across the AI life cycle and when you think of risk mitigation across the AI life cycle what does life cycle mean it means that if I'm building

an AI application fine-tuned and generative AI model what stage Stage one when I conceived it. Stage two when data in use. Stage three when I'm actually putting the model in sort of building

the model. Stage four when it's out there and I'm getting real input of how it's operating. Right? So that's how we have to think. We have to think of life cycle. We have to think of the we have

to think of law as a part and then we have to think of the plus. And a very short answer what is that plus right? When we talk about plus if you look at the Indian governance guidelines annex

five or an extra four actually site relevant voluntary frameworks for India that help in &gt;&gt; implementing responsibility and what are those voluntary frameworks they get

divided into three parts three four parts. One is TC standards which TC like fairness standards second ISO standard third audits fourth voluntary frameworks. So when we look at

standards, there are nontechnical standards about sort of integrating AI and an AI management system. And then we look at frameworks, we have risk mitigation across the life cycle. So the

short point being organizations have to think of the feature, think of the cycle life cycle, think of who they are as an actor and embed risk across the design of their AI product and that's how you

achieve the compliance. Uh I think uh voluntary standards and frameworks have an excruciatingly important role to play uh when it comes to responsible AI uh

uh if I can uh bring you here uh I think SI has been advocating for a global AI safety commons framework right because the cost of compliance especially for startups and small and mediumsiz

enterprise that are typically resource branched uh most of them can't bear it right uh why do you think it is important what do you think should constitute the foundational scope of a

safety commons architecture u I think if you can limit your response within 2 minutes or even less than that'll be awesome because we are running for time l I'll just start with

a very simple example so for example um the MIT AI incident reporting tracker right so they have a scale for this classification or severity classification which says your financial

it says if your financial if you're facing a financial loss of more than $1 million it is actually classified as high severity incidents. So um all these kind

of classifications are centric towards the western region of the world. Right? So how do we make it contextualized to the Indian environment? All these socio cultural and socioeconomic background

should be integrated into your assessment and that is what we are trying to generating or creating uh safety comments from the perspective of the region specific things specific

things which will enable the assessment of AI models which is getting deployed at scale. So for instance we are trying to assess multilingual multimodel uh text be it audio video kind of things to

towards the assessment of safety. So what kind of uh vulnerable things or violent things are present in those content and how are we going to identify. So uh for example we have

certain things that are present in the text but not in the audio but still when you combine it together you will have certain things which will lead to a harmful impact.

those kind of assessments we are trying to do and come up with an evaluation matrix which will help us in developing some comments which can be used across the group and so certain things are

still contextualized to read it which will be helpful for people who are adopting AI to evaluate it based on &gt;&gt; got it I think we'll be remiss if we are talking about province framework and not

mention DPI um and I think I'll give the closing to uh sub here uh So both given our experience in DPI uh how can we shape uh the next two to three years of technical priorities especially when it

comes to uh making AI safety measurable auditable by design u and is there any candidate architecture uh that you can recommend uh for potential inclusion in uh the commons framework that Gita just

spoke about. Um &gt;&gt; yeah I think I think the solution to that question I mean I I'm not all knowing but from what I can understand is the solution lies in uh what Abin

spoke about what's written in the PSAS which is technical governance of AI when you are when when the consumption of AI is going to happen at population scale um how do you ensure um that is behaving

as a trust in a trustworthy manner. So u that's in where you'll have to start thinking about can I automate or semi-automate the process of um analyzing whether this

system is behaving behaving according to its intended specific behavior whatever that specification may be and that's where you have to start thinking about legality

liability auditability provability explanability not just as rules written down on paper but enforcable as um you know machine enforceable as runtime checks or static

checks right um and that could essentially be orchestrated right at you know before the system is deployed so at the next level or export export it could be a combination of two but the real

idea is that you have policy implemented as you and that's the way to go about doing it. um in the in the I mean as suggestions what I would say uh which could be segue into what um Gita spoke

about um perhaps I'll go back to the things that I said in my initial remarks perhaps uh looking at low cost immutable evidences some sort of a uh an architecture which has hooks into all

stages of the AI pipeline uh monitoring as the system evolves. Uh keeping the evidences for audit and then having you know um explainable or liability allocation analysis components

bootstrapped over these evidence something &gt;&gt; I think it's as elaborate as it can get. Thank you so much uh for that brilliant insight. U I think we can close uh the

panel now. Uh thank you all uh for joining and thank you to the audience. If we have raised more questions that we have answered then I think the ecosystem is alive and well.

&gt;&gt; Yeah. So thank you Raj. Following the panel discussion we have a responsibly text presentation by Abilash. So please stay back for that and then over to you. &gt;&gt; Looking forward to the most interesting

part of the session. So request people to stay back. &gt;&gt; Uh it's going to be whatever we saw now technological framework. How do you implement it actually on ground is what

you're going to see. It's one of its kind. First of its kind framework. If you can have the slides on the screen so

we company. Sure. &gt;&gt; Yeah. We are a company called Preas APL. We are world's first full stack responsible platform covering all the

stages mentioned by PSA's paper across uh data collection, data use protection, AI um risk assessment in uh inline protection. &gt;&gt; Yeah, inline protection. Can you hear me

at the last Okay. Now is it better? &gt;&gt; Okay. Um, so how do you build this? How do you implement it? Think that you are a bank, right? We have been working with

global organizations like I you know we are accelerated in Paris by India, Saudi Arabia government, Accenture and across multiple entities across the so why, what and how? First why? Why is this

very important for enterprises? Think that you are a bank, right? Um you have to deploy AI responsibly. Why? So if you put privacy, think of Apple, right? The first thing with the advertise is Apple

with the lock, right? Um so if you put privacy at the center and responsibility at the center, people will believe in you and give more data. When you give more data, you can build more

intelligence and when you build more intelligence, you can create more value. And if you create create more value, it's a positive vicious cycle. This is very foundational for every business

unit. And we are not speaking philanthropy, right? So it's actual business but how can responsible accelerate your business is what we are speaking now in this what these are

things that you have today right um so the ones which are in white for example you have the comput infra you have LLMs and you're building agents right what is missing is understanding the risk

mitigating the risk for data in use protection AI risk assessment safety and security when you build this your stack is complete so how we do This is the whole life

cycle that is mentioned by the PSA. This is globally first of its kind. the data is putting out um responsible AI framework which is you know much broader better than EU's AI impact um U AI right

so at the data collection there are different phases in the life cycle at the data collection level let us say you're a bank you are building a model for loan approvals right can you take

people's loan approval documents and directly train the model you have their health information sensible information their financial while your kid is studying all the information can they

use it and train the model right at the data collection level you have to understand the risk in the data Then the data in your cell you got to mitigate. For example, you got to do

anonymization, synonymization, um differential privacy, different kinds of techniques which are this is these are techniques that the big techs are using but this has to be democratized

for all enterprises which is very foundational right once that is done once you build a model such let us say you have built on top of Gemini or llama you built a model how do you validate

that after you have trained the model the guardrails are not broken right it's very foundational so you do a assessment of the model and quantify the risk in the parameters that are important for

you, right? That's the model training part. And once you deploy it, there can be new kinds of attacks which are coming up, right? Which can breach your guardrails and extract sensitive

information out of your model or users. For example, let's say your bank and your bank employees are putting loan approval requirements directly to an LLM which is external to your ecosystem,

right? Which will breach your fundamental requirements. So during inference, how do you protect at the user level at the model level becomes foundational? Then agenting brings a

completely different dimension of obligations because agents can hallucinate and and hallucination can result in any of these risks of privacy, safety, security. It may think of you

know ordering pizza then end up ordering a bomb. Right? It may hallucinate and do different things. So when you are allowing it to do things in the real world it is exponential risk than an

alarm which is just a chatbot. Right? So this is the why part of it. On the what part of it, this is what I think um you know professor also mentioned earlier. So I'm not going to discuss more but the

these are the stages of the life cycle. So why humanities tomorrow you know um the human rights have to be protected against the models. You want to be a algo slave or you want to be human

holding your consciousness is the question right. So hence um the human civil rights are very foundational on how do you convert these civil rights into technical requirements which is

privacy, safety, security, fairness and what is the quality of these requirements? How do you en make it explainable, transparent, provable and enabling is going to be foundational and

this will be mapped to different stages of your life cycle. Right? This is how we are doing it. And there are some very foundational principles. For example, what is the difference between security

and privacy? Um security is data at rest and transit while privacy is data in use. And most of the security architects are you know experts here. You would know the CA triad confidentiality,

integrity, availability. But from a privacy perspective, the similar triad is for dissociability, predictability and manageability. How do you dissociate data before using it for let us say

model training, right? We discussed about a loan application use case. How do you dissociate the user from the data and then use it for model training, right? That's pretty critical

requirement and AI is a classical example of data in use. So how do you ensure protection of data in use is the fundamental question that you have to ask. Um now that's the why part. How

part happy to introduce you privacy and par platform we are launching couple of new products on this the whole stack this is first of its kind globally having a single stack solutions across

the life cycle right at the data collection level how do you quantify the risk right from consent right without consent if the data is going to an external ecosystem or without protection

across the legal obligation it becomes a problem understanding the risk and quantifying the risk and doing AI impact assessment at data level data protection impact assessment which is a legal

obligation uh in DPA Right. Followed by technical safeguards includes anonymization like you know mathematical grade anonymization like K anonymity t closeness differential privacy synthetic

data differential privacy. These are foundational for you to get exemption to use your data in model training process followed by a assessment where you have a impact assessment done and you do

retaining on the data for example an agent doing an interview of a model before you deploying a model. So the agent will ask the model questions on various aspects of privacy, safety,

security and fairness and quantify the risk. Right? Similarly, at the inference level, how do you put a firewall around the model both at the user level and at the model level and protect the model

from both these dimensions of sensitive information reaching the AI ecosystem. Similarly, preventing attack reaching the ecosystem and ensuring that the model does not pass out sensitive

information to downstream parties. Right? Then subsequently the most recent layer which we are also launching today is the agentic trusted uh ecosystem where we are you know working with

massive institute of technology in building in in a nanga ecosystem. We are working on building protocols and standards for trusted agents like the traditional world we are getting in the

internet era we are getting into internet of agents and in internet of agents like DNS we need to have protection and control for agents as well. Can you help like something like

DNS for agents? How do you quantify it? How do you list it? Like yellow pages, right? How do you list it? And how do you control agent from hallucinating? Especially now it gets into the

hallucination control. How do you build contextual based hallucination controls at the agent level is what we are doing. That quickly covers the whole spectrum. So we are happy to announce that we have

built first of its kind privacy threat modeling globally. We have built first of its kind unified privacy enhancing technology platform agentic a retaining small language models including

synonymous inference and agentic DPIA with contextual boundary. So this is what broadly we do and these are some of the you know visualizations because we have to move forward beyond from uh text

to action which is all about the impact summit right. So how do you capture constant how do you capture the data flow in the ecosystem right uh with different multiple downstream parties um

and how do you quantify risk like professor was both was speaking earlier how do you quantify risk at privacy safety security fairness methods and then how do you do it for structured and

unstructured data then how do you do interaction like agentic DPA interacting with your DPO or your business user requesting for data uh followed by mathematical proofs of let's say K

anonymity t closeness differential privacy when you are feeding data because this is going to be critical for your transparency um followed by synthetic data with differential privacy

where the utility of the data becomes a proof I'm created a synthetic data but does it have the same utility as my original data is a fundamental question you need to have mathematical proofs of

it to even take it to your business data analytics team followed by red teaming how do you quantify this compare it against other models against privacy safety and other attributes that you

want in your business context um then followed by inline inference like how do you put a firewall between let us say claude and Gemma between your end user so that it can identify the risk at the

prompt level and the rag level how do you protect security uh at prompt and rag and interact with the external downstream model right um and across the life cycle how do you do it and

especially on the agentic side right um so agents have couple of two folks one is at the LLM side and one is at the API side so on both sides How do you control the flow? What information can flow and

what information cannot flow? And how do you build contextual boundaries around it? Right? This is going to be the definition of responsibility technically in the future. And this is not a cost

center as we were discussing you know and this is a fundamental problem right this is a profit center because as you unlock data you can't even touch data today for example healthare data or

financial data you can't even touch it and without it the model is useless. It gives you insight which are process but if you unlock the data then you can take the data for model training then your

model can become gold right and this is what is going to change the world in terms of responsible enterprise AI adoption this is going to be foundational other than no very

different from the traditional you know my child drawing cartoons with open AI right that is very different for enterprises you need to have models on prem and that has to be speaking your

language with your data and for that responsibility is critical and this is going to changed the way the world is going to use AI right as an analogy like when the internet came we all were using

blogs but after that when it was digital banking it was a very different ball game than putting a blog right same is going to be the difference between normal responsible AI and enterprise

responsible thank you [applause] &gt;&gt; thank you thank Thank you for that presentation on platform. So I request to come over to hand over

I request the speakers to be here for a minute. So thank you everyone for the audience for being here and listening to us. Thank you so much. What do you want?

Million lights. Fine.
