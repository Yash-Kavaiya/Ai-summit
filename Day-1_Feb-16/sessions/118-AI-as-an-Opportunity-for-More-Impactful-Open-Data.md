# AI as an Opportunity for More Impactful Open Data

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 16:30 ‚Äì 17:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 15 |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/A0nac3miRrQ?feature=share) |

## üé§ Speakers

- Christopher Maloney, Gender Equity and Governance Program, William and Flora Hewlett Foundation
- Fran√ßois Fonteneau, PARIS 21
- Mercedes Fogarassy, PARIS 21
- Randeep Toor, Google
- Rohit Bhardwaj, National Statistical Office, India

## ü§ù Knowledge Partners

- PARIS21 (OECD)

## üìù Summary

This session explores how AI can strengthen open public data systems to deliver greater social and economic value. The session explores the policy, institutional, and technical conditions needed for AI-ready data ecosystems, highlighting the role of National Statistical Offices, transparency, and collaboration in building trust, while simultaneously improving data quality.

## üîë Key Takeaways

1. This session explores how AI can strengthen open public data systems to deliver greater social and economic value.
2. The session explores the policy, institutional, and technical conditions needed for AI-ready data ecosystems, highlighting the role of National Statistical Offices, transparency, and collaboration in building trust, while simultaneously improving data quality.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/A0nac3miRrQ/maxresdefault.jpg)](https://youtube.com/live/A0nac3miRrQ?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

Good afternoon everyone. Great. Thanks for the thumbs up. It's wonderful to see everybody here today. I need both. This one's on. Okay. Wonderful. Thank

you very much. My name is Mercedes Fugarasi. I am uh here today with the pleasure of moderating this session which we have entitled AI as an opportunity for more impactful open

data. Welcome everybody here. It's great to see so many faces and so many people uh here from the very beginning. Uh this session and and this conversation is going to be part of the theme on AI for

economic development and social good. As we all know AI is very much often described as a transformative technology but AI doesn't emerge in a vacuum. It really depends on fundamentally on data

and with that in particular on high quality trusted well-governed public data. For decades, national statistical offices have been really at the at the core of all of this and they have been

invested in producing open data as a public good. Open data to inform policy, to strengthen accountability, and to support inclusive development. Today, AI changes the whole scale of that

conversation. It also changes the speed that everything is happening and of course it changes things in ways that data or in the ways that data can actually be used, reused and add value.

But all of this raises which is a big conversation that and finally how do we ensure that this transition that will determine whether AI truly

enhances the public value of open data or not. With that, I'll introduce our speakers and then we'll have some opening remarks uh on a video here. So, in terms of our speakers, we'll start uh

just directly to my left, we have Mr. who is a senior policy adviser at Paris 21. Part of Paris 21 stands for the partnership in statistics for development in the 21st century. That

happens to be where I work as well. We're very delighted to have my colleague here. Volswis is an adviser on AI and data to the chief statistician of the OECD. He has over 20 years

experience advancing innovation and capacity building in statistical systems particularly in the global south. Next to Mr. Fontino, we have Roit Bardage. Roit is the deputy director

general at India's national statistical office where he leads innovation and emerging technology initiatives in official statistics including AI and data dissemination reforms.

Next to Shardwage we have Christopher Maloney. Chris is a program officer at the Huelet Foundation where he supports the use of high quality data and evidence in policym particularly in

Africa. He previously held senior roles at USAD and the Millennium Challenge Corporation. And then finally at the of the line there we have Mr. Rendip. He is a senior

technical leader at Google and leads infrastructure strategy for the data common. The data commons is an open source initiative making global public data more accessible and AI ready. So

those are our four panelists here today. It's a pleasure to have everyone and as I mentioned we will have a quick uh opening remark from uh Mr. Stefan Barhus. He is the well I was going to

say on the screen there but I'm on the screen. I am not Stefan. Let me just pull up his video there. He is the other speaker that you may have seen. We can turn the screen to my screen. There we

go. Uh, so we have Mr. Stephan Beerhost here who will introduce himself and kick off the debate. &gt;&gt; Hi, I'm Stefan Host and the co-founder of the Grav Lab and of the data tank and

I'm also a research professor at the Tanden School of Engineering at New York University and I'm delighted to be part of this panel. Unfortunately, I could not attend in person but eager to share

with you what we've been working on which is ultimately the third wave of open data. Over the last few years, we've been working on trying to change the way we make decisions, improve

decisions in terms of legitimacy, in terms of effectiveness by seeking how we can leverage data, make those decisions more data driven. And so that's one of the reasons why about 15 years ago, we

started to work on this whole notion of open The first wave of data was freedom of information. The second wave of data was where governments were providing access

to their data on their platforms. And the last wave, the third wave I would say is ultimately where private sector also started to provide access to some of the data for public interest.

Suffice to say that all the waves were building upon each other. It's not like one replace the other wave and they all brought new concepts in the table. Now despite the fact that we have seen those

three waves over the last 10 years or so, what we also have seen and this is especially the case the last few years. We actually have seen a backlash against data what I have called data window.

Despite the fact that we are in AI summer, we may actually also in which I mean a decrease and especially decrease in the fact that data is made accessible and here is of

course where Gen AI comes into play because what we are seeing that AI can actually be a force for democratizing data as well but open data can also be a force for improving AI is being trained

or AI is performing as well and so the fourth wave of open data is really the wave where AI and data is intersecting with each other both in terms of open data for AI where we have seen different

scenarios play out where open data can play a role whether it's to improve AI training or whether it's ultimately to augment uh some of the data sets that are needed in order to provide context

specific context and context specific uh answers to particular kind of prompts. We have tried to catalog a lot of those scenarios that we have identified where open data can be a force for improvement

of AI and of course some of those as already mentioned some of those examples really enable for augmentation using rank models for instance that would allow for public interest use cases such

as for instance using GI for disaster response as well and here indeed we have seen many use cases over the last two years or so where AI has become a force for democratizing

access to data or democratizing the use of open data by the general public. One use case is of course where we have seen that AI can be used to provide conversational interface with data don't

require a data science degree anymore to query data. You can actually do it through a chatbot or through other AI interface. Other use case where AI can be used is to of course provide

synthetic data. The synthetic data can also be used in order to fill in gaps. Typically data might be hard to get where it might be sensitive or it can fill in gaps where there is a lack of

representation to a large extent. We also have seen that AI can start improve or at least detect challenges with regard to data quality. this whole movement around data contracts that can

be automated using AI in order to make sure the data is the data that is ingested complies with a certain kind of data quality criteria question then is of course very briefly how do we write

the first one is to really make data AI ready what are the requirements for data to be able to be used for AI and that's also why we have working on kind of updating the player framework. We now

have also suggested we should add ready for AI. The second element and that's somewhat related to the first one is that we also need to improve problems of data. We really need to understand where

does the data originate and also what was the context in which the data was collected because that will be very important for subsequently AI training and for also also making sure that the

data does not elucidate. We also have to start prioritizing public use cases for generative AI use and for the use of AI for open data and this of course requires us to invest not only in AI

literacy but also in questions literacy spoken at length about the need to become more sophisticated in actually defining what are the priority use cases for which AI data can help. We also have

to start thinking about innovation in And so as already mentioned a lot of concern in today's data ecosystem is concerned related to extraction and that's why we see the emergence of data

comments which are kind of a new institutional way to provide access to data quite often a way that seeks to align access to data with preferences and expectations of communities as well.

So let me conclude by saying is that we are in this weird moment when it comes to open data. On the one hand we see a massive backlash. So we could say this is actually the worst of times when it

comes to open data. Data has become more closed than ever before. But on the other hand we also can be optimistic kind of best of times emerging where AI can really become a force that enables

us to democratize data in ways that we've never had before. So with that, I hope this wave of data open data sets the stage for the panel and might also be used as a way to really understand

what is needed in order to ride the wave and especially to prevent from happening. Thanks so much for having me and best of luck. All right. Well, thank you, Stephan.

Hopefully, you're you're going to watch this later. Not exactly at this moment because I think it's about 4 a.m. for him uh over there in North America. But I think that was a really great way to

kick us off here. Uh and before we really make that technological shift and talk about AI from that perspective, I'd like to start with the institutions. So, turning over to you, tell us about the

role of national statistical offices when it comes to data and AI. They've custodian and the trusted data steward. But what does the NSO really bring to this debate?

&gt;&gt; Thank you. Good afternoon everyone. Good evening. NSO national statistical offices actually bring a lot to this debate and actually much more than many of us would

think in this fantastic summit. If you check the program national offices you do know keyword search happens only for that session. So congratulations, you made it the most important session.

Absolutely. But I'll try to explain to you why NSOs are so central and why again we need them to to make the open data work. And if AI is transforming the way data used, this would be NSO will

ensure that this transformation strengthen public trust in data. Long before open data became popular, long before GI NSO were putting quality data out and their core mission is simple but

very powerful producing data good data reliable data about people's lives the economy the environment today and we heard it from from we we live in a world of data overload there is more data

everywhere than any time before AI can generate text can generate summaries can generate data sets fake data but I would say this data flood this is the NSO to actually play a

unique role they produce data that are validated that are quality control that are transparent and comparable across time and countries and that's the first thing which any bring to the AI

debate they bring the quality fuel which AI systems need to function and that's a very significant contribution. Second, NSO brings structure and standards. The AI we know it works best when data

are machine readable, well documented and structured. NSOS produce standards on how to do this. They produce classifications. They maintain consistent time series.

They provide metadata which explains the context for numbers. In short, they structure the reality of the world which AI can then digest. So that's the second contribution. What

data plus standards and and structure third the NSO and that's very important in in the world we have. They operate under very strong legal and ethical framework.

They protect confidentiality. They follow principles of neutrality. They serve the public interest, not commercial. So in the AI era, all these matters more

than ever. Again, statistics are optimized for social value and not for So that's what NSO and that's quite a lot when you think about it. Now, how is AI changing the game for NSOS

in quite optimistic here Stefan you know balancing the first and the minus I'd like to focus on on the positive we see two areas of

opportunities for for AI for NS the first one really is on data production if you look at data development data a lot of data gaps and AI gives us a chance to produce more

useful more timely data there's which we are doing together with Google and NSO in Mongolia to produce better quicker population and housing data using AI and satellite measuring.

The world is driving innovation based on AI. The second area of importance is really access and I think India is a good example of what you know serious people

can do good with this AI and we all agree and was mentioning this data is very context to understand and you know a clever chatbot and put good data at the fingertips of of users the media

the population or policy makers that's a blessing of course you need serious people to make it work and I think it work in I would say that we see AI as an accelerator

We are hopeful and we have reasonable hope that AI can be a force for open data in the end AI I think not AI will reward those NSOs which are modern

and AI will make NS want AI to open data and data data then it's important to invest Thank you very much. That was really really helpful. I like the way that you

framed it with the three things especially that the NSO brings to this conversation as well as the importance for for investment. Um so now I'd like to hear from the NSO here. uh Roz if you

could tell us in this this AIdriven ecosystem especially when it comes to an evolution of the role of NSOS as mentioned should the NSO act primarily as a data producer or as a steward and a

regulator of data &gt;&gt; thank you thank you I thought half the work has done thank you very much once so let me just discuss it in two different manners so

one is In present role NSOS cannot have a binary or exclusive role that I'll be only data producing and somebody else will take up the uh owners of being a data. It has to be it has to be a

combination or intersection. Now source role has to evolve and it needs to evolve from being only a passive data producer to something which is orchestrating the entire data ecosystem.

That is how I look at NF. uh to begin with uh let me talk about the production capacity which comes from my colleague talked in detail but I just want to emphasize on few points one is that what

NSOS produced that cannot be produced in any other ecosystem NSO has continue to produce data which it produces the GDPs the CPIs the inflation the socioeconomic indicators the surveys it's just

impossible for any other any other organization to produce so much of data so data needs to be proded by NSOS and we need continue to produce those data most importantly that we have been a NSO

India let's talk about NSO India NSO India has a history of almost 75 years and now we are in our 76 years over a period of 76 years different types of different types of classification codes

templates have been created best practices have been absorbed over a period of time so it it its legacy has the capacity to produce quality data

which I feel no other organization has and most importantly let me tell you one more thing here that NSO is supposed to produce data for analysis if you really think about any other source of data the

primary use is not analysis administratively it's for administration you special it's for something else not for analysis so NSO is the only organization which

produces data for policy making for analis As a result, the NSO's data is comparatively I'll say nothing is perfect comparatively has better quality, better standards, better better

production philosophies in in comparison to any other data. But having said that, I don't mean that other datas are less important and our data is equally important but we need to work with those

data to bring it to the standards on which the NSO data is produced. Okay. Now, uh second point and M you are free to tell me when my time is over. Okay. So uh the second point is that but

what is the evolving role of NSO? So we have to understand that there NSO has a evolving role and that evolving role I saw it in four four pillars. One is data governance which we all talk about

talking and how to govern any data. Then then second is which is to me is very important is validating a data set. So administative data has been has been created but whether that data is up to

the mark for analysis for use by AI that is something I would say is very important and NSO plays that role to validate that data set then orchestrating the entire data ecosystem

and as a I mean what do I mean by that the example is that on 24th of this month we are going to call every state government to talk about how to best use their administrative

so orchestrating them orchesting the orchestrating ecosystem to work to better the data for AI and lastly uh which is which is also very important is ethical requirements trust and all that.

So this is what and we at TSO India have been working on it. We have our own national metadata standards. We have own our quality frameworks. We have our own class and classifications. We have a

digital bouquet of data dissemination because if data cannot be accessed there's there's no point you know talking about data. So the very the very logic of NSO existing is data

dissemination that is why we are we are working on it and all of you must be knowing most of you may be knowing that NSO India has come out with the MCP server as well. So you don't have to

come out with your own and it has your own environment. If you have a cloud subscription work there in the cloud and get it connected to our database and do whatever you like with that. So this is

a few things I wanted to tell you. So we have we cannot be any more binary. It has to be a mix of both. That's what &gt;&gt; Thank you Rob. I think that that's a very logical answer and thanks for

laying that out. &gt;&gt; So now we've spoken about the roles the mandates but let's get practical Randy from a technical perspective. the ministry wants to publish an open data

set tomorrow. What are three things that really need to be in place for this data to be AI ready and for responsible reuse at scale? &gt;&gt; Thank you for that question and I I know

in a couple of uh previous comments these topics may have come up. Uh I'm going to emphasize on those again here. Um the three key principles that we at Google and data com are putting in place

is uh using and enriching metadata. In other words, you also mentioned metadata is so important. Um and once the data is enriched with right amount of metadata that data will be easily understood by

agents in today's world. Um and then comes your standards. We can't just operate in different 20 30 different ways of organizing the data. We have to have certain set of standards like

schema.org, SDMX, NMDS, but they have to be interoperable. We have to build adapters so that these standards can talk to each otherwise we will still be in like my

standard is best standard or not. So that is a that is a very important aspect. And last but not least from the third thing I would say is the problems. When I say provenence, I'll I'll explain

in a very natural language is when you take any food product and you read there is nutrition label. It tells you a lot of information about where that food was produced, what is the content, what kind

of nutrition information, something like that data needs it too. Data needs provenence which tells where it originated from uh what kind of usage this data could be used. Is it public?

Is open? Is it restricted? And in addition there could be additional enrichment on provenence is as data travels there has to be some sort of a history of that travel and if there are

any enrichment or changes have been made. So that way we not only know where data is coming how it has traveled but how you can use it as well. So these are the three key points I would say that

would make your data AI ready in in a reasonable way. The agents can talk to this data. &gt;&gt; Thank you. That's uh yes claps all around. That was really great to hear

especially for the work that we're doing. I think that answers a lot of my questions uh in terms of my day job. So this is very helpful for guiding me as well uh in terms of so now we've

discussed the institutional roles the technical conditions uh but taking a bit of a step back over to you Chris what's actually new in AI in this moment and what's new to this debate today?

&gt;&gt; Sure. Um so as opposed to my esteemed panelists I am not a technical statistician or or AI guru. uh I come from the world of philanthropy and specifically I am most focused on

working with grantees primarily in Africa that are helping unlock the data revolution for better policym across the continent and connected with that also ensuring a strong robust data governance

around that and what's been interesting is in the four years of overseeing this portfolio we've gone from talking about data revolution and data governance to AI revolution and AI governance but the

issues remain largely the same and that shouldn't be surprising because at the end of the day Data is just the lifeblood that's making AI work, right? And so I sometimes wonder like 30 years

ago would we have thought about Excel? We put data into an Excel, we got an interesting insight. Now we put data into an LLM, we get exciting insights. So what's changed is we've gone from the

narrow to the general. We're going from something that was very expert and niche to something that in theory anybody could use. We've gone from something where we would use a little bit of data

to now we can use all of the data in theory. Right? So it's it's this speed and scale which I think is making it different. But coming back to sort of thinking about at least in the context

that I focus on in Africa, my main concern is that a lot hasn't changed because unless we're worrying about some of these more fundamental issues, I do worry that AI is in some sense a storm

in a teacup at least in parts of Africa and other parts of the global south and further enhancing this wedge between the north and the south. So we still need to work worry about connectivity. We need

to worry about electricity. We need to worry about infrastructure capacity. We need to worry about multist stakeholder uh approaches for participation and governance. We need to worry about

sovereignty. And we need to worry about accountability mechanisms. All of these big fundamentals. That's just one whole piece. And then the other side of course is having that data in the first place.

You need local data. We need local languages in that data. We need the trust and safety behind that data as well. And all of those fundamentals are still very very weak in many of the

countries that I'm working in. And so that's the question that I keep coming back to is are we moving the needle on these fundamentals for this to truly be meaningful.

&gt;&gt; I think that's a very very important question and highlights some of the the things that I think the rest of the conference this week are really going to talk about in terms of the connectivity

importance, the capacity importance uh and of course also the importance of local data. Uh but one of the other elements I think that always comes to mind especially for official statistics

is trust. Uh so Roit like to hear from you again especially as AI systems actually generate their own insights. How can governments ensure that official statistics remain the real trusted

reference point for decision-m in policy? &gt;&gt; I tried asking this question to AI and and what I got was you know really didn't make me happy. So it talked about

legal, it talked about audit trail, it talked about sorry it talked about provenence and it didn't make me any happier because I we need to talk about AI readiness of data first in terms of

how people understand it. We keep talking about in technical language as jarens. We remain in these bubbles where and and we all understand what bubble is. A few are inside it and rest of us

all of us are outside it. So uh that is why the problem I see. So what is I mean I am perfectly in agreement with my panelist here when they say what is AI readiness of data it is in two lines it

is data and metadata problem. So if I want to make my AI data my data trustworthy how should I do it? So I cannot stop any person going to Gemini or or a or a charge GPT or anyone

seeking an answer about my about data. Now I need being an NSO I need to ensure that that that question is directed to my database. My database is the preferred way to look

at to search for that answer. That is the trust. That's how I'm going to generate trust. And as part of it, we have done many of the activities. We have to make we have ensured that

metadata is available. We have now we have MCP server. So anyway, if you plug that tool in, it straight away goes to my database. So I'll say uh Mis that it is important that we see this trust

thing not from the legality uh spectacles but from the part point part of point of view of AI readiness and if data is AI ready believe me it's going to be used by any any AI system. So

interoperability is one and lastly uh public engagement and literacy is also very important. If we can't do these two things uh I think it would be difficult. people should know whether it is right

to ask a question to a AI or go to that NSO's web portal to seek that answer. So these are the two points I wanted to say &gt;&gt; definitely I think the the data literacy is absolutely an important part and back

to that that that question about AI readiness as well. I think that brings up what Randep was saying earlier. I know uh uh so Randy maybe just back over to you if there's a concrete example

that comes to mind where AI actually improved the quality or usability of public data. Um I mean I don't have to go too far. Data comments is that example uh where

we have implemented and brought in data from various different organizations, NSOs, aggregators, um government agencies and we've connected the dot between um what is the and I'll use an

example like when uh there's a diabetes what's the correlation of diabetes to food scarcity or surplus of that and and having that ability to ask that question in natural language has been a key

aspect that data comments has also enabled as Rohit Gi was saying right people should be able to talk to this data in in the language that data understands um so in terms of with the

data commerce implementation bringing the data into a a single standard format or interoperable format uh enriching with that metad data and then serving it through natural language is what data

comments is an example and and just also reverse playing that we implement these principles internally at Google as well for our enterprise data where our business operations can use natural

language to make sure our internal data is also following these same principles. So we're not really um walking we're not just talking the talk but we're walking the talk as well.

&gt;&gt; Thank you. That was the exact uh wording I was going to use. for walking the walk, not just talking the talk. So that's great. Thanks for the example. Um, Frana, at the global level, how are

things changing in statistical systems and and how is the global statistical community adapting to this conversation? &gt;&gt; Thank you, Mercedes. Um, again, I think the statistical community what we can

observe has understood that AI is going to stay. it's not going to go. And um um you can see the the community really tackling the issue seriously from understanding together what it means for

them to be AI ready defining as as Chris was saying what comes in the cloud of NSOs what is outside of the cloud of NSOs you know electricity connectivity you know at times you know data

governance in in in in the country as well and what they can influence uh to make it Turkey. So the community is very um serious about it. Uh we have started to update our systems. Uh there were a

few examples here from from India using MCPS for for dissemination. We have started to update our people as well. There's a lot of training going on at times where the financial situation of

that community is not super rosy. You have some lead NSOs in the world which are know which budget are cut by one/ird. So you know this comes at a time where the you know the statistical

industry really needs to to to modernize and and adjust. Uh one thing which was said by my panelist was the need to invest in citizens. Um you have a few countries where NSOs are actually taking

a lead role in this both you know the normal layman on on on the streets you know trying to to make them aware of the beauties of quality data. uh what what uh Randep was saying you know

understanding the difference between something which is trustworthy which is coming from documenting sources and something which is fake um when you check the actual ability of people

globally to do this uh it's not going on the right direction neither you check the numbers from the OECD in terms of mathematical ability of students or people the trend is downward so that's a

bit worrying yet you know you have a few to countries where where you know by specifically addressing statistical literacy uh you know you can make a difference. So I think the the community

is is in a good position uh is aware of what needs to happen. And there are lots of exchange of experience in the UN in the OECD in Paris 21. Uh you know a lot of um I would say aggressive piloting

and a lot of wise deployment because the NSO the statistician are wise people. They not go for the the hype and the the heat of the moment but they are serious about improving quality of of the data

they they release. I like that uh a aggressive piloting and wise deployment. I think that's maybe another uh key word we can keep using. Um so looking ahead, Chris, given

everything that's been discussed here today, and I I really appreciate your you saying that, you know, you're not a statistician, but I think you bring immense value to this statistical

conversation that we have here. Uh but would you say that we're entering a renaissance for open data, or are we more at a time where AI can actually undermine the value of public official

statistics? What do you think? similar to my colleague here when I got this question I was like god I don't know I'm gonna ask chat GPT [laughter] so so no jokes aside I it's a tricky one

of course because there is no right answer but I do think it's interesting to sort of think through what are the pros and what are the cons and then sort of map that to a couple examples that

I've seen in real life so the first is you know what makes us think that this could be a positive renaissance well if a country has really strong well ststructured high quality data then AI

is obviously going to increase the value of that because it'll help reduce hallucinations it'll help reduce bias you have this really powerful tool but you require that fundamental there first

similarly um if you have a robust ecosystem of researchers of startups of civic innovators similarly once again now you've created this wonderful tool where you could overlay AI on top of

open data and help foster that community even further and finally you've democratized to some degree accessibility so you know now all of a sudden you don't need a PhD to have to,

you know, sort through all of these data and statistics and that's great. But to all of those positives, there is of course a con and it goes back to what your fundamentals are. So if you have

synthetic data flooding the ecosystem, that can rapidly erode trust if that's not being properly managed. Um, similarly, I was thinking through platform substitution. So you know how

many of us back in grad school suffered through navigating through WDI or com trade. You knew your data backward and forward. I didn't have the luxury of asking some platform to do that for me.

And so do we risk um sort of creating this death spiral of of knowledge in a sense. And then finally, if we have these magical systems, does that create a sort of perverse incentives in

countries with limited budgets wherein we actually decrease funding for surveys, for censuses, other things like that because well, we can just have AI do it, right? And so there's no right

answers here, but these tensions are something that every country, every NSO is going to have to be grappling with. And so I was reflecting a little bit on my own sort of experience of this on the

continent. And you know, those of you who are familiar with Africa, you know, 10 years ago, there was literally a law in Tanzania that if you published a statistic that was not blessed by the

NSO, you could go to jail. There was an experience in Uganda about six or seven years ago where the World Bank classified Uganda as an LMIC, sorry, as an LIC when the president had gone on

record by saying they were an LMIC. And this created a massive sense of trust issues. So now that if you basically do anything Uganda with data, there has to be someone from the NSO on that team

that's working on that. So I think to myself with AI now turbocharging this, does that now create an incentive for even a further crackdown on things or does that sort of go in the other

direction? One thing that gives me a little bit of hope is seeing this explosion recently of citizen generated data. So people who are actually now trying to bring rigor and voice uh to

the data that they themselves are experiencing could collect every day to help augment what's going on. So I do see this little bit of a bridge there. Um but again no perfect answer but just

some of the tensions that I wanted to highlight. &gt;&gt; I like the way you framed it with the tensions there and also the mention of citizen generated data which um as Roit

you mentioned you know the data sources all of them are important. Of course official statistics are what we're talking about mainly here but uh the tensions that we're looking at need to

be addressed and and citizen generated data can often be an interesting add-on. So uh we do have about 10 minutes left. There is a big clock that I'm very aware of. So I want to keep to time. I do want

to see if maybe anybody here has a question. And while people think about putting up their hands, I will one provocative question to the whole panel. Um, but when it comes to uh the risk

that we're all talking about in these tensions that you mentioned, Chris, is there a risk that AI companies benefit more from open public data than the citizens who actually funded its

production? So, have a quick think on that. And then if anyone here does have a question, I saw a hand go up right there. So, I'm going to call on you. You are the lucky question asker. Please uh

if you could tell us you my friend if you could tell us who you are where you're coming from. &gt;&gt; My name is Premi. I'm a final year student at TA University. My question is

for Rohit sir. So so NSO deals with lots of data right? So the biggest issue that AI has in data collection is data biasness which completely changes the model's efficiency and how does AI

impact the decision. So what are the standards that NSO followers for data biasness and reliability of the data? &gt;&gt; Just before we answer that, I'll take maybe one or two other questions. I saw

a quick hand over here as well. So hi I am Shahed and I am representing uh uh data science in your nonprofit called data kind today and my question is uh to uh the panel is that uh given

that uh AI ready data needs to have both data and metadata. So what are the most priority components that the metadata should have like there is provenence or there is possibly the source but what

&gt;&gt; all right I'll take I don't know which one is working &gt;&gt; okay hello um so my question is Mr. Christopher um I'm a Ganch uh I'm an aspiring researcher and I often face the

issue uh that was mentioned by the previous questionnaire too that there's a lack of granularity within the metadata that is given to the same uh to the same repository and when it comes to

open data that becomes an even more important challenge considering that there's no fixed pipeline because of multimmodal nature of the same data and so my question was how much granular is

enough so that we could crossverify, we could implement our own check sums if you will for the data that we have for instance for sensors for for IoT devices.

&gt;&gt; Thank you. And we'll take one last question here. &gt;&gt; Thank you. I'm Dolly [clears throat] Musim. Sorry.

So this question is basically on open innovation. Now there is a lot of uh open innovators who have been working on different uh aspects of innovation and they require a lot of data. uh how is uh

what is the kind of uh status on using of this data to really do open innovation on uh uh for like say for example a sandbox on uh fintech or a sandbox on uh tourism or sandbox where

there is a data is distributed but if you really want to do any new innovation using AI you require a lot of data set now what is the policy on that uh that's the first part of the question. Second

part of the question is that how do we govern uh when when we are in a distributed environment? How do we govern that data? Especially if uh one central body is really holding that

data, then it becomes easy. But if there are multiple people who are holding that data, how do we really govern that data? Thank you. &gt;&gt; All right. So, that's been some really

interesting questions. And then I know I had a little provocative question there as well in terms of who benefits most. We've got six minutes left, so I'm going to turn it over to the panel. Answer any

of those questions. Uh, and if you want to offer a quick closing remark, it's up to you. Randy, I see you grabbed the mic. So, I'm going to go with you first. &gt;&gt; I'll I'll take a point on the your you

provide pro provocative question. Um, so in my opinion, um, data public data is available today in in its silos. uh large I would say corporations like

Google should put in an effort to like bringing that public data into open world. So through open source projects like data commons through this project we are making this data openly available

through open APIs providing technologies like MCP server and agents which will enable this open public data to be interoperable having connectivity between the data as

well so that normal people can talk to this data in the natural language and get that information they want. So while I do see that um this is only possible when multiple of these large

organizations will come in and start making this public data using all these standards and principles we talked about making it openly available through open source platforms

open APIs and MCP servers and stuff like that. Um I'll try and link the two questions actually on granularity and governance because I think they are actually

linked. Um you know I think this is where it comes down to I think two things. One, what does the community want and need and what are the mechanisms in place in a country to

advocate for that need and I think that varies from place to place quite dramatically. But I think one thing that I've been very focused on in particular countries is, you know, working with

institutions that are really helping define national and some cases even subnational data strategies that are articulating some of these questions. I sit in Menllo Park, California. I have

no idea what they are, but I certainly have some amazing grantees who do know what they are. And so I think that is a really big question is how do you really localize the questions you're asking so

that what you are thinking from a policy standpoint or a governance standpoint, it's fit for purpose. One last thing I know is I love your provocative question and I can't resist because that's my my

my favorite space is you know I'm of two minds here. If you have a robust thriving ecosystem of innovators and researchers and and and other actors who can leverage the data then yes they can

punch just as much as with any private company. Um but at least in the context of the African continent too much what I'm concerned about is for AI to get better and better it needs more data and

Africa is the last frontier arguably of more data. And so you're seeing all sorts of strange and my opinion somewhat unethical things going on right so if you look at Kenya in the past two months

in the you know the massive bilateral health agreement that's been signed with the US that health agreement is is tied up because required within that is the turning over of tons of Kenyan health

data right there's a reason why that was included in that agreement you think back to about two years ago you have um tools for humanities worldcoin which was essentially two steps removed from open

AI's crypto that was essentially scanning Kenyon farmers irises to build its proof of proof of personhood LLMs, right? And with very very sort of shady governance behind that. So when we're

seeing those practices in a in an environments that are very power asymmetric, we have to be quite concerned. Um but again there's no like yes or no answer. It's going to be very

context dependent. So &gt;&gt; okay. So u quick points on data access. So uh as you all be aware government of India has something called AI course. I request

all of you to visit that. Lot of data is available there. May not be a perfect data but lot of data is available. Then we have open data. Then we have our own eaniki portal, national data portal.

Then there's a micro data portal. So I'll request all of you to just search all of these are available and you don't need to have any AI tool for that. It's all available through APIs. You are

innovator, you are developer, access the data through API and use it. So that's part one. uh to your question I think in shorter term uh yes I think there is a real threat that those who for I mean

the data is created with the public money but perhaps the benefit might just flow more to somewhere else but it's very easy to handle that we should have it defined look open data is not free

data we at times we feel open data is free data open source is free source it's not that way open data should be open data it should not hamper open data movement at all but we should have a

very very contextual agreement of data contribution. We should have we should have a data dissemination policy. We should we should put a cost to our data and probably then going forward we'll

have easier way forward. That's what that's how I look at it. &gt;&gt; Thank you for the very good questions. I'll go with the ones on metadata and how you know how much enough metadata

one can put in the system. The good news is the statistical community is speaking with the tech on what is good metadata so that the tech solutions eventually can find the good

data which is produced by the raw hits of this world. Last week we were with NSOs in Africa to actually help them put the metadata in the form which can be digested by the AI machine and this will

go a long way. So the answer is not five tags. It's not 200 tags. It's as much metadata tags as the machine can help you to put along your metadata your data. So it's read by the by the machine

and and they are you know very working level solutions for this. I will skip your difficult question uh because of time obviously not because &gt;&gt; obviously because of time. Uh well thank

you. Thank you for all of that. Uh just on a quick ending note there, I think Roit, what you hit on about, you know, open data is not free. It's just free in terms of money. It's out there, but it

was built on something. And the costs that go into it might not look like the tangible coins and bills or credit cards, but it is built on something. And in the work that we do with countries,

that's one thing that that we do talk about quite a lot and that we emphasize is there are data out there, but how free are they really? And so I think it's an important thing for us all to

consider when we do think about AI accessing, using and making use of these data is really where is the value going and who is benefiting from that and who is funding that information. So I hope

we can leave on that note and a big round of applause for our speakers here. A big round of applause for you all for coming out. I see in big red letters time is gone. So I will release all of

you. Thank you so much for coming. Have a great rest of the conference. Really good. Very good. Amazing. [clears throat] &gt;&gt; Thank you very much.

See what I'm thinking.
