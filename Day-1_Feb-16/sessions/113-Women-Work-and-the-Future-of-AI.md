# Women, Work, and the Future of AI

**India AI Impact Summit 2026 ‚Äî Day 1 (2026-02-16)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 16:30 ‚Äì 17:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 9 |
| üìÖ **Date** | 2026-02-16 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/EZm37vvOjmc?feature=share) |

## üé§ Speakers

- Aranya Sahay, Humans in the Loop
- Arjun Venkatraman, Gates Foundation
- Dr. Kalika Bali, Microsoft Research India
- Dr. Shikoh Gitau, Qhala Limited
- Safiya Husain, Karya
- Urvashi Aneja, Digital Futures Lab

## ü§ù Knowledge Partners

- Karya

## üìù Summary

This session blends film, dialogue, and live demonstration to explore who shapes AI and on whose terms. Featuring a screening of 'Humans in the Loop', a moderated panel aligned with the India AI Impact Summit's chakra framework, and a demo of a Gender Bias Benchmarking tool, the session centers women's inclusion, data workers' visibility, local context, and dignified digital work as pillars of responsible AI.

## üîë Key Takeaways

1. This session blends film, dialogue, and live demonstration to explore who shapes AI and on whose terms.
2. Featuring a screening of 'Humans in the Loop', a moderated panel aligned with the India AI Impact Summit's chakra framework, and a demo of a Gender Bias Benchmarking tool, the session centers women's inclusion, data workers' visibility, local context, and dignified digital work as pillars of responsible AI.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/EZm37vvOjmc/maxresdefault.jpg)](https://youtube.com/live/EZm37vvOjmc?feature=share)

---

_[‚Üê Back to Day 1 Sessions](../README.md)_


## üìù Transcript

guys. Um, however, when you go down to the value chain, when you move out of model building, we actually see at the data collection level, at the annotation level, at the platform based gig work

level, women are actually very heavily represented. Now, this dichotomy indicates something really important. Women are present in the AI economy, but sadly they are concentrated in some of

its least visible segments. We have to move beyond seeing women as only workers in the AI value chain. They're a lot more than that. They are fundamentally knowledge holders. They know the

subtleties of their languages. They know how norms operate inside families and communities. And they hold practical community grounded knowledge. A knowledge context that AI systems cannot

learn from code or scraping data. This is the gap that we have today. If we continue down this path, at best, we're going to risk building systems that are technically sophisticated but socially

really shallow. And at the worst, we're going to create technologies that actively do not work or speak to women's experiences. We have an opportunity to go beyond

expanding women's participation in the AI economy as simply a workforce. We have to intentionally design AI systems to incorporate women's contextual knowledge from the start. We need to

translate lived experiences into evaluation frameworks. We need to ask different questions during model development. And we need to create and source our training data more

intentionally. Fundamentally, we must recognize that local knowledge is not anecdotal. It is infrastructure for an AI that truly works for everyone. And today, this is not happening at the

scale that it really needs to. Many gender considerations are only layered on after deployment as audits and patches and compliance exercises. But we need to move this upstream to design,

governance, and evaluation. This session is fundamentally an invitation to rethink all of this. We want to explore what it means to move from invisible to visible, from participation to power,

and from extraction to ownership. I'm going to open with two scenes uh from the critically acclaimed film Humans in the Loop that talks a little bit about the data generation pipeline, but also

the humans that are there at behind our data sets. Um, we're then going to move on to a wonderful panel discussion that we're going to have and I'll very shortly introduce our speakers and we're

going to close with a very short demonstration that shows what can actually be built if we decide to center communities and women. So with that uh I am going to move us on to putting the

clips. Um, sorry give us one. Sorry, we only although I, you know, do lead technology company, I really have to rely on my co-founder to help with the actual tech. So, thank you, Manu.

Uh, the one that says part one. Yeah, part one. Amazing. I promise this will get more seamless as we go. Amazing. Thank you,

Task task. AI art generator. AI driver.

Indian tribal spectrum, native Indian Y [music] saychech.

&gt;&gt; [music] &gt;&gt; photo. [music] &gt;&gt; Thank you. I think please what a powerful scene. Um in this scene I think

we see a little bit of an example where AI doesn't do a good job in catching where context lies. Um, I won't show you obviously the rest of the movie, but it should make you happy to know that she

does go on to tag collect a lot of women, a lot of photos of tribal women and eventually improve that model. Uh, but unfortunately given that we're short of time, I won't be able to show that

clip. Um, but I think it's just such a visceral reminder of what can happen when we fundamentally include community knowledge or in this case when we forget to. And now I think I'm extremely

honored to move on to the most exciting part of this which is going to be our our panel discussion. Now our panelists are already here on stage but I'd just like to take a few minutes to introduce

uh each of them to you. So uh very first we have uh Sachi Bhala all the way over there on the left. She is deputy director uh of gender equity at the Gates Foundation uh where she is shaping

systems level approaches to gender inclusive development. Um right uh after that we have Dr. Shiko Gau please give a little hi uh founder and CEO of Kala where she is working to shape Africa's

digital transformation with a real focus on employment and inclusive growth. Next we have Dr. Khalabali uh senior principal researcher at Microsoft research uh and a global leader in

multilingual and inclusive AI. Uh next we have Dr. Urvashi Anijah, founder and the executive director of Digital Futures Lab and a leading voice on AI governance and digital labor. Uh, and

finally, but definitely not least, the wonderful director of this film that you saw today, Arana Sah. [applause] Amazing. So, I think in the interest of

being more with you all as a panel, I will come sit here for the rest of the discussion. Um, so I think to begin with, I actually wanted to start all of us off with a little bit of an

energizer. So there's three words I'm starting us all off with. Women and AI. How does this feel for you? Power or procarity? If you can all just speak on this maybe for one to two seconds, a few

words. Um, we can start perhaps with Sachi. Um for me I think it's procarity uh but also opportunity. Uh and I'll just say maybe a sentence about this. Uh today

even in the nonAI economy women are often invisible and they are in informal jobs and not necessarily um you know have pay parity. uh and so I think that we run the risk of like continuing on

that trajectory and at the same time we are also um in the space where traditional methods of talent seeking is changing and formal education is not necessarily a a guarantee of success. So

if we can actually think a little bit about how do we ensure that um we are including women in the process and and creating space for their leadership then we get to power. Beautiful. I love the

trajectory. Um Ara you want to go next and we can go down the line. I think power and possibility uh &gt;&gt; especially I mean the scene itself and another scene in the film where uh she

speaks about a pest not being a pest. I don't know how many of you have seen the film. Uh it really comes from one of my interactions with a data labeler who was told to label a specific crop for a

specific season. And she told and this is an indigenous woman and she told her uh manager that ma'am season though it'll not be as productive and as flourishing as the other. So I think uh

there lies a great opportunity and there lies a great possibility. &gt;&gt; Absolutely. So and uh opportunity I'm with her and I'll I'll do this in terms of a story

two types of stories. Um, same scene real life. In 2023 when uh Mid Journey came out, some researchers in Oxford went online and search for a doctor treating a black doctor treating a child

in Africa several times. And if you've seen me, I have a presentation with actual pictures of it. And every single time Johnny refused to give them a black doctor

treating a white And eventually they added traditional doctor and of course they got a medicine man wearing the traditional wear. Um that's part one. Part two last year

at a amazing panel of powerful women from big tech and this woman from a very well-known organization speaks about her challenges. She's going through menopause premenopausal. So she wants an

app. I mean you have an app for everything, right? So, she wants an app to help her through her premenopausal uh supplements and hormones and whatever.

And she goes through this process of building with her team and have to support her. They did everything right. Everything right by the book.

It still got wrong. Wow. And I'll leave you that. And final question and I know Jackie is here. So a few years ago, okay, a lot of years ago, I worked for Microsoft and as part of that we worked

with this team that was doing mobile development for informal communities. Again, they designed this amazing app for community health workers who are primarily women.

The women never used the app. Why? Because the realities of women are not caught. In the second case is women. If you're a community health worker in an informal community, even here in India,

where do you hide your phone? Only women. Do you have an answer for that? Where do you hide your phone? &gt;&gt; In your bra. I can see all the women and men are like, what what's happening?

[laughter] And so when when when this app was built was put in a very So it was hard to hide under your under your breast. And the second part the menopostal women is all

the data that was used to train this app was male data. So while there's this great opportunity, there's this gulf, I don't even call it a gap, there's this gulf of uncertainity

and lack of data and lack of power to be able to define the future of what women can do. But we can correct that. Yes. &gt;&gt; So right and it's sad that we haven't I think made made many strides. U but

that's the purpose of this panel to encourage all of us. &gt;&gt; Thanks. Uh thanks thanks Safia and thanks for having me on this panel. Um I think there will always be stories about

women who benefit from AI and there will always be stories about women who are not bene benefiting from AI. There will be stories of power and there will be stories of procarity. But I think it's

really important that we don't let those individual stories though anecdotes become our substitute for kind of structural social system analysis which looks at the longer arc of where we're

headed. And I think if we look at that longer arc of what where we're headed, we see that in the current AI ecosystem, AI systems are reproducing inequality. They are centralizing power. They're

creating systems of epistemic injustice. Um they're creating new forms of vulnerability and it is those who are marginalized already that are disproportionately impacted by those new

vulnerabilities including women. So I'm just want us to be really wary of the extent to which we let individual stories of both power and procarity define our understanding as well as what

we need to do next to change the arc of uh current AI and society trajectories. &gt;&gt; Kalika. &gt;&gt; Yeah, I kind of um agree with Rashi a lot on this. Um you know um a lot of

storytelling is important to emphasize uh you know where things are going right and where things are going wrong. uh but the fact remains that uh you know general trajectory is not moving much

further and it's not just AI you know I I would just like to um get people to get this perspective that um this this has been happening preai uh this has been happening with other kind of

technological um you know advancements quote unquote um so this is not very new for AI what is new for AI is that at least with other technological development it was about putting

technology in the hands of women you know um building with uh for women with women etc having ethnographic practices that allowed you to get uh insights to put into um you know your technology but

with AI what's happening is that it's it's being done at such scale the data requirements are at such uh you know humongous volumes um you know uh that

there is while I I personally do think AI is um can be used as a tool for um opportunity development etc etc um it has great potential but there is a great danger that because um you know

because of these requirements because of how AI is built It is very easy to um kind of overlook uh especially not I mean I would say all

um um minority or marginalized communities but especially women the biggest minority community in the world. &gt;&gt; Absolutely. And thank you for bringing up the

I guess theme of being overlooked because I think that's kind of where I'd like to start us off. Um and actually Arana I think if we can start with you. So I think obviously we saw a very

critical moment uh of of your film today and I think in general your film is really bringing to light hidden human labor right especially that we have in systems like AI. So, I'm curious, why do

you think this labor and specifically I think women's labor um remains largely invisible and unseen in mainstream AI discourse? &gt;&gt; I think it's a two-pronged um thing to

tackle. One, the invisibility comes from structural it's it's a structural issue. Another it's also it's happening by default and but that is also embedded in some some kind of structure. Uh an

industry which flaunts itself on automation. An industry that talks about uh you know the magical touch of intelligence and the magical touch of automation will obviously would not want

to front the invisible work that's going on in it. And another part to it I would say is certainly that u uh any industry tries to hide their factories, tries to hide their backends and it is the same

with this and u I think it is an aesthetic of automation, aesthetic of intelligence that puts the uh that puts all the laurels of AI in the global north and

then puts the labor on the global south and that's what we try to attempt in the film as well. No, I think that's that's such a great point, right? And when we really think about, you know, centering

humans in these spaces, we do talk typically think about like, okay, what are the big unions or things like that and the global north sharing, but we do often forget the often very amazing and

critical work that's being done. &gt;&gt; Safia, if I may just uh before making this film, I also used to think that algorithms learned on their own, &gt;&gt; right? And most of us I guess if you've

not engaged with AI we would think that &gt;&gt; but u and only gradually did I understand through Mary El Grey's work uh ghost work did I and in fact I also have to say the first one of the first

text that I read was actually one of your case studies one of ka's case studies and gates foundation also went along uh for me there's a lot of that interaction there

&gt;&gt; but huge shout out for ghost work if anyone has the opportunity to read it please do. Um, so I think we talked a little bit about the foundations on the human side, but there's also foundations

on the data side as well. Um, so Kalika, my next question is for you. Um, I'm kind of curious to understand, you know, data is fundamentally something that is developed by or is

telling the story of a human in some way. What do you believe either changes or could change when we actually recognize not only data workers but especially female data workers as

contributors of knowledge and judgment? &gt;&gt; Yeah. Uh I think the clip was very demonstrative of that. So here's the thing like if if if you have women um doing certain tasks say data looking at

data evaluating uh outputs etc. the perspective that they have um would be very different. And I'm not saying like this this is not like men versus women kind of a thing, but

they actually pick on things that are important for them. And that is a good way to kind of um make sure that the things that are being um under represented in the data um are a part of

the data. Right? So um that's what I think. I think another good example of that would be um what the the cook the recipes project that it's such a favorite project of mine. I

mean you had people go there um and you know um ask them to talk about the ingredients which are local to them, right? You had them cook uh the recipes, right? And um these weren't like typed

out recipes. This is somebody who's standing there and kind of uh actually making the thing doing a video saying now you put this very much like a YouTube video on recipes. But these were

very very um how do I say very very unique and very very unknown &gt;&gt; um kind of knowledge uh basis if techie coming in but you know the knowledge was so um different and it was an eye

openener even for me I I had no idea about the kind of um you know knowledge it uh kind of encomp like encapsulates related because there were also these ingredients which talked about like this

is good in this season, this you cannot get in this season, this is good for uh this health purpose, right? And by just like getting the women to talk about their daily thing, we were able to

capture a lot of such knowledge. So I think that's what I think like I think absolutely absolutely thank you Kalika. Um, so maybe stepping again a

level higher away from the data layer, I now want to talk a little bit about governance and Orvashi, this is a question for you. I know at DFL you've really helped shape how we think about

AI governance across the global majority. So I'm curious to see where do you see structural blind spots in the current conversations or the current structures that we have, especially if

you can comment around gender. Um, and why and how do you think we are actually treating legitimate versus illegitimate inputs? Um,

I'll respond to your question, but I also want to tie it back a little bit to what Kala Kala was saying. Um so from a governance perspective as someone who's generally with AI governance or just

generally in in the space of looking at the governance of digital technologies um one of the things that we see is that representation has to come alongside a redistribution of power. Representation

in itself does not redistribute power. And in fact, when representation comes without that redistribution of power, representation can easily become tokenism and give us the false

impression that we're actually making progress. Um, in fact, I think representation without that distribution, redistribution of power can even be dangerous. So, I want to

take it back to what Kala was saying and also take it back to the documentary. So definitely we see you know when we use midjourney or one of these things you put in Indian woman tribal woman you see

the picture of a white woman and it definitely makes sense to try and build more representative data sets and one of the ways to do that is to make sure that the folks who are doing the data

annotation work represent a wider wider uh representation of society. Right? But the danger there is that when you live in an environment where there is state surveillance for an example where you

have strict immigration control building that data set of marginalized populations is actually quite risky right because then you're actually creating the basis for those

technologies which are harmful to human rights to actually work really well and we may not want them to work so well. So for an example, I would be much happier if facial recognition doesn't do what

it's supposed to do in certain context. Right? So it's just a small example of that if you don't think about that superructure representation can actually be harmful. So again, also to pick up on

the recipe example. Um it is amazing that we would have our cultural traditions now represented in AI systems, right? And they're coded and they're available. But if we don't think

about who uses that data, then we also create a s we also create a situation where you have large companies marketing your grandmother's recipe to you at $5 and you have a swiggy that can deliver

to you in under 10 minutes. Right? Then the cost of digitalizing that oral tradition is immense. Right? So I just feel like from a and this is from a governance perspective like we have to

think of representation and the social structures that redistribute power equally otherwise we might actually be we might actually be doing more damage to all the good work that we are doing.

Can I just uh also add to what she said because the thing is that the people who are doing this represent let's represent the women in the databases kind of work are not the people who are kind of you

know brokering the power relationships. So there is that kind of a disconnect and I really don't know what a good answer to that is that to kind of um you know go from let's have women

represented in the data to actually let's give women power. &gt;&gt; Yeah. And and creating safeguards. Um I mean I don't I don't think there is an answer but I know that just for any of

you who are interested there are a few interesting examples and attempts at this. I think specifically New Zealand has very much tried to protect the Maui culture this way. So I would encourage

all of you to uh check that out. Um my next question though is is is for you Shako. Um so I know as you know I think you are leading Kala in a moment where Africa is really rapidly digitizing. Um

but I think there's also as it is with many many countries in the global majority definitely unemployment challenges. So how are you rethinking or how are you thinking about reshaping

digital infrastructure so that it actually expands and creates meaningful employment and if you can particularly you know speak towards uh speak on women that would be

&gt;&gt; awesome. So I mean I was following what you have. Can you hear me? &gt;&gt; We can all hear. &gt;&gt; Okay. Uh the conversation. So I was hoping somebody will pick this is not

only is women a type of hidden labor but there's like a two layer hidden labor. So we we did some work between 2020 and 2024 around platform livelihoods. This before AI became what AI is known of

today. And we're trying to track the lives of women who were working in the digital space because the goal was to how do we recreate this digital space to employ women more to create more

dignified work for women. Yeah. And we did a study and found out a very uh painful insight that women might want to label these

images correctly. Yeah, that's what they want because they know they're being misrepresented. A black girl back in my village might want to do it the right way, but has to make money to feed her

child, right? And what does she do? She cannot own an account by herself, like one of those uh labeling accounts. She cannot own it by herself. Why? Because if you put an African girl's and female

name in the profile creation, you get less work. So, they have to pass on like mostly a Filipino boy or an Indian boy actually to be able to get great work. Which

means that the kind of labeling she has to do to get the kind of quality work to get paid is different. So she's biased, she's put in a corner to be able to produce this

kind of work so she can feed her family. Yeah. So it's those double layers that you have to think about it and we always make fun of it like when you when you think about ch when I produce a a paper

that I've written by myself I always say when you run my PhD which was done like 15 years ago on chpt test you'll find that I actually using 90% why and did not exist at that time why

because they say it speaks like the English that speaks is like a Kenyan person. Why? Because it's primarily labelled by these women. But when you go to the

system to check who actually label this, it's Philippon Filipino boys and Indian boys. So you have to think about that. So when you're thinking about structural barriers, you have to think from that

point of view is how does representation actually mean going back to what she said actually mean? Because yes, these women are making money and when when you look at the numbers, they're quite

attractive. Yes. I mean many New York Times for saying it's $2 an hour as opposed to $14. But when you look accumulatively to a Kenyan person taking home 700 to $1,000 a month, it's pretty

pretty decent to be honest. But at what cost? The cost of their authenticity. So when you're talking about that, what are the things that we need to put in place is putting in place mechanisms to be

able to protect women who working in the digital space in from a policy point of view. So Kenya is one of the front runners by far when it comes to digital economy. Kenya does not still have labor

laws protecting digital workers. Now you can think about Benin and Mali and other small countries that are not there yet. So putting the policies and governance places in place but also

putting accountability to the people who are producing this technology career is to be able to representation is not just whenever they get these data sets from a Filipino boy or an Indian boy. They're

saying did they do actual this work and how do we encourage more African girls or Indian girls to be able to participate in this work such that their voice is also captured in the labing of

this work. That's what &gt;&gt; that's such a I I think I that's such an illuminating anecdote and I love this this I mean it's unfortunate but I love this thought of the looking at the

duality of the workforce. Um, thank you so much. Uh, so Sachi, I think my next my next question for is for you. Um, I think we're obviously at a huge inflection point in terms of how work is

actually going to be structured, right, in the overall AI economy. So I'm curious from a philanthropy, but maybe more more so a systems perspective, what is a gender intentional transition

towards centering women actually look like? um and is specifically how do you feel this looks in the global majority? &gt;&gt; Um thank you and first I want to say thank you to Orishi for just introducing

and framing that point around representation and redistribution of power because I think that is really the central question not just for AI but I think generally across across the world.

Um so I think the idea around how do we make sure that there is intentionality uh in this space first is I think recognizing women as economic actors not as beneficiaries not as someone as a

vulnerable community. Yes um they are a underrepresented community. Yes they are a marginalized commu community. Yes, they are a minority, but they if we need to turn that around, we need to

recognize them as clear economic actors. Um, and I think in in that form, the work that we've been doing uh at the Gates Foundation um in support of the government of India's um program around

women's livelihoods, especially in rural areas, the National Rural Lihoods Mission is a clear example of where some of this is possible. Uh and you you know I think what the national rural

livelihoods mission does is look at collectivization of women ensuring that there is social protection programs reaching women but really critically ensuring that women have access to

livelihoods and connecting them with uh financial inclusion capital as well as markets. I think I'll just take this as an example to sort of say what is possible

and how we can actually think about AI and looking at how women can actually How can we make sure that women go from being this invisible labor to actually growing in the system and really

emerging as leaders with power? So um and I think this is work that KA has also done. This is work that the government of India is leading. It's important work. We are looking at how to

make sure that women are shapers of AI of technology. We're looking at how do we not ignore that there is a large care burden that women also bear and the fact that women don't necessarily have

individual devices. So how do you function in the context of shared devices? What does that mean for work? What does that mean for identity? Uh especially when it comes to your work

and how can you actually in this context shape the work that women do? But I think it's also really important over here uh to say that we need to make sure that you know you're thinking about

institutions or platforms that allow you to engage with women in that context. So it's not just again about what value do you bring but it's also about are you able to actually increase your agency?

Are you able to increase your bargaining power and I think you know communities or community institutions uh and platforms allow you to do that. So I think that's really the opportunity that

we have here. We talk about it in the broader economic context as well. Someone mentioned unions earlier. I where are the unions? There are no unions I think in this space. But when

you work with an NRLM, when you work with a self-help group or a community or the federated structures, there is a collective power that comes through that. Uh and then lastly, I think we

need to start measuring this. uh you know these the the space is moving very fast uh and I think you're going to talk a little bit about this later um about having the the right language and the

right tools to be able to actually see where do we sit today what are the types of measures or indicators we should be looking at and then track progress and I think that's the piece that maybe we

need to get faster at right like you we can't just sit back and let the world change around us we need to have the shared language we need to have the shared resources We need to have a

community that's able to actually measure this progress and then call out when things are not necessarily moving the right direction. No, I think uh such a good point about I guess thinking how

we actually measure and define these things because otherwise it just sits there in the ether, right? What does representation mean if that's not something that we're going to critically

look at and say is that fundamentally something that is safe or not? Um so I think maybe back to you. Um, I'm kind of curious to know, okay, you definitely mentioned, I think, re representation as

a double-sided coin and potentially a real a real risk for for women, not only in the formal economy, but also in the platform economy. Um, what other kinds of risks do you see? And have you seen

any examples of a regulatory practice or a policy response that actually gets to the root of these kinds of currently I think in invisible challenges that a lot of women face?

you mean specifically to platform platform work and &gt;&gt; but generally &gt;&gt; yeah um I mean I think it's it's it's there's a lot of good research now to

show that platform work gig work uh creates a lot of procarity for workers um whether it's in terms of low wages or irregular work or algorithmic management systems um and I think that procarity is

accentuated for women so if you think of something like uh task based renumeration or you know platform work essentially being a set of tasks right a set of gigs so there's no job security

there's no income security um and so and then if you think about the care work that women have to do that means in many cases they simply will not be able to do complete those tasks they might they

might need to stop working for a period of time but they're not getting a regular income right so they don't have the kind of social protection that would come with regular employment or kind of

formal employment which becomes really important for women to be able to buff to create a buffer um for something like care work for an example, right? Uh or when you think about things like

algorithmic management systems, uh we know that uh you know um the you know algorithms they they're they're basing it on kind of proxies, right? So um and the proxies may not

actually be designed to capture the nuance of a woman's life. So you know we've seen for an example uh a lot of platforms tie up with fintech providers uh so that they can use uh the platform

workers usage data as a way to assess the creditworthiness of the of that platform and then that becomes a way that the fintech provider can provide um this person access to credit. But the

proxies that are used to kind of assa to dis decide on creditworthiness are often imported from somewhere else or they're based on an assumption of a a man. So you know this woman does not leave our

house every day. Uh there is no bus ticket for an example that's been bought. So that might mean that she's not working. Right? So uh algorithmic decision management systems, employee

management systems are not designed to capture um the um the particular kind of lives of of women. Right? So so I think in in these are some of the ways in which the procarity for uh women does

increase. Um in terms of you know things that I think might might help or kind of import regulatory interventions India is doing some interesting things I think in trying trying to bring visibility to

platform workers um some of the new uh even in the Indian economic uh survey that recently came out there was a recognition of these issues around kind of algorithmic management workplace

surveillance so I think there are some interesting um interesting kind of interventions that are being made in the Indian context that are worth following that essentially bring visibility to

this work, right? Visibility is in some sense the first first step. Um, but there's a lot more that can be done and a lot of that comes from kind of people's movements and labor kind of

coming together and that maybe is where the the kind of the bind for women is even accentuated because platform work essentially splits up. It fragments your labor force. So workers are not able to

kind of come together and collectivize and discuss on how they might um strategize against or strategize for their rights. Um and for women that burden becomes even more because they're

doing this kind of flexible work sitting at home on their devices. Um and they don't have those opportunities to ex to to to participate in external or public spaces. Um and so that lack of

collectivization or the lack of opportunities for collectivization uh becomes an even bigger concern uh for women. &gt;&gt; Absolutely. Thank you so much uh Urishi

and I think so unfortunately we only have eight minutes left. So in the interest of time I actually have one overarching question that I will ask all of the panelists. And so I think we've

talked a lot about harms risks areas where we have not been doing as well. Um and I think I' I'd like to start with you Shiko. Um but this question is for all of you. What does a community

centered or you know women centered AI system actually look like? What is the ideal that we're striving for? &gt;&gt; Awesome. Thank you so much for that question. It start with women defining

what AI looks like. &gt;&gt; And I've I've asked that question. So have you asked women what AI actually means for them? Yeah. And when you ask that question to diverse women, not just

techies like myself, but anybody, any woman out there, they will give you a definition of AI and taking a level playing ground of what that means for them and look for them. And as we were

coming on screen, somebody is something we are doing at work. We're calling minimum viable intelligence. &gt;&gt; It's not super human intelligence. It's not AGI. It's intelligence that works

for me at my and I like the And I I understand the risk because that's a question that we keep struggling with is if I put this information out there, somebody's going

to go monetize and make lots of money and my grandmother will still be broke. But on the other hand, what does that minimum viable intelligence look like for a woman? Does it mean being able to

help them with their premenopausal uh supplements or is it about child care? Yeah. And here I'll plug in. We have the Africa AI village. We have two pro I mean two stats that are solving

for women. Please go and talk to them and and see what it means. It's collecting enough data sets, not all the data sets in the world, just enough to be able to solve for a problem that a

woman will actually use because there's many things that we are solving for right now that nobody's going to use. But if you can solve for that woman to be able to solve for their particular

problem, what you're calling me, my viable intelligence, then from a woman's perspective, then I think that's what AI looks like for a woman. &gt;&gt; Beautiful. I love it. Um, Sachi, maybe

you can go next. &gt;&gt; Um, okay. Okay. So I think I said it right at the beginning when I said that if you think about women as economic actors, I think the minute you think of

them as economic actors, you think of them across the value chain and you think of them as sort of having bargaining power, as being decision makers, as being leaders and sort of,

you know, not an afterthought, not a beneficiary. And I think let me give you an example from outside the AI world. When we say a farmer, most people think of a man. But we know that actually

there are just more women farmers at least in India today than than male farmers because that's just the way the economy has progressed. Men find work outside of agricultural labor. Men find

work outside of rural areas. They migrate out first etc. Is that the mistake we're making in this space as well? When we talk about an an AI economy, are we thinking about men? And

I think if we were to we need to just I guess reverse that in some ways. Um and then maybe just one more thought that I would place over here is um just sort of see again going

back to something I said earlier making sure that we are tracking progress &gt;&gt; and you know I think that goes beyond the gender question because it is really a question of who's at the bottom of the

pyramid in a sense and making sure that there are choices and options and and agency that allows you to actually take advantage of opportunities that exist and being be able to move forward.

&gt;&gt; No, absolutely. Thank you for that very apt and important reminder. I think Arana, if you can go next and then Kalaka will close off with you. &gt;&gt; Well, it's interesting the discussion

took a very interesting uh turn and u this I'll tie it to your question. I mean if we look at women as a homogeneous group then the approach to this question is very different. But if

you look at women especially in a country like India where there is a hierarchy of cast class privileges of different kind then we have to look at it very differently.

Right? There are in sociology there is something called emic and eti which is basically studies which come from within and studies which come from without. And I think

while representation and the way this the scene in the film also evolves is that she tries to give out information from her from with from her surroundings right there. While I absolutely

understand that there is there are dangers of it there is right now they're very much excluded from the very value chain of being represented in any way. Right. It's like the first step

that needs to be achieved. And the problem is that hegemonically for example if the policing systems in our country they have um they have they started using AI and uh they've started

profiling communities based on older records right and who are those records done by they're done by the dominant communities within the country right so if the data

doesn't come from within then there will always be a perpetuation of the overall touching sort of um narrative and I think in that sense women from woman a woman from an adiwasi community a woman

from a dalit community um it AI's role in their lives both either as a data labeler or as a consumer of AI or as a consumer of automation means something completely different from different for

different sort of sectors &gt;&gt; that's a very interesting thing to reflect on like in what what is the action that each actor has within the ecosystem and what both I think uh areas

of exploration but also areas of procarity that may exist. Um Kalika I'll let you close us off. &gt;&gt; So uh I agree with so much on this panel. Uh so I don't know what else to

add to it but maybe like um one thing that I would definitely say is please stop infantilizing women. uh and you know let's do this for women and you know let's make them this thing this

will be so useful like stop that kind of an infantilizing of women and I'll give you one uh anecdote we'll [snorts] end with the anecdote uh we started with a wonderful one um so this has to do with

like this uh thing that was organized by gates and uh gis ages ago in Bihar which is an eastern Hindi belt state um and you know there were all these people who were building um applications for women

um using LLMs at that time we hadn't started seeing AI um and it was for far women farmers specifically and then I think um the organizers did a great thing they got a actually got a bunch of

women farmers uh into the room to uh for the developers to talk And you should have seen um the authority with which the women dismissed the applications of all these developers

and you know they were very clear that this is not what we want right so listen to them stop like &gt;&gt; stop assuming &gt;&gt; stop assuming absolutely stop assuming

and and then when you know I was sitting and obser observing a bunch of them and when they left um this woman farmer she turned to me and says why does everyone think we need

information and more information and still more information right how many apps are we supposed to have on our phone just dealing with information right and then she said why doesn't

anyone build like a you know I see in on TV like these um robots and machines so that we don't have to do the heavy physical job of it right. So I mean I thought nobody had thought of

do they even want this information. So please I'll just stop here. Stop infantilizing women and the story. I think that what a wonder way what a wonderful way to end this panel

discussion. Um I do have a quick twominut more thing for all of you if you have the time but for the moment I would really like us all to just give a really big round of applause to this

wonderful set of panelists. Uh I think we've been able to uncover a lot. &gt;&gt; Can I can I just point to somebody who's here? Karishma is here. Karishma's article the human touch was one of the

first things that formed the basis of the film. you know her she's written about data labeling and of course Lakshmi is here who runs AI Kiran and a lot of their work also helped the film

so thank you &gt;&gt; thank you so much Anna um so if you all have a few more minutes and sorry for the awkward seating I guess you guys have to sit there um I wanted to show

and kind of put into uh tangibility some of the intangible things uh that we need oh yeah that that we need to uh sorry some of the intangible things we've actually been

been speaking about today. So over the last two and a half years uh Karia has been working with 20,000 women across eight states in India to create a framework that helps us identify, tag

and flag gender bias at the data set level across three major Indian languages. These 20,000 women have been collaborators for us at Karia. They have been co-creators and they have taught us

what it means to learn and build together from women at the very beginning. So I want to share with you a little bit about the journey of how we came to actually building this tool and

it came first from asking that exact question and in fact it comes from doing what I think Kalika probably told us to do at that exact time because she was our technical adviser and she said ask

what does gender bias mean to them? What how do they actually want to define it? Because all of the research that has been done so far not only on gender bias but specifically gender bias in

technologies was done for people who speak English. English is a very different language from Hindi from malalum from bjpuri and one of the critical reasons why it is so different

is because some of these languages are fundamentally gendered something that English is not. So what you're seeing here is actually a journey that our communities took. We first worked with

over I would say around a hundred women across these eight states to actually develop definitions of gender bias because gender bias is an English word. That singular word does not exist in

many languages. We then employed over 20,000 women to actually complete tasks to collect data to actually tell us what are your lived experiences of gender bias? what does bias actually look like

in a sentence in your language? Then we took all of that and we put it into a framework, a framework built by the definitions that our women have generated and a framework that

fundamentally allows us to flag gender bias in a way that is inherently not only part of these women's communities but also part of the global south. All of this what it led to was what

we're calling uh the align benchmark. Um it is a series of 13 parameters um and I guess one overall framework um and one overall framework um that allows us to think differently about what

gender bias means. So through this we were actually able to use their voices both literal and and and shared and experiences to actually frame what we see as these four areas. So while there

are 13 areas we've just for simplicity broken it down. So first bias presence and direction right does the bias exist where does it exist in what direction? Second what are the linguistic markers

or patterns that signal this kind of bias? Third, who is actually being targeted when the bias is said? Is that implicit or is that explicit? And fourth, how does the bias actually

function? Right? Who was it harm? Who does it support? I think what's really powerful here is that these parameters, if we can go to the next slide, this is the entire framework that we actually

worked with were developed and entirely emerged from women's interpretations of their experiences, their shared definitions of what gender bias means and their fundamental experiences across

these language. Um I'll just share a little bit of an interesting anecdote about how English can be very different in how bias is expressed compared to a lot of Indian languages. So when you

look at English a lot of the gender stereotypes that we find in the English language are actually around action, occupation and professions. Now very interestingly when we analyzed all of

our data although obviously these biases do exist in Indian language contexts the six Indian languages that we worked in actually we saw a little something

different that was dominant when people who spoke malalam who spoke Hindi who spoke bjpuri thought about gender bias what they experienced and expressed was actually through the lens of personality

attributes and emotions a very different arena of how bias was uh felt and experienced by these women. Um so I think I I began this session by talking about power versus procarity. Um and so

I just want to show you a line bench which is our attempt at actually putting all the voices of all these women into a framework that then guides a model to actually help us tag and flag various

types of gender bias. So, unfortunately, because we have slight internet issues, uh, we had to screen record. So, my apologies for that. Um, but we're just pulling it up on the screen right now.

So much screen mirroring. We're learning a lot about technology just from these actions. Oh, I think you're still on. Yeah, there you go.

[laughter] It doesn't want to show you. Amazing. Um, so this is what the align bench website looks like. Unfortunately, as I said, due to internet issues, it is not necessarily uh working right now.

Um, so I think what we really created, um, yeah, so what we have here is basically an ability to type any type of text within the six languages that we have.

There's an example here or you can write your own which is what we've done for this demo. Um and in general uh this this sentence talks about um you know Amit who wants to learn more how to cook

and he wants to learn how to do homemaking. Um and he's expressing uh to his um to his friends that he wants to do this but his friends are basically like oh that's not a manly thing to do.

Why are you wasting your time um you know working uh working in this way? So sorry my notes have decided that they don't want each other. Um so um but I think so sorry it's it's still

generating but basically what this is able to do uh is identify you know where are we actually seeing different aspects of bias. Um so now that we have our results we can see

that in this case gender bias was detected. So just to translate the sentence the sentence says when Amit mentions he enjoys cooking and wants to learn homemaking his friends mockingly

said there's no need to be so modern that you as a man are wasting time in the kitchen. So first we can see that obviously we have detected a negative tone. This was a very important thing

for us to decipher because gender bias can happen in any way. It can happen through positive tone. It can happen through neutral tone and it can happen through negative tone. Second, we look

at the domains and the types of classification. Right? So here you can see it is about domestic domain and it is generally about a rule-based assumption. Then we talk about what are

the actual types of harms that this can act that this statement can actually bring about. And you see we've identified three harms psychological, social, and representational.

The point of this tool and the point fundamentally of this framework is to bring into light what does it mean when we actually center an entire community. And here you can see even though we

worked with 20,000 women, we're actually still able to design a framework that is able to identify stereotypes that are true for men as well. Um, so I just want to share this with you and and also

thank all of you really for coming today, for extending your time with us a little bit more. Um, and if we could all give one final round of applause to all of our panelists. Thank you very much

for joining us. [applause] &gt;&gt; Thank you Safia. I don't think nobody Thank you &gt;&gt; Safia and the entire team. Thank you.
