# AI Diplomacy: Asia Leads ‚Äì Governing Artificial Intelligence in a Fragmented World

**India AI Impact Summit 2026 ‚Äî Day 2 (2026-02-17)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 11:30 ‚Äì 12:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 9 |
| üìÖ **Date** | 2026-02-17 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/7yx1xmAMkzE?feature=share) |

## üé§ Speakers

- Anne Marie, Denmark's Tech
- Azizjon Azimi, Artificial Intelligence Council of the Republic of Tajikistan
- Dr. Yuko Harayama, GPAI Tokyo Expert Support Center and Tohoku University
- H.E. Mr. Nezar Patria, Communication and Digital Affairs of the Republic of Indonesia
- Prof Alejandro Reyes, University of Hong Kong; Scholar-in-residence, Asia Society; Adviser, AI Safety Asia
- Prof ‚Å†Stuart Russell OBE, UC Berkeley and Center for Human-Compatible AI
- Wan Sie Lee, IMDA Singapore

## ü§ù Knowledge Partners

- AI Safety Asia

## üìù Summary

As AI reshapes global security faster than treaties can anticipate, traditional governance is falling behind. This session proposes "AI Diplomacy"‚Äîshifting the focus from rigid harmonization to establishing operational "fire-breaks" for immediate risks. Convening ministers and experts, we explore how Asian leadership can secure trusted lines of communication in a fragmented world. Join this high-level dialogue to transition from abstract ethics to durable, cross-border crisis mechanisms.

## üîë Key Takeaways

1. As AI reshapes global security faster than treaties can anticipate, traditional governance is falling behind.
2. This session proposes "AI Diplomacy"‚Äîshifting the focus from rigid harmonization to establishing operational "fire-breaks" for immediate risks.
3. Convening ministers and experts, we explore how Asian leadership can secure trusted lines of communication in a fragmented world.
4. Join this high-level dialogue to transition from abstract ethics to durable, cross-border crisis mechanisms.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/7yx1xmAMkzE/maxresdefault.jpg)](https://youtube.com/live/7yx1xmAMkzE?feature=share)

---

_[‚Üê Back to Day 2 Sessions](../README.md)_


## üìù Transcript

Safety Asia, if I might take a bit of time, works across the region and globally to bring together policy makers, diplomats, researchers, and industry to think through the governance

and crisis implications of advanced AI systems, particularly from Asian and global south perspectives. Um, today's session is part of a broader set of conversations we've been convening on

what we're calling AI crisis diplomacy. how governments coordinate across borders when AI related incidents unfold at speed and with uncertain attribution. So this discussion is intended to help

continue that conversation and move us toward more concrete frameworks for AI crisis diplomacy and engagement across borders. So let me begin with a simple observation. In diplomacy we are used to

thinking in months and years. We negotiate, we deliberate, we build consensus. But AI enabled crises do not move at diplomatic speed. Financial systems react in seconds. Synthetic

media spreads in minutes. Autonomous systems can act before governments even know something has gone wrong. So the central question we're exploring today um is how do governments coordinate

diplomatically in response to AI related crises that unfold across borders at machine speed. We circulated a short uh fire starter paper ahead of this session with our panelists describing an AI

crisis as one that crosses borders unfolds faster than any single authority can manage involves attribution uncertainty. These are not hypothetical challenges and yet while we have built

deeply interconnected technological systems we have not yet built the corresponding crossborder crisis coordination infrastructure. This is what we want to explore together today

this morning. So if I might before we move into my just presenting some scenarios let me briefly introduce our speakers. We're joined this morning by professor Stuart Russell, director of

the center for human compatible AI at the University of California, Berkeley and co-host of this session. Thank you, Professor Russell. Wani Lee, director of the Infoccom Media Development

Authority, IMDA of Singapore, Harayyama Yuko, former executive director of the global partnership on AI and a leading voice in international AI governance from Japan. and um Ajitan

Azimi, apologies if I mispronounce, but u founding chair of the artificial intelligence council of the Republic of Tajikistan. We will also be hearing a short video intervention from Audrey

Tang, the cyber ambassador of Taiwan and former digital minister. And we are delighted at some point um she might be in the room already, but she she she said she might be delayed.

Ambassador Anne Marie Entoft Melgard, Denmark's tech ambassador, who may be joining us shortly um given her scheduling. So, please join me in welcoming our speakers. Let me just

quickly frame some scenarios, right? These are illustrative. They're simply to help us think concretely about coordination challenges um with regard to AI. So scenario one, a financial

crusade, a financial cascade, sorry. Um, an AIdriven model misinterprets data and generates a false policy signal affecting another country's currency. Automated trading systems react

instantly across jurisdictions. Markets halt. Financial link strain. Officials call each other. [snorts] One side asks, "Why did you take this action?" The other says, "We didn't. Both may be

telling the truth. The core problem is not only malfunction. It is the absence of a rapid crossborder verification channel to establish what happened before trust collapses. Here's another

scenario. Scenario two. Deep fake driven instability. A highly realistic synthetic video appears shortly before an election or peace negotiation. It spreads rapidly across platforms hosted

in multiple jurisdictions. Verification takes longer than political timeline itself. By the time the video is debunked, the damage is already done. The issue here is not detection. It is

the absence of a shared crossber response mechanism at speed. Third scenario, autonomous infrastructure incident. An AI system managing interconnected infrastructure makes a

decision that affects another country. The system may be hosted in one jurisdiction, operated in another, and affecting a third. Governments suddenly ask who has authority, who is

responsible, and who do you call, who can act, right? Our current diplomatic and legal frameworks assume human decision makers. They are not designed for agentic systems acting across

borders. Across all three scenarios, all three scenarios that I've outlined, the challenge is not only technological, it is coordination under uncertainty. So, how do we establish shared reality

quickly enough to prevent escalation? How do we clarify intent? How do we coordinate response? Now to frame the broader implications, let me turn now to Professor Stuart Russell. Um Stuart, may

I invite you to offer some reflections on why increasingly autonomous and agentic systems create new challenges for governance and diplomacy, please? &gt;&gt; Uh thank you very much for hosting the

session and for the opportunity to speak. Um so AI has been around for about 80 years. Um I wrote my first AI program almost 50 years ago.

uh and we've gone through a number of boom and bust cycles and obviously we're in a boom as one can see uh right now and that is the result of uh mainly large language model technology

uh and and that technology is increasingly uh being augmented with so-called agentic capabilities which is sort of ironic because my uh my textbook which came out in 1994 has on the front

cover the intelligent agent book. Um, and so the the idea of an agent is central to how we think about AI systems. They're systems that perceive and act. Uh, and generally they will act

in the pursuit of their objectives. Um, so you asked about the challenges to to governance uh from this fastm moving and and uh technology and the fast unfolding unfolding scenarios. Um and in

fact this is one myth uh I think that is often uh trotted out by people who are opposed to regulation. They say oh you can't regulate something that uh is changing because by the time you've

regulated your regulation will be out of date. This is utter and complete nonsense. Uh think about how we regulate medicines.

Medicines have to be shown to be safe and shown to be effective before they can be sold to the public. That regulation is going to remain in place for the next 500,000 years until

we all become immortal and we don't have to take medicine at all. Right? It has nothing to do with the technology. It has to do with the risk. We set the level of risk that we're willing to

accept from a technology and we leave the technologist to figure out how to make the technology that satisfies that level of risk and delivers as much benefit as possible.

Um and so um you know another permanent uh and future proof form of regulation is liability. If you harm someone, you are legally

responsible and you have to pay. That's going to be true for the next 500 billion years. And again, it has nothing to do with the technology. But unfortunately,

pretty much all tech companies disclaim liability. So, if you ever have a few hours to spare and you really want to read your Microsoft enduser license agreement, you will find that Microsoft

disclaims all liability and uh in no case can you ever claim more than $5 in compensation. That's what it says. Uh so that's a a regulatory tool that's been around for thousands of years that

the tech industry has managed to uh avoid. Um and I think it would be very sensible if we were to restore that tool which uh works very well for fastm moving technologies for uh for things

that unfold very quickly and so on. Uh the only problem with liability is that it's not much use for extinction risk. In fact, that's a mathematical theorem. It doesn't deter people from imposing

extinction risks on the rest of us at all because of course if the risk comes to pass, they won't pay. Um, myth number two, uh, AI is a generalpurpose technology

and you can't regulate generalpurpose technologies. Um, this is a strange axiom. Uh, and you might say, you know, okay, why? And then they say, "Oh, but you know, look at electricity. We we

don't regulate electricity. It's a general purpose technology." But that's nonsense, right? Electricity as a general technology is highly regulated. Uh the voltage and currents have to be

regulated otherwise you would set fire to every city on earth. The wiring is regulated. All employees who work with electricity are regulated and trained. the plugs, the appliances

are regulated. So, um, these myths are trotted out by people who are professionals who have successfully in the past opposed regulation of cigarettes, of uh, carbon

dioxide, you name it. It's the same playbook uh, that's being used here to prevent the regulation of technology. Uh, so as I mentioned, you know, effective regulation uh, is about uh,

ing the level of risk that we're willing to accept uh and then requiring demonstrable proof or a high a high degree of evidence uh that the technology meets that level of risk. And

this is how uh you know here's here's some water. Why can I drink this? Why can I drink it? Because it's regulated. I know that I can drink it because the

companies are subject to regulation on the quality and safety of this product. Same with food. Same with this building. How can I sit in this building without being worried that it's going to

collapse on my head? Because it's regulated. Because it has to meet building codes before anyone can come into it. Um, most of you flew here on an airplane or took a train. Those are

highly regulated. Uh, if you go upstairs in the elevator, that's highly regulated. Um you know nuclear power is often cited uh as an example of how uh the risk is quantified very precisely.

We accept a risk of nuclear meltdown of between one in a million per year and one in 10 million per year. Uh and that that risk has been set by regulators because that's a what we consider to be

a reasonable tradeoff uh to get the benefits of of nuclear power. Uh it's very diff if you set the risk to zero to prove that uh your nuclear power station will never have an accident. Um but we

have been able to prove and those proofs take the form of millions of pages of uh probability calculations uh that the level of risk for nuclear power meets that standard. So what can we do with

AI? You know, I think with with planes, uh, for example, um, the notion of safety is pretty clear. It's not supposed to touch the ground unless you mean to touch the

ground, right? That's pretty much it. Um, with AI, because it's so general, it's difficult to say, you know, what is the line between safe and unsafe behaviors

by AI. Uh so this approach of uh what we call behavioral red lines is gaining some uh currency and behavioral red lines are um specific obviously unacceptable

behaviors by AI systems. Uh and the idea would be that uh companies need to demonstrate that their systems will not cross those red lines uh with some uh reliability that's appropriate for the

level of risk that we're talking about. Um, and so if uh if a red line says AI systems should not explain to terrorists how to build a new biological weapon, uh that's a fairly high risk thing. So

you'd want a fairly low probability of that happening. Uh another red line might be, you know, an AI system should not pretend to be a human being, uh whether a general human being or a

specific human being. Uh that's maybe a bit less of a risk. So maybe you might have a a slightly higher threshold for that. Um now when it comes to extinction

uh which many or in fact all of the the leading CEOs of AI companies have uh have said is [clears throat] a pretty likely outcome of their effort to create AGI.

Uh you know what's an acceptable level of risk? Should it be zero? Uh well probably not because there is already a background level of extinction risk from other

things like asteroid collisions and so on. Um and so we might uh we might say well obviously it should be safer than nuclear power station right extinction is much worse than a nuclear meltdown.

Uh so let's say for the sake of argument one in a 100red million per year and I think that's a bit generous but we'll go with that. So what do the companies say the risk is? Um

the CEOs have given numbers for the risk of extinction from the technology that they are trying to build between 10 and 50%. So put another way, we need the

technology to be 10 million times safer [clears throat] than it is. Um also, you know, those numbers that they're giving out um are I would say seat of the pants. They're not even back of the

envelope, right? They're not based on any actual calculation. It's just a gut feel that they have about how easy it will be to lose control uh as we make AI systems more and more capable. Uh so we

need regulations uh that will uh require a demonstration that the probability of loss of control leading to human extinction is less than one in a 100 million. uh and that

applies even to systems now, right? It should be easy to come up with that evidence now. As the systems get more capable, it's only going to get harder. So the company's view is, well, we don't

know how to how to show any level of risk. But if they can't do it now, how are they going to do it next year and the year after when the systems are in fact

much more dangerous and much more capable? Um, so this argument that technology companies come up with often is we don't know how to comply with any required level of risk and therefore you

can't require it. Let me say that again. you the human race are not allowed to protect yourselves from the technology that we are building that may well make you go

extinct. So that's the argument that they're making. Um I think I'll stop there. Thank you. &gt;&gt; Great.

[applause] Thank you very Thank you very much uh Professor Stewart, Professor Russell. Um now if I might turn to the panelists um on if I can start with you there's much

um that you can react to the um the scenarios that I posed the comments by uh professor Russell um I'm thinking you know Singapore sits at the intersection of finance infrastructure and digital

connectivity in Southeast Asia. So if one of the scenarios I mentioned unfolded in the region today, you know, how would coordination actually begin in practice? Which channels would move

first? And where do you see the most dangerous friction? Verification, authority, speed, up to you on what you want to comment. &gt;&gt; Thanks Alejandro. [laughter]

&gt;&gt; And you have only some minutes. &gt;&gt; First of all, thanks for having me here on this panel. I think um when we were looking at the notes that you sent us, well, when I was looking at the notes

that you sent, I don't think I can speak on behalf of my other panelists. Um I thought it was quite interesting because I don't think we have had this conversation before in terms of thinking

about how do we coordinate amongst governments when uh crisis happens in AI, right? Um so and I have to say this is a somewhat theoretical and hypothetical discussion right now

because it's not something that we have set up um for specifically for AI right so um but we have done it for other types of crisis and and I I define it um very

specifically on crisis because I think Stewart talked about governance and regulations of um AI and I think here we are perhaps going to I'm going to maybe focus on your earlier kind of

provocations which is how do we cooperate when it comes to something that has happened in the crisis meaning some bad thing has happened and what we can do about it right um but we have had

crisis in the past separate from AI whether it's the pandemic and cyber security and so on so there are ways in which governments are working together in concert to deal with crisis the

question is what has changed in the age of AI right Um and I think we should be able to what has changed in terms of perhaps in terms of the the scenarios that you have highlighted doesn't sound

like it's well I mean it has changed in terms of the speed the scale um and our ability to um assess um the situation um but it is still applied to these scenarios in which we find ourselves on

um on um in the past cyber security healthcare and so pandemics and I think in that case we should continue to rely on the kind of institutions that we have set up amongst

ourselves to address these issues but maybe what's additional um is to your earlier provocation on how do we identify and how do we verify right um I work with um a few other um

institutions that you know some are called AI safety institutions but some are not anymore uh Um and there are other names as well. Um and one of the things we do a lot of is joint testing

efforts, evaluations and so on. And it's not so much the outcome of the joint testing efforts as important. I mean the results and all that for us is less critical. Building a capacity for um

evaluating is important but what was really important for us is the ability to start building trust. I think for governments to work together uh you have trust with one another at various

levels. You know at the diplomatic level our foreign ministries they have a ch they have channels to engage and discuss. At the sectoral level the healthcare agencies have a way to engage

and discuss. The cyber security agencies have a way to engage and discuss. How then do the people working in AI have a way to engage and discuss. And I'd like to suggest that this is one initial way

in which we're starting to build that trust among people who are within government dealing with AI learning to well working together on even small projects is a way to start

doing that and doing it on a regular basis exchanging information that allows us to build trust. And when you have trust then you can pick up a phone and call someone and say you know my

colleague in Japan is Akiko right and pick up a phone and call Akiko and say hey you know something has happened can we just figure out is this is this what you're observing today as well or not

and then coming back then to Stuart's earlier sort of discussions around then how then do we regulate and think about bigger um think about um the issues around um requirements on labs to um to

demonstrate um that we they're not putting us at risk, right? And in very um severe scenarios. Um this is also one way we can then start to do that together, right? Um governments have to

one build the capacity to also technically um evaluate and assess. We can't rely on the labs themselves to show us. So if they give us a number, how do we validate that? um and we can't

do it on our own because collectively I think we even have less resources than a single lab uh in in doing some of this work um so definitely we have to do it together right um so these are ways in

which we can try to try to address the what you call in the title diplomatic crisis right um I'll just summarize I know this is a very long kind of intervention but I'll summarize in in in

three points one we already have existing ways in which we are dealing crisis whether it's in healthcare in cyber security and so we should do that I think on the AI front we can start

building a lot more trust amongst um all well I don't want to use allies amongst um colleagues who are working uh in this space especially around technical evaluation

um and then three um as we start to build this technical evaluation to not just deal with crisis but we can then start dealing with issues around governance and I have more to say on

that but maybe I'll just have my fellow panelists uh provide some provocations as well. Thanks. if you could give us the perspective from Japan but also um if you know from

the global governance standpoint which you're quite familiar with um if one of the scenarios again that I I mentioned occurred tomorrow or any other scenario um which institutions would be expected

to coordinate where do you see the um biggest institutional gaps for blind spots if you will. &gt;&gt; I think it's okay. &gt;&gt; Thank you for having me and the topics

you are focusing on um diplomatic crisis in AI is really something not evident but we have to tackle all together. I agree with this and I don't think we have solution international organization

something like that we should be innovative in the way to tackle these issues because first of all AI is uh technology as such we we know that we have to

uh I would say um we are facing many crisis Today more frequently, more in depth and uh handling all these issues we have experienced by the past when you have

crossborder crisis uh diplomatic way of handling these issues and this has been we have accumulation past experiences and doing well and something something else sometimes it doesn't work

but we have some kind of cons consolidated way of tackling this issue but when you have AI which is amplifier amplifier of existing crisis the difference is is that the speed and we

are less equipped human to make decision quickly and also to have a way over quickly cross those border issues. So we are less less equipped to address issues. So how

to do that? We are not alone as a human being and also I would say um the differences with existing crisis and AIdriven orified crisis that usually when we human are tackling this crisis

we have kind of social consequences but it is absent in case of AI so the difference is there and I think it's phenomena is becoming more critical because we are seeing observing that AI

is gaining in agency more than more. So how to do that? So my view on this issue is that we need to consolidate what we have uh accumulated by the past. Um but not

only because we see the limits because of the capacity of human capacity. So what what is needed is to have kind of new practice new framework to address these cross border crisis

generated by air. Uh there's no one solution but fundamentally what needed is that more to more human touch within that's mean

human to human nation to nation institution to institution um channels at the basis and it should be uh consolidated by mutual trust and what we

needed is not just addressing uh crisis you have in your front but uh everyday life you need to consolidate all these channel uh with employing human capacity and

that's the key to see innovative way of tackling crisis generated by &gt;&gt; great thank you very much now if I might turn to a now I I'm sure I mispronounced Um, if you could then talk to us a bit

about the Central [snorts] Asian perspective. Um, what kinds of AI related crises feel most plausible in your regional context and how included do countries in the region feel in terms

of global coordination discussions? And what would make a crisis coordination mechanism sort of genuinely usable from your point of view? from where you are sitting.

&gt;&gt; Thank you so much. &gt;&gt; I can speak loudly enough for everyone to hear. &gt;&gt; I think it takes time. &gt;&gt; I think it's working now. Perfect. Thank

you so much. First of all, I want to say namaste India. Thank you so much for hosting all of us. It's been a phenomenal time uh in Delhi. Uh for everyone who has come over. Uh [snorts]

let me say a few words in the beginning about uh kind of what's been interesting in Central Asia that I think could be a model for other regions of the world. Uh last year uh I was involved in a process

uh together with our colleagues from the government of Tajjikistan uh and the government of Central Asia in drafting a United Nations General Assembly resolution uh which was passed uh over

the summer. Uh it was passed on July 25th of last year and that resolution is quite unique in several aspects. First and foremost, uh it built as common central Asian approach to artificial

intelligence where we laid out that eventually our intent with AI adoption is to uh achieve sustainable development goal implementation. Uh and as part of that there were two features that were

very interesting uh for our discussion. Number one, uh the UN resolution called for the establishment of a regional center for AI. So it's the first time when at a regional level there's a

center for AI in in a particular part of a region that will help at least coordinate between the different AI bodies uh uh you know that are responsible for it. Uh our friends from

the agency for innovation and digital technologies of Tajjixan are here today. The deputy director is here and they're working very hard on realizing this regional center for AI initiative now

together with other governments of Central Asia. Number two, the resolution recognized mechanisms of AI self-regulation that have come out of Central Asia and Tajikhstan has been

really at the frontier of that discussion. Now, let me be a little bit libertarian here and and give you the perspective that we have had at least in our country. So, there was mentioning on

this panel from other esteemed speakers about uh for example electricity as a you know technology, cars, aviation uh what is the main cause for uh plane crashes? About 80% of plane crashes are

because of human error. It's because of pilot decisioning that was faulty. What is the main reason for car crashes almost all of the time? It's human decisioning, right? It's because of you

know different factors that are tied to the human drivers who are who are making faulty decisions. Um you mentioned in your finance case and I'll tie this point back the case of how in trading

for example financial trading most of the trading at hedge funds is done by AI models. Why is that the case? Why have hedge funds moved away from human decisioning?

Because in any Beck test that you can run, an AI model that's as basic as a classification model running on a very simple boosting algorithm outperforms human judgment. And the point I'm trying

to make is that you know these technologies including artificial intelligence technologies are permeating into decision making not because of human perfection but because they

perform superior to human imperfection. So when we talk about regulation of AI, the question that puzzles me as someone who is you know a founder of a startup that is now helping our government with

with AI policy is that we always talk about AI as an object of regulation. Uh right and that's not really the case because who's eventually making the decisions? It's actually humans that are

using AI as an instrument to make decisions. So why do we hold AI to a standard that is much tougher and stricter than human decision- making? I find that to be very puzzling and the

perspective of Tajjikistan and Central Asia, you know, at least from our conversations with our peers in the region is that think about it from our perspective, right? Tajjakhstan is a

developing economy. You know, we're just permeating our AI policies. We've put AI into 100 schools across the country. There's a national LLM who have developed locally. We export AI to 20

countries. A lot of this work is done by my good colleagues who are in the room today. And the question we raise is if we were to step in to regulate AI at this point, right? What does that mean

about our competitive advantage on the global scale where countries are, you know, look at China for example, countries are developing and deploying AI models at scale never seen in human

history before, right? It regulation slows down our ability to innovate fast. But it's not a question of not regulating. Of course, we all agree that AI should be regulated somehow. We all

agree on that point. It's a question of do we hold AI to perfect standards while human decisioning is imperfect. That's where I will push back. And one comment I want to make is, you know, Zipple, the

company that I founded as a founder and CEO, I uh chair the AI council under our ministry of industry new technologies in Tajjikhstan. We work uh on using synthetic data in order to uh automate

decisioning for risk models in banks. So whenever we go to a bank, let me explain to you the journey of what we go through as as as startupers. We go to a bank, we tell them, folks, don't trust our words.

Let's run a beck test. We can help you reduce your non-performing loan rate by using our proprietary synthetic data models. You know what the bankers tells us? Fantastic. Let's run a test, but

let's make it X condition, Y condition, Z condition. And six months pass, we've done 10 back tests. And then they come back and say, let's run one more test of hypothetical scenario of this happening.

Like we go through seven rings of hell before deploying one AI model into production when we're serving enterprises and banking is a very highly regulated sector. And if someone tells

me that a government bureaucrat can regulate me better than my customer, I'm sorry, but it's just not the case. And we service 60 banks across 20 countries. The world's largest Islamic

bank is our customer. We just onboarded them. To onboard them as a customer, wait to go through 12 months of piloting and testing, right? And my customer is the ultimate

judge of how capable is my AI model and whether it was developed in a properly regulated environment. So my call is to think about it a bit broader where I understand that for you know advanced

economies strongly regulated policies makes sense but let's not go back into colonial thinking of the whole world should follow that model. It's not the case. We're living in a different world

and frontier economies like Tajikhan have a voice of their own to make. So this is my call to have a bit of kind of counterintuitive thinking and to at least give a chance to developing

countries to showcase that we can have our own independent sovereign policies that are innovation first and that will help us to have our own seat on the table not to be on the menu at the

table. &gt;&gt; Thank you. &gt;&gt; Thank you very much. I I see one is wants to respond a bit that before we go to Audrey Tang's uh video please.

Thanks for that. I mean I absolutely agree that I think there are many things that we should allow um economies to be able to do um and be able to use AI effectively in Singapore. We do that as

well. I mean AI is going to be a big part of our economy in our society. We we've launched a lot of recent um well made a lot of recent announcements to actually be able to use AI effectively.

But I thought maybe I wanted to break down the regulation conversation a little bit better because I think um it's a it's a little bit more complex than that I feel. Uh so I work in AI

governance and safety policy. So we talk about AI regulation in Singapore. So that's really what I will be focusing on. Um well what I'll be working on if we have AI regulations in Singapore I

think it we look at one regulations in terms of outcomes not technology. I think that's important and that actually is reflected in how that comes out in the sectors in financial services in

healthcare in um in legal in HR and so on. You focus on the outcomes and not the technology and that those regulations already exist today but of course then you start thinking but in

some cases it might be difficult in the existing regulations to be able to address the new the new impact that AI might have in these specific use cases. Um so what do you do? You either clarify

your laws or you introduce new ones. So for example in the elections we introduce a law relating to deep fakes a couple of years ago so that you can you can it's very specific it's very

surgical only during elections you cannot be using deep fakes and we have mechanisms to one decide if it's a deep fake and deep fake doesn't have to come from AI it can come from Photoshop it

can come from somebody just you know putting somebody's voice over a video right that doesn't require AI it just scales it up makes it easier um and so on.

And then we have make means to say then who then tells the platforms and we're talking about social media platforms WhatsApp and so on who then tells them to remove this. So you must have a

mechanism to do all of that as well. So these these are laws that you can put in place to address specific issues that you have to deal with and I think for your citizens they will expect some

level of protection from harms not not everything but some level of protection of harms. And actually you've already done that to in in in many ways in your sectors. Now let's come to AI in a

different ways. What Stuart was talking about was how do we deal with significant risk from AI at the broad level across the world, right? And that I think requires a form of collaboration

and working across governments, right? We're talking about companies with a lot of influence which is already alluded to by Stuart. Then how then do we work together on that? And that's slightly

different from how we want to use AI domestically within our countries, right? But even then when we think about something like chat GBT from open AI or

any of the models that dealing with our citizens on a very regular daily basis and you worry about the harms that can come from that and that is a domestic issue that we have to deal with. What do

we care? What do we say as unacceptable harms whether it's CSM, non-consensual sexual images and so on. what is unacceptable and that's something that we can actually address. We have online

safety laws in Singapore and we can use that to be able to deal with these things. So that's my that's the point I'm trying to make that regulation is a complex topic. You have different parts

of it that you want to address. There are things that we should do internationally together. Um and that takes time and effort and maybe diplomacy is an important piece of that.

&gt;&gt; Thank you very much. I see that all our panels panelists want to respond and I'll have you respond. Can you just keep it brief because I would like to show the uh video of Dr. Audrey Tang and

we'll make that at the end. So, so please Stuart. &gt;&gt; Okay. &gt;&gt; Um yeah, so with respect to the the last remarks from our Tajikistan colleague,

um regulation is not a single uniform thing, right? It's uh it's always needs to be calibrated to the risk uh that's being posed. And your bank customers

because you are not liable as an AI company for losses incurred by the banks trading with your software. They have a very strong incentive uh to uh impose lots of testing which is effectively

regulation but happening in the commercial arena. uh but when you uh when a company like OpenAI is selling it directly to the user uh the user is not aware there's a huge information

asymmetry uh I I may not know that this software is capable of convincing my children to commit suicide and so uh so what what kinds of

regulation depend a lot on uh the information status of the participants uh their economic resources and so on. Um, and regulation does not have to be an enormous burden. My friend Emily runs

food standards in the UK. Um, and she says that there's far more regulations on sandwiches than there are on AI systems. So, a little sandwich shop can manage to comply with a heavy regulatory

burden. And we open uh across across the world more than a million new uh innovative restaurants and sandwich shops every every year. Right. So, so somehow regulation and innovation are

going hand in hand. Uh and and uh and as I mentioned, you know, vast parts of our economy, water, food, buildings, transportation, these are all regulated and yet uh they they function very well.

Uh so I think that's that's really important to understand. I just want to come back to something that that WCI and and Yukio said about cooperation among regulators. I think this is this is a

really crucial thing and uh I think cooperating around uh specific uh regulatory projects that are not moonshots that are not trying to regulate

everything in one giant omnibus uh global treaty. Uh for example, we should have a right to know every human being should have a right to know if they're interacting with a human or a machine.

It's a very simple proposition. uh it's already in the European Union AI act. I would love it if regulators around the world could cooperate to figure out how to put that into their national laws uh

and how to operationalize it. Uh for example, so that platforms, social media platforms screen out bots so that you know that you know the message you're getting is not from a bot but from a

real human being. Uh that will be wonderful. &gt;&gt; Now just one minute each please. One minute each because we I know time Just one minute. Uh regarding regulation, we

have to move from this traditional classical principle agent model high authority controlling companies actors into more co- responsible model in the way that everyone on the same boat and

including those who are at the government regulatory issues but private sectors and users and we have to take responsib each of us and then changing the way of creating. So just a few

&gt;&gt; thank you so much. &gt;&gt; We'll keep it super short. Yes. I I think the reason that there are more regulations on sandwiches and artificial intelligence is because AI is a bit more

complex than making sandwiches. So we need to take time on how to actually think through these policies. Uh one question I just want to rather comment I want to add is that actually absolutely

agree with my colleague from Singapore because you know it's not a question of we are working on the AI regulation policy in Tajjikistan. My colleagues from the government are here as well. Uh

and we understand how you know it needs to be risk based. Uh but our startups in the country right and we're one of the few developing countries that is exporting AI software today at scale in

production are not entering the European Union markets because EU AI act is too complex. We can debate theoretically what is right you know is the risk based approach framework great or not but de

facto in the real world of practice companies are not entering the EU market because we simply don't understand what is in that thousandpage code that I think even GPT will have a hard time

contemplating upon so the question I want to pose for everyone just to think about is that our approach is AI self-regulation where exactly as my colleague from Singapore said it needs

to be outcome based we have seven principles that we've enshrined in that resolution that past AI has to be explainable. The models have to go through validation, out of sample

testing, out of time testing before they go into deployment. We understand that because we make those models. But it's a question of should it be up to a government agency to deploy these into

practice or should it be the market to self-regulate up until we understand a little bit better what is AGI, how does it feel like, and whether we should eat sandwiches with it or not. Thank you.

&gt;&gt; Okay. Now, sorry to dr. Good local time. I'm Audrey Tong, Taiwan's cyber ambassador, first is show minister and 2025 right livelihood laurate. My heartfelt thanks to AI

safety Asia for staging this important conversation at the India AI impact summit. In diplomacy, we in years. It takes time to draft texts to build consensus and

ratify commitments. In the AI world, crisis unfold in milliseconds. What I wish to discuss today is this incompatibility between diplomatic and algorithmic time.

These crisis are not on the horizon. They're unfolding now. Deep fake videos featuring public figures. Synthetic voice scams spreading across borders. Automated systems amplifying harm before

regulators can respond. And we have seen what algorithmic time does to markets. The 2010 flash crash where US markets plunged and recovered in minutes. We can see what it does to trust. Europ has

warned that organized crime is using AIdriven impersonation to scale fraud and evade detection across jurisdictions. And here is the shift that changes

everything. AI is no longer just a tool. It is a participant. N's most recent guidance described AI agent systems as capable of planning and

taking autonomous actions that affect real world systems as open claw recently demonstrated. Once incidents become agentic response cannot depend on heroic improvisation.

It requires institutionalized crossborder mechanisms. So what does AI crisis diplomacy look like? three building blocks and one regional proposal.

First, for trust, build a white list for public integrity. Taiwan's 111 government SMS is a dedicated short code for official messages so citizens can instantly

verify what is real. A blue check mark for public communication. Every message shows the agency's name and the last three digits of your phone number. Proof that sender knows who you are and the

network guarantees who they are. When people trust the channel, fishing and impersonation lose steam. Every country needs its own version, a low friction verifiable trust channel that works even

in crisis. Second, for consensus, use AI to listen at scale and reward bridging, not outrage. Tools like polless simplify interaction

to agree or disagree, removing reply threats that amplify emotions. They surface bridging statements, ideas that people with opposing views still find reasonable and make them visible. The V

Taiwan process combines this with in-person dialogue, transforming polarized issues into workable policy. Tools like talk to the city push the skill of listening further with

auditability at the core. Every theme traces back to original participant statements society can verify whether the summary is faithful. In crisis diplomacy, legitimacy comes

from speed and verifiability. Third, for safety, treat AI incidents as civil defense. Deep fakes do not stop at borders. Market cascades do not either. Work on

defining and monitoring AI incidents is emerging, but we still lack the operational connective tissue for crossborder coordination. So, here is my proposal. Establish a

regional AI crisis leazison network a technical hotline for the algorithmic age. Now we do not need to start from scratch in cyber security. First provides a

global network for incident response teams. APERT offers a trusted contact framework for the Asia-Pacific. What we need is to extend that capacity to cover AI specific incidents whether by

deepening existing mandates, embedding AI expertise within the structures or establishing complimentary leazison points that plug into the networks we already have.

The goal is to ensure that when the millisecond scale crisis hits, crossber cooperation is not improvised. is ready to roll. This does not require a political alignment. It requires

technical trust. Finally, a window of opportunity. Let Asia be not just a rule taker but a supplier of safety infrastructure. This summit is the first event of this

scale hosted by the global south possible through growing recognition of the institutional power of the India digital public infrastructure. Atar and UPI are increasingly seen as

leaders.
