# Navigating the AI Regulatory Landscape: A Cross-Compliance Framework for Safety and Governance

**India AI Impact Summit 2026 ‚Äî Day 2 (2026-02-17)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 11:30 ‚Äì 12:30 |
| üìç **Venue** | Sushma Swaraj Bhawan | Nalanda Banquet |
| üìÖ **Date** | 2026-02-17 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/53CSK_Up9_A?feature=share) |

## üé§ Speakers

- Aarthi Sivanandha, AZB & Partners
- Dr. Lalit Patil, International Neurodegenerative Disorders Research Center (INDRC)  "
- Dr. Mohammed Misbahuddin, C-DAC Bangalore
- Dr. S. D. Sudarsan, C-DAC Bangalore
- Dr. Vit Dockal, International Development Research Centre (INDRC), Czech Republic
- Mr. Ramesh Naidu Laveti, C-DAC Bangalore

## ü§ù Knowledge Partners

- Indo-Pacific European Hub for Digital Partnerships (INPACE)

## üìù Summary

This session examines the evolving AI regulatory landscape through a cross-compliance lens, comparing various national frameworks for data governance, safety, and accountability. It highlights practical approaches to implementing trustworthy, safe AI, drawing on large-scale project experience. The discussion aims to support policymakers, practitioners, and institutions in aligning innovation with regulatory compliance, ethical standards, and responsible AI governance across jurisdictions.

## üîë Key Takeaways

1. This session examines the evolving AI regulatory landscape through a cross-compliance lens, comparing various national frameworks for data governance, safety, and accountability.
2. It highlights practical approaches to implementing trustworthy, safe AI, drawing on large-scale project experience.
3. The discussion aims to support policymakers, practitioners, and institutions in aligning innovation with regulatory compliance, ethical standards, and responsible AI governance across jurisdictions.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/53CSK_Up9_A/maxresdefault.jpg)](https://youtube.com/live/53CSK_Up9_A?feature=share)

---

_[‚Üê Back to Day 2 Sessions](../README.md)_


## üìù Transcript

important points from compliance perspective. Uh as I understand that uh it's it's afternoon and we all I mean uh Indians uh we all go for lunch and for this time as well. So uh okay uh and

today I'm speaking on safe AI from cross compliance perspective. Uh cross compliance here is India and EU is the perspective I am considering for this session. Uh India with uh 1.4 4 billion

people and European Union with 450 million people. So it's a one country with 4 1.4 billion people and it's a union of states countries with uh 450 million people. They have dedicated EU

AI act uh prior to innovations and prior to AI uh developments. Uh but we as a 1.4 4 billion people we don't have dedicated particular AI act it seems like okay we

we we are something okay there is a gap but it's not it's actually uh a blessing we can say it's a because uh we are focusing more on innovation than regulation presently because if we

consider EU AI act it's one of the strictest act in the world and if you consider your AI uh developments or your company to work in EU you have to comply with EU AI act with uh stricter

regulations and uh so much of compliance but uh it's actually necessary from point of view if we develop something uh it has to be very safe uh especially with the AI

so considering that uh this uh I mean it depends what's safe AI for you and what's safe AI for European Union so there are some criteria from European Union AI act that we might consider that

these are the parameters for safe AI. So it depends like it's it's solely based on risk classification. You are a company working in India. It has implications. Your users are from union

European Union. It's still you have to comply with EU act and which makes a very uh difficult for companies to companies uh to do crossber compliance. Are you okay?

Mhm. Yeah. Yeah, it's okay. So, so it start with classifying risk. If you are a company works to who wants to work in EU and makes a cross border compliance, it start with high-risk AI

system. de correct classification of systems which is high-risk AI systems limited risk minimum risk AI system. So if you know that your system is very basic or high

risk it depends and you are getting categorized to categorize for which compliance and what compliance are you you might need to consider for your AI uh even if you do this compliance that's

okay you're you you categorize under high-risk AI system such as military uh AI or AI system developed for military or something social uh uh

profiling or something. So it still fills under this uh your personal data is being shared with uh this AI system. So you need to consider that high-risk AI system or lower risk AI system

depending on it uh we the AI act is something that commisss considered very uh stricter regulations and considering that the all of the companies in Europe are demanding

uh getting extension for compliance. If we consider same with India I mean we are open we don't have regulation you know like uh every time you use what when we consider AI we mostly use uh

Gemini chat GPT and the kind of AI models which are available like okay but there are many other things that we need to consider and nobody reads their AI policies or privacy policy it's just

very common it's very obvious that we can't read their privacy policies throughout word by word so that's why if the regulation makes any system or any application or any AI

safe by its regulation or by compliance. It's it's it's that users don't have to bother about whether it's safe or not. It's regulated by some regulation which is very strict which is very uh reliant.

So it's it's better to for rely on such uh applications. So uh even if it's now we the second parameter could be um risk classification if it's high we go with the continuous risk management. Now

you you have passed the system that okay it's a high risk it's a moderate or a lower risk system you still have to continue with this classification continued for the till you are using it

and till the companies are deploying it. Now if you're a company, you're not just classified as just a company or just a AI user or AI deployer. It has classification like AI deployer, AI uh

manufacturer, AI promoter depending on it, you are making some changes into AI systems. It another categorizes you into using your trademark or using your uh system which makes it reliable that who

actually is operating this and who actually deploying it. Now as I said that the continuous risk classification gives a track record of identifying risk mitigating them and not

the just reduce those risk and for our use so that it get comply now for developing such AI in European Union or any other it's like we depends

on a data because all AI is based on data data which is being shared by us and data sets that's available on public domain or is a private. So if their data sets or these data sets are biased

somehow the results are going to be biased. Let's consider we have this data that works on um how our platelets are going to increase or decrease and if the European Union or the different race

people or our Indian race people or any other if it's something bias then actually it's going to give something different. So the authentication of this data set continuously uh is one of the

parameter that we can consider for safe AI. Another is a technical documentation. What system you are using? What detail design you are going to detail in it. Uh

and risk management uh steps along with testing methods. If you cannot explain your AI system that means you know that you cannot prove it that it is safe. Coming further is transparency to users.

Uh now we are at the end of AI summit. I guess we will become with some rules in India as well that all of users or something which is created by AI should be mentioning there like it is been

created by AI. So, so this is transparency to sis users that okay what we are looking what we are witnessing what results we are are generated by AI how they are developed and how they are

you know being what purpose it is developed for and um human oversight another parameter human oversight you develop something AI for developing a website you develop a

any application for this and you have to continuously oversight side every time you edit your prompt in the chat GPT just like putting your edit option there and you can change that that's a human

oversight anytime between the system anytime between the development if there is a human oversight that considers okay it is safer that it is not generating something different that we want it is

not if we if there is something risk we are going to mitigate in between [clears throat] so that's one of the parameter like human oversight it's not just we are going to get this AI system

completely automated every time in between we can interact and change according to us that's the human oversight that's one of the parameter it is a safe AI

it's okay it's okay it's okay it doesn't last that no problem yeah now accuracy of the AI robustness and cyber security uh you we might have used and most of us heard

chat GPT but how many of you have heard LU mobile Proton it's a Swissbased privacy uh chatbot or or GPAI general purpose AI. So sometimes we give prompt to chat

GP or any other AMI model it gives some uh accurate or some results but if we give some kind of another like lumo or any other which are developing themselves for privacy perspective and

there is no uh they sometimes give okay we are not working on it like uh we can't give something error comes out that's accuracy. So why this is important? You you're working your

health data. You're using that health data or getting some outputs from these AI deployments or AI agents and you got okay my blood pressure is just 120 but why it's showing from last day it's 180.

It's generated by AI. So accuracy is more than important. You don't want to get wrong medication by when we use AI in health sector. At INDRC we use this health data to find

a a probable solution for Alzheimer's and Parkinson disease and simultaneously cyber security. So, so every time I visit most of these public places, they have something uh

passwords like Delhi at 1 123 or something kind of right everyone. But that's was the cyber security for AI just uh just in time something uh happens like a data poisoning. uh we

have these data sets and if they are poisoned somehow to get something some results which we we are not going to believe that okay these are the data we just know the output of the data but

considering this poisonousness of the data which must have secured cyber security standards which is the one of the parameters that uh we consider for safe AI

and post market monitoring so we you deployed the AI and uh even if the postark post market monitoring is very important uh considering uh the feedback from the

users, monitor real world problems and performance and take correction action where it is required. AI system evolve and monitoring is uh ensures ongoing safety just like models we are receiving

day by day. This is uh XY Z model with another version. So these are the parameters of safe safe. There could be more parameters from AI perspective from India perspective because uh Indian laws

or Indian legal system is not very strict or lenient considering our political and personal our constitutional value. We might have more parameters add or subtract from these

parameters along with it. Now I come with now this is the first part like okay safe AI now it's a cross compliance. Cross compliance is just you're uh it's like you're working in

another country with another users for their according to their values according to their standards. So for this cross compliance it means okay we don't have to read GDPR we don't have to

read AI act it's nothing like that if it's if your company if your AI is working in any other countries uh for users we have to comply with their values we have to comply with their acts

so how can we de do these things okay is we have developed something for Indian market we have developed something for European market there could be possibilities that some of the safety

standards are different but either we have to inclusive all of these I don't we have to consider only common points but that's not the situation the situation is okay you have to comply

with the EU act then you have to comply with all of their required procedures like oh whether whatever I mentioned for high risk lower risk and according to we will get classified to it and make a

compliant and technology standards like this so so every time I mean you're physically outside of EU or any like even they are sitting the European in innovators they are sitting

in Europe opinion they have to comply with Indian AI act maybe we we might witness an AI act in upcoming years so so that's a cross compliance and this cross compliance perspective you have

something common uh safety standards that's that's the minimum that we need think and addition and substraction to that it's a collaboration between multi- states that we need to consider

in conclusion like companies operate across India and EU must comply with relevant regulation in India and each territory and more importantly they must design AI system that satisfy not just a

cross not just their territory but it's a cross compliance so as per our regulatory standard as well uh so if we understand the risk uh use good quality data keep humans involved

protect systems from misuse and monitoring performance. Uh we can create AI uh that people feel confident using and that's the our uh that's our line for this AI summit as well like welfare

all happiness for all. Yeah. I think I okay um I'm I'm undone with this uh presentation in presentation is mostly like okay uh it's the same what I mentioned I forgot to fly slides it's

cross compliance technical meaning which which is some of the gray areas some of the considering data law we have different data law now we are getting compliant with the IT act with just 606D

and uh with rules from the AI uh we are just relying all there but it's okay we are we are innovating and for that we have this sandbox box that world can consider. Okay.

Uh I think I'm summing up. Yeah. Uh thank you Lalit for discussing about uh the complaints part. Now I invite uh Vid DL Dr. Vid Dul so to discuss on uh AI safety especially with a perspective

on cross compliance uh Indo and Europe. Thank you very much. &gt;&gt; So ladies and gentlemen, good morning. I'm very happy to be here. I'm happy to

be here actually and I'm very thank you Romesh and CDC team for organizing such a important event and I also thank Indian government and his excellency prime minister Modi for

organizing AI summit which is very important and uh we are participating there also in our booth. So if you want to see check boot in expo you are welcome to come to hall six. My name is

Vid Duchal. I'm uh my background is law as well. So I think it's be session of lawyers. That's great. And let me please uh give you some onsite of our projects and how we actually are ensuring u uh

trustworthiness of AI systems. Let me speak first about Clara. Kara is a center of excellence funded by uh European Union and Czech state by more than 43 million euros and we are trying

to make a benefit of emerging technologies uh not only a IML but also large models and uh uh HPC and quantum computers as well to understand things and aspects which actually we couldn't

understand before because we didn't have these technologies in our hands obviously. when the chip was released you know 2022 I guess yeah and first quantum computer

in Europe I'm not speaking on the IBM's one but at least the academics ones they were actually installed last year so now we have these technologies in our uh in our hands and we can try to understand

very very different aspects very complex aspects multimodal nonlinear aspects in our brain within with within the scope of these technologies which Great. But we should not forget the compliance side

because if we are trying to understand the processes to understand why neurons for example are not living for more than 100 years although they are supposed to live for 100 years and why other cells

for example which are supposed to die like cancer cells are not dying. We need to understand the optimal performance of the system at the cellular level and understand more the signals signaling uh

cellular signaling pathways. Therefore, we need to use very large amount of data which means privacy which means compliance at least from the perspective of lawyers. But actually in Europe and I

believe also here in India not in not in all countries in the world but we agreed in Europe and in India the technology goes hand inhand with trustworthiness of the system.

January 22nd in Prague, we organized a official pre-summit event uh which was actually acknowledged by AI summit AI impact summit India and you might see that also first secretary of

the uh Indian embassy in Prague was visiting this event because actually it's very important to discuss how to ensure the transportiness of the systems Even in the development even in the

research as you know AI act it's applicable for um technologies which are entering the market but if you are doing errors within the innovation or bias or something you could never ensure the

trustiness and AI act compliance when you are entering the market. So we need to think about that well advance although it's not applicable at that time but it will be applicable. So we

need to think about that right away. So actually there are two main questions when you are dealing with AI systems in in healthcare at least in Europe. It's not only about AI act, it's also about

medical device regulation because AI system is a medical device or could be understood as medical device. So the first question if you are a developer of such a system serving uh the clients in

EU is whether there is the applicability of medical device regulation MDR because if it is then you need to do more paperwork. Then the question is the second question

is if it is applicable MDR then how do you set your metrics to validate the transport how do you assess the transportance what are your metrics bias

detection you know accuracy how do you how do you actually prove the regulator and notified body that uh it's it's explainable and it could be used so we actually develop and discuss and

Actually at that meeting there it was public bodies or public authorities there uh notified body the check methological institute it's it's a it's a body who's actually putting the the

approvals. So we discussed the four layer framework how to define uh this let's say process even in the research and innovations period. The first is some kind of due diligence you know

premarket validation. It's a kind of due diligence to risk risk assessment risk classification both things what actually was explained. The second is that you need to track the process the workflow

because you know there is a lot of aspects in the workflow cross-sight compliance must be assured during the whole process. So you have due diligence, you have the process workflow

assessment and then you have to assure that it's actually valid across the time because some data are longitudinal. So it means that you actually the population changes you know and and and

it should be actually validated across the time as well. And last but not least is the human- centered validation because uh you know who would be liable when errors occur you know so what would

would your physician said oh sorry I did bad medication because the system no it's always about the human and uh you might not know but in some cases we go even beyond the AI act with respect to

ethics which is called digital humanism principle. Because the statement is that that actually we are failing. We are failing because the technologies

sometimes you know um are in front of us sometimes but we cannot fail and the digital humanism principles was signed uh five years ago in Poisdorf in Austria also by the Czech Republic and and it

was acknowledged not only by European Commission as a driving principle but also UN United Nations. So it's not an act, it's not a regulation, it's an ethical princip.

Uh big data needs big processing capacities. So I'm very happy to announce that very soon May 1st we will launch AI factory um AI factory project.

It's 40 million uh investment 40 million euros investment for three years to support AI services in Europe. So imagine you have a spin-off company or a startup

and you need AI service some maybe experts from university uh that will help you with maybe validate your language model and you need robust processing uh robust computational

power. You just call us you go and you get it. Of course you there must be it must be a good success story. It doesn't it's not for all but for great ideas but you basically receive this service for

free. IMDRC uh is responsible to provide to be the entry point for AI services in healthcare and biotech and Czech

technical university which is my second affiliation is for uh advanced manufacturing industrial AI services. Moreover, uh, INDRC with with kind help of LIT is

actually the entry point for startup support, business development and legal and ethical uh, um, services. So one question to you, how you know innovators from India could have access

to these services? You know European Union and India signed the landmark agreement, the trade agreement just three weeks ago

and we need to implement it. We need to accelerate the business and we have tools to support it. And this is my last slide. Just yesterday

you see in our booth in expo hall six under the opiece of her excellency Madame Jigua her excellency ambassador of the Czech Republic to India. We signed the contract actually and

established the Indian company to support Indian innovators to come to Europe to support them to scale up the business through the Zelen impact accelerator program. Maybe you don't

know Zen, the city of Zelen. It's some 300 kilometers far from Prague. But you should you definitely know Bacha Bacha he established a great business

some 100 years ago and it was in Zelen he was born there actually so it's a it's the city of his family and uh we think that it's a great uh opportunity

for Indian startups Indian innovators to establish a company in the Czech Republic or subsidiary company in the Czech Republic or receive a special support from the Zelen

regional government uh uh to to receive accommodation for free for one year to support the visa process and moreover with venture capital fund we are

collaborating these companies can after due diligence and if they are if they win they can get the support up to ‚Ç¨100,000 for preede activities and up to‚Ç¨1

million for seed activities. So imagine you get for the program it's open now you apply you are successful we want successful stories you enter Czech Republic establish a company get the

funding there get the support there you scale up and then after three years maybe you come back to India for IPO and get rich and that's the partnership for everyone on mutual benefits and

interests and this is the partnership I believe which is full of trust between Czech Republic, Europe and Harat. Thank you very much. [applause] &gt;&gt; So now I uh invite welcome Dr. SD

Sudson, executive director uh Cedak Bangalore. So thank you very much sir for coming to the event. Namaste and uh good afternoon to all of

you. So today we have assembled here on a very specific topic that is directly impacting uh the adoption of AI across the globe

for the welfare of all happiness of all. The welfare and happiness that we look at essentially happens when we trust the system any system.

So the trust is built on few fundamental things. One of them is partnership and collaboration. So that is the reason we thought under the

uh inpace umbrella under which five different Asian countries and European Union all the 27 countries together. So more than 30 countries as

single consortia is working towards making the digital society a reality in the right way. So this is the basic collaboration

partnership agreement part like any system has three aspect a also is not different the legal the moral the ethical. So when we look at that

we need to look at a framework of governance that is acceptable across. That is where this joint framework of people working together and knowing each other through mutual

engagement which will fundamentally reshape humanity as a society. So whether it is the uh horizon call that is going to come up in the next

month or the deal the Indo-U trade deal we signed very recently. So all of them are looking at making a digital society a reality. But to make it a reality, it

cannot happen without AI support because we would like humans to engage in activities at a higher level than routine jobs or as they call blue collar kind of jobs

to be done by the AI. So humans can live for a higher purpose in life. So it is not about losing jobs but we create a framework of governance and uh

compliance for a trusted society. And when we do really that what happens is we live our life as humans. The A framework fundamentally is

throwing a challenge to the traditional uh software stack that we have been used to where there is a set of known requirements and you test against the requirements and we say it is pass fail

or you know there are some acceptable limitations we roll it out but with AI particularly the geni of of the like the system is continuously learning and adopting.

So the traditional requirements versus testing is not going to work because uh the functionality is continuously evolving and changing. So suddenly we are looking at a

framework uh which requires complaint based on some prescriptive analytics rather than uh standard complaints of dos and don'ts, SAT, acceptant testing and so

on. This also brings us together to a new era of uh transdisciplinary work. Suddenly engineering, science and arts are no different. They need to talk

to each other. We need a model that is based on sound mathematics. We need a user interface that is based on communication that is essentially humanities and art subject.

But the implementation is core technical which is engineering and in the end the wellness and welfare is related to healthcare. the ergonomics,

the strain and stress on the eyes when we look at the blue screen and the reinforcement kind of learning which can actually create a larger bias which is one of the challenges in AI

which means we are looking at even psychology and uh that kind of subjects. So suddenly AI is integrating all the disciplines under one number law

which is also very good because when we combine all this suddenly we see there is a need for natural intelligence to be in action to make the artificial intelligence a

reality that is legally and morally acceptable. So the purpose of today's gathering u uh thanks to it u the idea was conceptualized when we met in September

in uh Singapore for the inpace annual event uh thanks to Arty and Lalit. So we immediately thought it is very important to present to the larger audience so that such a framework governance

framework can be evolved. This requires support of people from all walks of life and all sections of the scientific or the STEM and whatever communities you

call it arts, society, social all of them. So it's a kind of a I will not say 360¬∞ because it is a 2D affair. Uh A is about tensor which is multi-dimensional. So we are going beyond 2D. So evolving a

framework of governance which will ensure the AI while it is adaptive it functions within the human societal guardrails which will make anyone and everyone

accept the decisions of AI because it is based on sound principles and the actions are transparent. In the end we live happily ever after through that

governance and we do collaborate beyond this uh country and continent level engagements through standards bodies as well. So we have uh engagements with academic universities

standardization bodies like ITLE eiat ISO all of them but the standardization happens only when the dialogues are done and we reach a consensus through inpace we have been

organizing several workshops across the globe to discuss such a digital societal framework and the policy implications. And it is great that not only it is multi-country

but it is also multi-dimensional. So you can see the industry here, you see the uh federal agencies here, you have seen the academia. So all of them are under one roof. We are sitting across the

table to decide. So I don't mean to take larger time than uh this and uh glad I could make it before the session ended from Barat Pandom to here and and uh uh I think I had to shuttle back again uh

thanks to the schedule uh being like that. Uh but thank you for joining us and special thanks for flying in for this event uh with a short notice because uh anybody planning a

international trip in less than four months through approval processes we know how it is. Uh thanks V, thanks Arti and Lilith and uh Romesh for coordinating and uh all of you are

welcome to join hands uh both for the inace work the horizon call and just the Czech Republic what the open invitation given by VIT and within the country Sedak is looking

for collaboration under the India umbrella under the national supercomputing mission under the India a semiconductor mission. All of them we national quantum mission and so on. All

of them are very strongly involved in partnership of private, public and others. Um so welcome to reach out to us and uh though not present we also have IIT Gojhati as one of the key partners

from the Indian side and uh you will hear them on the 20th we have another session uh evening 4:30 Barat Pandapam. So there we'll also hear from the IAT goati side and a few more colleagues who

will join because we had to make sure everybody gets a chance within the available slots. So do do think over we have two more days 20th we can actually chart out the next course of action once

you make up your mind. We are looking for collaboration. Uh thank you so much for the opportunity. Hand it back to Ramiskar. Thank you so much sir for your wonderful

insights from your vast experience in AI and uh digital governance. Now I call upon Arti Sadhanandanda ma'am. So for providing us insights about uh AI and data governance in India also focusing

on DPDP act and how it can comply with the uh European acts like GDPR. So please take over ma'am. So, uh good afternoon everyone. Uh standing between you and lunch and law.

Uh I'll try to keep this brief and really uh would appreciate a very participative audience and uh happy to take questions you know in the uh middle of my talk or anytime a thought crosses

your mind. So feel free to uh you know just raise your hand while I'm talking. Um and you know my previous speakers have uh really laid out a lot around healthcare and you know international

cooperation that India has uh you know seed has done a great job of telling you you know what they've been doing and so there you can see there's a real passion to do stuff right across countries and

you see this actually across the globe uh we've had uh conferences organized you know pri prior to this around AI obviously at the Bletchley Park which started in 23

Then you have the soul conference, you have the Paris conference and now you know India is trying to organize a global conference at our level. So I think from a navigating regulatory

policies, challenges, safeties and governance. U the first question is why do we need to regulate it at all? Right? And I think it's a very important question to understand because several

countries have taken several approaches to it and the most prominent ones of course is um you know US has a very light touch sort of independent sectoral regulation um and they and they're going

full scale to say let's just you know let let innovation and market forces drive how our AI tools are developed and how our consumers work to that. um and what they famously call the AI race,

right? I don't know what the finish line is, but there is a race and China, no one really clearly knows what's happening there. Of course, we know deep sea came out of there a couple of months

ago. So, we see there's a lot of uh model architecture and advances that's happening there. And then you have the BRICS nations and uh India leading a very separate and a charter for their

own people because of the diversity of languages because our needs are so diverse and our access to some of the basic technology that really propels AI is also very different. Um and then you

have other countries you know um so as a part of ACB and partners which is a national law firm uh I also head our Taiwan desk so which means that I have some exposure to what's going on on the

east side as well. So South Korea um Taiwan and that region right Japan. So they are planning you know their own approach to how they uh will regulate and govern safety and uh of AI models as

well as tools right and then you have the European Union who was the leader of the pack and I call it leader of the pack because they were one of the first um you know sets of people to really

look at it from a citizens point of view and look at it as saying you know there are fundamental rights for our citizens and then there's technology that's developing at an accelerated pace. So

what does this technology really mean? And remember most of this technologies right now coming out of the US right and everyone else is you know uh keeping pace or trying to trying to keep pace.

Europe has you know a couple of uh lead players including Misral. Um India is still you know looking to see who's our player but uh matey has done a great job of seeing you know how they can um uh

invest through compute and non-comput resources into selected startups and see how that's going to uh really take off. So you can see all these disperate attempts um that's all converging at

these summits to for everyone to get together and see if we can answer some of these big questions and uh geopolitics is not completely divorced from what we're doing. So a

large part of the conversation is geopolitics and how different countries um for one a better word weaponize regulation to make sure that they are staying ahead uh not staying ahead of

any particular race that's determined by a single country but how they can keep pace for their own people so they're not left behind in uh whatever is happening right in respect to access to these kind

of uh public goods. AI is literally a public good now and India's had great success with uh the India stack and having built the payment architecture through which 1.4 4 billion people can

access financial inclusion right um of course we rely on phone pay um you know G pay Amazon pays all of these various platforms but they build it on the roadway or the railway that India built

for it and they're trying to see if we can do that in AI by saying what's our foundational AI model so that's a little bit of history I think uh we should or rather background we should keep in mind

into why our policy making is going to be quite different and why our regulations are different and what is a challenge of our policy makers right um and so I will you know touch upon uh a

few aspects and during this time you know feel free to ask me uh a spate of regulations that have come out uh that may have caught your attention but why is it important right uh so the risks

from AI models is largely into three categories um and that's why we're talking about safety so when you talk about safety safety from what like what are we

guarding ourselves from? So it's important to understand what that risk is and what that risk is is um so bifocated or triocated into um AI models that essentially malfunction right uh or

just misuse and we know misuse we see it all the time um and we see also systemic risks in how these models are being developed. So this is the risk we're talking about, right? Malfunction,

misuse, and then systemic risk. And we're trying to cover all of that. &gt;&gt; Um, what do I mean by malfunction? &gt;&gt; Okay. Need my picture before I make my point. So

&gt;&gt; Okay. Sure. Um so at any case so we're talking about misuse, malfunction and uh systemic risks. So um you know the more and more

AI tools have become accessible to the general public uh the lower the barriers have become to really use it, deploy it. We've had you know issues of deep fakes, non-consensual uh imagery of people

that's getting floated everywhere. And this obviously has the effect or impact of influencing people and behaviors of people. It can be you know influence in uh elections. It can be influenced in uh

health data. It can be influenced in perception of how you look at uh data or or even you know how you form opinions. So some of the surveys of chart GPD said that the largest use for how they use

chart GPD and India has like 70 million users for any of these platforms is largely from you know asking it advice uh asking chart GPD or any of these models for advice. It was for writing

and asking it for suggestions for ways to do things. So we're not really using it for any kind of fundamental you know um as yet proven productivity gains at least in India for enterprise AI

deployment and so what are we really using it for and what is the risk uh associated with that. So what is the challenge for a policy maker in India? So we know what the problem is uh we

know it probably needs fixing or it does need fixing. So how do you fix it right? What is the challenge of the policy maker? So um any any guesses what's the challenge in making the law? We all say

let's make laws for you know let's stop uh uh doing this let's stop the deep fakes let's stop uh influencing people's behavior so much um let's stop uh ch children access that's a big uh question

now as well. So what is a challenge you think of a policy maker when you try to make a law for any of these things? &gt;&gt; Any wild guesses? consensus.

&gt;&gt; Consensus. &gt;&gt; Okay. So, object definition, what are you trying to really tackle? &gt;&gt; Yeah. &gt;&gt; Can you just speak up?

&gt;&gt; Law should fit to all like you said, we have children, we have education, we have different sectors. So, you have to make something which is generalized for everyone.

&gt;&gt; Okay. Yeah. Object fit for all purpose. Also ma'am uh I think while making laws the policy makers always think that we have to keep up with the innovation and growth at the same time we have to think

about individual and society at large. Yeah. Yeah. So you want to fast track development but at the same time you want to balance uh priorities for your people as well. Yeah.

&gt;&gt; It can kill innovation. &gt;&gt; Innovation. &gt;&gt; Yeah. It can kill it. &gt;&gt; Okay. So are you saying like no regulation at all because you don't want

to kill? &gt;&gt; Not really but uh the challenge is that how to make it so that we don't kill innovation. &gt;&gt; Okay. What is like the perfect recipe?

So it's not overcooked or undercooked. &gt;&gt; Okay. &gt;&gt; And the practicality of implementation it looks great in theory you can come up with laws but how do you implement it?

But what I feel like it has to be very simple because India by its magnitude of people using data today, consuming data today, you need to keep it simple so that uh uh they understand.

&gt;&gt; Okay. So we have a great policy makers here. We all you know we collectively came up with so many reasons right and so you know like in larger forums and countries that are thinking about it.

This is true. All of these are being taken into consideration. But one of the most critical aspects of why we are not really able to make you know really robust laws and there's no you know like

litmus test for countries to say hey this is how you fix it and the reason why you can't do that is because we have under reporting. Okay. So where is the data to say yeah okay children right the

lady back there said that uh you know you said it has to be practical and then so where is the data to say these many deep fakes were generated and or you know these many children were affected

or this particular use case is prohibited and then how many people or who is the content creator for all of this are you able to bring down to who the so the first problem is when you

make a law is the data Right. Uh we have completely uh under reportported and inaccessible systemic data. That's number one. Second is um what what is the um you know sort of detection tool.

Okay. So do we somebody said right like the innovation is going so fast it's all fast-paced but you still want to keep balance. So when you do that how do you figure out you know what are the tools

as the technology keeps advancing. you know law always lags a little bit behind because you're trying to catch up to what's happening. So what's the kind of tools I use to detect that something has

happened right? So who's investing in those tools the AI safety tools. So that's the topic. So what's what's the safety predicative and who's developing these tools? Who's funding it? Right?

Because this takes uh time, money and objective. Uh so being very clear on that uh the gentleman in the first said you know consensus you know objective what what are we actually doing? So that

is the second one on AI tools. And then the third one is you want to keep pace. Somebody said that already also. You want to keep pace with what's going on and not left behind. And then um if if

all of this can be deployed, can you then really um say that we have the basis to build uh India view or an India policy on saying how it should be regulated? Right? And what's really the

next challenge is that you know as these models scale the persuasiveness of these models also accelerate very highly and what I mean by that is that you know uh and I'm just taking an example right

like a Grock or a claude from its first version to its fifth version it's trained on more and more compute uh it's trained on obviously the transform architecture or any of the newer models

And what it happens is that with that increased training and the compute power and the horsepower that's gone behind it, it's become so much more capable of persuading its user to tell you what

it's saying is the holy truth. So there are so few people who question what's being said, right? So when that's going on, &gt;&gt; even when you have a policy that says,

you know, this is not transparent, users be like, that's fine. Let me still use the model. Right? So can you actually stop user behavior uh or are we just mouthing you know these words of

transparency or safety or fairness. So what really goes behind you know the seven sutras the Indian government came out with that as a light touch voluntary governance guideline. So with that what

goes behind it? How do you actually you know determine those things? Um and you've all heard about AI agents. Um so who so you know how do you uh discriminate liability between the AI

agents and in multi- aent model architecture can one agent just move the blame to another agent and how do you actually affix liability over here so these are questions that's already so

the questions are getting formed faster than the answers right so that's really where we are over there and when the AI system fails right and what I mean by failure the most simple case of failure

is hallucination But when you have other cases of failure like basic reasoning, right? Uh you could have that you could have um failure in the kind of tools you use

because a lot of the third party tools uh are being used on already created models. So one minute everybody over time. So how do how do you do that right? Um with that so how do you affix

uh liability? So the policy needs to also you know take that into consideration. Um and so you know I would in summation uh because we're running out of time a bit is to really

think what is India's policy uh for safety and governance. We had a policy uh when we became a software giant right we made a policy saying we will not patent softwares and so somewhere that

led to multiplication of companies their models of how software get exported to the rest of the world our models of creating value uh over here so we had a policy for pharma we had a policy for uh

you know manufacturing we had a policy for industries we had a policy so what is India's AI policy right what is the basis for that. What's the safety and governance aspect of that? Um, so we'll

just have to remember that we have information asymmetry between the models and what users are really coming up with. That's number one. Second is we've spoken about this need to be fast uh

with everything. So it's a market possibly a market risk or a market failure where you're trying to keep pace but the risks don't have enough time to get wetted out. Um and so you have to

account for that. Third is institutional design right uh any institution what's really the design um formula there what's what I call the evidence dilemma and um in February the international AI

safety report was published and it's a superb document if you've not read it before it's a little uh heavy at about 200 pages but it's one of the best uh documents that's been put out by the

international community uh it's not prescriptive in what companies or countries should do but it's very enlightening on what ought to be considered when you make these. And uh

last is of course you know coordination challenges. We have so many ministries looking at so many things. So interministerial cooperation and also at a countrywide uh cooperation because

it's not just uh geographical barriers now it's technology barriers. So how do you go through all of that? And um and lastly, you know what what the report also calls as a defense in depth

strategy. How do we defend ourselves against all of this onslaught of technology? There's no like a single answer to it. But it really means that you um bring together a combination of

technological, organizational, societal, you know, um institutions coming together and that's really what the summit is about. um you bring a whole range of measures and um AI models

in the broader ecosystem will obviously evolve and keep evolving like everyone has said uh but policy makers will step up on how governance and safety need to work. Thank you.

&gt;&gt; Thank you ma'am. &gt;&gt; So thank you so much uh at ma'am. So it's very insightful and lot of areas regarding policy and governance are covered. So since time is very short, we

can take quick one or two questions very fast. &gt;&gt; Hi there, I'm Mang U. I lead digital investigations and data governance at JS Held. Um

sorry &gt;&gt; at JS Held. Uh it's a consultancy firm out of the US. Um so I I kind of wrote down this question. Uh firstly, thank you so much for the talk. It's it was

very insightful. Um this question I had on the liability gap between the frameworks in the EU and India. Obviously the EU AI act places quite heavy conformity obligations on the

provider of the models. Uh but under India's DPDPA u the full liability pretty much sits with the uh data fiduciary. And uh I'm just thinking of the nightmare scenario where you have an

Indian bank that deploys a compliant say European high-risisk model and uh it starts hallucinating and leaks personally identifiable information. Uh the provider says we met EU standards

and the Indian regulator says well no you failed DPDPA due diligence. Uh now how can an Indian director sign a compliance certificate when they technically can't audit the black box

that they're renting in this scenario? Um or &gt;&gt; sorry which is the black box? &gt;&gt; That would be the the the the AI model that they've taken from say the

U the EU uh something that's EU compliant LLM model from you know from from uh let's say from from Czech Republic for that matter. And I guess the other point is are you

seeing contracts um that successfully force forensic access onto the providers because um or or are Indian boards basically just simply inheriting this blackbox liability at the moment.

&gt;&gt; Uh due to time is ma'am I I would request to close it. Actually another session is planned 12:30 team is waiting outside. So uh we can close one question and remaining we can complete in

entrance area. Uh it will be available ma'am. &gt;&gt; Um am I audible? &gt;&gt; Okay. So I'm not sure it's an Indian

board problem. I think it's a board everywhere problem. Uh especially the US. Uh and I think uh governance is you know like I said it's uh each company's approach to it. Um and from an India

board perspective, I think the boards have gotten really smart about how they do this. Uh they definitely don't take blanket acceptances on EU compliance. Uh it has to comply with DPDBA. If the

companies in India, the only exception is if you're not processing, you know, Indian people's data like the data principle is not Indian, then it doesn't matter, right? But if the data principle

is Indian, of course, it matters and it does not matter what system or tool from where you're using. So to that extent uh I don't personally buy the blackbox principle and I think you've got to uh

go with the stick liability standard over there. Yeah. &gt;&gt; Thank you ma'am. I think yeah please. &gt;&gt; Yes. &gt;&gt; And no we're not seeing contracts on

that. &gt;&gt; Hello. Uh so for as for banks uh uh is banks are regulated by RBI which has like a uh service legal agreements and it's like a

uh mandatory compliance for it is just uh recently they come up with some updated uh guidelines by RBS is like &gt;&gt; yeah yes yeah so I think you can look for it so to get it compliant. Yes.

&gt;&gt; Yeah. Uh so in the interest of times I thank all the participants for being here physically and we have lot of thousands of online participants you are watching on online YouTube live. Uh this

is just a beginning. So with this uh workshop we would look forward all of [clears throat] you to participate in uh developing a cross compliance framework across the countries. Uh so we have the

details of all the participants. We will circulate our action plan. So the call for action will be circulated to all of you and all are you welcome to participate in this noble cause and let

us make the AI global. Thank you so much Jind. &gt;&gt; Thank you all for joining the session. Your exit is in back side of right hand side comment.

You have something.
