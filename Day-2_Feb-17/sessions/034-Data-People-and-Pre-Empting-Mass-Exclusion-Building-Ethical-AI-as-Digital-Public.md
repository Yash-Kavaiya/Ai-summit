# Data, People, and Pre-Empting Mass Exclusion: Building Ethical AI as Digital Public Infrastructure

**India AI Impact Summit 2026 ‚Äî Day 2 (2026-02-17)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 10:30 ‚Äì 11:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 8 |
| üìÖ **Date** | 2026-02-17 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/oeJ9FFoWMT0?feature=share) |

## üé§ Speakers

- Arpita Kanjilal, Digital Empowerment Foundation
- Dr. Bhavani Rao R, UNESCO W4EAISAC
- Jayesh Ranjan, IAS, Government of Telangana
- Nicolas Miailhe, AI Safety Connect
- Osama Manzar, Digital Empowerment Foundation
- Paola Galvez Callirgos, Globethics
- Ram Papatla, APAC Trust and Safety, Google
- Ram Papatla, Google India

## ü§ù Knowledge Partners

- Digital Empowerment Foundation

## üìù Summary

This is a focused session examining how AI in public services can unintentionally exclude vulnerable communities and how such risks can be anticipated and mitigated. Anchored in community-centric design and inclusive data governance, the discussion will connect grassroots realities with ethics, policy, and safety perspectives, positioning AI as a transparent, accountable, and people-first Digital Public Infrastructure that enables social empowerment rather than mass exclusion.

## üîë Key Takeaways

1. This is a focused session examining how AI in public services can unintentionally exclude vulnerable communities and how such risks can be anticipated and mitigated.
2. Anchored in community-centric design and inclusive data governance, the discussion will connect grassroots realities with ethics, policy, and safety perspectives, positioning AI as a transparent, accountable, and people-first Digital Public Infrastructure that enables social empowerment rather than mass exclusion.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/oeJ9FFoWMT0/maxresdefault.jpg)](https://youtube.com/live/oeJ9FFoWMT0?feature=share)

---

_[‚Üê Back to Day 2 Sessions](../README.md)_


## üìù Transcript

Let's give ourselves a minute to settle down. 30 seconds. Can I request everyone to sit please? who recorded my audio.

&gt;&gt; Okay, that's a good note to start today's session. Uh hello a very good morning to each one of you who is here early in the morning uh going through all the cues I'm sure uh from your

respective gates. Uh thank you so much for coming. This session is organized by digital empowerment foundation. It's a Delhi based uh nonprofit civil society organization in partnership with Globe

Ethics. our uh dear partners uh from Geneva and AI safety connect our dear partners from Paris and global yes yes so from the global governance uh uh networks and uh we have UNESCO's women

for ethical AI South Asia chapter as our inclusion and equity partner uh so it's our great privilege to have all of you here and our esteemed panelists um Jes sir is not here yet because there was

some medical emergency in his flight because of which he just landed in Delhi. So he is on his way. I hope he makes it by the end of this session. But meanwhile I think we should uh start our

conversation today. As you would all know our panel discussion is titled data people and preempting mass exclusion building ethical AI as digital public infrastructure.

This session will explore the risks of algorithmic exclusion that could potentially perpetrate perpetuate uh in welfare, healthcare, education, finance and governance and examine community

centered frameworks anchored in digital empowerment foundations grassroots work in India with 2,400 plus community information resource centers. We have now been

working with department of telecommunications, government of India to actually build uh implement a project called the Samrid Gram pilot which is basically a very good example of what a

DPI enabled village can look like in India. So um our conversation today it's going to be very multi-to stakeholder because we have our panelists from different sectors. So everyone will come

in with their insights of how they approach the question of DPI right and then can we arrive at a definition or a framework for a people centric DPI. So that's the objective of today's session

and um I am very honored to introduce our panelists. Um uh Osama Manzer uh he's our founder and director of digital empowerment foundation. Nicholas he's the co-founder of AI safety connect.

Paola AI ethics manager at Globe Ethics. Dr. Bhavani Rao, professor at Amritita University, also co-chair of UNESCO's women for ethical AI South Asia chapter and Ram Papetla, managing director uh

Asia-Pacific region trust and safety Google India. So we welcome all of you to please join us on the stage and Jesh Ranjan S who's uh uh uh like he's is important to talk about uh I hope he

reaches uh very soon. He is special chief secretary uh to chief minister's office in government of Telangana and has been very uh instrumental in in building Telangana's IT infrastructure.

So yes, thank you so much. Um so today's discussion is basically like uh divided into three segments. So to start with like what are those key framing points that shape our

discussion right one that DPI and AI enabled DPI is not a neutral infrastructure because it actively shapes access el eligibility and voice in the digital ecosystem

and exclusion today is algorithmic exclusion today is silent. There are many hidden costs to today's digital exclusion and very hard to contest because most of the systems that are

being designed are systems that we can't see, we can't you know uh debate or we can't contest as citizens, right? So and DF's grassroots evidence has always uh you know given us this learning that

exclusion often harms before harm is visible is tangible is measurable right so that is where we begin right so through this panel discussion we'll attempt to address if exclusion is

increasively preemptive and invisible and where should accountability sit in data design deployment or governance Right. So here we have three segments. First segment we want to understand

whether and where exclusion can potentially enter the AI DPI stack. Right? Segment two will look at how AI can be governed as public And third segment will be focusing on

how can we ensure inclusion by design. So that means like how do we locate gender in design? How do we build trust by design and what is industry's responsibility which houses the public

infrastructure in today's AI ecosystem. So in the segment one we start with Osama. Um Osama I I'll be asking questions from here. So uh this will be easier for me with my iPad. Yes. Little

bit. So uh Osama uh the first question to you is from villages and community centers. Where exactly are people falling out of AI enabled public systems today? And what is your understanding of

what a peoplecentric DPI can look like? Ia is [clears throat] having some grudge with me I guess. [laughter] Uh I always speak at the end because I make my notes during the session. So uh uh but anyway

so I I I would like to just simply uh do do I have three four minutes? &gt;&gt; Four minutes. &gt;&gt; Four minutes. Okay. So uh you know uh we uh I I want to take all of you to uh

you know um away from the hype AI and AI can change the world or digital can change the world and all that because I have seen all this in the last 30 years you know how do came and went and how

mobile came and created hype and didn't really put up to uh you know the expectation of the people it's very simple the masses live in far away places simple you know the people who

want to serve whether Google wants to serve technology want to serve anybody wants to serve they all live far away means they are not equipped with any kind of big infrastructure

neither it's road nor water or anything right now digital is saying that I want to become your infrastructure digital is saying I want to become your infrastructure so that everybody can

reach you means the mass who live away who who is saying this. The corporates are saying this, the government is saying this, right? So number one is that what is this digital? That means

you have to be replaceable by road off the road, water, electricity, housing and everything. And all this has to become data. And all this has to become a serving infrastructure. That means the

technology or the data or the pipeline or the bandwidth has to be available to the people. so that your data and service can go. That's what the public infrastructure is. Public infrastructure

I what is public infrastructure? I get out of my house, the buses stand is right there or the metro is right there or electricity is right there, right? Water is in the pipe. But if you say

that public infrastructure is 5 kilometers away, 10 kilometers away and only available when you buy a 10,000 rupees infrastructure device, then only you can access that public

infrastructure. then it is unaffordable, inaccessible, not cheap and especially to those people who live far away. So what I feel is that we have reached to a point where

that information which is the most powerful thing that what is information that means if I am a ration seeker my authentication of my biometric should be easy even if it is required.

I should not pay 100 rupees to get authenticated then then get my rice and uh you know lentils and then go home right that should not be the situation but that is how it is now at the moment.

So digital infrastructure has to be available at the doorsteps. The question to me is that is the public digital infrastructure available to the doorsteps like a post office or a thing?

No, it is not available. It says that it is available in on the internet which is available 5 kilometers away which is available at a cost which is available in the language that I don't know which

is available after filling one hour of form in the language that I don't know that is the problem with any infrastructure especially the digital infrastructure so when AI is coming here

AI is saying AI is actually saying that we will simplify all this how will How do you simplify? I have no idea how you'll simplify unless you have the real data, trustable data,

you know, and every AI system is now extracting the data of the people rather than extracting the data that has to serve to the people. So data of the people or data for the people what do

you want? So these are the question that I'm posing and that is what at DF we work with. We are saying that digital public infrastructure has to be a affordable, available and in a format

that people can consume. Right? If AI can help, fair enough. If AI cannot help, thank you very much. We don't need AI. It's it's very very simple. And I feel

that AI is based on raw data, right? As much as data. And AI's job is believable. It has to be trustable. If it is not trustable then it is not good. Right? And it should serve people not

use people to serve themselves. I think most of the time when you see that the technology or anything is being pushed by the corporates and uh government or anybody make sure that it is going to

serve them by you and not that they are going to serve you first and that's where I think people like Niko and other people who are talking about safety ethics and all that is very

very important and therefore we need to discuss how we build the digital infrastructure with the help of AI must know that it should be available at the doorstep of the people especially in a

country like India who are unconnect where 65% people who are not meaningfully connected should be uh at the at the at the at the subject of it I can come back if if there is uh more

time uh in the second round &gt;&gt; yes um I mean I didn't have grudge but I think I wanted to set this context for the larger conversation where key stakeholders from different sectors s

will now re respond also from their vantage point on how they approach the questions of safety, trust, security, responsibility because these are the words being used today in abundance. We

are using them across sectors but which are those frameworks that define these words uh is very important. I think that is where I think our session will focus on. So here I think I'll bring in

Nicholas. Um from a global perspective, global governance perspective, most AI safety debates focus on frontier risks. Um and and what are those slow harms of AI infrastructure for welfare and

governance systems? Or like if I have to rephrase paraphrase that question, what governance mechanisms exist today that can uh you know actually stop these invisible harmful systems before

deployment. &gt;&gt; There you go. Good morning everyone. Thank you Arpita. It's always a pleasure to be back in Delhi. Uh some of you may not know that I started my journey in

digital governance from Delhi in 2009. Uh and I had the pleasure of collaborating with uh with Osama and getting to know about DEF and one of the early and earliest player in that

conversation. So you a lot of wisdom, a lot of experience uh to provide perspective and I think I agree with with most of the things that Osama has been painting as a context. I'm going to

be building upon it. I'll try and also hopefully provide a bit of constructive and useful contrast to your question and then I'll move beyond your question. I'm going try and keep it very quick and

structured. So the slow harms are the same that we have seen across the move towards a digital society, the move towards an algorithmic society with higher speed and scale is algorithmic

discrimination new. No, it's not. access to credit, access to healthare, access to education, all of that mediated by algorithms is not a new problem. It's just intensifying

with speed and scale of deployment of advanced algorithms. So the questions of who gets access and not access under which conditions, which accountability, which redress mechanisms

is gaining in intensity, but it's not new. What I think is new is the move massive move of the algorithmic society that we are building from a

social technical blackbox meaning social technical meaning it's a blackbox for people on the fringes people at the base of the so-called uh power pyramid but those at the top of the so-called power

pyramid used to understand it well rule-based algorithms um we are now moving in a society of machine learning blackbox box where the depths of the neural networks with trillions of

parameters. This upscaling of compute and data into a token society with deep black boxes has rendered these boxes black for over for even those who designed those. Neither Joshua Benjio

nor Joffrey Hinton nor Yanaka the three awardies from Alan Turing award for advancing deep neural nets into a society can tell you today that they understand how the system operates. So

we are building a civilization on top of black boxes which are gaining in power and strength in influence in capital in a way that those who design them do not understand well and those who are

engineering those systems into solutions are claiming not me they are claiming that they're advancing towards super intelligence systems and so my main fear in terms of the slow harm moving into uh

uh fast harm is that they may not want AI Osama at the fringes because AI does not serve them but we're unfortunately going to reach a point where whether you like it or not

AI will have you and there won't be much of an alternative and because of machine learning either really really really either you're at the table or you're on

the menu and the alternative is going to be Oh, I don't want to be on the menu. Yeah. Okay. But then you are moving deeper on the fringes of society. Remember I remember I don't know what's

the case in India. But when the smartphone became popular, meaning affordable, accessible, used and popular in my country, there was this debate of, oh, you don't need a phone. You can

choose not to have a phone. There's freedom. Where is that freedom? No phone, no work, no phone, no labor. The same is likely going to happen with those um advanced systems. Oh, you don't

need to have chbpt. Do you get the pattern I'm going out there? And so those who are in a situation to refuse it either because of political uh resource that they want to

mobilize against that AI that they don't want either in a national project with a sovereign push or because their community are saying no to the to the deal, no to the social contracts being

pushed on them or not. The price to pay to remain out is going to get super high because those who have access to super intelligent or very capable intelligence systems to plan reason and and create

value in the service economy, India is a service economy. So those who are creating value in that economy uh are going to get too much of the pie. And so what I'm what I'm worried about is that

really now is not the time to be left behind and left out, but the price to pay to be in and the value system we have to trade in and abandon if not sacrifice. The deal to me is looking as

a very bad deal for the values and the hopes and the dreams that my grandmother and my grandfather have given to me when I grew up. Thank you Nicholas. I think uh it almost

feels like a catch22 situation uh because uh absolutely right that uh see when we talk about safety trust security uh for a citizen an ordinary citizen who is giving away their data they have no

idea where the data is going for what purpose it is being used where is it being stored will it be used for their social welfare or for commercial interests We all know as citizens uh

sitting in this room that even the elite classes are unable to grasp what's happening in the AI ecosystem as citizens and then we talking about scores of masses of citizens who are uh

deprived of lot of resources uh in India in global south globally. So um this it is a catch22 situation because for AI to thrive we need algorithms for algorithms to thrive we need data sets and datas

data sets from communities. So the question is how do we ensure a safe AI a secure trustable AI where communities are able to share their data not give their data it's not extractive by nature

but is more co co-created co-developed uh that we ensure that citizens have autonomy and agency in the ecosystem rather just being passive recipients chasing what's happening you know this

otherwise feels like a forced digital transition that we are going through and here I think I will bring pa uh on the question of ethical AI. So um ethical AI pa these principles do exist right but

why do they fail to protect communities in practice where is the implementation gap and how do you see that because you do have a network of uh organizations who are uh in the global south and you

have also authored a report where you have uh in detail looked at how civil society organizations who are working with communities are fairing in this ecosystem. Thank you. Yeah,

&gt;&gt; thank you Arita. Good morning everyone. Um it is a pleasure to be with you and on behalf of Globetics is it's it's not Are you listening to me? &gt;&gt; Yeah. [laughter]

&gt;&gt; Well, I was just uh thanking uh the detail empowerment foundation for inviting us uh for globetics. It's really an honor to coorganize this session because this topic is really

timely. Um I am Peruvian. Um so I'm trying to bring the voice also from the global south here in particular. I am I feel very welcomed by India. So thanks so much to the host because and I do

believe that this AI global summit will and has all the potential to bring a difference because uh we've been working on you know bles safety and se as well last year in in France on action and

this year on impact. That's where we were the impact whether it is bringing impact or not. Are we leaving behind? So this this conversation is is timely. Thanks so much. Uh and as I said I'm

Peruvian. So I'd like to share with you a story that happened in 2019 uh in Peru. A rural woman was uh denied her government subsidy for food uh as a biometric authentication system didn't

recognize her fingerprints. But what happened there? What we don't know is that this woman had been working for decades in agriculture and that's why the digital footprint had worn down

and Peru by that time in 2019 it had a data protection law we have a very big rhetoric on inclusion and by that time Peru had also signed the AI principles by the OECD so ethics were there right

but what failed what what happened why why this woman was left behind So what failed here in the system in my opinion is that ethics were not enforcable. What's happening there and

and for the past decade we've seen huge number of frameworks and and I myself uh work helping with implementation of the UNESCO recommendation on the ethics of AI. Um then where's the gap arpita um

with the implementation gap as you said and first of all we need to recognize that all of these um charters on ethics the ROM call uh even victimology companies have published charters on

ethics right but all these are non-binding they're no mandatory so what happens when this is not complied in fact nothing unfortunately um so I would say these

This is one of our biggest um failures I would say where is weak right um let's take the case of the Dutch uh childcare benefits um a scandal maybe you've heard about it uh some years ago the Dutch

authority used a algorithm to identify or flag fraud risk um to give some subsidies and because um this model was embedded with some discriminatory assumptions. So the data was not

completely diverse and quality up to date. Actually it recognized and falsely um put some families as of fraud risk leaving thousands of families mainly from migrant backgrounds aside. So once

again that's where we're we're failing and in DPI context this gap becomes bigger because when there is one ethical failure this becomes systemic. So DPI for me it's it's it's a wonderful

opportunity to really start um bringing people that were usually left behind but if we are not putting ethics into practice operationalizing it then we're failing. Um another point that I would

like to touch here is um power because and you mentioned it briefly uh when you introduced me when we see power asymmetries and that's our reality those power asymmetries are at the

moment shaping who builds the models who sells the defaults who benefits from this and whose data we're using. Um I was remember yesterday very proudly that in Latin America we have already

launched LATAM GPT which is a model that was made with our own languages and you know our history we have so many indigenous communities. This is a model that is still, let's say, progressing

because it has a lot to to improve, but it's at least one big step to bring our own culture and heritage into the digital world because what Nicholas was saying is true like we really need to

come to this digital era with our values and with who we are, right? Um so we if we want to have these DPI and AI solutions more ethical and really avoiding um mass exclusion we really

need to work with communities and bring them since the very beginning since the very design and for government. Do we have government uh representative here? Can you raise your hand?

&gt;&gt; Wonderful. Yes. So so this is a call for you. So this building is [laughter] uh indeed indeed and and this is a call for them to do participatory processes

right let's work with citizens and communities since the beginning and now that you mention it arita this is the publication that I did it's called path pathways to inclusion advancing the role

in civil society in AI governance you Nicolas and and in this document I discussed um with civil society from all over the I want to thank you Nicholas. That's fine. We can just put it here.

[laughter] Um what's happening with civil society from the global majority? Um how are they participating in AI governance? And in particular, there are three main

barriers that are preventing civil society for meaningful and impactful participation. What are those? First, unfortunately, we go on resources funding. um we need more funding for

civil society and academics to do more research to bring voices from the global south. Second and this is interesting even though there are networks working on there are there is a lack of you know

coordination and coherence because sometimes there are only a few that are invited to some big meetings. So how do we do to bring all the voices right? So we need to start working on more

cohesive and cooperation within us. And the third one is um knowledge and technical knowledge on artificial intelligence. And well as Nicolola said even the fathers of AI are still

understanding how these black boxes work. Imagine civil society organizations right but we are the ones that bring the voices of the one most impacted by artificial intelligence. So

we need to have that voices in the table. Um, thank you so much Arpita and I hope this is just some food for thought not for you to be negative and leave this room in an in a negative way

but we really cannot afford not to be optimistic. So let's think that yes technology is here to help to improve our societies but if we want to bridge the gap we really need to start

participating every organization that you are civil society private sector academia we have a role to play here and it's just a matter of let's start talking to each other and I think this

meeting is a good starting point thank you &gt;&gt; thank you pa that was very energizing talk uh so here I think u two points that I think PA mentioned here. Uh the

takeaways are ethics without community in the loop will only be a procedural compliance right and safety without public accountability can become a technocratic control. Now uh these are

the key takeaways right and and here I think uh when we talk about community in the loop the first problem as PA highlighted is that even civil society organizations are do not have a seat at

the table for AI governance conversations. So this is what I think DF also keeps always reiterating and always tries to occupy places where we can take our community voices through

storytelling. uh uh so we should acknowledge that together as all stakeholders that uh we should have more csos in the room then csos can bring communities in the room. There are so

many gates uh like different stages at which there are roadblocks and we have to break them all maybe step by step sometimes and maybe all of it uh like question them more critically and uh

redesign our ways of building uh frameworks definitions etc. So here uh when we talk about inclusion by design we come to the segment three where uh we will focus a little bit on the question

of gender in AI right and here Dr. Bhavani plays a very uh important role like woman for ethical AI South Asia chapter has been instrumental since last one year. She has her report today which

we are very honored to launch from our platform and you can have a look at the report uh after the session. uh uh they are the network which I am also part of is raising lot of important questions on

how automated uh AI uh audits or you know creation of data sets how they are creating lot of critical concerns for women in healthcare in hiring process and then of course like if you go deeper

70% of India is rural uh we have to look at women as a commu heterogenous community women with different marginalities, transgender women, self-identified women, there are people

from different marginalized genders, what are their experiences with the current AI ecosystem. That conversation also always have to be centered in our uh uh discussions. So here I invite Dr.

Bhavani uh with a question. Uh in AI enabled public systems, where does gender exclusion hide especially when systems claim neutrality? Please.

&gt;&gt; Good morning everyone and thank you for being here for this uh uh discussion. The thing is that uh if you listen to the the phrasing of her question, it says where does gender uh where where do

where does bias hide when systems claim to be neutral? I in all honesty, I don't think a system can ever claim to be neutral, right? Because I mean this is thanks to uh my technical colleagues.

They said that it requires some 13 billion parameters to train large language models. That kind of data we do not even have for the kind of people who are who are left. So there's no way that

any system can actually claim this neutrality. So obviously then the biases hide all over the place. And I think we have plenty of now well-known examples especially in healthcare. uh like she

was saying that you know that you know medical systems they do use male uh data to create for their pharmaceutical needs and and this is all established uh and and this is still in countries where the

data sets have actually been used to train the algorithms let alone countries like us where we haven't used and therefore it's it's uh it's it's not possible that it is not neutral so it is

definitely biased and these biases are there but also human beings are biased Um, so I'm just uh I'm just going a little bit off topic over here, but I just can't help it. Um, I tend to

sometimes give the opposite argument just because it is there in front of me. We've come through civilization. I mean, we've just been through colonialism. We've been through feudalism. We've been

facing biases all our life, all the time. The bank manager who's sitting over there is more likely not to give you a loan because you're a woman than an AI algorithm over there. Right or

wrong? Right. You can't say technology has failed its promise. It has done its bit and technology at the end of the day is only an amplifier. It is going to amplify a

good story or a bad story. technology in itself is biased because of the biases we bring to it is this is what I feel and therefore the question of of context the question of values becomes like

super critical and and I don't think as human beings we're going to shape up in such a dramatic way that suddenly we're all realized beings and we're going to throw biases out of the window and and

we're going to have this purely egalitarian community. I mean I don't think that it's going to happen in my lifetime. If it happens well and good but um

&gt;&gt; yeah One of us has to be Mahaarish Vishnu for that to happen if you ask me is that that's a joke that not not everyone will understand but forgive me for that.

Yeah, but but I I think a lot of the topics that were put out by the different speakers, whether it's in terms of uh who owns the data, the data is owned or the data is shared uh who

co-creates uh uh the the services that that that AI provides the the possibility to to to also uh manipulate or use the AI as not to be uh not to be a s its master but it you it should be

able to serve you. These are all very important concepts. I think all the critical concepts have been brought up by the speakers already and uh I and and I think these are the most important

ways to address the biases that would come out that we can actually have an eye about it. But uh in terms of the masses, if you ask the masses because I know you work with the masses and I work

with the masses, they're mostly not even aware of it. They're already their lives have been completely based on biases of others. They may not even recognize a bias when and they won't even know how

the bias affects them. It's it's those of us who who have seen those patterns and are concerned about the patterns that are making that much of a noise about it. And I think um um god I think

I've gone a little bit off topic but uh I think the most important things is that uh when you're talking about these large systems it is important to have um data sovereignty and I think Dr. Krishna

Shri is uh which is right here she is talking on on the 20th and she's publishing a white paper especially on the south Asian context on data sovereignty this becomes very critical

for us because our context is so different our ethics and the way we what is important to us is different also the way the way we say how we how do we define poverty how do we define a lot of

these things u that are intangible that are u that do not have definite and finite values these are all uh critical considerations that are very contextual to us and therefore data sovereignty is

a very important conversation when we talk about uh designing systems that are uh are somewhat neutral. Uh and and I still believe that it can never be totally neutral but at least it'll be

relevant to us. I think that would be a better way to say than than calling it neutral. uh the other thing that I would say is that uh there is so much of the population that we work with that is in

the informal sector and they data never ever gets reflected uh into into our system. So how are we uh ever going to consider neutrality uh and uh biases and and and those

conversations when we don't even know and that part of the population is actually invisible. Um so I'm I think I'm leaving with more questions than answers over here.

&gt;&gt; Yeah, absolutely. Uh and I think again like just summarizing all these conversations in the context of our larger conversation about DPI, right? Digital public infrastructure to build a

citizenric, communitycentric, peoplecentric uh DPI. um what are the steps that we should not follow to uh repeat the mistakes that are already uh at uh design level uh operating in

today's ecosystem. Here the most important uh voice I think in this panel is of a technologist. Uh and we have Ram here from Google and uh here Ram the question to you is what does responsible

and trustworthy AI mean uh uh from your vantage point uh when systems affect access to welfare identity or services and all the uh other discussions that panelists have been talking about.

&gt;&gt; Cool. Thank you Arita. I don't think I'm the most important person here just just given what I heard from the amazing panelists but super excited to be here and thank you also for our collaboration

on capacity building workshops you've been a great part of that I thought maybe I'll take a slightly different pivot for a second just to give you maybe how we build products and maybe

that'll be helpful maybe everybody is aware but I've done this for the last 25 years last 5 years at Google where most of the time we're thinking about products in a way of you know can we

obviously it's a commercial sense so we're always thinking about can we get broad distribution but if you leave the commercial sense for one second that broad distribution is all about

inclusivity right and keeping it very contextual in India that means that can we have a large language presence can we make sure that you know people can derive value in their own language so

for example Gemini and notebook LM and AI overviews are in multiple Indian languages right that's just one example of like kind of reach the second would be an example of you know how do we make

sure that beyond our AI principles that we have in testing and like making sure the algorithmic you know um you know transparency is there. How do we make sure that the society civil societies

can actually give us that stamp of approval right? So one simple example is we partner with uh center for responsible AI with IIT Madras you know where they give us that kind of feedback

to say that here are the feedback sloops you should consider as you're building for sort of Hindi language right so uh again we need this input without this input there's no way we can build

products that are loved product that are safe that are inclusive right that's just an example and there are examples beyond that right to for example if you want to make sure that hey we've built a

basic platform for education we've recently launched you know, Gemini going to help you with J prep, right? Again, that's a big deal because what we're saying is if you

don't have access to a tutor, if you have a shared device, please use Gemini to at least make sure that you can start prepping for uh, you know, J. You don't need to have, you know, a fancy, you

know, uh, you know, tutor and, you know, expensive equipment to get there. But to even get there, we worked very very closely with institutions like Vadwani AI to make sure that they can give us

that grassroots feedback on what does it mean to actually build for education. So the point of my story is really to say that we cannot do this without uh you know what the esteemed panel was talking

about earlier like we need to be one part of this entire kind of ecosystem. We're very very excited to you know learn from the ecosystem build for everyone make sure that it's inclusive.

So that's the part about product building. I think there's a second part which is about you know a lot of the conversations is about also about data and transparency and you know the impact

of algorithmic kind of you know benefits right which I agree we're seeing some of these examples and they're pretty sad to kind of hear I think that's where we need fast feedback loops we're a company

that can react to feedback the moment we have feedback we move very very fast and this is just beyond complying with like data laws and regulations in every country that we do right but if we

actually have feedback from civil society if you have feedback from you know uh educational institutes or like the healthcare industry to say that this is what we're seeing in terms of you

know uh confirmed biases this is an area where you can act very quickly and so I would encourage us to have a stronger dialogue with Google you know give us a seat at the table I mean I'd love to put

you all on a speed dial next time we're building a product to quickly check on like some of these things because this is I'm like a kid in a candy store right now so this is very very exciting for me

to be in this moment uh but yeah I think about this a little bit more optimistically and I think we can actually come together and solve for India in a much much better and a

scalable way. &gt;&gt; Thank you. That's very assuring. And uh uh on that very positive note, um uh we have 15 minutes. So we would like panelists to go through okay 13 minutes

to go through another round where I'm going to just ask uh because of lack of time one concrete change right uh or a suggestion policy level design level or governance level that would prevent

preemptive exclusion or reduce these hidden harms costs if implemented uh say in the next two years. We are looking at a DPI model for a village for to make a village smart instead of a city. Uh so

yeah one round please start with Ram. Yeah. Okay. &gt;&gt; No go ahead please. &gt;&gt; Fresh off my last talk. Yeah I think I probably I'll keep the same theme in

terms of feedback loops. The one change I would say is you know can we have stronger feedback loops and stronger connection because you know there every parts of this like you know big huge

elephant right we all can touch only a few sides of it and not realize it's even an elephant so what we really need to make sure is you know we get you know very rich you know insightful

conversations about where we are seeing these challenges and not necessarily the solutions just the problem statements and sometimes I feel like those problem statements are very hard to even you

know identify and distill Uh so I would request you know all the uh you know the panelists here and you know other civil society orgs to you know you know reach out to us give us that feedback and make

sure that we're able to build that bridge. Uh that would be my main takeaway for today. Yeah, &gt;&gt; it's hard to give only one but um let me say for governments um let's start

having multistakeholder tables so we can set what is our minimum acceptable risk right because as Dr. Dr. Rose said they're not going to be uh systems that are absolutely neutral

neutral. But we as a country, as a society, we're entitled to say, okay, this is what we accept or not, right? What's what's what's the minimum risk or or minimum um assessment that you as a

company before launching your system, you need to do because I don't think it's acceptable to harm and then say, "Oh, sorry. I will I will then fix it." because there are already hundreds of

people that h that that were harmed. This is first one. So no more um launching and especially in developing countries where we don't have the regulations. So companies just launch

and then test with us. Um and for that multistakeholderism process, participatory process key uh and second uh if our civil society here, let's work together closely more closely because

yes indeed Google organizes uh sessions open loop from meta does well. I participate sometimes when we can discuss um how technology and you know now no new functionalities that are

going to launch but sometimes and and you know those are the privileged ones that have the contact with. So how do we do to involve more people there? Right? So sometimes it's super hard to have to

have the thousand of CSOS there, but if we work closer together as communities, we can bring the voices of more people to the table. So I think let's connect more. I would say those two are

&gt;&gt; mine will be connecting to the part of the question before I didn't answer which is what can we do about this black box. So the way in which we're doing and in the daytime I lead a Paris based

startup that does frontier AI safety evaluation. So we're trying to break into the black box to understand its behavioral dynamics and so we develop behavior elicitation technologies.

There's a movement happening from Blesley Park to here which is the establishment of what we call a safety institutes and they are organization led by the public non-regulatory trying to

advance the science of understanding the behavioral dynamics and the related risks of this system started in the UK uh and spreading around the world. Obviously India is into that

conversation and matey is into that conversation. I wish one year from now is that what is right now a very fledging endeavor at the uh India AI safety institute become something much

stronger and indeed multistakeholder and indeed baked into the the the parodynamics and the economics of building products because indeed you're hearing it the industry is willing to

engage and I think he's willing to engage from a position of honesty into understanding the risk and say okay now they're feeling the competitive pressure okay and that's why we need regulation

in my opinion but but India's a specific case I'm happy with the HGI that has come out last week. we can discuss but that's why I won't put it my first my my my urgencies capacity building on AI

safety institute to bring science sai what they're doing in IIT Chennai is amazing we need academia to come to the rescue of governments to establish a pathway a strength in the science of

evaluation testing evaluation verification validation of these deep black boxes of the machine learning algorithms be large language models be vision models

And we need to move the what I need you to understand is very important. The evaluation of AI is still today despite open air clocking 50 billion uh US dollars in revenue and entropic a

clocking 15 billion revenue like as we speak is still an unsolved scientific challenge unsolved and so without that it's going to be difficult to ensure that the values that this bias

mitigation or bias redressal or bias directioning that we want per society is going to happen because we simply don't understand and so building a a local base of A safety institute networking it

with the global south not in a way which is anti-n north but baked into a global endeavor that UNESCO has been immediating is to me the urgency of the day. I'm not seeing today the capacity

that I wish is available because we're talking immense scientific challenge unsolved. &gt;&gt; Um I would uh come back to the the hand that's holding the gun and not the gun.

And I would come back to ethics and Paula said a very important thing is that you could have systems in place but who's actually enforcing ethics? Uh in a small community where it's like a

village which is the context that you set it's usually the community that does the enforcement that does the monitoring that has the oversight. So it is I think very critical for us to really empower

communities at the base on what these ethics are and how they can be responsible and then all the words like participatory and all these things get pulled in but really the

accountability for ethics and the human element of it is critical. If we do not know what values are, how are we going to know? Are are we going to say that once we train a AI and we have oversight

over AI and then AI will do the perfect thing? No, but we still need humans to know what it is. So at first we have to my and it's always going to come back to this ultimately humans have to become

ethical people and this is where I would still it is the source and that's where I would put my focus on and the other focus is that I would say that industries have to stop being extractive

no matter how inclusive industry says that they are how much of inclusion that they at the end of the day if their agenda is extractive it's not going to save the story And I think also large

institutions are still made of human beings. So valuebased, ethics-based, human orientation is probably the root solution that I would recommend. End of matter, I guess. [laughter] No

more AI. It's as simple as that. That's what you said. And I totally I I I to I I I totally agree that human in the loop has to be brought in, right? Uh yeah, but why do we need to have to brought in

for something that we want to create in any case is not trustable. So I I actually made a doodle of the virtues of AI. You know everything is virtue. You know human being you know everybody will

say that okay this guy is you know good or you know ethical and all that and virtues are uh you also use the word bias by so many times so many people will so bias inequality data mining

algorithmic manipulation prejudice exclusion hallucination environmental degradation discrimination fabrication profiling are the virtues of AI

but everybody thinks that AI can change the world. You know, everybody thinking that AI can really really really can make a difference. And why not is because

everybody's human intelligence is saying AI is more intelligent than me and we want it to become a digital public infrastructure. So imagine we ourselves are creating a system which we

are going to say is full of bias, discrimination, exclusion and everything and better be an infrastructure for the future. And who is pushing it? If you see who is pushing it, it's not God,

right? Humans are being pushed by God or a creator. But this AI is being pushed by what she said is mostly now the corporates and the government. mostly if you see and the corporate means profit

making that means your motive is to make profit out of who whose data your data and my data it's very simple so I I just want to get to the root of the thing is that this

requires 1 million ethics for developing one microscond of you know uh AI or something so much ethics is required so much if you want efficiency if you want everything and and If you want them to

become a infrastructure then it is really God saves us what will happen. So basically we are creating something that we will keep on fighting in the future you know that that's the root of the uh

discussion but just get into the f you know I I'm just want to put one minute into saying that let's come to the real brick and mort day-to-day uh issue which we are talking this is a technology

which is claiming that it wants to solve a human problem or a human efficiency problem or something like that right And more often than not uh Ram talked about feedback. Uh you know why don't we do

feed forward rather than feedback. You know feed forward means you already know what you are developing and you know enough that how you are developing rather than when you develop and push

and then you say tell us did it harm you or did it not harm you. So in that manner you know it it is very important that anything that we are building first look at exclusion as a primary uh you

know outcome. We always say inclusion inclusion inclusion digital inclusion human inclusion everything is inclusion and sounds very good and if you see as inclusion yesterday we had five people

using internet to today 10 people then tomorrow 100 the graph is always up so it's very positive inclusion at DEF the way we calculate is that anything that you create first you calculate how many

people are left behind for not using this and what is their life. That's exactly what we are saying and I'm just saying it's not negative. It's a positive way of thinking is that

whatever you are building first think how many people cannot use it whether it's a broadband whether it's an internet whether it's a digital infrastructure whether it's an antenna

whether it's a broad road whether it's a toll road whatever how many people cannot use it and if you see that number you it will always be larger than the number that people are going to use and

the people who are going to use is always will be progressive and not day one it will always be progressive And that is all I want to say in this one is that digital public

infrastructure when we are talking about we must talk that when once we make it as a nec and and I'm also saying it is not progressive because you are making it necessary because you're making

policy for example coming here is is a QR code QR code is a digital digital means it required device right so there are so many infrastructure requires to do that

rather than just human being I'm already an infrastructure. I'm already a face. I have already have different face. I have already have a unique face. I already have a unique voice. And we still need a

technology which can be deciphered. So it's it's very interesting that we must see that anything that we are developing is it excluding or not excluding on the basis of that if you make it a public

infrastructure it will always be positive. That's all uh that we are saying. That's all we are saying. And public infrastructure means it's not individual public infrastructure. It's a

public infrastructure that means things for the masses not masses for some things you wanted to getting excited &gt;&gt; uh just one thing please go ahead &gt;&gt; we are out of time the next session

organizers will request us to move out but please Ram go ahead &gt;&gt; I'll take I sorry I'm more of a case study example guys I wanted to give you one more interesting example so we've

rolled out this technology called synth ID uh because your comment about feed forward triggered me to say this synth ID is a watermarking technology ology to figure out whether it's AI slop or not

and you know the way we built it was of course the technology was rolled out but the way it came to life was to actually work with Jagrron and really make sure that the Indian newspaper community and

the media community can actually give us feedback on like the applicable usage of Indian you know media right and we learned so much from that and that also improved our global models so I think

sometimes yeah it's okay to put the technology out but we also need that kind of engagement to make it real as well and make it very local and contextual so I'm excited excited about

that. Excited the opportunity but over to you. &gt;&gt; Thank you so much. We have the launch. Please can you please hold the reports panelists and take a picture and we have

moments for the panelists and thank you so much audience for patiently uh sitting through the entire session. Uh thank you. [applause] YES.

&gt;&gt; Anywhere we have to stand. a little bit. I'll give the moment. &gt;&gt; Thank you. Over to next session organizers.

Just a moment.
