# AI and the State: Policy and Practice in Government

**India AI Impact Summit 2026 ‚Äî Day 2 (2026-02-17)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 10:30 ‚Äì 11:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 9 |
| üìÖ **Date** | 2026-02-17 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/zxNUsv9xN-Q?feature=share) |

## üé§ Speakers

- Christine Custis, Partnership on AI (PAI)
- Gaia Marcus, Ada Lovelace Institute
- Jaan Tallinn, Estonia
- Prof. Alondra Nelson, Institute for Ethics in AI at the University of Oxford
- Rumman Chowdhury, 
- Stephanie Ifayemi, Partnership on AI (PAI)

## ü§ù Knowledge Partners

- AI Policy and Governance Working Group (AIPGWG)

## üìù Summary

The panel will explore how governments can approach AI deployment differently from private sector actors, including how procurement decisions can influence market outcomes in the public interest. It will examine the role of safety research, measurement, evaluation, and community participation in supporting responsible public sector AI deployment. The discussion will also consider how red-teaming, ongoing assessment, and evaluation processes can strengthen transparency and accountability.

## üîë Key Takeaways

1. The panel will explore how governments can approach AI deployment differently from private sector actors, including how procurement decisions can influence market outcomes in the public interest.
2. It will examine the role of safety research, measurement, evaluation, and community participation in supporting responsible public sector AI deployment.
3. The discussion will also consider how red-teaming, ongoing assessment, and evaluation processes can strengthen transparency and accountability.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/zxNUsv9xN-Q/maxresdefault.jpg)](https://youtube.com/live/zxNUsv9xN-Q?feature=share)

---

_[‚Üê Back to Day 2 Sessions](../README.md)_


## üìù Transcript

Gaia Marcus is the director of the Ada Love Lace Institute. Prior to joining ADA, Gaia held senior level civil service roles in data and AI roles in the UK government as well as serving on

government reform and as head of the UK's national data strategy, having previously led on digital innovation, network analysis, and data strategy in the charity sector. Ga, will you join

us? [applause] Yan Talon is a founding engineer of Skype and Kaza. He is a co-founder of the Cambridge Center for the Study of

Existential Risk and the Future of Life Institute. Yan, will you join us? Dr. Raman Chowri is a globally recognized leader in data science and responsible AI uniquely positioned at

the intersection of industry, civil society and government. Please join us up front. Thank you. [applause] And finally, we have Stephanie If. She is the senior managing director of

public policy at the partnership on AI. Previously, Stephanie served as head of digital standards policy in the UK government, where she built a new portfolio focused on technical standards

for emerging technologies, including AI. Come on up and join us. [applause] And I am Dr. Christine Custous. I am the program manager of Professor Nelson's Science, Technology, and Social Values

Lab. Please join me in welcoming our esteemed panelists. &gt;&gt; [applause] &gt;&gt; They've asked us if we would like to if we can please stand for a picture before

we start. So, and we also would like one more microphone, please. Okay, thank you so much for joining us again. We're excited to uh talk here today about this important issue. The

first question that we have is for all of our panelists. Um we framed this panel around a core tension. Governments are both AI regulators and AI users, but you've each approached this question

from different angles. Policy development, implementation, research, entrepreneurship. So from where you sit, is that the right framing? What's the most important consideration governments

miss when they think about deploying AI systems? And I'll just start closest to me and we'll we'll go down the list. &gt;&gt; Hi, good morning everyone. So think about this question. I think

when governments deploy AI systems themselves, I think of them essentially dog fooding their regulatory system. So governments tend to be held to a higher standard than say the private sector

because people don't tend to have the choice to opt in or out of using their services and people tend to be at least in the UK where most of our research are very attached to how the state shows up

and so when you have gaps in your regulatory system you essentially find that those are exacerbated for government. So if there are liability gaps as we see, if there are gaps of

governance, if you're not governing at the foundation model layer and therefore there are risks that might be pushed downstream, government often suffers from that. And so I think when we think

of government as regulator and then government as service provider, you really there's a real tension there because they are both and if the system doesn't work for likememes, it's

definitely not going to work for government. &gt;&gt; Thank you so much, Yan. What do you think about that? &gt;&gt; Thank you. And good morning. Uh I do

think it kind of like is different uh for governments that are hosting the companies uh that are uh doing their kind of frontier AI uh development and countries that are not doing that. uh

and I think for the countries who are hosting uh there is actually even like a more general and a more intense tension which is those governments are kind of dependent on the companies increasingly

uh for the competitiveness u on both the economic military etc. Uh on the other hand they're supposed to kind of regulate and oversee uh the development. So it's like uh really tense situation.

one one way to kind of like uh see it without actually I'm a European I don't know what exactly is going on in the US uh but like you just see the kind of the incoherence uh of the decisions uh when

it comes to like export controls for example like part of the government wants to kind of limit uh access uh of the chips uh to some other competitive country or competitor countries on the

other hand they kind of yield to the what it looks like uh kind of pressure from the companies uh like Nvidia uh to actually not do that. So like they they seem to be trying doing both.

&gt;&gt; Thank you. I'm American. I don't know what's going on in our country either. So [laughter] &gt;&gt; um so one thing I think about quite a bit having worked with governments on

implementing AI systems and in particular I think this is relevant for India and any of the countries building sovereign AI is there's always insufficient investment on privacy,

security and responsible use. Um so currently there's a lot of interest in data centers, owning your own data, collecting your own data, building your own models, etc. But I see very little

to no conversation about ensuring you have the infrastructure for test and evaluation, responsible use, privacy and security. One of the most important things uh and this is what I work on

with human intelligence um public benefit corporation is that genai requires a very different type of evaluation that is quite different from machine learning. These are not

statistical models. These are probabilistic outputs. We have just scratched the surface and we don't have exact sciences around how we should be testing and evaluating. So what I would

love to see is investment from the government on developing new methodologies and making these methodologies public. So to ensure that not just government models are being

built well but that it sets the tone for how companies are building their models as well. &gt;&gt; I think that last point is so important uh in terms of uh government as a

regulator and user. I think this tension really depends on to what extent uh government is able to demonstrate best practice on both sides. So um I've been in government myself and now in civil

society and I think for me it really has to be grounded in the the principles that government has to be able to deliver uh AI deployments that are fair, trustworthy, safe and secure but also

that accountability is at the heart of those deployments too. Um and so I think for us in the work that we do, if we just take agents as an example, if governments were to deploy agents, how

do they do that in a way that is safe, secure, and trustworthy? We released a paper recently on uh real-time uh detection and monitoring around agents. And uh one of the traditional principles

in civil service when you're thinking about government deployments is uh the chain of accountability. So you really need to have a means through which you can document uh practices. So you need

to be able to uh trace back accountability in the actions that civil servants take. And so as we think about agents for example, the whole premise around agents is that they are more

autonomous and that governments can take a step back. So what does that mean when we think about principles like that chain of accountability where you're able to kind of demonstrate step-by-step

processes that can ensure that governments and and their actors are held accountable. And so uh that begs the question as to what extent agents contradict with some of the traditional

principles that we've thought about when it comes to governance um by governments. Um and so I think there are just some fundamental questions. How do governments resolve those types of

questions? Uh how do they have good monitoring infrastructure? How do they have strong documentation practices? Um, and if we're able to get those right, then maybe the tension isn't as

prevalent, but it's dependent on those good practices that they can demonstrate. And maybe the last thing I'll quickly say is I think building on Raman's point that um in governments

have a really strong role to in in demonstrating best practices and to what extent can governments also be able to build uh an ecosystem around assurance that uh really is able to show that is

built on their own best practices around assurance and can demonstrate to industry the practices that they should be adopting too. And so I think there doesn't have to be a tension if

government also leans into a kind of testing evaluation assurance ecosystem and uh can ensure that not only themselves but the practices of the the industry uh also demonstrating safety

security uh etc. Thank you. &gt;&gt; Good morning. Uh great to be uh here with you all. So I'm going to pick a thread I think from both Yan and Steph who queued me up on accountability. I

think um the tension between government as a user and also as a regulator of AI is depending particularly if you are a country that has leading labs exactly as um Yan identified is something akin to a

conflict of interest right on the one hand you have um all of this sort of financial business scale up the sort of race that's national competitiveness that's economic competition um that in

some ways mitigates against the kinds of you know guard rails and responsible kind of infrastructure that we need for AI So, it's a tension that we are certainly

dealing with in the United States that I think um won't be able to go unresolved for too long, but we'll see. Um uh and I guess I'd want to offer a couple of other ways in which it's different. I

mean, I think that, you know, citizens are not one of the things we are as consumers, but in our role as citizens, we should have other expectations for the state and its use of AI. So

including um that you can have a right to understand how the state is using AI, how decisions are reached rendering that you might not have that same expectation in the commercial sector. But if a state

is using a government, a local government, a state government is using these technologies, there's an expectation that you have a right to sort of understand the work that the

government is doing on your behalf in your name. Second, you can't change like so you might prefer, you know, Gemini to Claude or to another chatbot. So, as a consumer, you can switch between chat

bots. As a citizen, you can't really change your state in most instances, right? And so, you're really stuck with um a state that is using these tools for a certain kind of governance. Um and you

know, the state has kind of sovereign power over the decisions that it's making unless it is willing to participate in some of the democratic accountability and some of the insurance

and evaluation that Steph was talking about. And then, you know, last I would say, you know, particularly in the United States, but I think that's true of a lot of other countries as well,

we're in an incredibly low trust environment. And I think the use of of algorithms and AI and government sort of is only compounding that right now. It doesn't always have to be the case.

There are ways to, I think, develop and deploy and use AI and government that offer more transparency, more accountability um into how the systems are used. But right now, the risk in a

low trust environment of introducing into government tools that um are more obscuring of the practices of government is a real risk to democracy. It's a real risk to social cohesion. Um and uh so if

a you know if a you know a chatbot or a AI system kind of fails in the commercial sphere and this goes back to something that Gia was saying you know that's a failure mode and it's a problem

for for your brand. It's a problem in the consumer space. Um if it fails in the government side either because you know autonomous weapons are you know destroying communities or people are

being wrongly sorted into social welfare benefits etc being discriminated against the erosion to democratic societies the erosion to society is really profound and so I think that we really need to to

pay attention to that. &gt;&gt; That's amazing. We're going to come back to Alandre in just a moment but I'd like to ask Gaia a little bit about that. I know that Ada Love Lace has a lot of

evidence on government use of AI and governing with AI as part of your uh there's a report that you did. One was called learn fast and build things and just would love to hear more about the

lessons that you learned from those six years of studying AI in the public sector if you wouldn't mind sharing. So &gt;&gt; hi uh yeah thank you. So we did a really nice summary piece of work called learn

fast and build things and also looked at public perceptions of government use that looked at 32 reports over six years of uh government AI and data use in the UK and I think there are just some

really important things that we learned from it and it's been really important to sit with that analysis and think about what is useful for this audience. Um so our core tenant is as Alona was

saying you know you cannot opt out of using government. Therefore you really do you are held to a higher standard by the citizenry and you really learn a lot about what people's true preferences are

rather than what they will accept when it's a private business and they're kind of trading convenience for say lack of lack of privacy. So we see a lot of things. The first thing I think we've

seen is that at the moment lack of clear terminology is really really messing up our ability to understand what's actually happening with AI use. So something we're seeing in the UK, but I

think we're seeing everywhere is that you see examples of very narrow specific use cases of AI being used to justify investment in generative AI and used to justify roll out of generative AI which

is not actually having the same effects. And to Roman's point, it's incredibly hard for the government at the moment to really evan to evaluate and understand what's going on on the ground. And

because of the like hope and hype cycles that we're in at the moment, we find that any work to say, hey, slow down. Is this use working for you is often met as doomerism. So we recently published on

transcription tools used by social workers, which is a really delicate use case. And we find some really interesting things. Social workers love using these tools, but they do introduce

hallucinations. So this is potentially erroneous things going to care accords. We have to be able to say it is right that you want to save time. However, if we do not understand the

rate of hallucinations or how you do human the loop, you're inserting real problems into the use case. And I think what we're really seeing is because people are so attached to AI one way or

the other, it's really hard to just steer that steady ship of let's evaluate, let's understand what works, let's understand what works in real life, not just in the lab, and then

let's develop typologies and ways of rolling these tools out in such that you're not creating new risks. And that seems like a really hard conversation to have at the moment.

&gt;&gt; Absolutely. And I wanted to circle back to Alandre uh because she has uh some information about the work that she did with the White House Office of Science and Technology Policy. So, Alandre, you

you saw these tensions firsthand. You you saw the stakes that were fundamentally different when governments deploy AI versus when companies do and how commercial failure and you know as a

business problem is different from government failure which becomes a democratic accountability problem. So, what does that mean for how governments should approach deployment?

means we need regulation, I think, is the long and short of it. But but that can look um a lot of different ways. I think um it can be a commitment to to funding evidence-based

research about, you know, evaluation, transparency, things that help us to know more about the tools, the big, you know, computer science problem of explanability and what we're going to

how we're going to sort of um chip away at that problem. Um it can look like uh certainly, you know, laws uh like the the uh EU AI act or the some of the legislation that's happening in the the

UK, some of the legislation that's happening in the United States. I live um in part in New York City um and New York State where we have uh you know an an AI safety bill that was passed in the

fall. Um as well as um legislation that says you've got to be notified if algorithmic pricing is being done. So there's a kind of suite of regulation I think that can be brought to bear. Um

and I think different from uh you know our colleagues here in India. If you look at the sort of polling data on how different nation states feel about AI, um the United States, the sort of

measures are very negative about AI, often 50 to like in the sort of 50 to 70% which suggests that these are bipartisan misgivings about the about AI, not just, you know, that sort of one

group likes them and one groups doesn't. It's a a kind of majority um uh of folks who have concerns. And so, you know, people are really asking because of um problems with CESAM with young people's

mental health. There's just like a whole suite of kind of daily injuries and risks that are happening and to in exist in addition to those that are um sort of looming both in the near term and in the

distant future. Um and there was a real desire for that. So, I think that you know governments are going to have to respond. Um you know, one of the things I worked on and the Biden administration

is something called the AI bill of rights which was a a year of public deliberation that we distilled from academic researchers, from the um commercial research community and from

just stakeholders, from just you know local communities in the United States. Um principles that they thought should be um the sort of high level principles for the United States as the AI was

being built very basic things like system should be you know safe and effective kind at a very basic level and um you know it's now the case in the United States if you know the the

political a little bit that Ronda Santis, the Republican governor uh of Florida, has advanced a bill of rights for Florida. Right? So, just to give you a sense that takes up some of the

principles that were in the policy work that we did in the Biden Harris administration and expands those uh further actually. It's actually quite um interesting and good legislation that's

being proposed. So, I think you know governments actually need to pay attention to um the fact that people are becoming that these are becoming very clear political issues. I mean, another

way AI is becoming a very clear political issue is around the energy grid, is around data centers. And so, there's a now I think anchors and language for citizens to be able to, I

think, make different kinds of claims on government about how they want AI tools and systems to be used. And frankly, you know, saying that we don't want a data center here puts a choke choke point um,

you know, and the sort of, you know, in the AI ecosystem in a way that's not inconsequential. &gt;&gt; Thank you so much. I'm I'm fascinated by the afterles of the blueprint for the AI

bill of rights. It's it's extraordinary. Yan, I'm going to come back to you. Uh your work at the Future of Life Institute focuses on governing transformative technologies and

mitigating catastrophic risks. When you look at governments deploying AI across public services, from benefits administration to medical research to national security, what

failure modes concern you most? Thank you. So the the biggest kind of failure mode actually doesn't happen in my view in the application level. The way I look at it of like life cycle of

of AI of of like frontier AI. I think there are like kind of free three free frontiers where there are kind of are creating uh unprecedented situations on this planet. The the final of them is

deployment. basically like once you have uh AI that deploys and kind of uh diffuses and basically has like social effects which is clearly an important frontier but I think this is like almost

all the focus of of uh regulators uh currently I think there are two other frontiers like first frontier is where the das are being trained uh and this training regime these days is called

unsupervised learning unsupervised learning for a reason it's not supervised uh the uh so like we get like basically this random capability abilities uh and we kind of rely on lack

of capabilities for containment. So that's like it's not going to last forever. The second one I think is the most dangerous frontier. This is the internal deployment. Uh this is where

like I call it AI hits the soft tissue uh where it kind of gets access to people uh and and can kind of manipulate them and do whatever. Uh so and that the fact is that actually the most powerful

models are no longer published. Um, so they are kind of kept inside the companies and there's a very pragmatic reason for that. It's just they're too expensive to run. So like the actually

get like the most powerful models that are kept inside and then you can do distillation on that make it make it kind of the sort of like well behaved cheap models that then are published as

frontier models. So like in some ways I think the biggest failure mode that really is the lack of awareness what is happening in the inside the companies. also like additional thing to add add to

it is that uh the kind of evaluations are now getting saturated. So so it's it's like you have like this uh regime that has been built on on evaluations uh to kind of uh create some

kind of awareness within the companies and outside and like these like the evaluations just come up like come back with like 100%. What do you do now? &gt;&gt; Right. Right. That's that's interesting.

Definitely. Raman, I want to come back to you. Um, we've talked about how government AI failures become accountability problems. You've been on the ground working on this, coordinating

with the Biden administration to bring thousands of people to Defcon for like a a hackathon uh to Red Team AI systems for OpenAI, Google, Microsoft, working with the California government on

evaluation frameworks. You weren't just looking for technical bugs. You were testing for bias, manipulation, privacy violations, and other things. From that experience, tell us what you learned

about what governments actually need to assess. &gt;&gt; Great question. Thank you. And there's two studies I'd like to talk about. One was the Defcon event where actually we

designed the study around the AI bill of rights and um and the second is actually something called uh project Arya which was a project of NIST and it was a a red teaming that was actually launched to

any citizen in the United States. You could participate in a red teaming evaluation to look at all of these frontier models and the input would actually help NIST create their

standards and their policies. Um and the difficult task at hand was actually operationalizing these terms. So you know Alandre and her did an amazing job of saying hey these are the priorities

we should have privacy security um you know data protection etc. Now it was our job to take that and create something measurable. Now if we were going to to Jan's point to really try to evolve the

process of evaluation we actually have to evolve the process of measurement. So by background I'm a quantitative social scientist and we take terms that are difficult to understand, democracy,

social capital and we put measurements on them and this is actually one of the biggest things to think about. So you know Steph was mentioning things like um you know manipulation uh and Alandre was

talking about things like overliance, critical thinking, right? And we worry about how young people's brains will be influenced by all of these systems. We worry about how you know our livelihoods

will be impacted. And these are uh concepts that need to now be measured and quantified so that we can go ahead and say how should artificial intelligence be implemented in schools

so that young people are not becoming disillusioned by using AI but instead are using AI to augment their learning and and all of this requires deep scientific intervention. Um so to like

what government should be thinking about how they should be thinking about it. Yes, they should think about the hard you know safety, security problems, but they also need to think about

manipulation, bias, discrimination, as well as now these these conversations we're having about parasocial relationships, mental illness, uh because who else will do it?

&gt;&gt; Right. Right. Interesting. Like I'm thinking about how we quantify impact on cognition. That's like a an important thing to think about but something that we need to to measure. Stephanie, I'm

going to come back to you. There's often a tension in government between the pressure to modernize and appear innovative versus the responsibility to get things right and protect the

people's rights. You've seen this up close. Tell us more about the work that the partnership on AI is doing in this space. Um, so there are so many examples here and I think sovereign AI is

probably a theme that we'll hear and we have been hearing throughout the summit and I know Alandre just mentioned a barriermental impact infrastructure data centers and I think there's probably

going to be a lot of discussion at the summit in relation to this kind of pressure to moni modernize the pressure to kind of have your own kind of sovereign infrastructure but how do you

weigh up the kind of costs I think geopolitically in terms of what those dependencies mean in the long term uh but also things like environmental impacts but just because I think that

that's going to come up a lot in the summit. I'm actually going to switch gears to something as basic as documentation. I call it basic because I think it's quite foundational and it's a

kind of tool that I think many of us recognize that facilitate all of the things I think Raman talked about eval testing. I think we've all recognized the importance of having that type of

assurance infrastructure. Um about a year and a half ago now we did a report uh which was looking at the interoperability of documentation requirements. So we looked at eight

international policy frameworks and this include the NIST RMF and I see Alham here who is the kind of architect uh of that um in in the US government we looked at the soul frontier commitment

so the cell summit we're now at the India summit the commitments that came out of that the EU AI act and what we found was there was some consistency in what doc in which documents or artifacts

that policy makers called out so technical documentation was something that was called for from companies incident reporting was another one for example And so if governments I think

there's a shared uh kind of understanding and G7 code of conduct was another one of some of the the artifacts we want to what extent are governments holding themselves to that same standard

that they're they're calling for and I think that that's a useful example because there's a it's a clear demonstration of we have public you know strategies and and policy frameworks uh

that governments have drafted themselves but I think there's still a gap in those kind of basic documentation tools that governments should also be being be putting out there and utilizing

themselves. Um I think a second example is another kind of basic tool but I I call it basic because these are the kind of foundational tools that we should be leaning on. Second is international law.

So uh we uh published a paper in I think September now which was looking at how the role of international law in governing AI agents and there's still so much to be I think discovered there. Uh

and I think going back to this this what is unknown about how governments might think about agents and questions around accountability. Um we were looking at questions of how agents might operate at

the international level when utilized by governments and responsibilities that governments and states have to each other including things like transboundary harms. Um one of those for

example is non-inference not getting involved in election interference and manipulation. um uh uh rules governing um how one state might um harm another's critical

infrastructure for example. Um and so how do we ensure uh that there are kind of rules and frameworks that govern failures that might happen depending of course on how governments adopt or use

agents in the future when it comes to some of those kind of critical questions. Uh but again that's another example of how do you uh adapt or think about the role of tools like

international law that to date have governed those actions or interactions between states and how might that need to be uh readapted in the future and I think again that's just another example

where if we just return to the basic kind of legal principles and think about how those apply uh we'll make a lot of progress easier said than done but again foundational tools first

&gt;&gt; definitely a lot of amazing amazing work coming out of partnership on AI. Thanks so much. So, we're going to have a bit of Q&amp;A in just a moment, but first we're going to do He's ready. [laughter]

&gt;&gt; First, we're going to do a a lightning round. So, this is a question for everyone. 60 seconds. Um, we'll start on this end. The final question. We've talked about accountability, evaluation,

capacity gaps, and the unique stakes when governments deploy AI. If you can ensure governments get one thing right over the next five years, whether in how they regulate AI or how they use it

themselves, what would it be? You need to have an understanding and a plan for what AI needs to mean for your country and your jurisdiction. So by just using a current

tools, you are playing into an ideological basis that comes from labs essentially in Silicon Valley. You need to have one strategy that says we think AI will do a thing for us. This is the

thing and we will we will get to that through our industrial strategy, through how how we evaluate, through how we regulate because at the moment we have whack-a-ole. You know, you're hitting

different parts of the stack in different ways and we're just seeing governments respond when the public is sufficiently exercised. That is not a plan. That is series of actions. We need

to know your plan. &gt;&gt; Love it. Love it. &gt;&gt; Thank you. So yeah just to building build on my last answer that uh uh also for the countries who are not

hosting the leading models it your future also depends on what's happening inside those labs. So like putting pressure on on the governments that are hosting hosting those and and kind of

making sure that they actually understand what is happening in the labs and in particular I'm like sickeningly worried about the thing that now labs are explicitly aiming for which is

recursive self-improvement making sure that like we can remove humans from the loop of creating further uh further AIs and loss of control that potentially could come out of that. I would say

investment in the community independent community of practice of algorithmic evaluators and there's two reasons. There's the the ethical responsible use public interest reason and there's

actually a purely capitalistic reason. So the ethical responsible use reason is that this is how we get people from 70% not liking AI to actually being more trusting of AI systems because we now we

need to create independent evaluators. And this is not you know a crazy thing to say. This concept exists in just about every major industry that is impactful on people. Whether you're

talking about finance, education, aviation, uh medicine, there is no industry in fact that just lets companies do what they want and build things that are so societally impactful.

And the capitalistic reason frankly is this is a massive market. You know there is a lot of ask from not just the frontier. We focus a lot on the frontier labs, but actually most of my work is

done with Fortune 100 companies and you have insurance companies, automotive companies, banks, you name it. The any industry trying to adopt AI that has these very very big questions on

independent evaluation and they have nowhere to go. So investing in that market is also about creating new job opportunities. It's about ensuring responsible use and it's actually about

creating better innovation. just gonna uh plus one that point and build on it actually which I think it's such an important point around assurance. We're actually releasing a

paper on Friday um which is focused on how you build an a robust assurance ecosystem and I think that this is so important because we talk about these principles uh like justified or

calibrated trust which is that uh what tools do you really need to calibrate your trust in a a system uh not just kind of told to have blind faith that that it's going to operate in this way

and that it's safe and it's secure but you should have the tools that you need to determine uh whether you trust it. So, one random example I'm going to use now is that I was ill recently and I got

a prescription from the doctors and uh I trust the doctor, right? And I I get the prescription, but it also came with uh this document around some of the limitations or potential risks of using

this medicine, but then I can then weigh up like do I still want to take it? Yeah, I trust the doctor. I am aware of the potential risks. Um and so how do we ensure we have documentation around

limitations that models have and then you know whether you're a citizen whether you're a government sort of thinking about your own needs uh what do you need to calibrate your own trust and

I think that part of that uh comes from building this you know strong assurance ecosystem and for us particularly an independent kind of third party testing ecosystem uh where we have evals across

the value chain um and particularly there I think uh Raman made a really important point around the fact that this isn't just thinking uh about model providers, we should be thinking about

deployment across sectors and what is distinctively need needed within those sectors too. So um I think a lot of progress could be made um in that area. &gt;&gt; Yeah. Um I guess I would say uh I'm

going to so you gave us fiveear window. I'm going to be aspirational and say something that's going to sound almost ridiculous but I want us to kind of move the Overton window I think on a couple

of things. So, one um just on the like a phrase like recursive self-improvement like is it improving? I don't know. I mean, I guess I want us to like I think be really careful around the language

particularly as um you know, agents are you know up and running uh you know on our locally on our our laptops or in between nation states or whatever and you know is recursion always improving?

And so I think we want to like I want to hold that question open. Um, and I want to, you know, we, so, you know, Chow GBT gets introduced November of 2022. We're immediately start talking about the race

to the bottom among the AI labs. And I worry very much about a race to the bottom among nation states with regards to AI. I mean, you know, laws that were hard-fought, regulations to get on the

books are being eroded or being clawed back, etc. So, the race to the bottom has moved from the private sector to the public sector. I think it's a real concern. And so my aspiration here would

be to um ask our countries to be uh the best model of how these tools and systems could be used, not the worst model, right? To sort of have them be the guy is laughing the the sort of

again aspirational Overton window. Like imagine that our you know nation states could be the best models of how they're used with the best safeguards with the most safety like could model to industry

could model to um you know other nation states how we might do that. So that's my that's my polyianaish clothes. &gt;&gt; Join me again in thanking our panelists for today [applause] and we'll open it

up for some Q&amp;A. I don't think do we have any microphones in the audience? &gt;&gt; Thank you. It was an enriching session and I really enjoyed the conversation. So I'm a researcher from Ashoka

University and we often talk about um how extensively government framework should be in ethics but in practice compliance is often very voluntary or manual and it is bypassed by enterprise

blocks very easily. So in your view, how can we move from governance intent towards system level un bypassable constraints that prevent data leakage automatically at the user level so that

it can be done pervasive and on a large scale and with collaboration of all countries we're talking about polyiana right so we are very you know you we need to move

from situation where we have voluntary commitments to a situation we have mandatory standards s at the international level that are deployable and enforceable and ratified at the

domestic level. It is very hard to know what the path from A to B is at the moment. We have a panel tomorrow at uh 10:30 in room 4A if anyone wants to join where we have four or five countries

talking about the exact question like how can middle countries or otherwise aligned countries come together and ensure that there is an enforcement toolkit that is applied now and I think

without that we're just going to see regulatory whack-a-ole forward back sideways responding to domestic pressure it is very hard to know what the path is but I think there are many people

working on it so join us maybe &gt;&gt; I just say transparency and slowdown Yeah. Uh only thing I'd uh say to echo that is important for in order for all governments to be on the same page for

alignment. We published a a paper last year where we were talking about the AI governance stack and the different tools and mechanisms that we've been using to to to govern to date. And in 2020, I

think we map that there are eight levels in the stack through from principles down to enforcement. And by 2025, I think we had 13 levels of that stack. And so we're just adding more layers and

we have the institutes in July. We'll have the UN scientific panel playing another role. I the the network of institutes has changed its title now to focus on benchmarks and evaluable

um are we all working towards the same goal. So I think just keeping an eye on on that alignment element will be important. &gt;&gt; Good morning everyone. Namaste. My name

is David Karal. I'm from the United Kingdom. I work with the uh community cohesion sector particularly uh young people and women. And uh my question to you, how do you see AI co-creating

for the quality outcome? Is the question more like how are people working with AI to do this &gt;&gt; collaborating with AI? I think that's a very interesting take and I think one of

the predecessors that is necessary is to know that these systems are trustworthy and reliable. I think you know Gia was just using an example of theoretically like social workers want to use Genai

systems for transcription and transcription one would assume seems to be a very basic use case and yet uh there were a lot of questions that you know would require a human in the loop

oversight. So I think one of the the precursors to co-creating a better future with AI is can we ensure that the AI's output is consistent, reliable and helpful. Um for that we will actually

have to design uh mechanisms that will ensure that you have some sort of like a reliability check. Uh in particular because often when we're talking about designing better futures, we we are

implying that we want vulnerable communities, communities that are often left out of the conversation being in that room. uh and that's one of the most important things but also these are the

communities that tend to be the least literate as it relates to AI but and also often the ones that are not as comfortable raising their hand when there's a problem. So these are the

things that would have to be addressed. &gt;&gt; We've we've just published and I I can follow up afterwards but we've just published with enough Foundation a piece of work called Grownup which is about

the experience of 14 to 24 year olds growing up with digital technology. So it's not just AI, it's more broadly and they really need adults to step up is the thing that we're really hearing like

we are letting young people deal with these systems without having proper mechanisms in place really exposed to like the most gruesome harms on you know Reddit and and such things and like on

the other hand they can't imagine a world where that isn't what's happening for them and they have really nuanced understandings of what they do and don't want for things like digital

technologies and AI and I think the more we have mechanisms to listen to people and to ensure that our governance and regulation reflects what they want and what they're experiencing the better.

We're really happy to follow up afterwards. &gt;&gt; I would just add add to that. I'm most interested in kind of uh collective decision making uh help from AI. The

work of organizations like collective intelligence project uh I find very interesting &gt;&gt; from center for AI and digital policy. One of our flagship work is uh looking

at the national strategies and practices of countries every year. So we want to make sure that we see the gap between commitment and implementation. There's always a gap obviously. This year we are

seeing that gap widening because some countries are focusing more on AI AI first rather than humans first. So my question maybe Dr. Nelson and Yantelin uh at the international level we already

mentioned like consumers citizens making that um impact from within from the international level. uh what you see are like the most key points levers to use for countries to keep each other

accountable and for the implementation of the frameworks that we all contributed to. &gt;&gt; I mean countries still have nukes right including this one. So there is like

some uh power over each other. So yeah, using pressure to for getting you know transparency about what's going on and also helping with slowdown like uh both Dario and Demis last month at Davos

called that like called for slowdown uh but like at the company level just can't do it because there's in such a cutthroat competition uh which with each other so it needs to be like a uh

intervention and I'm afraid that the intervention might actually have to come from like third countries because the the countries hosting they're just like so conflict

conflicted about the situation. &gt;&gt; So I think it's really important to focus on the gap. I love this question and to identify it and press on it. So I mean one of the you know we were I think

many some of us in this room um around the same time last year were in Paris for the AI summit there and you know there were um civil society folks brought into the planning of that which

was commendable but I think the conversation that took place at that summit was not about welfare for all or happiness of all or inclusion you know so I do commend the framing of this

summit for bringing those issues about trust and about safety and guardwells and wellbeing sort of back to the table of these conversations but there's going to be a gap and we've got to press and

identify the gap just like shine a light on it and I think ask you know countries to sort of meet you know this is what you promised I mean I think this way often about bylaws and charters of

organizations that are old it's like this is what we say we do or we believe in and how do we do that so that's just a philosophical thing but I think you know what we've been seeing in the

United States with regards to AI policy which is being called light touch or deregulatory are a whole suite of levers that countries are using or can use to form

alliances or to put pressure on others. Export controls on semiconductor trips um uh you know immigration policy um uh you know trade policy tariffs and these sorts of things. So these are levers

that the United States has been using um sometimes as a budgeon on other countries. But I mean I think it opens up a different field of play for how we think about what levers are for air

policy you know amongst countries and between countries. So we might consider some of that. &gt;&gt; Thank you so much. I'm going to see if we can get a couple of people maybe in

the middle or in the back. And I want to encourage all of you that have questions to visit with our panelists after the panel is over as well. &gt;&gt; So the panel focused on democratic

values throughout the session. uh what I'm worried about um and this plays to the fact that there are uh you know the research on these frontier models as mentioned um are hidden in labs and like

they're not brought out to the public u so for organ for countries or organizations that don't respect these democratic values as much what do we do then and does that then then mean that

the countries that do value these democratic values are then forced into being playing this game because the strategic and uh military advantages are way too much for us to ignore them. So

how do you then play those uh and especially because it's so secretive like everyone's so secretive about the models and about how how much progress is being made. Uh so if it's not public

information how do you control it? We're talking about gaps but how do you identify the gap if it's not public? &gt;&gt; I can I can get a start and I think everyone else in the panel probably has

opinions on this as well. Um you know we talked a bit about you know transparency in these models and the thing I worry about is what do we then do with this information right so even if you know

all of the frontier labs came out today and said like these are all the things we're doing what power and agency does anybody in this room have to do anything about that these are massively basically

hedgemonic powers at this point that can unilaterally act in with extreme prejudice so that that is something that I do find concerning now Alandre talked a bit about you know hard regulation.

We've been talking a little bit about soft R regulation, right? What are the soft power mechanisms? And it is fascinating as a political scientist to see technology basically fall into the

world of geopolitics and and negotiation. Um, and you know, there are multiple different levers and bodies that are engaged in this. But to your point, you know, and again to Paulre's

point about this race to the bottom, right? How can we aspirationally have government represent the best of the best and not really fall victim to this race to the bottom? Um the other thing

is maybe from a purely capitalistic perspective. It has been interesting to me in my corporation as well is set up as a public benefit corporation and that presumably holds meaning uh TVD I was

also at Twitter which was not a public benefit corporation and if you are familiar with the saga of Twitter it is pretty much that you know it was in general agreement by the board that Elon

Musk was not the right person you know that people knew what was going to happen if he took over Twitter but they were they had the fiduciary responsibility to sell to him. So, you

know, I have less uh I am interested in the fact that Anthropic is set up as a public benefit corporation. I think it's an interesting experiment. I don't know if this is how I want to be experimented

on, &gt;&gt; but I do find it interesting that there are new modalities of for-profit corporations being explored as well that may be able to answer your question. I

too am now going to be a little bit polyianish about it because maybe we do need to be on this panel. &gt;&gt; Maybe I'll just quickly add that um I think that came to mind is the role that

economic competition will play. So I do acknowledge that there'll of course be bad actors. Um but economic competition means that most companies regardless of the country that they are within will

want to deploy uh within our other jurisdictions. And so for me one quick key question comes back to something like technical standards and what role can they play when we think about things

like benchmarks uh if we're thinking about uh defining what good looks like and ensuring that we have a means through which we can hold companies accountable against that technical

standard across countries. If we think about the standards at places like ISO uh that provides us a means to kind of drive accountability across countries in a way that is interoperable and agreed.

Um and there's an incentive there because whether I'm a company from the US or the UK, I'm sort of thinking I want to deploy in the EU, I want to deploy in Japan and so that might be the

incentive and that's of course separate from bad bad actors. Um so I think technical standards could play a role but of course I know that uh there's still a lot of work to be done when we

think about metrology and benchmarking. My question focuses on the initial comment that we can't switch state and my question is which state? Uh you know Roman finally talk down to hegeimony. We

still have the article 109 of the UN charter pending for a good 70 years. That's the root cause of hegeimony. Just a short comment on AI. you know it's a doublefaced uh you know uh it's a

scho schidophrenic uh you know multi personality disorder on one hand it's a melifuous symphony &gt;&gt; you know music if you put it in hands of India and kutum if you put it into the

hands of a hegeimon under a monstrous regime the dawnro doctrine you know it's a frankenstein monster waiting to gobble up humanity as a

whole. My question to professor and uh I'm sorry Alandra who is more Indian than many of us here. Let me just compliment that. Vasuv Kutumbakam is a concert of civilization that needs to

break away from the clash of civilizations that Samuel Huntington unwittingly trapped us into. My point is that you know could it be just a fleeting superficial con conversation or

could we unite even a small group of us to call for article 109 for the UN review to move away with the veto powers and the permanent seats in the UN security council and article six to do

away with the repeat violators of UN charter namely uh the United States of America which incidentally is not a state it is a
