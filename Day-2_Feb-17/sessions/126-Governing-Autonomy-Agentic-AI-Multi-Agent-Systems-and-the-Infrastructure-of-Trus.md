# Governing Autonomy: Agentic AI, Multi-Agent Systems, and the Infrastructure of Trust

**India AI Impact Summit 2026 ‚Äî Day 2 (2026-02-17)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 15:30 ‚Äì 16:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 7 |
| üìÖ **Date** | 2026-02-17 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/Uz0tWvm2XAU?feature=share) |

## üé§ Speakers

- Alpesh Shah, IEEE Standards
- Amir Banifatemi, Cognizant
- Apoorva Goyal, Insight Ventures
- Ellie Sakhaee, Google
- Praveen Tanguturi, Cognizant AI Lab
- Ramesh Raskar, Massachusetts Institute of Technology

## ü§ù Knowledge Partners

- Cognizant - Responsible AI Office and AI Lab

## üìù Summary

This session will be focused on addressing governance gaps in emerging agentic and multi-agent AI systems. The session will introduce a new public‚Äìprivate governance framework and showcase the Agentic AI Governance Stack through a live demonstration, followed by expert's discussion on safe autonomy, accountability, and standards. It will conclude with the launch of a global Agentic AI Testbed, inviting multi-stakeholder collaboration led by India's DPI strengths.

## üîë Key Takeaways

1. This session will be focused on addressing governance gaps in emerging agentic and multi-agent AI systems.
2. The session will introduce a new public‚Äìprivate governance framework and showcase the Agentic AI Governance Stack through a live demonstration, followed by expert's discussion on safe autonomy, accountability, and standards.
3. It will conclude with the launch of a global Agentic AI Testbed, inviting multi-stakeholder collaboration led by India's DPI strengths.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/Uz0tWvm2XAU/maxresdefault.jpg)](https://youtube.com/live/Uz0tWvm2XAU?feature=share)

---

_[‚Üê Back to Day 2 Sessions](../README.md)_


## üìù Transcript

to say probably a few words about what they do. Uh I'm sorry but Elisa from Google. Um go ahead Ellie. &gt;&gt; Hi everyone. I'm Ellie Safi. Um I am policy manager working on AI and

emerging tech uh particularly agentic AI and robotics. &gt;&gt; Uh hello good afternoon. Uh this is Mahes. Uh thank you all uh and thanks to Amir and Provin for having us here. Um

and also they are generous enough to put my younger version in the picture and uh so I'm from project Nanda uh we are pioneering the uh building a foundational infrastructure uh for a

internet of AI agents you know like it's a new kind of internet is born or getting born from last year onwards and u yeah and and one of the most uh foundational issues uh the way I say

that trust that is me you know like that's my photo the same thing you know like how we are going to solve this problem you know like using in this internet of AI agents and those

primitives needs to be solved at a foundational layer it's not like after the fact or after the thought uh solutioning it has to happen as a first principle yeah

&gt;&gt; thank you alash sha I am the managing director of the IT standards association uh and a member of the uh manag management council at IT E. Thank you. Thank you. I'm Apu.

&gt;&gt; Hey guys. Hi. Um my name is Apu Goyel. I'm a principal at Insight Partners. I help lead a lot of our US India investing efforts. Just as context, we're one of the leading global

technology investment funds. We manage close to $90 billion in AUM investing out of a $12.5 billion fund into leading global software companies around the world including a bunch of companies

based out of India. &gt;&gt; Wonderful. Thank you so much. I must be getting to uh Pravin is my co-odderator. Provin is the head of AI lab uh uh uh co in Bangalore. But before we start

talking about it that as of raise of hand, how many of you have built an agent or play with an agent? How many of you are scared of what's going to happen with agents?

What is probably like 5% 10%. So it thinks that 90 or 80 or 90% of you don't feel that agents are going to be dangerous or anything else. Have you followed what happened with open cloud

recently on notebook? Do you feel this is cool or exciting or energizing or fun? Who is excited about what happened? It's a beautiful experiment, right? So, we're

learning about all these things and as we talk about agentic world, the concept of autonomy scares a lot of people. Why? Because we don't fully know how they're built. We don't fully know how they're

going to react. If their objective going to be changing, if they're going to be mixing with each other and creating new babies, we don't know what's going to happen. And they got rid of autonomy a

lot of decisions that enterprises take and society's functioning may not be able to put to be put in the hands of agents. So the concept of governance which is a concept which is very

encompassing about how we control the outcome of systems and procedures and ownership and risk management becomes central again uh in the world of agent which was not the case about even a year

ago. So today we're going to be talking about this concept of governing autonomy. How we can actually make sure that this world this future world going to be fully autonomous. Not only is our

hand to some extent but we can somehow trust it and we don't know to extent. So to get started um maybe we can quick demo uh what do you think? &gt;&gt; Let's start with the uh questions

happening with autonomy and then &gt;&gt; okay perfect that's a good thing. So maybe I can start with Ellie. Um Eddie in your vantage point can you help us understand what's different

uh be between multi- aentic environment and systems and traditional AI that we've been used to how should we think about it? &gt;&gt; Yes. Um so with the raise of hand that I

saw in this room um I think we are uh talking with a group of experts. So um from my perspective uh and you many of you have already worked with uh Mosbug and uh have seen openclaw and have built

agents. So you know better than me that the way that we think about agents it's not a specific line or a specific point in time that like this group of things are agentic and this group of things are

non-aggentic. rather thinking about them as a spectrum of agency or autonomy uh going from basic chat bots that can do a little bit of research and at the end present the uh result. For example, uh

Google deep research uh does a research um uh given your prompt and finds the information and presents that information that has some agentic

features but it doesn't take actions uh compared to let's say going all the way towards the end of the autonomy that is uh for example uh autonomous cars that are completely end to end and they take

action in the real world. So um I think seeing agents as uh a spectrum or a continuum of autonomy across different uh dimensions uh including memory including planning and long-term

planning short-term planning uh uh planning horizon that they can uh plan and execute things and where and also uh autonomy itself. So what I'm trying to say is that um we can't say we don't

want to say agents are the ones that take actions. We want to say agents live on a continuum from uh very basic autonomy all the way to um things like autonomous vehicles that are fully

endto-end at times. &gt;&gt; Um Mish uh you started talking about interesting uh issues but let's follow what Ellie said. What? How do we know agents could make an error? How can

visually even know? Yeah. So, uh she used a very nice word. It's a continum, right? So it's not like a single point responses that you can keep monitoring you know like after the

agent took an action and she another thing that she mentioned they become a network actors they will be acting at every single stage uh visibly or invisibly and that makes it mandatory

for us to make it uh I would say that it has to be a runtime governance rather than you know like wait for uh like let it happen and then we'll reverse. So the traditional uh governance models uh that

we have or exist were very much evolved from our understanding of uh machine to machine interaction then it evolved into a SAS model and a cloud model uh then we evolved to the microservices

uh architecture and we kind of assumed that you know like somebody else is taking care of those uh governance after uh we reach uh at a scale you know like so when autonomy will reach at a scale

uh then we cannot think of like okay we need to solve this problem now now we are thinking of like agent as like oh okay charge GPT is one of the agents I'm just looking for the search answers uh

rather than uh and slowly saying that okay it can just go and do some action it can crawl some web pages okay then we say that okay then there will be multi- aent systems agent to agent interaction

collaboration that will happen uh and just we are adopting it without addressing the foundational issue that the safety and the governance uh has to be addressed. Uh that's one

that along with the trust but also to make it u like how it can become a deployable across the enterprises uh or a public services or a civil society uh without sacrificing this is very

important factor without sacrificing the openness of this execution and the accountability. So those are the two points I would like to mention. &gt;&gt; So you give us more information than I

asked for. &gt;&gt; Yeah. Thank you. Uh when we talk about incident that happens, of course we remove bad actors, right? People that are using agents for bad purposes. We're

not talking about that which is a different issue. Security, cyber security or control. We talk about agent themselves making an error or having a mistake.

Uh some of you know that we're talking recently about agentic identity as a way to make sure that who they just agents where they coming from [sighs and gasps] what's their training what data have

been used who created them who they belong they have a passport how they going to talk what language they speak and we talk with each other same language and so forth which brings now

the concept of certifying agents and I'm going to ask to al because in in uh stands in a working lot and many many uh concept even ahead of his time on energetic. How do you think

about certifying agents in a world of autonomy? How how should we think about that? &gt;&gt; So I think this works [clears throat] I

guess that's one of the most important points, right? Knowing when it's on and when it's not. I I say that in just but this is also part of when we're thinking about these

various models and how they interact with each other. Not not having the ability to understand what is the end result or what good looks like creates a difficulty in understanding the

parameters by which you can state this is working as it should. which means that very much and and already we we've seen this done quite successfully in a number of um uh the use cases we've been

involved in in prototypes and uh implementations around this that we've done is that first making sure you have a very strong governance structure in place is

critical. Second is making sure that you have um a very clear process by which you are determining what does and doesn't go out or is ready to go into the sandbox or

what you're ready to even put into that runtime state as you were sharing earlier. It also requires ensuring that you have an iterative approach to it. uh these

are not one-time exercises because you won't know necessarily when there might be a shift or where things may go in a different direction, right? And so this requires sort of a constant monitoring

approach. Adding to this is also the need to contextualize a lot of what you're looking at. Some of the reasons why you may see the behaviors of uh the multi-

aents may be because there was a trigger that you typically do not see or are unaware of which means that the bounds that you currently had in mind may not be the right bounds

and therefore it's important to also ensure that you have transparent accountable not only logging but people in that process ultimately at the end what it is that

you can evaluate let's a at a much um more [clears throat] level of confidence is the means by which it achieves the output. You won't know 100% if the thing that is

out there is perfect but over iterative series you begin to clearly understand right this out of the what is it Monte Carlo sim simulation or whatever model you choose to apply here

at some point you have enough data to know what what good should be right um so our certified program uh allows for this at the same I'm we have a series of standards uh that air uh was

was instrumental in being involved and a number of folks in the front row here as well uh which are focused on uh data transparency focused on uh age appropriate design uh focused on also

the critical elements around what does it mean to be accountable transparent what does data privacy mean in the context of systems that we don't know what they were to

Right. And what we found the greatest power in is people. It was the community of people that worked on this. They were heavily representative of a mix of lawyers, doctors, artists were even

involved with engineers and technical [laughter] people that really understood the contextual problem so well that it allowed for us to better understand and appreciate what some of those variances

might have been. Thank you Aresha. You invest in many many startups and companies and you are uh in the first line to evaluate them and and and see if

the company is going to have solid technical background and the teams is right. But because Apes mentioned the concept of governance, we're talking about governance and governing AI. How

do you imagine a stack? Because we talk about the technical stack. We're all familiar with the technical stack. What would be a governance stack look like when everything is autonomous?

&gt;&gt; Yep. I think the way we think about governance stack in this AI world is a 5k clear model. Um if I kind of go first is like the build time around the build time the idea is okay how is the company

architected around the idea of data governance model versioning. Um you know when you kind of come to the next stack which is basically deploy time how do you think about the idea of policy uh

tracking permissioning secrets management um when we come to the third piece of it we think about this whole idea of runtime how are you ensuring that your your architecture allows for

realtime observability how do you ensure that there is an idea of kill switch if things go bad how do you ensure that you are able to cut it off at the right point of time then we think about the

whole idea of How do you drive remediation? So do you have the right audit trails to be able to do a postmortem? Do you have the right incident response architecture to

be able to attack that? And then how can you drive whole accountability lay which is around the whole idea of who drives accountability? How how do you have the right reporting structures? How can you

conduct right postmortem? Do you have the right set of tools to be able to do a compliance mapping there? So I think we think of governance stack in today's world in a holistic 5k clear model and

anybody who thinks about this holistically is where we feel comfortable about the fact that okay this is the team that is truly thinking about governance in a holistic way and

as you think about the best AI native teams governance is becoming less of a compliance issue but more of like an issue which is more product and GTM because you know whoever is embedding

governance into product is truly winning on the GTM side um and that is becoming a competitive advantage and so it's becoming a key part of how we evaluate a lot of the companies. Some people

jokingly said we uh a few years ago 10 20 years ago we talk about internet inside when the PCR was that now we talk about governance inside. Would you agree with that? Oh

&gt;&gt; 100%. I feel like uh with so much of uncertainty I feel like every enterprise that is procuring today like the upfront conversation before they take on any business through multiple layers of you

know evaluation is talking through a bunch of questions around what is your auditability what's your traceability what's your data handling practices are the kill switches in place because the

costs of things going wrong are so high that people are willing to spend millions of dollars in ensuring right governance while even buying a half a mill contract. Uh so I feel like it's

becoming a core part of &gt;&gt; so how startup sorry this is a question I didn't ask plan to ask but how startups that don't have all the fundings and all the resources necessary

could comply with this level of scrutiny or requirement. &gt;&gt; I think the way I think about it is when you're starting an agentic AI business in today's world I call it like you need

to have a minimal viable trust tag. You need to be at a high level. You need to be able to tell what is this agent supposed to do. Uh is it actually doing what it's supposed to do? And if not,

can you actually have someone stop doing what it's supposed to do on a real-time basis. Uh and that becomes critical. And for that, you basically need to have what you call as a clearly defined agent

identity registry. You need to have the right set of guardrails at the orchestration layer. You need to have a clear realtime observability architecture. And lastly, you need to

have very clear set of defined oversight. I think if you don't have any of this, I don't think you should be launching an agentic ecosystem into production because you're bound to meet

more failures than success. And it's not that expensive to do these four basic things. And of course, as you scale, you'll have a lot of compliance start building into your product. But this is

the minimal viable thing that any startup that's looking to start in the agentic AI needs to do to be able to go into production. &gt;&gt; I think you should write an article

about that because a lot of us would need that little four stack layer to to get started. You to use the term orchestration uh I would like to pass on to Pravin uh as we as we talk about

orchestration and coordination because as we define multi- aent systems they need to be coordinated in some ways to do certain things but also to not do certain things. So um uh Previne um

&gt;&gt; is you ready for the for a demo quick demo? I think it's very tiny. So maybe you can explain um &gt;&gt; right. &gt;&gt; So uh we at Cognizant and truly believe

that the next phase of is going to be called the agent system. Am I audible? &gt;&gt; Sorry about that. I thought these was working. Yeah. So, uh we at Cognizant and Cognizant AI later truly believe

that the next frontier is going to be multi-agent systems and uh in that endeavor uh sometime in May 2025 we sort of opensourced our multi- aent uh framework uh which is called NeuroAI

multi-agent accelerator which you can see on the screen here. The uh it's a low code no code framework that's designed to accelerate uh the prototype development to scaling. It's a

production grade environment. Currently it's available uh under Apache 2.0 license on GitHub. Um there's an active community that is sort of uh working on building and enhancing this framework uh

as we speak. The the some of the features and the capabilities of this accelerator is that it's LLM agnostic right uh it's uh it's cloud agnostic. So you have you as a user will have the

control over which LLMs that you want sort of the the agent network to talk to. uh you have a control on which environment that you want to deploy on that and it's interoperable. I mean

currently you know we support multiple protocols from agent to agent communication our own uh our protocol as well as A2N MC and CCP server uh and most importantly as we as we heard about

the you know governance autonomy and you know the guardrails security is built into it. So we build this from security at the core of it not not as an afterthought. So in the next one minute

and and most importantly we've made it so simple that even a seasoned uh an a IML engineer to a CXO who has minimal or no coding experience can start building their agent network in a matter of

minutes and that's that's all the whole our reason is that we want to make this as a de facto standard and we want everyone sort of adopt it and sort of you know not just experiment it actually

build this agent networks at scale and deploy them. Right? So, let me just do a quick demonstration. And today, what I'm going to do is quickly white code an agent network in front of you guys. So,

this is our Vibing environment. So, let me just quickly Anybody wants to give me a prompt? Anybody? [cough] &gt;&gt; Anything? Anything you want prompted?

Yeah. Uh, AI submit schedule manager. [laughter] &gt;&gt; All right. &gt;&gt; Can you read because it's not easy to read.

&gt;&gt; Yeah. I'm I'm saying create a multi- aent. &gt;&gt; Can you activate the microphone? &gt;&gt; Yeah. I'm saying create a multi- aent network.

for India AI Summit New Delhi, right? It's working. It's working. Okay. So, what you can really see is just give it a few more seconds. What you see is that there's there's a multi- aent architect

and a designer which is working behind just to understand the intent and the intent that I gave us is create a multi- aent network for India AI summit New Delhi that's currently happening. First

thing that you will see is that the large language model understood my requirement and it created set of agents. It's defined the agents. Now what it's actually doing is that it's

actually connecting those agents to an orchestrator. It's still building through it and as a next step what will happen really is that basically now every agent

needs to def uh be defined. It needs to have its own roles responsibilities and what it should do and what it should not do. So currently that's a step that we are actually going through it where you

can see that you know uh in a matter of moment all these agent network uh agent individual agents will be populated with descriptors you can see that right so those descriptions are nothing but its

definitions what it should be doing at the end of the day right and it's still finalizing the agent network the two things that you need to see at this here is that there are agents which have been

specifically created for that agent network and then there are there are a few of them which under the leaf nodes orange in color. So basically what the designers uh understood there are

certain agents which were already created and they were available in my environment. So it just connected them directly. So instead of rebuilding them reinvesting in creating the agent

network it just connected them. So you could use it. So it also allows as a designer of agent network is that you have a control auditability and traceability of it. At any point you

think that that leaf node is not required in that agent you can just delete it and save it. the server understand the changes and your new agent network is ready to be played

around with. So now that the agent network is ready, what I'm really going to do is that launch it you can see that the agent network is uh made available again just let me blow it up a little

bit here and before I sort of fire a query uh just to sort of and again this is a development environment. The reason we are showing a development environment is just to showcase a couple of

capabilities. What you see on the uh the right hand side panel is basically your chat window and then you have an internal chat which is basically shows in real time the communication between

the agents what kind of information is being uh transmitted what the nature of it. So you as the designer have the control over it as well. So you can take a dump of it later on and you can sort

of go through it review it to see if any confidential data is being leaked through a large language model got access to it right and then you have the logs. The logs are pretty granular in

nature in the sense that it gives you to details to a level of the token usage and the cost associated with each of that communication. And last but not the least uh I mean most of the folks here

are pro ESD metrics right so you you also get a score of energy usage carbon footprint and the cost associated with each of the uh prompt that you are firing so let me just

so so I'm asking the agent it was so can you provide a schedule for the keynote speeches at the India AI summit right so I think it's okay let me see if I can get so currently it's not grounded. It's

just talking to a large language model behind. So every agent network that we create here uh by default it talks to uh GPT4 but you as a user have a control as I said is LLM agnostic. So if you in

your organization have a qualified large language model that you want this agent network to we can. So globally each and every agent network has access to one uh large language model but the framework

has has a uh coolest capability where every leaf node agent also can be empowered with two specialized or a general purpose agents uh LMS as well. So they act as a fallback and and if you

have a specialized agent like for auditing or governance or compliance right you want higher accuracy you can sort of connect a specific large language model just to that uh leaf node

agent as well. So that's sort of the and and today it's al uh it's available on GitHub uh it's been there for last 5 months now we have an active community uh the GitHub repository gives you

access to the code base under the Apache 2.0 the license and this framework has met the highest standard of GitHub uh security as well today right

so that's an example of a you have no queries to run &gt;&gt; yeah so that's an example of a coordination which basically takes into account all the consideration that was

discussed and you said it's open source right I mean &gt;&gt; yes it's open source &gt;&gt; open source so everyone can use and play So here's the QR code

and if you need more so here's the QR code that you can sort of scan it will directly take you to the uh the GitHub repository that we have uh you know we encourage you to fork it clone it and if

you like it give us a star and if you don't like it let us know why and we are here to uh sort of work with you on that. Sorry to interject. Uh I just had a question. [clears throat] If you have

multiple agents running at the same time, all of them have different contexts and different memory. How do you manage that in a single platform? &gt;&gt; I mean time. Let's come back to it

offline. But we'll get into that. We'll get into it. Right. &gt;&gt; Great question. So that was just a demo to basically because the audiences they were assumed what what these things look

like. But this has been certified as compared with this framework as well. So there's a lot of back behind the scene. what Abra was talking about has been taken into account in terms of

observability, loyalty, monitoring and so forth. Um going back to the um the issues of incident and tracking incident and and danger that could happen. There are examples in other areas such as

aviation and nuclear power etc. uh maybe Ellie can give us a uh view of how the other industries have dealt with these issues because this issue is today we talk about agentic autonomy we have

these issues before in other sectors how have we dealt with them &gt;&gt; yes um absolutely so the technology may be new but the concept of safety especially in safety critical uh

industries is not new we have seen that before and we've solved for it um uh let's take the example of uh aviations and uh drones for example example. So drones are kind of newer technology and

um a lot of um regulatory bodies are looking at regulating drones. Um and the interesting thing about looking at drones and comparing them with agents is that um when you look at let's say uh

the regulations that uh at least in the US uh aviation industry is uh considering for drones it was originally based on what uh FAA uh or federal aviation administration calls uh vloss

meaning um visual line of sight meaning that a pilot has always uh is in command and they have to keep visual line of sight with the drone. So uh and that is how they they are accountable for

managing that drone. However, as the safety and as the system safety based on using AR technology develops, uh now the VOSS regulation is moving to beyond visual line of sight and that

means that now the safety of an AI uh detect and avoid system has gone above what a human can do. And based on this advantage of like say advantage of detect and avoid based on AI system now

the regulation is moving from a pilot has to always keep visual line of sight to beyond visual visual line of sight and um using that as as a way to maintain safety. So drawing a comparison

to agents um if you think of uh when we talk about human in the loop human in the loop we're basically talking about v loss human has to keep approving every step of the way that agents are taking

however that undermines the utility of the agent right so as agents become safer and more reliable similar to what detect and avoid in the aviation industry got better and better over time

we we have to be moving from human in the loop to pilot in command in aviation industry. In this case will be human [clears throat] in command. So human

will be uh doing the supervision but not in the loop approving everything that agent is supposed to do. So drawing this kind of parallels with respect to other uh industries that are moving from

safety of human making those decisions make a lot of sense to apply to agentic AI because uh similar to um aviation in this case we see that human maybe keeping the human always in the loop is

not the best thing. um similar to detect and avoid there must there could be better safety systems that AI can provide that keeps agents even safer. But I'm not saying that that

applies today. I'm saying similar to other industries that moved from one side to the other. Agentic AI would also move now we are in human in the loop and then we have to move towards human in

command. &gt;&gt; Thank you very much. &gt;&gt; Thank you very much. As you talked about the similarity between drones and agents and FAA as a regulation, I'm going back

to the rest of the audience, the two of you to ask a question. Is that in your opinion, what's the balance between engineering guidelines and technical design at the core versus regulations

and norms and standards, right? Because both of them are media. We're not discussing that. But today with what's going on with agents in what the balance of that should be 80% engineer design

and good practices like observability measurement etc or should be probably little bit less controlled engine let innovation go but also provide a framework of um certification and

standards. How do you balance these two? &gt;&gt; Yeah. So um so I'm I'm having an analogy with the early days of internet you know like how internet even evolved at first place uh they make sure that it's not

owned by uh individual player or anything you know like so it was all inclusive and they very much focused on uh solving the uh the foundational primitives you know like identity

discovery uh trust and that was in a federated system. It was never uh a centralized system. So in this uh context you know like when you said uh okay so compliance uh it should be uh

heavy compliance will kind of uh sabotage the innovation you know like how the tear off uh uh we will do uh and my uh uh my opinion on that is you know like we should be uh keep the innovation

open you know like and that remains uh as a data plane okay but the governance has to be the control plane so they has to uh built in parallel. We cannot choose one versus other. So yeah, that's

that's my thought. Yeah, curious to hear P's and AU's opinion on that as well. &gt;&gt; Yeah. I mean I I think the Amir if we took some real world examples, right? There are some policies regionally uh

that some have claimed were too too early in the days and have led to uh unnecessary regulation and the effects of that unnecessary regulation have compromised perhaps the

innovation that could have emerged as a result. I'm not so sure whether or not I agree with that. I think, you know, I'll I'll let time uh, you know, claim claim that

one. But what I would say is that it really just depends what you're trying to do. If you're early days, you're just trying to understand what is possible and the bounds within your own sandbox.

You should be able to test it out and understand you. You need the chance to even observe it before you can jump to a conclusion that it's something that's ready to go to market.

Uh at the same time when you get to the proper maturation point as as uh my colleagues have shared you know this is when regulation starts to make a little bit more sense right from a requirements

standpoint. Policy can also function though in an earlier stage to begin framing and the framing is important to know just so you have a sense of where is too

far right but framing is different than regulation and at a certain point as as uh I'm sure prov there there comes a point where there's

a market acceptance of what is normalized And at that point in time, that's when the market differentiation really starts to play a role. That's when you see the

major uptick. Prior to that, you're not necessarily going to see everyone buy into paying $100,000 or $150,000 to get something certified, right? The costs become a barrier to

entry. But this also means that there is perhaps a different way of approaching this. You know I go back to the comment air and uh we're were speaking to

earlier of the mini trust stack. Uh you know in many ways we can also consider this the economic problem of how do you make it easier for startups to come into play. And one of those ways

could be let's say theoretically but maybe some of us are working on this uh is the integration of the required standards and policies baked into the fabric from the get- go.

And then as you build and stage stage gate beyond that what you've already done is you've bought time for those startups. You've also given a level of trust for the governments as they're

playing here. And you've given regulators at least something much clearer to measure against. It's very very difficult to ask someone to measure something and hold you accountable when

they don't know what they're supposed to hold you accountable against. And it's the same problem for companies. They don't want to waste time on something that makes no sense and all this market

play. So uh air I hope that helps answer your question but that's my take on it. Thank you. &gt;&gt; Can I ask a followup?

&gt;&gt; Please go ahead. I'm the ones to answer too. &gt;&gt; Actually in your experience &gt;&gt; captured most of what I said so I would save the time. Yeah, [laughter]

&gt;&gt; actually I wanted to get your perspective on how do you see founders actually balancing this? &gt;&gt; I again I I feel like the idea is everybody's working with the certain

idea of what that framework looks like and I feel like there is not a lot of mutual tension between deploying responsibly AI and what leads to high commercial deployment. You know, it's

like if you need to be a half a billionaire business today, you need to be able to sell into enterprises in mid-market and those mid-market solutions are selling into either

government institutions, large scale enterprises and all of them today have a much larger business to protect and so if you are experimenting with a new technology that is evolving very

very fast and if you're not able to provide for certain frameworks of those responsible guidelines, it's like hey I I want to make sure my data is being handled well. I want to make sure my

secrets are being managed in the right way. The agents have the right set of permissions and policies. Those guidelines are being and and I think today those frameworks are very

individual are localized to the company that you're trying to serve. But if you kind of distill that those will broadly come down to like 15 20 large principles and most of the innovation that is

happening is happening within the bound of those 20 25 frameworks. I think to Alisha's point that is going to start solidifying a lot more as a technology matures. Uh but I think people always

try and frame being responsible and commercial innovation as a trade-off. I feel like with the way the world is moving it's it's not a tension it's actually very very aligned and that's

how even most of the board conversations happen is because if you can't do that you would not be able to scale. So on that note staying with you Aurora right you you talked about the

trade-offs you talked about the investment right if we switch gears into the economics of it right uh so when when do you see the governance actually becoming a competitive advantage and not

a a cost burden to the organization to be very honest in very early stages of the company we have a lot of our portfolio companies at seed series A stage start selling into enterprises day

one so if you are selling into the cognizance of the world or if you're selling into uh the Googles of the world and all of that a bunch of these companies today and just how innovative

they are and how fast they're moving like day one they sell into you know a half a million a million AR contract into a very large enterprise and when you're doing that you can't do that

without uh governance like so 2 used to be that version uh so to compliance for like your old age SAS framework there so I feel like most of these conversations become pretty much very very embedded in

your seat series like the moment the company's about to sell if governance is not an embedded conversation in your product it will not be a competitive edge for you in the GTM uh and and to be

honest I meet 100 AI companies a month the best AI native teams today have actually embedded it so deeply that you know like security it's part of their weekly operating cadence they'll talk

about the idea around eval solid are your eval reviews are you doing it on a weekly basis. Are you making sure they're like meeting the standards that you want to espouse

serving into an enterprise customer? They're thinking about red teaming in a very solid way. They're doing very high quality postmortem reviews if things don't work the way they are supposed to

work. So I feel like governance is like security today and those start playing out pretty much in a seed series A conversation. If you're selling into uh an enterprise, I feel like that

conversation becomes later. If you are if you're a PLG business that is serve serving like individual creators, individual designers, I don't think if you are like trying to buy a $50 per

month contract as a designer and using that to create videos, you're thinking as much about like, hey, is this meeting those governance standards that I need to? But anyone selling into a mid-market

enterprise solution, this becomes a very early on conversation. I have just uh a follow thought on that right. So uh our digital infrastructure story like when we built Aadhaar we built uh UPI payment

system we solve identity problem we solve the the payment problem then we were like okay what about the data privacy then we have the dig locker then we have uh OCD and then uh now what what

is happening that's a very important success story and very much aligned with this agentic AI because the government uh took a stand saying that hey we are going to standardize is the interfaces.

Okay. And what we really want to do rest of the people or the companies they can do any application that they want on top of it so that government plays a very important role in this uh agent

evolution I would say. Thank you. &gt;&gt; Thank you. Uh at the beginning we talked about governing autonomy and we discuss about autonomy discuss about governance to some extent I think. Um what could be

in your opinion um well there are two aspects to that question. When is it if we can't about trust what is infrastructure for trust because it's so so fragmented between different pieces

and then how should we think about um the one single thing that would enable us to go to the right direction because we talked about again many things but in your opinion what be the one single

thing that we have to do today given what you expect what you mentioned the chance that we need to leave innovation open we need to experiment we need to have some general galleries and framing

of course, but besides that, what would be one little thing that we need to really be pushing collectively, not as individuals, but collectively to to to make everybody progress besides open

sourcing it? Of course. &gt;&gt; Ellie, do you want to start? &gt;&gt; Sure. &gt;&gt; Uh yeah, I can start. Um maybe one of the things that we can think about today

is how can we evolve um testing methods and evaluation benchmarks for multi-agent systems. And the reason I say that is that um uh we have a paper uh called distribution distributional

AGI safety meaning that um the paper discusses the fact that AGI may not emerge as one monolithic big powerful system but it may actually emerge as a lot of sub AGI agents specialized agents

working with each other in a multi-agent system. So we may be talking about multi-agent systems that reach AGI level capabilities and for that to be successful what we what should we be

thinking about today um when it arrives. So um in that paper we discussed like four different categories of um safety uh including market design including baseline safety for each agent itself

including inter agent communication etc etc uh please feel free to refer to that agent but um refer to that paper but um I think what we need to do today is making sure that we built evaluation

benchmarks that are relevant to multi- aent systems and for Frontier Labs that are developing these um models that are core to the agents, it's important for them to and by them I mean Google Leaf

Mind being one of them um to report uh test against these benchmarks, report transparently about the limitations, report transparently about the capabilities um and I think that's that

is uh one of the goals that Google lip mine has been pursuing and a lot of that information is available all on the website. Please feel free to uh read our frameworks on um on AGI safety as well.

I stop there. Um so I just uh again follow on that right uh one of the important factors that we're missing in a multi- aentic system there we are assuming that it's a

single model agent right across the multi- aent system I'm using one model but when we extend that thought that what will happen when I'm using uh models provided by different players

open source uh or nonopen source you know like all of those models will try to uh so single model responses are easy to verify you and have the matrix or all the telemetry associated with it to

verify the uh whatever the agent is claiming. But when it goes across those boundaries and it becomes like a multimodal system, multi- aentic system and every single uh company saying that

hey trust us you know like because we are uh so the one of the thought experiments or the way we try to solve it is like get every voice heard meaning that let's hear from all the academia

how they think about this problem uh get all the uh LLM uh or the big uh companies get involved in this discussion. uh the same thing with the system

integrators and hyperscalers also participate in it and not just them but including the policy makers as well as the the younger generation who is going to be the uh Asian AI native you know

like so how are we preparing uh for that that's very important uh so the problem doesn't uh get solved uh with the single model uh responses so the only solution that we kind of lean towards to make it

enough decentralized or enough federated system so that we can have those uh uh trust monitors across every single thing. Yeah. Thank you. Okay. &gt;&gt; Upper.

&gt;&gt; I think for me largely the thing which I think can solve a lot of it because that kind of distills enough information to people start working from is the idea around

auditability and traceability to be able to capture and accurate reporting in an immutable way of a lot of what is happening. And as long as there are more open-source platforms that allow for a

lot of those, you know, anonymized way of auditability and traceability and the incidents that's coming uh to come up, I think it drives for more collective learning at a more global platform

level. I don't know if there are and this is a conversation where when we meet a lot of startups they're actually learning a lot from talking to each other and they keep ruing about the fact

that there not a lot of global platforms that just talk about just learnings around audit auditable trails or what is happening how is it happening where do systems fail at what scale uh and level

of infrastructure and I think something that solves for just a collective platform to learn from uh will go a long way it's it's a very innovative environment ment that's in around the

world and what's happening. So as long as you provide the right infrastructure for even knowing what's going wrong um you know I think there's a lot that can actually accelerate the learning and

development there. &gt;&gt; That's a cue for our session. &gt;&gt; Oh I thought that was where you programmed the AI to do. &gt;&gt; Yeah. So I

I I think there's two two key points. Um I I'd like to just leave everyone with. you know uh the first Amir runs a very uh innovative uh organization called uh AI commons that many of us have been a

part of over the years and really the the the critical nature of what they have been looking at which very much aligned with uh a lot of what we were looking at as well was uh the idea of

having these technologies treated as a commons public utility Because if you don't do that, you can't do exactly what everyone here is speaking to. Only those that have access

to that can do that. And the point on education is very critical. Uh but the point I I would just bleed into this then is the thing to do today. We have to spend a lot more

time getting rid of the jargon. Even the title of today's session isn't something that's accessible readily to the same people we're asking to make use of these technologies, right? And so a

lot of the effort needs to be spent on demystifying what we're talking about and show the value of it because if we don't do that then you're going to have a lot of fear and a lot of the fear is

unnecessary and I think the these would be the critical things. Um I would also love to use your app to turn off that alarm. Uh so maybe you can teach us later

[laughter] about that. Thanks. &gt;&gt; Thank you very much. Thank you. Thank you everyone for coming. [applause]
