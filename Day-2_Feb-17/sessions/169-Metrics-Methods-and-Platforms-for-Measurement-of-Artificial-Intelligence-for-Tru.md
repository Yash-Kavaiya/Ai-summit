# Metrics, Methods and Platforms for Measurement of Artificial Intelligence for Trustworthy, Reliable and Explainable Applications

**India AI Impact Summit 2026 ‚Äî Day 2 (2026-02-17)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 17:30 ‚Äì 18:30 |
| üìç **Venue** | Bharat Mandapam | West Wing Room No. 6 |
| üìÖ **Date** | 2026-02-17 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/NqCHhhSLXE8?feature=share) |

## üé§ Speakers

- Amlan Chakrabarti, Calcutta University
- Carsten Maple, Turing Institute
- Debdeep Mukhopadhyay, IIT Kharagpur
- Kavita Bhatia, India AI Mission, MeitY
- Lipika Dey, Ashoka University,
- Mainack Mondal, IIT Kharagpur
- Mayank Vatsa, IIT Jodhpur
- Partha Pratim Chakrabarti, IIT Kharagpur
- Partha Pratim Das, Ashoka University
- Richa Singh, IIT Jodhpur
- Siddartha Khastgir, University of Warwick
- Tanmoy Chakraborty, Indian Institute of Technlogy Delhi
- Wolfgang Nagel, University of Dresden

## ü§ù Knowledge Partners

- Indian Institute of Technology Kharagpur

## üìù Summary

The session will focus on Metrics, Methods and Platforms for Measurement of Artificial Intelligence for Trustworthy, Reliable (Safe & Secure) and Explainable Applications covering major areas including Healthcare, Manufacturing, Mobility / Transportation and Governance, as well as cross-sectoral issues. A goal will be to highlight how to translate theory into real Trustworthy AI applications.

## üîë Key Takeaways

1. The session will focus on Metrics, Methods and Platforms for Measurement of Artificial Intelligence for Trustworthy, Reliable (Safe & Secure) and Explainable Applications covering major areas including Healthcare, Manufacturing, Mobility / Transportation and Governance, as well as cross-sectoral issues.
2. A goal will be to highlight how to translate theory into real Trustworthy AI applications.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/NqCHhhSLXE8/maxresdefault.jpg)](https://youtube.com/live/NqCHhhSLXE8?feature=share)

---

_[‚Üê Back to Day 2 Sessions](../README.md)_


## üìù Transcript

out just all I have. Where are you? stupid. &gt;&gt; That's stupid. Thank you.

Good evening everybody. &gt;&gt; Yes. Okay. Please. Yes. Okay. It's getting dark. &gt;&gt; So again welcome. We start off this with

a set of a briefing of 15 minutes on five topics. So I request the coordinator professor partatin das along with the other four speakers uh namely professor mang professor I sing Singh

professor Amlan Chakraarti and professor Tanma Chakraarti to come on stage please and I hand it over to professor pipidas. Okay welcome everybody. So uh this is just a very lightning brief. So

uh I start with uh the imperative for cross sectoral AI measurement. So the whole idea is AI is uh besides whatever it is doing it is uh collapsing the boundaries between different sectors. So

it will future AI systems will or it's already will need to transcend governance, healthcare, retail everything. So in terms of measuring the performance of these AI systems their

robustness, fairness, usability has to happen across uh sectors not not in silos. So while uh we have been working in a team to focus on multiple specific domains like healthcare,

governance and manufacturing and so on, it is imperative that we design adaptable frameworks for evolving AI and complex human AI interaction. So that's been our primary cross- sectoral focus

in the whole structure. In terms of some specific uh research agenda, here are some of the core components we have been looking at. Sectoral world models, cross- sectoral metric description

language otherwise it will not. So it's basically the interoperation issues that that come in inter and intra sectoral metric flow recommendation of metrics and so on. We hope that adaptable

framework for AI evaluation will come out come as will happen as an outcome foundational metrology and governance for AI. So cross- sectoral being being the key key area integrated

testing with domain specific translation. So it's just not cross- sectoral. It should be translatable to specific different domains where we are hoping to do pilot with governance,

healthcare, manufacturing, logistics and so on. So that's uh the first first perspective I would request uh uh professor Rich Singh you can present from there also if you

want. &gt;&gt; Okay. &gt;&gt; Thank good evening everyone. Thank you professor Pippity for this. So so like uh professor mentioned the second

vertical is healthcare. Uh so we have five trust areas. The very first one is related to open platforms and benchmarking. Now if if you're building healthcare solutions for any

country, we can't have public general scale healthcare models that work for every country, right? Because the demographics uh the characteristics composition, body composition, all of

that is very very different. If you if you look at the average height of an Indian male and Indian women versus average height of a Caucasian person, it's it's very different, right? So any

kind of metrics that we are doing, any kind of measurements, any kind of solutions that that we are developing have to be more tailored, right? So that is what we're targeting in this project

and it started with collaboration between India and Germany. But it scales up to any country that we are looking at and has to be adapted to different countries. So that is why our first

thrust area is open benchmarking and creating open platforms where we can collect this data annotate this data with respect to the practices the medical practices the clinician

practices that that are being followed in that specific country uh point of care explanability. So uh in in any healthcare solution explainability AI based healthcare solution explanability

is very very important right whenever we go to a doctor we are always asking why this why that what is this what is that right? If if same goes for a doctor also that if we are suggesting if any AI

solution is suggesting a doctor that this is TB this is something else and I remember in one of the sessions earlier this morning where I was uh the speaker said that he had presented a CT to uh to

one of the doctors and they said don't do lumbar puncture thank you don't do lumbar puncture I mean the reason is why why should I not do the the the diagnosis that they got was correct but

then obviously what is the reason of not doing it Right. So, so what is explanability then personalized digital well health and wellness preventive health care in setting like ours

preventive health care or for that matter anywhere preventive healthcare is very very important right why wait for something to happen before you uh start taking any precautions and then public

health AI given that the the ratio of a doctor to patients doctor one doctor treating how many people is is significantly uh we need to work on Right. So, so can we do public health AI

to reach to the masses and these are the key challenges that we are looking at explanability there fragmented standards across countries. So, how do we uh how do we unify that? There's significant

data diversity gap and we need to come up with model metrics just specificity and sensitivity is not going to cut cut it. So, we need to come up with metrics which are targeted towards the

application that we're looking at. Right? And in all of this trust becomes very very important because in healthcare trust and explainability and accuracy these are the three things that

matter a lot. So with that thank you thank you very very much professor Richa Singh. I would uh next invite professor Jakarti to talk. &gt;&gt; Yeah.

So so what I will be telling I will be speaking on the metrics for manufacturing and what we look forward is that we are transforming from the industry 4.0 0 to 5.0 Which means that

automotive AI to human centric AI right and trust and resilience are some of the issues we would we wish to take care here and when we look that there are certain critical areas like the AI

models or blackbox and we have the biases and reliability gaps in terms of the failures of the instruments and the sensors which drives our okay machines right those are the those are point of

concerns and obviously the security risk in terms of the data security, privacy and the attacks, the poison attacks. Okay. And obviously we need to we have seen some of the examples like we have

seen that the cost of op the opicity okay okay for AI leads us to the okay biasness right biasness in the scoring and we have seen some examples of some of the count's AI systems right

and also we see that the systems failure is not that statistical statistical estimation of failure is not always the operational failure estimation in the right direction. So those are some of

the things we need to take care of right and so the directions should be that we should have an auditable standard and the the mechanism needs to be set okay for maintain the standards and

compliance for the AI and we okay need to look into some of the actionable things that why models have failed right okay the puncture in the data what caused the puncture in the data what

what caused the a failure security failure right and we need to need to compare the compare the the AI systems across the various platforms to make a static benchmark. Okay. And so the

target is that we need to look forward to the uh safe and scalable operations of the AI and if we want to go in that direction what we need to look when we can look on to the following following

four key metrices which we have uh seen and studied and done research a bit of research. Okay. So first one comes the transparency and explanability which uh okay tells that that means okay what is

the why the model has gap that particular result and what are the what are the okay what are the core issues of the model whether the okay whether the explanability of the model supported by

the model creation itself or it is just a okay just a sort of a statistical prediction okay right and we we need to have the demographics equality and we need to have the equity also so so when

the model is losing whether the model is losing for a particular group or or a demographic group or it is or it is creating good only for a particular okay particular sector right that means and

if the model is doing good or doing bad it should be okay it should be across the okay distribution it shouldn't be in a in a in a skewed manner okay the key metrics are obviously that means we need

to have the robustness and we need to have the drifts will be there in the in the in the manufacturing system lot of sensors are there, instruments are there, data drifts are there, policy

drifts are there. So can we adapt to those drifts, right? And obviously the the the okay the direction towards accountability is that we need to have the audits which should be on the

compliance okay we need to look into that means how many times the human has overrided okay because if human override the trusts on the AI decreases right and we need to look forward to that right

and whether we have a complete traceability starting from the data to the maintenance of the AI models right and if we look to look into the future future road maps we need to have the

standardizations there are standardizations in international level have done okay right we need to follow those we need to have this have this okay the adoption bymemes there are a

lot of challenges because we have lot of lot of AI requires the competence and the competent okay those competences might not be there inmemes okay so how they will adapt to those we need to have

the models which can be just go models okay which are explanable go models and that may be the strategy and obviously the real-time operations we will not go with the we have the edge computing

where we can look into the monitoring of the systems all throughout the lifespan. Okay. So those are the things we can look forward and obviously unified governance. Okay. Starting from the

starting from the capture of the data to the maintenance and at the different levels of the system we need to have a one flow of governance and we defined by the policy and compliance and I think

that's all from my side. Yeah. Thank you all. So next uh professor manga so my very wellware well clock that you

can see there please come up the three minute sure thank you professor so um um I'm so and um I'm going to actually talk a little bit differently uh think of this way uh

governments are going to use artificial intelligence in their decision-making right? Uh day in day out probably they're already using it. Right. So so but but if you if you uh if you ask a

government official today uh explain the decisions the government officials if even if they're using the artificial intelligence they are going to explain the way they have understood the

decision. Right? So, so, so the whole objective is that if governments have to use artificial intelligence and they have to use it then how to make in how to ensure that the governments

understand or the the governments is understood well by by the AI tools that can be explainable to uh population at large and uh more importantly like can can we actually audit uh the the

decision- making process and and uh the the whole uh circle of uh governance Can can can this be made auditable and that's where like the whole beauty of uh taking the uh AI to 1.4 4 billion

population in India and probably 7 billion population to the world that's what we can that's how we can make the impact right so so um if if you have to think about uh implementing a large

scale project for example a project like Aadhaar a national ID project which India runs uh if you have to take it to the masses you have to think of if a decision has been taken to issue Aadhar

or not issue Aadhaar or or for that matter any such large scale project in healthcare domain or for that matter any governance level domain. How do we ensure that people can understand a

decision? Right? What are the measurements to explain those decisions? Right? And that's where our our unified citizen interface program or or or the or the pilot that that we are proposing

comes into the picture. We are essentially highlighting that when we are talking about uh uh people coming from different backgrounds, it the the program or the the whole auditable

mechanism needs to be multicultural, multilingual. uh if you're talking about India 22 official languages and thousand more than thousand uh unofficial languages right so so so the government

ecosystem has to the government AI ecosystem has to cater to the needs of uh this diversity right similarly when we talk about uh um interacting with an AI system then how you going to interact

um if if people are not able to um type in English right so so so probably can we do this in in a voice uh control system right so and and finally like uh uh if you have taken a decision if if

you have like released a thousand page or 100page act how can you summarize and explain this to normal people that this is what the act means right the act is essentially a legal language how to

convert this into a summarization format and how this can be explained to to to people and this can actually cut across citizens judiciary policy maker equally right so so if if someone goes to court

of law then when the judiciary needs to understand what the law is or how the government has taken a particular decision and therefore the measurement across this kind of governance is very

important and that's where we are talking about having an evaluation framework when we are using AI it should not elucinate in a governance setting right very important and and it has to

be consistent um the risks according to different applications need to be classified uh between low medium and high-risk uh uh settings and and there has to be as Anan earlier mentioned uh

with respect to standards and materology uh this these things need to be uh uh well well defined and as a next step we are proposing and planning to actually do some small pilots to explain to to

create an entire pipeline end to end pipeline that can be then uh later on documented and utilized with that thank you very much &gt;&gt; thank you may our last in this series is

preet okay so everyone wants to design powerful models right uh when it comes to powerful ful how powerful the model is. How do we evaluate? We evaluate

based on the accuracy. We create our own benchmark and show that look on our benchmark our model performs 90%. And your model performs 89%. So our model is powerful right and people keep coming up

with more and more benchmarks and showing that look your you know their models are better than our models. So the fundamental problem that we are trying to address here u is how to think

of evaluation beyond benchmark based setup right there are multiple problems in benchmark based evaluation for example if you think of two task let's say there are two task uh in a same

group and both the task requires some sort of knowledge extraction let's say typical let's let's consider two very well-known benchmark uh data sets like evil and mlu these are very standard

data It's could be use and you see that you know even within the same type of task uh there are two different models which perform in two different ways right so which model to choose for a

specific task. The second factor is of course data contamination. We know that you know when these models get trained they basically uh you know uh curate data from in uh from intellect and those

uh curated data will be a part of their training and you know uh that that would basically leak your uh benchmark data. The third part is very important. If you look at uh you know let's say a same

model family let's say llama right? So if you look at let's say llama 8 billion and llama 3 billion right a same family both of these models perform very similarly on ei bench benchmark data set

whereas they perform quite differently in any value they they belong to the same family if you look at same model size let's say llama 3 billion and co 3 billion they are of same size but one

model performs better on one data set another model performs better on another data set So how do we choose which model uh is needed for your task? This is the fundamental question that we are asking

and we uh you know the the uh group of researchers uh here we're trying to hypothesize that along with the accuracy based measures which are which are which are needed based on the benchmark we

also need to report something called the latent performance profiling of models right and latent performance profiling we call it LPP. LPP is going to be a suite of metrics which would

characterize the internal property of a of a model. Right? Now here I'm showing two uh examples of LPP. One is simple simply look at the entropy of the last layer of words and the other one is

participation ratio which looks at how compact your layers are. I mean these are just some of the example metrics but you can also think of your own way own ways to measure LPP. And it turned out

that these measures are related to the internal properties of the task. Let's say you are interested in some healthcare task which requires you know uh the entropy measure of a model.

Right? Now if you know that this model is superior in terms of entropy, you will choose that model uh you know model one not model two. Right? So task profiling is important and then model

profiling is important. And we hypothesize that I mean this is our uh prescription we are we are suggesting that when a model is released in the model card the LPP matrix also should be

released so that people will understand which model should be chosen for which task. Thank you. &gt;&gt; Thank you. &gt;&gt; Thank you very much. Please give them a

big hand. We now invite Professor Debep Mhaba and Professor Minak Mandal to talk very briefly on a very very important topic the confluence of security and AI. So on

stage sure &gt;&gt; so uh so we will be uh you know like talking about a topic which is like last but not the least. So it is you know like measuring AI but with the context

of security and safety. So we know that you know like when AI systems have been deployed there are multiple stakeholders which are involved and some of them are affected by you know like security

issues while some of them are affected by safety issues. So the question is right how do you really measure them? So if you look at the landscape of various attacks which kind of thwarts the AI

systems there are concerns about integrity in the sense that you know like whatever you are feeding as input could be malicious there could be potential trojans. There are fear about

you know like that your IP could be stolen which are called as you know the the model could be kind of reverse engineered. The IP could be you know like stolen and at the same time there

are also like you know like there are concern about privacy attacks where your coated data can also be you know like be compromised. So NIST while NIST kind of provides several attack scenarios what

remains as a concern is how do you really understand. So here is for for example you know like a slide which shows you that kind of shows you how fragile our AI systems are. As you can

see that there is a picture of big but with you know like with little bit of noise it is kind of diagnosed as an airliner. So the good thing is that if you look at and you know like observe

the AI systems which are deployed on the various platforms you will probably get signals and these signals are absolutely important to kind of you know like kind of differentiate the proper inferences

from malicious inferences. At the same time, it is also equally important to kind of you know like equip your team with a red team versus blue team configuration which means that you know

like there should be kind of uh you know like measurements and performance of various attack simulations and and and kind of you know like also considerations of the defense strategies

to really kind of assess the AI systems that are deployed in various strategic sectors. So with this I would like to kind of you know like invite Minak to talk a little bit about the safety

aspects. Thank you Deepib. So hi. Um so I'll just go quickly over and try to systematize as brief as possible uh as a state-of-the-art for AI safety. So if

you look into AI safety which essentially means that AI system should behave as intended which is as subjective as it can get. Uh you can actually think at least today what

problems we know in three broad categories. First one is intrinsic risk. Second one is interaction risk and third one is societal risk. What is intrinsic risk? That is something that comes from

within the AI because they don't have some knowledge gap. They don't know something, they hallucinate, contraate and all these other problems. The second is external or interaction risk which

essentially means that problems that come up when uh the AI interact with the users. Users just believing the AI and jumping off the cliff that will be your interaction risk. And the third one is

societal risk. when a societal scale we have misinformation generated by AI or some problem uh or some automated usage of these machines to do some harm to society and the reason a core reason for

all of this problem is measurement centric because when developers create this AI they just don't know how it will be used slash abused in the future to that end what happens today is to solve

the safety problem in general people measure uh or people try to actually defend them by by a measurement based rule based defense which can be broadly categorized

as a guardrail based defense meaning that the AI should not really behave in a particular way. You actually put some rules what they should not output and that is actually great because it will

stop toxic vocabulary it will stop PII or the private information leakage. It is pretty easy to implement just you can think of it as a modified version of EEL statement. However, it will also block

benign uh prompts. It will also uh does not really acknowledge the knowledge gap of the uh LLMs and it can be easily jailbroken. That has been shown again and again every year 200 papers. So the

issue is that these guardrails are blacklist oriented and the safety to make AI really safe. We need to be more adaptive and this adaptation should be measurement based. So this is the thing

that people are working on including some of our recent work which looks into improving safety. So for addressing intrinsic risk we tend to measure the knowledge gap of AI. We have built some

system about that looking at the last layer looking into the prompt looking into the actual training data. For addressing the interaction risk we had work on measuring the appropriate norms

of how AI should be used. What are people telling? what are the problems with it using a datacentric large scale view and finally for uh for addressing the societal risk we uh develop judge

models to do better in terms of safety of AI systems but these judge models are generally done by the measurement result of norms and knowledge gaps with that we would just want to resonate

that the measurement is paramount of paramount importance for AI security and safety because measurement will enable us to know that what is wrong in terms of what is insecure or what is unsafe

and it will also pave the path forward to mitigate it. Thank you. We will take questions here. Do &gt;&gt; you have Okay, we'll get get back.

Please give them a big hand. And we now move to the last part of our uh session which is the panel discussion. May I request professor Lipikad moderator of the session and uh

the panelists this is an international panel along with Dr. Dr. Kavita Bhya who is the national uh COO of India AI mission. May I request professor Wulgang Nagal. May I request professor Ken Maple

and Professor Sadhart Kasigit to kindly commence it. Good evening everybody. So now we move on to understand how to measure the effectiveness of generative AI as it

gets deeply embedded into the decision making processes of organizations. [gasps] Uh we try and focus on the best practices in AI measurement happening globally and also try to understand what

are the focus areas that India should focus on and lead. So a brief introduction of our panelists. Professor Siddhhat Kasagir. He currently serves as the head of safe autonomy at University

of Warwick WNG. uh he's a member of various international standardization and regular regulatory committees including ISO, SAE and the United Nations

uh economic commission for Europe uh on safety of self-driving vehicles. Professor Casten Maple from the Alan Turing Institute UK is with cyber systems engineering at the university's

cyber security center. He's director of research uh over there and has worked with organizations in key sectors such as manufacturing, healthcare, financial services. uh professor Maple is a fellow

of the British computer society and vice chair of the council of professors and heads of computing UK. [gasps] Then we have professor Wolf Tang Nagel from TUD Dresden University of

Technology. He's the director of center for information services and high performance computing. Since 2014, he's leading the national big data competence center SCADS uh funded by German Federal

Ministry of Education and Research. Finally, we have Madame Kavita Bhhatia. She is scientist from Ministry of Electronics and Information Technology. She is the COO of India AI. She has led

transformative initiatives like the National Data Governance Policy and the National AI portal. uh she's the driving force at matey impacting several national service delivery projects like

Aadhaar authentication Aadhaar data vault e-ign and single sign on etc without which we cannot imagine our lives today welcome our panelists so without further ado let me straight away

go to the questions uh professor kasagir my first question is for you unlike traditional software AI measurability is not only about accuracy. Can deployable AIdriven autonomous systems be designed

to provide guarantees for robustness as well as regulatory compliance? &gt;&gt; Thank you. Thank you so much for the question. An absolute honor to be here. Thank you for the invitation. Uh I think

we need to separate out the two aspects of the question. One is the regulatory compliance and the other one is the guarantees aspect. So in the in the space of uh emerging technologies like

AI where the even the regulators are trying to understand what should they be regulating what should it be a guideline should it be a full-blown regulation so if you take uh the approach in in

globally you've got an approach in the UK and US which is more on the guideline side and then you've got an approach in Europe which is more of a very stringent EU regulation. So I think let's separate

out the regulation for a minute because I think regulators need to understand what they need to do thinking of it from a more scientific manner. I think when it talks when we talk about measure

measurement and guarantees we need to first identify what are we measuring uh before we go into the metrics itself. It's first we need to understand what are we measuring. So some of the work

that we've done in uh in WMG uh and we we work in a space which is a proper embodiment of physical AI self-driving cars and what the biggest thing that we came out was was in order to tackle this

issue you need to bound the problem you cannot be working with an unbounded problem and the way we bound the problem was try to make an argument for the training and the testing data sets as a

representation of the real world and we've come up with different ISO standards in this space so we can talk about that a bit later in but I'll stop there in in the main message is to bound

the problem when it comes to representation of the training and testing data sets. &gt;&gt; Great. So let's move on to post deployment. Professor Nagel, what do you

think should be a way to evaluate AI systems after deployment? &gt;&gt; Well, actually um we we have not looked into energy. I think energy is an important point.

&gt;&gt; Okay. So energy measurement I think is important because at the end somebody has to pay the power going into the systems and um as the question you have put at the

panel is a difficult one on a worldwide level. I would say nobody can answer it correctly. [gasps] We we are doing projects in that area. So nice options to to find out and

identify how we can do that. But there's nobody who has an idea. We have 70 years of AI development and the neural networks and the fast machines the HPC machines have enabled us to make very

good success and now since we can talk to Jet GBT uh everybody believes now everything is solved. It is not and it will take years before we can guarantee that there are things coming and I'm

pretty sure that we need an opportunity to make sure that we can evaluate even on time that the models are not changing over time like we have have to do it with software in general. So that's to

the answer. Thank you for that very important point to remember energy. Uh very important point. Uh so moving on to professor professor Maple uh you work extensively in the areas of healthcare

and finance which are heavily regulated areas also and given this uncertainty of the creative generative AI answers. What are some of the mandatory measurables that you think are needed for again

deploying AI systems? And I would again like to also add what roles will regulators play in this. &gt;&gt; So thank you very much and thank you for the organizers for giving me the

opportunity to speak about some of the many projects that that I have. Um we're fortunate in the UK you mentioned that it's heavily regulated areas and that actually um creates an engagement

strategy. Um and for for me one of the most important things is as we've moved forward so as as Wolf Gang said um AI's changed I I've been on a panel early this morning uh and a round table and

neural networks been around for a long time. Yeah, they died a death for a while. Um, and then we got better at at compute and algorithms and we could do much deeper. But the thing is um as AI

changes, we we need to think about where it's being deployed. And in those two areas that you mentioned um we have to move forward and think about the system, not the model. Because you saw some

graphs before from from different models. Well, in some context that might be fine because it's not really important. But un unless we can really understand how a model the these sort of

blackbox generative models very complex unless we can understand how they're behaving then it becomes very difficult to correct the behaviors. Um in the UK uh the the regulator I work most closely

with so I do a lot of work in healthcare but in the UK the financial conduct authority has really been very proactive in its area and one of the important um developments that they created for quite

some time ago now was the development of digital sandbox um and those sandboxes allow organizations to test their systems within a controlled environment.

They've also engaged with the whole community from academics from um industry to create a synthetic data expert group because at the end of the day if you want to test um and I think

Sedartha you just said some some great points about how we test we want to test you need data to test and that data must be the right data to properly test um the data. I think one of the most

important things is about conversations. It should be the community coming together because a regulator should be there to protect um individuals and society but at the end of the day it if

it can do that by talking with industry by talking to experts in academia that will get the greater result &gt;&gt; digital sandboxes. Very good point. So I come to Madame Party. What does

measurable AI look like in India's setting given the vast demographic and linguistic diversity that we have here? Which are the most important measurable metrics for us?

&gt;&gt; Thank you for inviting me. This is a very important question from the India perspective. I personally think it should be the inclusivity, language and uh robustness should be one

of the most important aspect when we talk about um metrics of evaluation. In fact uh India being such a large and diverse country to me language is one of the most important thing which should be

uh taken into account when we are uh doing u model testing. the uh in fact we have taken a note of this and as a part of India AI mission we have already started working on it we understand that

the um as it was also mentioned by professor Mayang that 22 official languages and every few kilometers there's a different uh dialect so which means that uh we need to make sure that

the models should have the cultural and Indian context as well as the data on which it should be tested then only it will bring the uh concept and clarity of what India needs. So we have already

started working on it and in this regard we have already um selected or we are working very closely with 12 organizations. Four of the models have been launched in this summit itself. You

can in fact have a look at the models um being showcased at the mighty stall. Uh the second aspect which I also think is that the data which is also very important. So we have taken a note of it

and we have already announced AI kosh which has 10,000 plus data sets which are Indian data sets. It brings into all the cultural and diversity aspects and also we are making sure that the data ha

has all those principles which was talked about uh should not be biased should be very uh uh diverse that all aspects have been done plus um with the uh mission which we have already called

pashini we are very closely working with organizations like police so that we can make solutions which are u understandable to them in their language and last I think the model testing

should be to evaluate the solutions on the low performing uh response based the model than the high. So I think language is one of the most uh important aspect which I think should be a point of

focus. &gt;&gt; Thank you very much. &gt;&gt; A minute each now for uh some quick uh quick responses. So coming back to professor Nagel you say that these are

new we really don't know a lot of it. So we would really like to know which important aspects of AI systems do you think we still do not know how to measure well and what should be the

international community prioritization to close the gaps. &gt;&gt; Well u we are addressing like you the important fields. So if you work on health care for example, it is clear

that the answer has to be correct and therefore typically in Germany it's not allowed the system alone can direct the diagnose. It has to be always a medical person sitting in front of it and

checking with his experience if this is correct or not. I think this is at the moment the most effective way that we control the answers. This is not scaling because you don't have so many

people if you have many requests. But if you have people which ask for diagnosis I think we have that stuff available and we'll do that. &gt;&gt; Thank you. Professor Kasager. I would

like to ask you the same question. what is it that we still do not know how to measure well? So I think uh bringing [snorts] yeah bringing the uh two uh thoughts together the data aspect and

and also not knowing I think what we've been able to do at in the UK is uh come up with a concept what we call as uh an oasis concept which uh which actually come up creates an ontological model of

the world in for for your particular use case and and that allows you to actually make an argument for completeness and representativeness of your data because you can always say that the data should

not be biased. Nobody would say no to that. But how do you at an engineering level prove that is is the space that uh the researchers and the community needs to work on. So not just saying don't

have biased data tell us how do we actually at an engineering level create a bias-free data and to that approach I think uh an ontological approach to actually classify the world is something

that has uh succeeded at least in the high safety critical uh industries like the one that I work in self-driving cars where we've been able to drive international standards on this

onlogical approach where people agree on how you classify or describe the world a similar thing will need to be done for other domains like languages and others aspect one last thing I would like to

say when it comes to standardization uh we've been very fortunate in some of the areas especially in self-driving cars and and automotive the research community has played a big role in

driving the standards I don't see that happening in other domains because I I I think the researchers are the uh stakeholders who have the maximum uh uh uh innovative

in knowledge in this space and they need to come on to those standardization platforms. &gt;&gt; Thank you uh professor metro. So now my question do you agree with him? Can

there be really common global benchmarks and metrics for AI or do they need to be tailored to each country's data laws and culture? What's low? &gt;&gt; Thank you very much. Um and it it's

interesting because in the last 24 hours we we've launched some work from the ML Commons uh initiative. Some of you have heard of that. And that was really about creating a defensible and robust and

rigorous and repeatable um methodology for creating benchmarks. So um I I think there can be some some models that that are global. I think we saw it in some of the earlier talks about how you've got

high level and lower level. So I think the principles based should be consistent so that we can have an understanding so it can be uh compared but in terms of different sectors and in

terms of different countries that has to be um tailored. Um and if you look at that uh ML Commons paper that's out um in the last 24 hours, you'll see that we talk about culturally aware and

culturally nuanced. So the example given in that paper is if you were to give a clock to somebody from the Far East, from China for example, that is not a good gift to give. So if you ask a large

language model about that in the UK context, that's probably fine. But it means that there's a funeral pending. Um so so not a good idea. So I think yes the answer is we can create global

standards but they need uh global benchmarks but they need to be tailored for specific cases. &gt;&gt; Thank you Madame Hatia. Last word from you. Do you think there can be global

standards and or they need to be tailored to each countries? I think that the uh global cooperation on the foundation AI principles is essential but it has to be tailored for

every uh country because there is a lot of diversity as I said that there is a lot of diversity and to have inclusivity. So we need to have a tailored um benchmarks for every

country. And in fact um just to highlight again that um as a part of India AI mission we have already announced an institute which is working with 13 um educational institute on

various aspects of evaluating the models to come up with the frameworks and all the aspects which are important for metrics. We are very closely working. So we thought that we will follow the uh I

mean global principles but we will have the Indian context and Indian diversity and have the solutions frameworks tools which will be uh developed on the Indian requirements. So I I want it to be that

way and that's why &gt;&gt; thank you that's very succinct. We can take possibly one or two questions. Yes please go ahead. You mentioned about bounding right and would you like to

provide some examples &gt;&gt; just on the mic? &gt;&gt; Sure. You mentioned about bounding is important right? Uh so could you give some examples which are closer to

production use cases? &gt;&gt; Yeah sure thank you. So I'll take an example of physical AI self-driving cars autonomous vehicles or automotive example. So one of the things that we've

been able to do is create an on ontology or a taxonomy of the world itself. Uh so we we uh then you divide the world into everything that is static, everything that is dynamic and everything that

changes in the weather. So we call it scenery, dynamic elements and environmental conditions and then you can create a whole tree around that. Then you use that tree to actually

create your training set, your testing set and then compare that to real world distribution of those a aspects. So what this what we have been able to do is convert this tree into an ISO standard.

So ISO 34503 is an ISO standard that the entire automotive industry is using and adhering to to actually bound the real world conditions. &gt;&gt; One more burning question anyone? Okay,

go ahead. Hi. &gt;&gt; So my name is Saksham and uh this question is for um so the whole panel. I how can researchers basically you

mentioned that researchers are the main stakeholders but researchers are not only situated in India they're all over India even me as an Indian I work uh in Germany. How can researchers outside

also uh contribute to the research that is going on in India being uh being in another city or something else? In fact uh uh we want uh the uh researchers who are working outside to

contribute in the foundation model which I said that it should uh the Indian foundation model and in this aspect we are actually closely working with many of the researchers who are outside India

but Indians we are working in fact um couple of uh foundation models which we have approved as a part or we are working closely as a part of AI mission are Indian uh researchers outside but

were helping us in developing and foundation model. We had announced a call for proposal. Currently that proposal we have stop stopped because we have received so many responses we are

not able to evaluate them but shortly we will again evaluate and another thing which I said that AI safety institute if a Indian researchers also wants to contribute in the safety of AI um we

have already opened one call for proposal um ethical frameworks and all if you are wanting to contribute in that you can submit your response to that proposal and we will look into it and

the committee will decide and accordingly if as per our lines it will be selected. &gt;&gt; Thank you very much. Thank you panelists. Thank you wonderful audience

for being there till the end of the day. &gt;&gt; So first of all uh I would like to thank all the speakers and the panelists. I would like to rem remind everybody that this

you know 1 hour session or 55 minute session is just the starting point of a series of events that we are planning on this topic. This already we had an Indo-German meet which came up with

certain ideas and then today other than our group ideas and adding security and safety this panel put up some very very interesting thoughts and we are going to you know take this up in a much larger

framework. So any thoughts, ideas, anything that you have, I think uh please write to any one of us and we will be very happy to you know include that and take that onto this. I request

all of you to give everybody a big hand. I thank the organizers especially Kartik Arpa for doing this and my institute has personally requested me to thank all the speakers on this behalf. So a small

token With this we come to the end of our session. Thanks a lot.
