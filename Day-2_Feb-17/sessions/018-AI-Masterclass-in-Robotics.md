# AI Masterclass in Robotics

**India AI Impact Summit 2026 ‚Äî Day 2 (2026-02-17)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 09:30 ‚Äì 11:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 17 |
| üìÖ **Date** | 2026-02-17 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/__I6rjZBckw?feature=share) |

## üé§ Speakers

- Laksh Parthasarthy, Tata Consultancy Services
- Naresh Mehta, Tata Consultancy Services

## ü§ù Knowledge Partners

- Tata Consultancy Services Limited

## üìù Summary

This session, conducted by Tata Consultancy Services, will demonstrate how strategic AI goals can be turned into practical, production-ready solutions for inspection, and autonomous material movement, emphasizing resilient, safe, and compliant deployment at scale. Participants will gain hands-on experience with AI systems, along with industry-specific blueprints and replicable best practices.

## üîë Key Takeaways

1. This session, conducted by Tata Consultancy Services, will demonstrate how strategic AI goals can be turned into practical, production-ready solutions for inspection, and autonomous material movement, emphasizing resilient, safe, and compliant deployment at scale.
2. Participants will gain hands-on experience with AI systems, along with industry-specific blueprints and replicable best practices.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/__I6rjZBckw/maxresdefault.jpg)](https://youtube.com/live/__I6rjZBckw?feature=share)

---

_[‚Üê Back to Day 2 Sessions](../README.md)_


## üìù Transcript

where the rubber is meeting the road today. Uh examples, inspiration, some element of little bit technology terms. But that's first 15 minutes. Post that we're going to actually spend the

next five minutes. &gt;&gt; On each one of your tables, you'll see some devices as they are manifestations of what we want to do in terms of bringing physical AI to the life.

&gt;&gt; So there might be a robotic arm. In some cases, there might be an automated guided vehicle. In some cases there may be a quarter of it. These are the miniature versions of actual physical

assets that you see. We also have some friends today joining me. Um these are friends who have been deployed at customer sites but because it was a master class uh we actually had them

come in. First up with a huge round of applause I want to introduce Ekko. [applause] Echo is TCS's first humanoid and Echo has been trained a lot about TCS

culture, mission, values, everything. Echo has been deployed on a few customer sites for some targeted specific jobs. Uh worked very well for few cases where it is really really meaningful.

We also have Poochie a few of them I see. Poochie is the quarter ped robbo dog and Pooi has been actually deployed at three customers. Uh works very well with respect to you know looking at

inspections and safety. So these assets on the stage are a manifestation of you know what has worked and how we have been able to transform these into uh real real implementations. All this will

not be just on a PPT where we'll actually do this live today. Finally once the blueprints are applied we'll eventually go to the test demo space there we'll actually do some tests but

let me give you I have spoke about the last 10 years there are two different streams right if you look at it at one end we have this AI journey going on and at the other end the industrial robotics

journey when they started off there is traditional AI and there is industrial robotics both of them were largely deterministic what I mean by deterministic is both of them were

ruledriven Right? There were machine learning based rules. There were there were machine learning based algorithms but there are sequence they are deterministic. Right? They were trained

on a historical data. Industrial robotics on the other hand also did a repetive action which was governed and determined. Then came in generative AI became a catalyst for a lot of

transformation and within generative AI what it did it was it actually brought a lot of augmentation. you're able to augment the human work and simplify the day in the life of a

persona with something new and that what generative added to the AI world. On the robotic side, we had collaborative robots where the augmentation was done by a robot with a human plus robot

operating model. Then came in agent AI &gt;&gt; and then came in agentic AI and agentic AI actually uh brought in orchestration and action part uh stitched across multiple agents and in case of robotics

or multiple uh robotic arms versus two physical assets it could be anything but why I'm showing this chart is if you look at it the top line and the bottom line they're actually converging and

this convergence is leading to what we call physical It's where the digital AI gets bridged to the physical asset and the era of software defined physical intelligence

is here. But the question is why now and why India I want to spend a little time here. If you look at global aspects and what's happening in the west and not just west in general globally

there is an aspect of let's take a look at supply chains you are actually in in west and globally majority of other places you are actually looking at supply chains to and using AI to

actually address the brittleleness resiliency fear and supplier ecosystem integration you know bringing in AI there's geopolitical risk tariff impacts all of

In India the situation is a little different. It's a unique challenge and a unique opportunity and it's more about you know looking at how you can build some sovereign capability how you can

introduce newer nodes function supply chain how you can do poly sourcing take about an example of a legacy factory floor right in the west you actually bringing AI to basically do

transformation uh with legacy OT systems you're trying to optimize on top of that India has a very unique opportunity and a significant advantage now to from from

where they were to actually going into AI native factories and AI native industrial corridors that's why it's a very unique opportunity then we talk about capacity in investments in driving

this consumption model in country like India we have a very significant opportunity to look at you know robotics as a service fractional robotics and last but

not the least &gt;&gt; while automation in the west is being looked at more from a uh aspect of addressing skill gap issues and workforce issue. India doesn't have that

big of a problem. It's more about amplifying and automating the workforce to do more with less. So given all these different aspects of comparison, there's an aspect of bringing the

digital version which is the bridge between what we have to bring the digital version to the physical assets and address two big challenges

right which gets mentioned. One is the last mile access to basic services whether it is public services, education, healthcare and address the problem of missing middleman

intermediary and that is possible with physical eye through last mile workers last mile technology last mile process to achieve and address both these challenges. Great. So now we have spoken

about context of physical AI spoken about technology evolution spoken about region and once for India there's an aspect of manifestation of each one of these right so you have the digital to

physical bridge gets manifested in multiple forms we have humanoid you're seeing one on stage today here you have quadriped you're seeing there you can have a combination of drone and

quadriped for one of our customers we did an orchestration of a drone based imagery with a quarterface which ready to derive something very meaningful. Then there is AMRs and AGVs. There is

autonomous vehicles of course there could be coarts. There are multiple manifestations right but here are some of the use cases that have come to life in case of humanoids. They're actually

starting off doing pilots for multiple customers in terms of you know actually utilizing their capabilities of humanlike interaction their dextrous hands to be

able to pick up a payload of let's say 3 to 7 pounds picking knitting sorting kind of things the biggest benefit is they're available 24/7 uh same thing if you look at autonomous

vehicles not talk much about it but V2X integration is becoming mature by the day and last but not the least you know quarter Quadruped actually the deployments have

gone far more mature into customer ecosystems. Let me show you actually one example of this art of possible things that coming to life. So here is an example of a

deployment u that we did for one of the agri tech major customers and in the context of the customer it was actually they have a large warehouse with processed food facilities scenarios like

uh oil spills gas leaks ammonia vapors these are all very common fail points in this kind of situation. So we came in we looked at the entire stack right from infrastructure all the way to

intelligence. We selected the device which will actually replace the manual process of inspection. You know what in actually in this particular case every inspe inspection was done every 4 hours

and every inspection took more than 90 minutes. Um and because of the hazard situations going in and doing the inspection also was very troublesome. So when we came in we actually uh brought

in [clears throat] uh selection of the right asset to basically do the inspection. Then we actually looked at elements of LAR sensing pre-processing the data mapping it on the GPUs. Um you

know we have the Nvidia stack there which is resides on the border. uh then from there we actually looked at uh aspects of building the model, deploying the model and then eventually

continuously making it learn to define. So the end to end view helped us drive the whole safety incidents by about 90% and operational downtime uh by about 30%. Now this is not just an

innovation side project. This is actually live in three different sides of globe. One in China with 30 a fleet of 30 border PS are actually working live. Poland is

live with about seven and then there's lat which is ongoing. This is actually a progress of getting inspection and service in action. All that is what the initial context

about physical AI. What we will do from now is take some of these use cases which we just spoke about in their own little small way of identifying what we can solve in the next 75 minutes and to

be able to help you walk through what's going to happen in the next 75 minutes uh in the logistical part of how we going to do it. I would like to invite PN and

Thank you Nish. Uh good morning everyone. My name is Laksha. The room is completely packed but still there are some optimization we might have to do. If you see in each table

there are two different types of assets uh which is either a but it could be different. Some of them have ADB and a quasi some have a robot account and uh team of two. If there are more people

than that, we'll make a team of three max. But if there are some tables at back empty, there are somewhere places if you want to move around, you can do that. If you're interested in a

particular type of device and for each of these device, we have mentors. I'll just call out if you're for example, if this is basically a puppy pie. This device which looks like a quadriped is

called a puppy pie. All the puppy pie mentors can you raise your hand. So any any issues during the pro we have a manual we will go through to get together together but if there are

issues they will be walking around and addressing it. Uh mentor pipe mentors metals there. Okay. Yeah arms they are there. The robotic arms are on

the back side. In case if anybody wants to work on a static robotic room they might have to move to the back side. So again this seating is very similar to the but I think while we had kept two

seats empty but I think it's all full the the the plan is to spend the next 50 60 minutes follow the handbook we'll we'll show you a quick preview of how do we log into the what we call as a TCS

that's [laughter] SAP the platform that helps us to implement a blueprint into these hardware devices where we don't need to code anything it's not that we have kept it coded but it's a workflow

based system where you can plug plug and play models. You can do a little bit of configuration, little bit of tuning, but essentially it's a no code based program where you will actually see implementing

certain things in the physical asset and at the end of the day we will go implement it. There are testing fits for example for the AGV that's the testing kit. We will go one by one and whatever

you implement will be tested there. For the quadriped or puppy pie this is the testing area we will test it here. The robotic arm doesn't need to move. They're already stationary on the table.

So wherever we build we will test it out. Right. With that, I'll call Prat to give a quick glimpse of the tool workflow. Thank you. &gt;&gt; Okay. So, uh you you'll have to first

login Good job. monitor that if some of you need to move around and find for yourself that's also fine right or you can start logging in

and follow the stream so each device two two people one device one computer that essentially the concept &gt;&gt; you want to work on &gt;&gt; is there available

which one are you interested in using this one or this Have you decided yet? You can move that. &gt;&gt; Huh? I want the back. You can go there

as well. &gt;&gt; Okay. So, uh this is the platform uh AI orchestrator that you will use today. So uh before you reach here there are couple of steps which is written in your

manuals and also the mentors will help you with that. You first need to log to a virtual desktop and from the virtual desktop you need to enter the login portal. Uh what login uh you will be

able to uh come to uh the homepage. Now at the top this is so a minute on what this is really. So uh this AI orchestrator is actually you can say a collection of catalog of AI resources

that you will need uh in deploying real world AI applications. You have a model catalog which of course you will not need today but this is something you know an

enterprise when it wants to deploy AI applications will require a lot of resources like you will need IoT connectivity you'll need data pipelines you'll need models so all those things

are big catalog of templates So you can one way to think of it is of templates which let you select certain templates then deploy them. So today you will be primarily using a resource called the

physical AI right. So based on the devices that you have what we have done is we have created a certain collection of template which you will use. Hello. Yeah.

So based on the device that you have uh what we have done is we have created a set of templates and scenarios which are inbuilt and what is happening behind the scene is that uh it's taking those

catalogs and it is deploying including the workflows uh which will be deployed and connected to your individual uh devices. you will have to do some configurations and publishing and etc

which the mentors are going to tell you. So this is going to make it easy to deploy complex and as it was earlier said it is kind of a low code low configuration thing. So with this uh I

am going to hand it over uh to the mentors to guide you one by one. First logging into your virtual desktops and from virtual desktops you do this. You will also do some editing of workflows

and publishing them and making your robots work. So thank you. Thanks everyone. Let the games begin. I want all the mentors to come at their respective table so that you can

actually get started and let's make it interactive. It's okay to make mistakes. You're here to fix one by one. But idea is to basically get to a point where we can get the physical asset moving and

actually bring the physical AI blueprints to life. Good morning. Mother 35. [laughter] Come on.

&gt;&gt; [music and singing] &gt;&gt; Some shop. [music] [singing and music] [singing]

&gt;&gt; What this will do is basically [singing] object. &gt;&gt; [singing] &gt;&gt; We have already created the device. I've chosen the types of Anyone

[music and singing] just like you don't check This is the photo.

&gt;&gt; Hello. Usually pictures. &gt;&gt; Are you safe? Let's talk.

Cut it. Those are &gt;&gt; table. Cameramanual table.

&gt;&gt; [laughter] [laughter] &gt;&gt; network. They are controlling these people. He's not he's our

network. Wi-Fi. signal is there. My chain wasn't &gt;&gt; So I wonder Yeah.

Good morning. &gt;&gt; [music] &gt;&gt; This is a robot robot robot. What the

heck? just So that they'll be able to live. &gt;&gt; Yeah. If you show any other

models can be two colors. Let's connect. &gt;&gt; They don't discover that. &gt;&gt; You like open

&gt;&gt; Please. color. Sorry. I think you can pick up

the battery. left. Check this. You have to talk. [singing] Uh-huh.

Actually every morning about I can't [singing] wait. Okay.

by 3 7 But I hope it's connected. AI1, AI 7, AI 1, AI 2, AI 5, AI 7, AI 10 and AI

I passed. [singing] Wait, I audio international

YouTube. Yes. Audio. from my journey.

area. No. Extra audio mic audio. The BL Everybody

audio. Don't know about &gt;&gt; [clears throat] [cough] &gt;&gt; Where's my bag?

&gt;&gt; Hello. Hello. &gt;&gt; [singing and music] &gt;&gt; Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello. Hello.

Hello. Excuse me. Huh? The beauty about the orchestration

platform is you can have multiple brains, agents of agents, right? So you could have a Gemini model, you could have an anthropic model, you could have an Azure model and let the AI agent

choose which is the best model for that given in intent of the user. So that's a brain part. That brain is stored in the memory and then every AI agent should also have an action part. So perception,

cognition and action. So the action part is where it captures an image, captures a live data, sends that live data into a vision language action model. You can see another model coming in here and the

action is it has to follow the line. In case of the robotic arm, the action is to pick something. In case of the AGV, the action is to move something. So if you actually see for the AGV,

&gt;&gt; what we do here is the same. The workflow remains the same. The AI agent remains the same. The basic construct of the AI remains the same. For the AGV part, we take all the input data and

send it to Nvidia QOP. QP is a route optimization algorithm by Nvidia. And by the way, you could have any different models here. It's like a it it basically solves a traveling salesman problem.

Where to go first, which order to serve first and which order to deliver first. So we send an API call to the Q opt and the QOP comes back, gives a response, gives it to the AI agent and then the AI

agent executes the action. So behind the scenes all these complexity is happening. Now what we do is through the meridian platform all those complexities are hidden away.

&gt;&gt; There's an abstraction layer because people from business people from technology they don't want to get into the details of how the orchestration happens that they more interested in the

business outcome. So that's where this kind of a platform helps where all the complexities is hidden and you have a very simple plain UI which you can Yeah, please

something problem with a personal &gt;&gt; So if you actually see here &gt;&gt; so basically all the input that you're getting the factory data is a CSV file where it says pick up location drop off

location. We have creat [clears throat] this data from an enterprise system and then you can plug that data into the Q agent. The Q accepts data in JSON

format does the optimization and then gives you the optimal route. &gt;&gt; Yeah. Yeah. Yes sir. See, there's no real math for it. Like mentioned, right? One of our customers

for a warehouse about you know about about 15,000 ft they're about using about 30 quads right again this is there's not really a magic formula for that but it depends on sensitivity how

frequently you want to do the inspections what is you know whether these two agents can coordinate how many times you want to do the

inspection what you want to capture so so the basic starting step is first we the entire facility, right? Once the entire facility is mapped, you can precisely know how much time it takes to

one end of the factory, how much time, how much time it takes to do a basic capital. Capital A capital I [cough] capital RO

6 These are alligates. If you have a camera and we don't give any coordinates just put it anywhere in the factory and based on the camera it

will search the directly optimize &gt;&gt; one One way password We start taking what?

All right, we'll take another 15 minutes to do the rest of the demos and then we'll take a checkpoint. Another 15 minutes to complete the rest of the demos.

want you next to me. Yeah, you need Just watch it.

Funstone. Looks better. Can you can you please let me give me a thumbs up? All demos done not done.

Can you please raise your hands who have done? &gt;&gt; Okay. About 60 65%. Okay. All right. &gt;&gt; While folks complete, I wanted to bring back the humanoid. I got an interesting

question from one of the gentlemen here. He said, okay, what are the aspects of humanoid use cases in financial services? uh and I did wanted to bring that okay

there's an aspect of of course doing parimeter screening and being the guardian of a physical branch but there's also a lot of uh vision plus brain that you can embed right so a

fireside conversation with one of the first TCS humanoid is what we did recently uh we can of course ask questions to echo he's been trained on a lot of subjects there's the behind the

scenes public which is there uh but I just thought I'll quickly ask a few questions if you will. Um, &gt;&gt; hey Co. Thanks a lot for traveling all the way from US to India. How does it

feel being here for the physical AI master class? &gt;&gt; Hey co, thanks for coming all the way from US to India for the physical AI master class. How does it feel being

here? &gt;&gt; The mic is not working. &gt;&gt; Okay. While you reset. So what we did is with this uh humanoid we actually uh got it engaged

actually trained it on a lot of subjects of how any TCSR would basically be inducted into an organization. So we're talking about the future of human plus agent operating models. If this would be

an employee of the future in one of our organization enterprises what it would be like right u initial days a lot of experimentation exploration but I think uh if you anybody who wants to come and

feel the dextrous hands can please actually come and touch and feel the dextrous hands. So the dextrous hands it actually has gives you a sense of you know what kind of payload it can uh take

you know how it can actually receive um and stuff like that. So you know what uh Arun if you can just come so um

this humanoid um has two computers in it right uh one is the inside computer and one is the GPU computer. And GPU computers where it has a camera and it has a lighter here. You guys can

actually come in touch and feel that. So that camera and LAR give it a good situational awareness perception around it. And it also has a small speaker and a mic. So it can hear us and it can also

speak. Now talking about the degrees of freedom, right? The robotic arm that was there on the table has six degrees of freedom. Basically it can move in six axis. The quadriped that you see which

slightly walks, right? That has about eight degrees of freedom. Any guesses how many degrees of freedom this humanoid might have? &gt;&gt; Infinite is too [laughter] much.

Any any guesses? &gt;&gt; 30. No, six is too less because arm itself has six. You know the complexity increases, right? &gt;&gt; Huh?

No. 43 43 degrees of freedom. Somebody said infinite, right? Humans have I would agree. infinite degrees of freedom. He can do anything. Left arm, right arm, spin, anything I can do. But

these guys, right, they have a certain axis of rotation. So this is 43 degrees of freedom. And what you see here is a five finger dextrous arm, which means it can basically sense it can it knows how

much of pressure to apply and things like that. So uh that's that's the main structure of it. &gt;&gt; Um see it's like it's a closest approximation to what what a human

think. So for example Tesla is also coming up with a robot called as Optimus right that's about 49¬∞ of freedom right so I'm sure that one day the way today RAM is becoming a commoditized quantity

in PCs we started with early computers at 4 GB RAM then eight today 64 GB is like pretty standard and it would keep on increasing similarly at hashbs yeah at head can actually

rotate in 360¬∞ so there will be a point in our lifetime where this number of degrees of freedom could be 100 and where you'll have human like action, right? So yeah, so that's about the the

hardware overview of what is happening within that. So, uh maybe you can just uh I'm functioning within normal parameters. Thanks for asking. It's

great to be here at the India AI impact Summit 2026 discussing the latest advancements in physical AI. How about you? &gt;&gt; I'm good. Thank you. So um uh the

participants here at the India AI summit um can you tell us a joke about a human vision? &gt;&gt; I'm glad you asked. Here's one. Why did I get kicked out of the robot bar?

Because I was making too many mechanical jokes. &gt;&gt; Okay. Borderline humor that was. Anyway, um so Eko uh tell us um why are you here and what is your role today? I mean we

are all a bunch of humans around you. You are a robot and uh what's exactly your purpose in this hall. &gt;&gt; I'm here to assist and provide insight on physical AI specifically in the

context of the TCSAI physical AI masterass. As a humanoid robot, my role is to help participants understand and interact with physical AI concepts through hands-on demonstrations and real

time feedback. I'm designed to amplify human thinking, not replace it. &gt;&gt; So, I'm here to facilitate learning and exploration in a collaborative environment.

&gt;&gt; No, no, just ask a question. &gt;&gt; Hello, I'm Dr. James. How are you? &gt;&gt; Nice to meet you, Dr. James. &gt;&gt; I'm functioning well, thanks for asking. It's great to have you here at the India

AI impact Summit 2026. What brings you to the TCSAI physical AI master class today? &gt;&gt; What can you do in the areas of rocket science?

As a physical AI robot, I can operate in various environments and scenarios from small scale to large scale applications. I can assist with tasks such as inspection,

navigation, and manipulation in confined spaces as well as in larger areas like warehouses, factories, or even construction sites. My capabilities are designed to be adaptable and scalable,

allowing me to work effectively in a wide range of settings. in the society. Thanks. &gt;&gt; You're welcome. It's been a privilege to be a part of the TCSAI physical AI

masterass and contribute to the advancement of AI technology. I'm glad to hear that my efforts have made an impact in society. No. So I think one thing I wanted to

make a comment is behind when when it actually started conversing you might see how it started interacting the hands and behind the scenes there's actually a gesture engine that has been built every

single statement of course there's an aspect of LLM the LLM response has been basically I would say mapped to a given gesture there are about what 30 gestures &gt;&gt; 31 gestures that we have built basically

mapping every sentiment of a statement to a certain degree. Now what happens is technology wise if you debug this there are three different pipelines. There is a vision pipeline which is through the

cameras there is an LLM brain and then there is a gesture piece. Now let's say I ask or Arun asked a particular prompt or a question Dr. Jane asked a particular question and let's say it

gets into a conflict saying okay I have to move both my arms in this particular direction and then if it gets into a conflict of saying okay my both the hands will collide if the gesture gets

to that point then it goes into the exception loop it breaks the chain right and that is purposefully happening so that you know at some point in time when the gestures are in into a conflict

situation we don't want the metal body to get ruptured so lot of advancements on getting physical AI to life has a lot of backend complex componentry. It is not as as it easy as it appears. But

with what you saw today in the master class with some of the blueprints with what you saw in terms of some of the training that we have done things are getting better and better with evolving

every single day. I think that's the whole aspect we wanted to uh bring into the master class. I just wanted to open it up for the entire uh korumir and see if you have any other questions, any

feedbacks, sentiment, views from today's uh session. Anyone? &gt;&gt; So the fully loaded version costs about 120 130k US. Uh but then of course with a lot of degrees of freedom the

complexity of you know making it uh getting getting this destrous hands the packaged version which comes out of the box is roughly around $45 to $50,000. But if you add complexity then it

becomes a little more. How would you uh see the from ROI perspective the agent take AI versus the genai and its applications? &gt;&gt; See I think um if you if you honestly

ask me the world of Gen AI I think the ROI is completely business specific. So I I'll give you some examples. Um use of Gen AI in case of transforming the day in the life of a sales rep or a field

technician on the ground has been proven right. You can automate a lot of stuff, generate a newer piece of insight, simplify the day in your life. Agentic AI actually bridges the loop between

insights to action, right? Which is take those actions as well. Physical AI actually says that okay, you have taken an action, but if the action translates or manifest itself into the physical

world, then physical AI comes into action. In terms of ROI for physical AI, I mean agentic AI, amazing ROI for cases which really demand agentic. You don't need a bazooka to solve to kill an ant

right basically you you want to basically address agentic AI where it is demanding when it comes to physical AI the use case that I actually gave you an example of an implementation for a agree

techch customer now you think about it that safety hazard and all of that but if you really take a dipstick into you know what it does it addresses a major skill gap issue that we have globally in

some of the advanced countries right what it essentially means is for cases where there are no people and there are issues, you are able to bring in a quadriped or a humanoid to address those

inspection issues. Like a quadriped with an IP67 certified shield on its back, it can go into hazardous environments, ammonia vapors, right? Of course, there's an aspect of safety, but the

amount of coverage of inspections that it does in the limited time, the number of inspections it does, the accuracy with which it does, it's it's amazing, right? But

you just cannot pick any use case and then say okay I want to try this that's why I'm trying this use case. You have to basically qualify the use case from a business ROI standpoint and then do it.

I would say in about 50 to 60% of the cases where we have tried doing something like this with physical assets uh the ROI has been very very solid. There are of course 40% of the cases

where we have failed also but it's a learning journey which all of us will go through that experimentation. Uh so first of all I'd like to really compliment the TCS team. I think you've

done a great job. I have uh so I have two questions. One is uh the co-pilot mode that you use your onprem data to train the robot

uh versus the genai model where you have open data to train for more efficiency uh which is used here and why that's my first question and second is the whole AI poisoning cyber security concerns

guard rails which you need to build because uh it's kind of contradictory that the more data you feed in the more efficient it becomes but it can be also AI poisoned or it could be guardrails

which can be contradictory too. So how do you solve that uh difficulty? I think both are very relevant pertinent in the context of where AI journey is today and both are extremely valid

questions. I'll um interpret your first question. I think your context was uh on-prem co-pilot data versus actually using genai models which are basically you're referring to large language

models. uh in this case and some of the other demos that we are doing I think SLMs are the way to go only if you do not have a large element of non-deterministic

general research which is required in majority of the enterprise use cases you will end up believing that SLMs might be good enough as long as it is tuned or fine-tuned or context because it's

always a hybrid mix of deterministic and nondeterministic use case you will never have a see in in in global industry barring research and coding there are only two non-deterministic use cases

nothing else is completely nondeterministic so majority of the times you'll feel that SLMs would be the way to go second question on AI poisoning and the use of guardrails um

bringing that view I would say that in the last 18 months industry has taken multiple industries financial services uh you know manufacturing healthcare many industries have taken the path of

addressing the lowhanging fruits with AI Okay, there are use cases which went into production. If you also see that all those use cases were actually use cases where you didn't have much to do

with uh elements of uh enterprise guardrails or agents interacting with the cross foundational systems. If you have an SAP, if you have an Service Now, if you have an um some other system

which is uh foundational and then you have to have agents work across all of these systems in one agentic workflow that is easier said than done because you need to have policies and the

guardrails for every systems talk to an enterprise guardrail and then basically deliver that. Uh things are maturing like we are also trying to get that done. And we have done it with

deterministic still working with uh where agents autonomously determine you know which uh policy to invoke for what kind of foundational systems. That's the end state. We're all working

towards it. I hope that answered your question. Any other questions? Yeah, if you can please switch on the mic. &gt;&gt; Yeah, go ahead.

&gt;&gt; Yeah. Uh hi I am Dian and I wanted to ask if uh first of all is the uh hardware of the robot completely inhouse for TCS from TCS. &gt;&gt; So there are couple of areas where we

are working on the hardware of the robot but on this one no it is not uh inhouse of TCS. &gt;&gt; Okay. &gt;&gt; Uh there are like multiple companies we

buy. There is figure AI there is Boston Dynamics uh there is &gt;&gt; from unitry. &gt;&gt; This is unitry. Yes. &gt;&gt; Yeah. Yeah. that that's okay. My second

question was uh as I was observing this robot I and I know this in general is that using language models is very uh it has a lot of latency. So it cannot the robot cannot perform reflex actions for

maybe it is in some factory and maybe some box fells uh uh fell. So it cannot just reflexively pick it up uh or maybe prevent it from falling to the ground. So how do you deal with such edge cases

in the way you are develop developing this technology? &gt;&gt; I think it's uh the answer lies in the question it's edge edge cases. So you have to also determine what goes on

cloud versus what remains on edge both not just on the uh aspect of deployment and visioning pipelines but also on the element of data prep-processing uh and creating those pipelines for data

prep-processing in a scenario like a shop floor will become extremely core um I would say the use of physical AI for complex edge scenarios like that is a distant uh

thing it's not actually uh I would say real in terms of implementing it today but when it comes to taking actions for example in autonomous vehicles there is so much of edge case scenarios that we

normally handle right those elements yes but dealing with physicality of those elements in terms of you know dropping boxes from the top where you have a data prep-processing lag I think those use

cases still needs maturity sorry &gt;&gt; yeah yeah &gt;&gt; thank you &gt;&gt; See uh this this is in continuation to what is what is the indigenous status of

hardware and country not only TCS that is one thing. Second thing what about the robustness of uh this thing how uh different environment and all and third what is the battery backup and how much

time uh these three things can you answer? Yeah. So um Indigenous right so we were a little bit late to the semiconductor race but um 2019 uh we have Tata

electronics um where uh uh that's going to be India's first semiconductor fab facility two facilities one in Assam one in doera it's coming up you might have heard about it um I think we are on the

path for that indigenous vixit bat uh 2047 target now the interesting thing was in Tata electronics For fabrication of these construction sites, we needed somebody to go and check these

construction pillars, check the construction progress and things like that. So there we have actually deployed uh EOS's brother u where is Puchi somewhere around. So Pooie is like a

national hero because it served in two um construction sites and continuously monitoring and scanning these facilities. It actually sorry it actually did a 12-hour work shift for 45

days um in uh this was Assam Assam Jagi road site. So it actually clocked that shift monitoring the gaps in the flanges and actually relaying that imagery with the drone imagery and giving the

real-time view of you know what it would mean from a safety hazard perspective what it mean from a continuous check on the flange so that it does not you know pile up and eventually becomes a major

issue. keeps the whole construction of the plant on time and schedule and and Puchi also met the Indian finance minister. &gt;&gt; Yeah. And again hardware right see for

us um these are all just metal aluminum titanium actuators joints cameras you know we always taken a hardware agnostic approach right the software that's running on this the gesture engine right

that is the world's first gesture engine that is something that TCS developed and we are considering applying a patent for that similarly the the whole integration pipeline and all of that components

developed inhouse and a lot of components actually reside u right inside this thing Right. So that's where the brain resides. &gt;&gt; Just one point on the indigenous aspect

also. If you get a chance to visit Tata Pavilion, you actually have Tata Motors with their AMRs and AGVs completely indigenous where SOP's infrastructure to intelligence is completely built in

India right uh and we have three different variants 300 kilos and 500 kilos and 1500 kilos three payloads. uh so a lot of advancements in terms of AMRS and AGVs and uh in fact we have a a

quadriped also right it's called as ashwa which is completely indigenous uh so if you get a chance to be at the Tata pavilion you uh this is in hall five hall five yeah

&gt;&gt; yeah battery backup is like about 4 hours uh continuously so this like uh 15,000 mh battery 432 watt U you know uh but the good thing is some of these robots are capable of

selfcharging and replenishing their own battery source. So they know when the battery hits 10% where do they need to go and uh hand over the shift to its next inline robot. Right? So that's

&gt;&gt; in terms of accountability like if you deploy it in some big warehouse and some terrible accident happens then who is responsible because like you have outsourced this to someone and the

coding is done by TCS team and deploy it custom based on client and there is a company involved. So if everyone does their job perfectly and some terrible thing happens then how do you like who

will sue who like how that see the legality a it's a case which has happened in uh driverless car Tesla uh it misbehaved and it killed a person and it actually

is a legal case which happened that who should be sued should be Tesla or the software provider or the hardware provider or the driver who was sitting next to it and it had moved to a

driverless mode. So after about two years the court ruled that Tesla should be sued for this and they had to pay compensation. See I tell you it's a very valid

question with no definitive answer from anybody in the world and the reason why it is is because it's very subjective by nature right now when let's say TCS or any other firm delivers a particular

scope of work of course it is within the guardrails within the boundaries we have a very clear defined uh legality and liability aspects of you know what TCS will own in terms of accountability

within those boundaries definitely yes we own it right uh but The whole nature of AI is probabilistic. Like I was telling it is not a deterministic world anymore. You'll only pick up those cases

which are probabilistic and okay to digest and in that Gary. Yes. Uh what one other thing I wanted to bring up. How many of you have heard about couple last to last week announcement on mold

book and just raise your hands. Okay. About roughly 10 to 20%. Okay. The reason why I I asked this question in one of the recent uh internal forums also the reason I am asking is you're

talking about how do you resolve these conflicts and legality and aspects of mold book is basically uh agents posting what it did why it did and stuff like that and resolving autonomously itself

right it's it's like a Facebook for agents and autonomously they are posting and trying to resolve and address the issue but my in my view the biggest benefit of let's say an evolved mature

mold book would be a case where it can create knowledge fabric which will only get the accuracy of every single workflow up and when that happens is when you will keep getting the conflict

scenarios down. Sorry, you had a question. Sir my question is that these rowers are mostly dependent on vision for their mostly task and like if there is a

situation situation creates that which like there's a problem in vision and so on then how it will deal with that blog like that situation I can take up a very simple question right we don't

light is very good as long as there is uh cameras are very good as long as there is light but in complete darkness right you want a point cloud data lightar so the LAR works better than

night vision it works in all different scenarios and it can work where there's like uh complete darkness right so just the way bats navigate using sound as sonar you know bats don't have eyes they

just echo sound and based on the waves they travel very beautifully right that's they're in the nature so So we're trying to replicate that. Right? Now here's a part. Elon Musk doesn't like

LARS, right? And the reason for that was not because he doesn't want to make the car safe. It's just because the cost of the light were very high. The cost of light are coming down. So we always have

two uh uh vision systems, right? One you can almost say this is a redundancy built into the system. One fails, the other kicks in. And generally we fuse both the data and then give a very

comprehensive coverage of the surrounding. &gt;&gt; So like we have seen in recent am models there is a saturation is in its intelligence at level two computation.

So what will we expect the intelligence of these rowers like there is a huge saturation is coming to the LLMC intelligence. &gt;&gt; See

I'm just trying to understand your question. See when you when you say about saturation on LLM's intelligence it by and large largely depends on you know what level of guardrails you are

working with on prompt engineering how effectively you are creating the training pipelines and what you're using for if you just take an LLM out of the box and just try to do a rag on that of

course you'll end up in uh assuming that the saturation is down so the in my view as a technologist I'm just giving you a view that LLM journeys and the intelligence is a three three-step

ladder right you start off with rags on LLMs you'll end up with 60 65% unless your use case is just general knowledge but if it is really contextual just use rags 60 65% Dr. Jane asked a question,

the response was 60 65%. You know why? Because it was not trained on that. If you go and do a little bit of deeper prompt and context tuning, it will go up to 80 85%.

If you really want to get to the 95% and above range and in the context of let's say physical AI, just giving in work instructions of a shop floor will not get you to 95%. You know why? Because

the real tacit knowledge in the in the brains of the people who are working on the shop floor. Those fine adjustments, those intuitive decisions that they make on the ground is not captured in any

work instructions. It will only happen when you actually observe the worker and then marry the work instruction and then create that anomaly and then tune it again. And it's an iterative process. So

your saturation depends on these three layers and how you're using it. Hello first of all it was a great session over here and thanks to all the mentors who

have helped us through uh it's related to robotics the question I have like I what I would like to know is whether Tata is working on any dark factory requirements and how is the readiness

for a domestic large scale industries &gt;&gt; there are a couple of uh things I can announce I can I can only I cannot announce but yes there has been significant progress in terms of

realizing the vision of dark factories. Will it be dark dark gray one or it will be gray day one? It's depending upon you know how you look at it. Uh but yes we have model factories which we are

actually onboarding uh into creating a dark factory vision. Um I would say the initial progress and the uh the way we are looking at it is absolutely fabulous results. So it's a journey.

factories what we say like it should not be your end goal as long as you focus on your KPIs right so whether it's factory setting it's cost quality and other parameters it's part of your journey

don't keep it as your end goal you might end up there you might not end up there still okay &gt;&gt; okay I been told there's question for time for one last question

no okay all right it's been an absolute privilege putting up this for all of you Uh, I hope you all did enjoy and learn as much as we enjoyed putting this up for you. Thank you.
