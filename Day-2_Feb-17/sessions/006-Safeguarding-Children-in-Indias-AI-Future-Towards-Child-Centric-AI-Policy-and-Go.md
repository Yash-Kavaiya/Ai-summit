# Safeguarding Children in India's AI Future: Towards Child-Centric AI Policy and Governance

**India AI Impact Summit 2026 ‚Äî Day 2 (2026-02-17)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 09:30 ‚Äì 10:30 |
| üìç **Venue** | Bharat Mandapam | West Wing Room 4 B |
| üìÖ **Date** | 2026-02-17 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/jLWs-CewTJg?feature=share) |

## üé§ Speakers

- Akash Pugalia, Teleperformance
- Ashish Jaiman, Nedl Labs
- Atish Gonsalves, LEGO Education
- Chitra Iyer, Space2Grow
- Gaurav Aggarwal, iSPIRT Foundation
- Maya Shermon, GPAI and Senior Tech & Innovation Advisor (ST&I Attach√©), Embassy of Israel in India
- N.S. Nappinai, Cyber Saathi
- Shireen Vakil, Space2Grow
- Uthara Ganesh, Snapchat
- Zoe Lambourne, Childlight

## ü§ù Knowledge Partners

- Institute for Governance, Policies and Politics, New Delhi

## üìù Summary

As India advances ambitious national AI strategies and positions itself as a global AI leader, the governance choices made today will have long-term 
consequences for children's rights, safety, dignity and development. Children are not merely 'users' 
of AI systems; they are deeply embedded within algorithmic ecosystems that mediate opportunities 
and risks.

## üîë Key Takeaways

1. As India advances ambitious national AI strategies and positions itself as a global AI leader, the governance choices made today will have long-term 
consequences for children's rights, safety, dignity and development.
2. Children are not merely 'users' 
of AI systems; they are deeply embedded within algorithmic ecosystems that mediate opportunities 
and risks.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/jLWs-CewTJg/maxresdefault.jpg)](https://youtube.com/live/jLWs-CewTJg?feature=share)

---

_[‚Üê Back to Day 2 Sessions](../README.md)_


## üìù Transcript

responsible stewardship. It is about moving beyond abstract principles to practical questions. I invite all participants to be bold, honest and constructive. Let

us interrogate assumptions, surface real uh surface real world gaps and imagine uh solutions that are grounded, inclusive and implementable. Thank you so much. Thank you all of you

and uh I must uh now invite uh Rena again and uh then we will uh begin with presentation and panel discussion. Thank you so much. &gt;&gt; Thank you so much Dr. Tari. So uh now we

are beginning with our presentation. So as highlighted by Manisha that we are doing this session in partnership with child light and space to grow. uh these organizations uh were set up uh the

expert engagement group was set up by matey to recommend on AI and child safety and they have come up with few recommendations. So I would now uh now invite Miss Zoe Lmberernet COO of Child

Light to present study insights and and share with us the evidence from the recommendations. Just &gt;&gt; good morning everybody. So today we're

here to talk about child centric AI and rightly so. India places enormous value on children not only as individuals but as a future parents, carers, workers, leaders who will shape the nation. If AI

is now the part of the fabric of of modern India, then child safety must be that golden thread that's woven through every stage of a young person's life. Alongside the benefits of AI, however,

does come harm. AI itself can create persuasive disinformation and give dangerous advice and generative AI in particular is increasing the opportunity for people to abuse and exploit children

as well as offering unfortunately advice to offenders on how to abuse children. AI is also being used to create both real and synthetic illegal child sexual abuse material often referred to as

nudification or perhaps deep fakes. This use of AI is unfortunately growing. Uh and in 2024, we calculated over 300 million children around the world were victims of some form of technology

facilitated abuse or exploitation. And in fact, even in the last year, we've seen a 1,325% increase in AI generated sexual abuse material.

But this is unfortunately also well understood by children and young people themselves who see that tension between benefit and risk, opportunity and safety. A childlike poll of 410 young

people across India brings this to life and I'll bring a few data points to show. We ask questions about their online activity, their use of AI and opinions on risk and benefit. So young

people in India see AI as powerful and beneficial but not safe by default. as shown by a clear majority who see AI as both good and harmful rather than just purely positive or purely negative. Back

to that tension and perhaps this reflects a nuanced and realistic understanding of the technology itself that's being used. And while many young people describe online safe online life

as enjoyable and helpful, only one in four say it feels safe. Young women in particular are notably more likely than young men to describe online spaces as unsafe, stressful, and mixed, and less

likely to say they feel safe online at all. This gender gap perhaps signals heightened exposure to risk, harassment, and image-based abuse online. But who is responsible for safe AI?

Nearly half of our respondents, so 48% place their primary responsibility for online safety on technology companies, followed by parents and carers and national government. And in fact, law

enforcement and schools also feel feature as quite responsible, particularly amongst young women. There's overwhelming support for acquiring platforms to automatically

detect and remove harmful content using AI and legally report suspected AI generated child sexual abuse images. However, interestingly, the majority of young people we survey do actually have

confidence in AI safety. And for us, perhaps that points to perhaps a distinction between AI technology itself, not inherently harmful, but rather how it is then used by people. So

safety cannot just stop at product design and and safety by design. It must continue through monitoring, rapid response, child helplines, and compensation for survivors.

because something will go wrong. When a child experiences sexual exploitation and abuse or abuse facilitated or or supported by AI, the impact unfortunately does not end in

childhood. The effects of that abuse can shape mental health, relationships, parenting and economic participation for decades after that. So if that whole system and I suspect

many of you in this room represent different parts of that system. If that golden thread phrase if fabrics, communities, nations weaken, protecting children in AI shouldn't just be seen as

simply a child protection or safety by design issue. perhaps a mantra that everybody will have heard over the last the last few days and to come but rather this should be perhaps if we go back to

the mantra of the AI impact summit overall one where we're promoting welfare for all and happiness of all. Thank you. [applause]

&gt;&gt; Thank you so much Miss Zoe. Uh I would also like to welcome Dr. Charu Malhotra. She recently joined us at the panel discussion. She is associate professor at e no so sorry

&gt;&gt; senior professor &gt;&gt; she she is senior professor from Indian institute for public administration New Delhi now I would request uh just a second

now I would request uh Mr. Gorav Agarwal from Isprit Foundation and Miss Chetra Ayar uh from uh space to grow to present expert group recommendations in front of us.

Thanks Ranja and thanks IP for having us here. So I'll just briefly describe the sort of you know what we did as a engagement group and then I'll uh pass it on to Chetra to share the

recommendations we have for the ministry. So we all know you know Zoe has done a great job in explaining how important the problem is. Uh so by the way like I'm Gorav I'm here in the

capacity of as a volunteer from IP spirit chairing this session engagement uh group on child safety on behalf of MITI. uh so I'll just tell you like you know the purpose of this engagement

group was to uh you know have a participative discussion with all you know participants in this important topic from not forgetting the the kids who we are talking about not forgetting

the parents who care about their kids in addition to the authorities and and and uh you know uh groups who care about child safety there are three four things dimensions what have come out and in

detail Chitra will mention the recommendations that we are making. First of all, we should probably change the name from child safety to child well-being. Uh right, I mean you know

safety is very I don't know patriarchal term in some sense like why who are we to yeah there are younger ones but still wellbeing is a better thing because especially in this world of AI I think

there are a lot of things that are that are beneficial to them especially you know kids living in villages who don't have access to good teachers. I think they can read anything these days. So as

as long as they are reading the right thing right and they are being exposed to the right things but but it is it is a double-edged sword. So we should worry about child wellbeing and not safety I

think. Uh second is um you know we should not leave out uh parents and community in general. You know I was having a chat with Chetra this morning like you know uh uh it will be

recordable let me say like my even at this age my father doesn't know that I'm a social drinker. if I hope he doesn't see it. Uh but [laughter] &gt;&gt; it's it's okay. It's okay. He's 82. He

he'll forgive me now. But there are families where like you know you know parents are much more uh okay with you know you become 18 and you do it. So who is right? There's there's nothing right

or wrong. So how can government or a civil society decide what is right. So not having parents in the discussion a solution that we create where parents don't have individual parent don't have

a say in what is right for their kids. I don't think it stand uh in a society like ours and globally in general. I think parents have a decision to make what whether what time you go to sleep,

whether you drink milk at the night or not. Then they should have a say in what is being exposed to you in a digital AI world. The third is in India the legal system

we know how it is uh you know so anything we do in ice actually we try to create a technolal framework for everything. The reason is it should be like as you say privacy by design should

be well-being I'm deliberately not using the term safety well-being by design it should not be left to companies or or or systems uh to to adhere to a to a regulation but rather the there should

be technical systems UPI is one example for you know all of us use it you don't know how it works but typically it just gets the thing done it gets the money transferred in a safe way similarly

system should be safe by design people don't or companies don't need to worry about them. They don't have to go out of their way to adhere to the system. Last but not the least, you know, we have to

make sure that we all come together in this in this very important topic. Why India is so important. We have, you know, we produce more children than any other country in the world. So I think

other countries can probably ignore this important topic. What we cannot with this I will invite Chitra to have her official recommendations above and beyond what I just mentioned. Thank you

sir. &gt;&gt; [applause] &gt;&gt; Hi. Uh thank you so much Goro for that. I just thought it'd be great to just uh speak about the process as well as to

how we really went about this. Uh so uh basically uh you know I spirit is the chair with mighty and uh space to grow and child light set up as knowledge partners for AI and child safety. I

think it's a big win that you know we have at an impact summit such as this a child safety as a track to really look at AI and children uh and uh the expert engagement group brought in lot of uh

stakeholders across uh the country and we also worked with lawyers, judiciary, law enforcement. We prepared the first draft, got inputs from uh various uh stakeholders and also set up a

pre-summit event on the 27th where we again got technology companies and industries to put in uh their uh inputs and finally the recommendations were sent to MIT and I'm glad to say that

there's a specific working group on AI and child safety where uh the team is actually looking at this recommendation and with this recommendation they would be actually announcing the declaration

for how India wants to stand on AI and child safety and how we become the narrative for the global south as well. So uh that's the overall uh process we took um uh the technolal architecture

got pretty clearly uh explained. I'll just jump on to some of the key recommendations and I know the the panel will also deliberate on this. One thing was uh can we have a child safety

solutions observatory? uh I've been working in the development sector for almost 20 years and I always feel that can there be a collective theory of change? Why do we all have different

theories of change and then every funer is funding a theory of change. Uh so I think this collective safety solution observatory is more about really bringing in all the uh you know

innovations and best practices into like one place where uh we are able to put in all the effort of what we have done as a country but at the same time we are able to become the narrative for the global

south as well and like uh you know Gorov just mentioned if we have the strength of children and also the children who are accessing internet in India are far more than any country we have solutions

right here. So can we really make that happen so it's more about aggregation. Uh the second is global south working group and I know that there is a global south network that is being launched uh

and uh the mighty uh is actually working on that. The third is uh a child safety innovation sandbox. We were thinking that you know what we could do is coming or a finishing of the summit and in Q2

if we can have announce a basic innovation challenge to look at how do you really combat uh digital harms and AI harms for children and really bring about innovations through a you know

innovation sandbox um nothing about children without children and I think it's important that youth safety advisory council is set up and this is not like a tick in the box

of saying that okay let's just get youth and let them talk and you know just do a tick mark it's not about that like really having that conversation so I I just want to say in one of the

researches that we recently did one of the girls in Bangalore said um I would rather speak to uh a AI chatbot and not even to my peer and my parents because either I'll be trolled or judged now AI

is making that space and if we don't have the voices of the children and youth on what this means to them in terms of say a well-being or in terms of harm we won't be able to really work on

really designing policies. So I think that's another area that we thought we need to really have. Finally, uh strengthen legal framework to address AI generated uh child abuse material. And I

think um nap ma'am is here so I don't I don't even want to open my mouth on this but all I all I want to uh you know highlight is how do we really uh demystify all the acts that we have but

at the same time we are able to have something absolutely clear when it comes to criminalization of cam whether it is synthetically generated or it is actually generated by an individual. So

really bringing that together and uh tech companies and the proof of burden on any kind of harm that is happening has to be on the industry and how do we really build in impact assessments of

really high interaction AI systems. Uh it's critical that we have that because uh today we don't we always talk about having a ISO certified thing. Can we have a ISO for safety on any kind of AI

impact system that is actually working for children? Finally uh bringing back to what Goro was saying about parents and guardians investing in digital resilience and AI literacy uh you know

as a preventive infrastructure you know there is cyber Olympia and artificial intelligence book in every every school which talks nothing about safety and it doesn't talk about well-being either so

I think how do you really work on curriculum and build that capacity of children and parents to be able to uh you know guardrail and navigate the AI space more safely and uh strategically.

Uh so that's all in terms of the uh you know thing and I just want to uh close by saying uh recently I've been actually in some other context talking to a lot of doctors and meeting a lot of doctors

uh and uh in and they kept saying that you know today when children come to us just introduce the panel. Uh so we have Miss NS Napina. She's senior advocate from Supreme Court of India and also the

founder of cyber sadhi. We have Miss Maya Sharma. She is STNI atache embassy of Israel and India. Miss Utra Ganesh. She is APEC head of public policy Snapchat. Mr. Mr. Ratish Gonalves, head

of product, computer science and AI product experience, legal education. We also have with us Mr. Akash Pagala. She's uh sorry, he is chief digital officer, co TP. Mr. Ashi's chairman,

co-founder of Needle Labs. He's uh the one who is moderating this session. We also have professor Charu Malhotra with us and I have already introduced her. Yes, thank you.

&gt;&gt; Thank you, Ranjel. And well, amazing amazing panel. I'm going to write into this. So, let's not waste time introducing ourselves, right? We'll just jump in into it. So, uh

I just kick it off by got a good presentation. So, I'm going to start changing my my child safety to child well-being. Uh but the interesting thing is uh you know our focus is child

well-being. uh and uh I personally think that you know it's it's we have to build uh we we love children in the playgrounds and and slides you know so we also have to start thinking about how

do we actually add the right kind of fences around the the the playground so that's what my focus will be and uh one of the other things is uh digital upbringing how do we actually

concertedly all of us together think about how do we raise the next generation ation in a AI first environment right for them it's an AI first for me it wasn't even computer

first right so now I' I've graduated to at least I understand that so with that I'm going to just kick it off uh so without introduction and the amazing thing that we have is the way I have

structured is we have builders right we have snap and uh we have Lego right we have uh guardians and policies uh you know utra uh mch Charu and and advocate uh napai

and then we also have an uh implementer right so we'll we'll we'll actually my questions would be around those themes so I'm going to start with builders because you know they are the ones who

always get the the other side of the stick all the time right so I'm going to start with them because they at least should should start talking about and I have my my things that I have to sort

now so maybe I'll go with utrau first Right. So how do you you know start thinking about well-being by design for your key demographic in India which is you you shared from 13 to 30 or 35.

Let's &gt;&gt; uh first of all thank you so much for that question Ashish appreciate it. Um the first thing I'll say is that at Snap um you know our fundamental start point

on product is that the design of the product or the architecture of the product has a far more powerful effect on the experience of the user than anything that we can do afterward.

Right? So design is crucial from that perspective. Um and let me open by and I I don't imagine a lot of you are Snapchat users. So I'm just going to open by saying what is Snapchat's social

purpose or social function right I know we all think about social media as this one you know monolithic category but it is not uh you might think if you look at think about all the apps you use Reddit

is different from LinkedIn is different from Instagram is different from Snapchat right uh some platforms are town squares that enable a a sort of viral sharing of information and some

platforms are onetoone commu communication enabling now the fact is that snap Snapchat is primarily a onetoone messaging app. Uh that in a sense, you know, because our social

function is not so much about being a town square where people perform for others for likes um and that sort of thing. Um it sort of minimizes the types of interactions young people typically

experience. Uh now beyond that as well, right, I think there's a a number of different things that design can do um as opposed to a postfacto content moderation approach. For instance,

minimizing contact from strangers that can do simple things like ensuring that people that are on the app need to have birectional acceptance of friendship before they communicate can be powerful

because it means that it's effectively a closed interface for communication. Number two, privacy settings that say, "Hey, your location is turned off by default." um you know such that the

young user does not have a cognitive load to make a difficult decision to change to find the the privacy setting and then change it right just having it turned off by default is a useful thing

to do for young people um so and the other the third thing I'll say is this right this idea of disappearing messages um now real life interaction is often [clears throat] ephemeral this

conversation is not you know being like the the words are not in the ether they're just sort of ephemeral we've tried to recreate that in the in the chat feature but also then for instance

give users the control to ensure that they can turn that feature off for instance. Um we also have something called family center where parents can have again right a modeicum of

visibility into their young person's interactions on the internet and online um without necessarily um compromising the young person's privacy. Right? So we try to be thoughtful about balancing

safety and privacy. We try to it's a it's I'm sure we often don't get it right. We think about this as iterative um and we think about this as design first. There's a number of and you know

we're constantly introducing new features every year uh sometimes uh every other quarter even to make sure that the way we think about safety in a AI risk landscape which where the risks

also are growing very fast you know we're moving quickly to address those as well &gt;&gt; of course so one of the things that I've always talked about is any innovation is

typically in the early stages are used by two sets of people right one is academics and the others are adversarial actors right so AI is in that phase right now. But that brings me to Atish

and at Lego uh Utra talked about transparency but but you know parents have oversightes. You have an amazing platform for kids actually where they can be creative. So how do you make sure

that that those right kind of transparency levers are built in into the product and parents or educators have the right oversight. F firstly thanks thanks for having me

and yeah so I work with Lego Education which is part of the LEGO group and for those of you who know LEGO very much values its brand it's a very respected company for the last almost 100 years

now um so with education our focus is very much on AI literacy and computer science literacy for for kids um and our perspective is AI is everywhere it's but it's not inevitable so I think I'll be

provocative here and say just because it's out there doesn't have to be uh it it it is not it doesn't mean that all the kids have to be using it as well. So our view is empowered that kids should

be empowered creators of AI versus passive consumers. Uh AI shouldn't be seen as magic. Um it's not a friend, it's a tool. Um, our approach to with building is give kids a screwdriver,

help them break apart this glass box or black box, whatever, and help kids to build their the future of AI themselves rather than using it. Um, we need to really look at the foundational

principles of what AI really is. Uh, and help kids to to we should help educators and kids to elevate AI literacy as with other literacies we have today. uh and and really support educators. I think

there's it's not a lack of tools, it's a lack of competence for educators and make sure that they are really uh supported um in I like the conversation around well-being and just maybe getting

a bit more specific around that. So safety, we do not generate text or media. So we don't we don't with currently at least with generative AI, we feel generative AI can be made safer,

not safe. So we just don't go there. Um, we do not anthrop anthropomorphize AI, which is a fancy way of saying we don't get kids to think AI is is a human being. Uh, not leading to unhealthy

emotional bonds. Uh, we want AI to be transparent. So, we only use models where kids are using pre-trained. So, we use things like pre-trained classifiers to teach kids about it. But those models

should have very transparent model cards. Where have those models been trained? uh what's the uh geographic diversity of people who've been trained on those uh so very clear data

provenence respect privacy so nothing so I was speaking to a colleague here in front he was like so which tool platforms do you connect with nothing leaves the kids computer device

everything is done locally uh nothing ever ever leaves there's no login information nothing goes to the cloud to third parties or to us so I think these are some of those very basic sort of

foundations I think we should I I don't think these are things we should fight for. We these are things that should be a given. Uh I think child safety and wellbeing should be a non-negotiable.

&gt;&gt; I I absolutely agree and and one of the things is like you know the the information does not leave the device where the kid child is playing which or or creating which is amazing but you

know harms happen right and I'm going to bring in uh our guard rails people uh advocate napani this is for you. So let's say the best of abilities of of both Snap

and and Lego but the kid or children get harmed for whatever reasons. So the two two-part question one is how is the Indian law being futurep proof and thinking about the potential harms but

more importantly where does the buck stop if that happens? &gt;&gt; Is there a buck? I don't know because he so so interesting thing so buck is a very interesting word gorov talked about

uh and then I think chitra was also talking about UPI you know why those work because nothing moves like buck moves right everyone pays attention things would work can we make child

well-being as important as the green buck &gt;&gt; well the flip side of that is the speedier your chance of moving your money the faster you lose it too of

course. &gt;&gt; So you know uh I love Gau's opening too where he changed the narrative from safety to well-being. I want to take it a little bit further on that and speak

about what uh you look at from the perspective of law and your opening statement also talked about how you have the makers the builders and god forbid if the guardrails there fail then how

does law come in? I would actually ask you to take a step back and remember that the guard rails are placed by law even for the builders to follow as a preventive and protective measure and

not just as a punitive measure. That's how law is expected to work and that's how law has been evolving over the last few years. So when we talk about be it safety, be it well-being or be it I

would say just existing in a digital domain with the assurance of uh ease of existence or with the assurance of let's use the word there's no harm in saying safety or a safe space rather right when

we approach it from the perspective of law how do we look at it as I just mentioned Now your narrative moves to preventive and protective measures and therefore

you have regulatory mechanisms coming in. You had recently the intermediary guidelines modified to include AI related synthetically generated imagery information, right? SGI and that whole

purpose of that regulation is preventive and protective and to place the accountability or burden on the uh platforms on builders on people who are offering products or services. there

it's predominantly services because it's for intermediaries. So when we speak about preventive and protective measures there is a lot beyond what meets the obvious two.

Who decides what is essential in terms of preventive and protective? Where do you place the guard rails in terms of to what extent should law step in to provide those preventive and protective

measures and when and how should it be left to technology to evolve at its own pace and I love to give this example always when it comes to law every technology person will say that law is

an impediment but it is not meant to be. So you know uh the example I give is you know you grow you evolve you reach a tipping point where if law enables it helps you

to grow further and if law becomes an impediment it makes you fall flat. Right? So till that tipping point, technology usually prefers to grow at its own pace and in an organic manner

and thereafter it may need that leg up from law and I am an optimist. I believe that law can actually provide that. Does India have laws to that effect? Coming to your last part of your question, yes

and no. As with every jurisdiction and for every technology that you can think of, as I told you, some governments do prefer to wait and watch because that is the organic phase of a growth which you

do not want to impede even inadvertently. But as growth happens and you identify pain points or issues or concerns and then you decide okay I need to step in

and when it comes to technology this is where a critical phase is also reached because you also have to decide what is it that you can interfere in or uh engage with and to the extent that

you're able to enforce it. A law which cannot be enforced is not going to be passed and it should not be passed too. So that's where just let me complete the proposition. I'm not going to take long

on that. So that's where India is poised. We have the AI uh regulations that's come in. We don't have an AI law the way you see in the EU. We have the data protection act which has been

notified but not yet implemented. And we have a very long road still ahead of us till May 2027 before it comes into play. And if you ask me are these enough and is it going to uh be effective? I would

say certainly not. It's not enough. Not even our criminal laws. But we have started somewhere and we hope that we will land safely eventually. &gt;&gt; Well, thank you. And uh you know I'll

just add to this is uh in my experience and what I'm reading and maybe reading uh something with chat GPT but law or regulations is always lagging so much behind the innovation pace that we are

in right now uh and the innovation pace is also dictated by the market what the market needs right you know it's been what three years and what 2 months till chat GBT was even announced and look at

it everyone uses Gen AI for a lot of things but that thank you uh and one of the things that I'm going to bring uh Maya in is like we talked about hey this is what India is is working towards

we'll have regulations but with in your experience uh like we can learn from others right you know you have you have uh you have great experience in global diplomacy uh so few things right one is

where do we look for to learn to you know make ourselves better in terms of our laws, regulations and whatnot and what are the key global benchmarks. So one is looking at the specific

jurisdictions and laws and then what are the benchmarks we should be thinking about as well. Thank you so much for the question again I think it's such a timely topic and you know um Gorav was

briefly saying that again the Indian who cannot avoid this topic because of the size of the population. I'll be glad to say this is a wicked problem. It's relevant for all countries. No country

should either neglect it at this point. I think that you know you were just mentioning about that um what India can learn from others. All countries need to learn from each other in terms of the

learnings in terms of the harms and obstacles. Um I would say from the Israeli perspective um we we we are known for our technology I would say the deep technologies startup ecosystem we

have some of the most brilliant technology in the world. I would say also again I've been living in India. I would say that especially in this domain there is fantastic work being done over

here. I feel that in many ways the problem is still in the delivery um meaning we are still seeing this kind of gaps even when we have very strong technology in this space and I think

that you know we were just asking about benchmarks there are different benchmarks but in many ways when AI is integrated we are speaking all of a sudden about

scenarios. We are trying to take a much more nuanced approach to child safety. So if in the past we had certain benchmarks on how we should look at child safety, what kind of platforms we

should moderate, today with AI, it's almost paradoxical. We know that AI is harmful. We know the level of content moderation, the potential harms, but in the same time you're also trying to

increase the AI uh dispersion. So what I would say in many way that I feel that benchmarks are needed but when we're speaking about AI in addition to um child safety trying to understand how

they can both be combined we need to think much more about tackling certain scenarios. What we've been doing which has been quite successful in our case um was to have this kind of

multistakeholder centers connecting our government to police centers to companies. One of them it's called center 105. It's actually a collaboration of our police unit with

some of our strong uh companies and the government. The entire point is to create fast cycles of action. Um and I'm saying this because this is not something will be dictated by a

benchmark. It's out of a certain void. Um benchmarks are here to guide and I think especially in the AI I think we've seen different benchmarks guidelines also in child safety space and perhaps

I'm saying this as as an ethicist. So I'm here to look for the exceptions in a way. Um and I I completely agree with as brightly said um law that cannot be enforced should not be enacted. The

thing is some laws will not be fully enforced because of the current situation. I think that's exactly the place here that we need to think a bit beyond the benchmarks. We need to see

how we're taking specific scenarios and bringing them the right stakeholders. And I think that in this case, what I hope we'll be able to reach is perhaps either more nuanced benchmarks to see

how we can tackle certain scenarios in child safety in which AI can tackle certain parts of it. But as as also said by the other speakers, we are aware of the limitations. We know that AI in the

in the end, yes, it's pervasive. It has its limitations. Um but in many ways I feel that the way going forward would be much more nuanced and scenario- based approach so that be

able to think slightly better on how can we make sure that this benchmark are actually fulfilled. How can we make sure that we're going a bit beyond global benchmarks and legislation that can

protect in times of need. It's very nuanced as well because you know you know when we talk about threats and and you know there's a generic way of lens of thinking about threats against or

harms against children but then it sometimes also get nuanced in the local cultural slangs as well right same thing could be appropriate here like I hope it's not the case but it is right so so

I'm going to bring uh uh you know the academic point of view here uh Vcharu uh just quickly before I go to the operational aspects is what are your thoughts in terms of uh the intersection

of we talked about builders uh enforcers or or legal and you know how are you preparing as an academic the next generation of both the enforcers the policy makers as well as

the users. We are not talking about kids here but at least your the kids that are the the students that are going through the learnings now will eventually start taking and and making those decisions.

&gt;&gt; Wonderful question. Thanks. Uh thanks to Manishi and IGP also. So as an academics uh I would just take uh Gorav's uh submission a step further. So he moved from safety to well-being. But I think

what we need to now look at is the rights approach that we should focus on rights by design. A right of a child to be having no discrimination, a right of the child to not be abused or previously

not being exploited etc. So once I have a rights approach in these systems and I think Chitra is also noting is we should have CRIA in terms of child rights impact assessment audits should be very

necessarily implemented by all the tech designers and service providers. Apart from that I would also advocate a whole of society approach in this where yes I'm so glad uh you take children on

board you you know ask them etc. But I think there are more stakeholders than that. There are not just tech flatforms, parents, guardians, children, but there are also doctors. There are also mental

therapists, counselors. There are also other alternative arrangements which we should look into. And this is trying to give them an alternative digital mentor which none of these uh you know building

blocks have so far considered. But let us remember that if a movie like her somebody can fall in love with an AI even a child is capable of falling in love with you know AI because it's

giving you prompts which are so easy which are it's like putting an echo chamber around what a child wants to hear because that's what algorithmic bias is so when I went through UNICEF

guidelines I went through Brooking Institutees guidelines Alan Turing Institutees guidelines OECD guidelines so framework which I came out was that there should also So be a blackbox to

open class approach where like somebody here mentioned that not just the parents but even the teachers have an access to what the child is looking without intruding into their privacy. Yes,

digital mentors as advocated by OECD is very very important and please neuroplasticity gap has to be bridged because the child is always looking for easier affirmation which AI gives them.

So there should be some frictional reward system in educational AI. That means don't just applaud them right away but give them this cog chance have of having cognitive development that look

you have to go through this. Last but not the least what is very very important for us is that the duty of care is also part of right. So duty of care by snaps and legos of the world is

an important part. It's not in favor it's a prerequisite. Thank you. &gt;&gt; No, thank you. This is amazing because you know again uh the the friction by design [laughter] sorry needs to be

built in. But the thing is that we all think that there is a friction between creators like Lego and Snap and the regulators. But I think uh Miss Napana actually was was very specific that we

should not think of as a friction but what is super important is we talked about hey builders but let's actually hear from uh uh our friend Akash in terms of operationalizing this whole

thing right so a few things there one is what is the real real situation here right and and one of the things I want you to focus on especially in the context of India where so much cultural

differences language differences how do to operationalize at scale of India. &gt;&gt; Yeah, thank you for the question and uh great to be here. Um I think AI has been there for the last I'd say 10, 12, 15

years since the time social media started, right? Yes, generative AI is new technology. I I'll talk about three or four things that are there and then I'll touch upon the India question,

India specific bit. The first thing is if you look at uh you know the harms that are happening it's not happening uh it's all happening online right so there are no boundaries online there no you

know you cannot have a law in India protect someone in the US and vice versa right the laws are extremely varied they're not consistent so it's not the lack of law not being there that harms

are happening it's the inconsistent enforcement ment that is happening why the harms are happening and again you touched upon it I think the panel also touched upon it AI is progressing really

really fast right so it's not about uh the gravity of the harm anymore it's also the instances of the harm that are happening so in a click of a button now you can generate synthetic media uh

which is in in millions of pieces right that is where law enforcement We work with we work with a lot of platforms and we work with a lot of regulatory authorities and we realize that the

challenge that is there is how you look at these synthetic uh synthetic uh data points that are coming out are they real not real etc etc right uh so there are challenges and then the last point that

I'll make is the differences between uh each market and each language So I know you spoke about India but you know we work across global clients. So we have people across the world who help with AI

safety uh who help with AI safety and AI training. But that is where I think uh regulators, policy makers, academicians, governments need to come together to say

what are the type of harms that are happening because the harms move from one place to another place. It's it's it's it happens. uh from a India standpoint, you rightly pointed out like

we have so many languages, we have shared devices, we have different levels of digital literacy that is happening and with all of that you also have like you know the generation just you know on

mobile phones directly like uh you know when we were growing up we didn't have any mobile we didn't even have internet so that's different completely different uh there are a lot challenges that are

there. I think the issue that all the platforms end up looking at is they have to manage engagement, they have to manage monetization and they have to manage safety. They have to manage all

three in equal regard. &gt;&gt; I agree. I agree. And and one of the things that I'll tell you when we talk about AI, it's not a recent phenomena. The first set of AI thinking actually

started back in 1960 when some academ academic academics actually uh thought about what they call symbolic AI where you actually map the world and it would always be

deterministic but thanks to Jeffrey Hilton and deep neural networks now we talk about probabilistic things as as AI which shouldn't be but anyway so one thing that I do want to actually close

out with is very very specific like one thing right we have to be very quick because we already have a Oscar music played behind us few minutes back so if one non-negotiable non-negotiable

safeguard that India should implement we'll start from you and we'll come here one non-negotiable quick &gt;&gt; I think uh there should be identity marks and all the AI generated content

so that a child knows this is a robotic [clears throat] uh thing on the other end and not uh Are you human? That's very important. &gt;&gt; Um

I think &gt;&gt; from SNAP's perspective, the one thing that India needs is an age appropriate design code. I think that we have a bunch of specific provisions and other

laws on data protection and the IT rules which is a horizontal law. What we don't have as a country is a statute that is exclusively and in a granular way focused on children's online safety. We

need that. Yeah, I [clears throat] would say uh faster reaction cycles through again multistakeholder approach making sure that companies are startups are also

part of the preventive measures as well. Um I think again it's key in India, it's also clean other places and I think that this is something that India should still fight for and I think definitely

perhaps one of the most pioneering countries and it's the linguistic diversity in design. the fact that you're able today to translate to all Indian languages. This is I feel a

benchmark for many other countries. Not everyone is putting so much effort. We're very small country. We don't have so many languages and still I feel that India is definitely bearing the torch

when it comes to language diversity in the space and it's it's crucial. uh zero tolerance for profiling of children and I think uh it's really critical for us to understand and know

and distinguish profiling from processing and understanding where processing for good stops and processing for good of commercial activity starts and know where to pause that

uh I would say that if if the tools are not if we don't feel the tools are safe enough, they shouldn't be in the hands of kids. So, I think I would I would say we should just legislate against that.

If if it's not safe enough, it shouldn't be used. But outside of that, I would say education and education like obviously our work is with kids, but all the way up to grown-ups, uh to teachers,

to parents, I think they need to understand this wave that's coming over them. They need to understand it in a meaningful way. And I think we need to get nuanced about AI literacy. It's not

AI literacy is not how kids or adults use tools. It's understanding how they actually operate. And I think that's a little different to just we know how to use chat GPD in a safeish way. Uh it's

it's they need to understand it a little bit more with a bit more nuance in that. &gt;&gt; I think I have a controversial one. So no targeted behavioral advertising for miners.

&gt;&gt; How would we survive in this no advertising world? I don't even know. Even Jack GPT is playing with some some ads. But anyway, so one of the things I I I I think you you're absolutely right,

right? Uh is literacy uh is very interesting word here, right? Media literacy especially whenever we talk about literacy, not just the users but the the users around the user, right?

Meaning educators, parents, do they understand how it works? And I like I I've been told that we have time actually because the next panel is running late which is an amazing thing

for the audience because what I'm going to do is my role is done. I'll have the audience actually will take like three questions. Uh raise your hands. All right. We'll start there second and

third. All right. We have &gt;&gt; Yes sir. Well ma'am the the way you mentioned about the regulatory mechanism that plays a very important role that alone

can bring awareness. Once the awareness is brought, children will automatically understand what to see, what not to see and the safety measures will be also taken point and at the same time the

legal and administrative procedures will be one of the most important issues and regulatory mechanism will be nuanced in the form of benchmark as you mentioned. I think uh that is one of the most

important point and we need to understand the most of the regulatory aspect and the most part of the and the most legal part of it depends on administrative awareness. It is

awareness and education the child should receive and then the safety and etc will come automatically that is intrinsic in nature not extrinsic. That's what is my point. Many laws now if you notice they

are bringing in awareness also as a critical requirement that uh uh stakeholders have to engage in compulsory. &gt;&gt; Yes. And at the same point was mentioned

by ma'am also and she mentioned it in a very withful manner that was there very enthusiastic. &gt;&gt; So good question. We'll go back to this lady here. Sorry one and then

&gt;&gt; hi. Hi. I'm Palvi. I'm from Guy Law. We're a law and technology public policy firm. My uh question is to Utra. Um yes uh you you alluded to this but I would actually love to understand how this

works in practice. So when you're thinking about the architecture of an AI enabled feature a bot how do you balance the the requirement for safety uh versus the child's privacy? So for instance for

promps for let's say self harm, what do you think about let's say alerting uh the parent or let's say a school if that's that's involved. &gt;&gt; Can you hear me? Yeah, that's great. But

thanks for that question. I think it's a really good one. I in fact I wish it was part of the panel as well. Um so even the we have a so on on on Snapchat we have an AI board called my AI. Okay, it

is a conversational AI that speaks with primarily young people. So as you can imagine very much topical for this particular panel right uh so when we built my AI we again took this safety by

design approach to its construction and we did that end to end okay oops sorry I'm just going to &gt;&gt; it's not me I tell you [laughter] it's it's the universe that's fine um so

let me just take you through the the process right in terms of safeguards while building my AI um it's not something we just retrofit into the app we have to

uh non-good faith questions of the AI, you know, like a strange question about sexuality that's age inappropriate or a question that's harmful or about bullying, etc. Um the AI will pause

communication with that particular person. That's again a built-in safeguard, right? And number four, essentially on family center, a parent is able to cut off my AI for their team.

Right? So really the TLDDR pali is that we ensure that the AI is aware of who it's speaking with. Number two, we try to ensure that it's responses are generally age appropriate. And number

three, we giving multiple mechanisms for it to cut off communication or stop when to speak with a young person depending on what's being detected. I hope that answers your question.

&gt;&gt; That's that's helpful. Thank you. &gt;&gt; That's amazing. We'll have one more question and there's there's a gentleman there and this is it. We'll we'll play the Oscar music. I promise.

&gt;&gt; This is it. &gt;&gt; Okay. &gt;&gt; I get to grab vouching for her. I've never rooting for

&gt;&gt; We'll give it to her. Yes, she does. &gt;&gt; Last one. Last one please. &gt;&gt; Thank you. Thank you everyone. Thank you sir. Uh my question is to Miss Chitra

and advocate Nabina. Uh ma'am I work with children in communities as well in school where we are working on school safeguarding framework and models. So whenever we receive SOS calls from

schools when it is related to digital safety it is usually that their classmates are either bullying or sexually harassing or assaulting them with defake photos or videos. They are

even chatting on all sorts of platforms but yet that age restriction is not there from any platform any space. Now once this offense has been comm committed obviously the school the

parent are in panic and they are reporting it to the police but yet under the poxo act or the IT act that takes time you know the the way the cyber security division engages is rather slow

even though it's a poxo case now because it's a child sexual abuse matter now in such instances how can school prevent uh such acts because in schools children are allowed to bring devices to what in

whatever capacity they are allowed to bring devices now the harm has been done so what can we what can the schools do next thank you &gt;&gt; see thank you for this question but it's

very close to my heart there are two things I've been emphasizing the first is to keep children safe but the second part is keeping children or others safe from children too because often we

forget that children can cause a lot of harm so it's important important for them to know the difference between a prank or a joke and a crime. Often they don't know the difference. Secondly,

they have to know that merely because they are minors, they are not out of laws, you know, perview. There is a law for them, the juvenile justice act. And what does it entail? And what harm can

come to the child who violates the law? You know that's awareness is also critical. So in my sassy tales book the second one this is what I emphasize saying that you need to know when the

line is drawn and you should not cross it. And secondly god forbid if you cross it then what happens. The good thing with the intermediary guidelines that have come out now incidentally and I'm

jumping to the remedial aspect of it is that the you know even earlier you had a 24-hour timeline as opposed to 15 days for take down of content which has nudity know that I can tell you how to

make it work. I am also a skeptic as much as I am an optimist. So we are realists rather you know we know how the law works and where it may fail but it is in each of our hands to make it work.

So I'm not saying the system is great but I'm just saying please make it work for you and this is how you do it. Okay, you have an online platform cybercrime.gov.in

it works very slowly. It will take 48 hours or more because there is a system or process it goes through. You have a 1930 which is also pretty slow because it'll take you back to

cybercrime.gov.in. If you want speedy takedowns and today the 24 hours stands reduced to 2 hours for that category. Go to a police station. Sit there and make the system

work for you. Take my word for it. I've done it. It works. &gt;&gt; That's amazing. And I'll give the gentleman last question. &gt;&gt; Yeah.

&gt;&gt; Thank you. [laughter] Uh okay so the question that I had is see there were several ideas that were floated by Miss Chitra and Miss Nepa and so on. Uh the challenge that I see is

that with every passing day the pace is picking up the spread is growing the old ways of coordination like this panels in an episodic way doesn't in my opinion is not going to cut it. So the question to

the panel is uh are there new mechanisms that need to be evolved in order to accomplish uh global coordination of uh responses? &gt;&gt; I'm sorry I'll chip in very quickly on

this. There is already go global uh action on this. Recently the UN ODC spearheaded UN convention on cyber crime has already been uh approved by UNGA. India is not yet a signary but that

pertains to only 10 substantive criminal provisions. What is critical about this cyber crime convention is it predominantly focuses on global cooperation from the stage of collection

of data to uh investigation and even trial of cases. So the whole idea is cyber crime is borderless. So now let's figure out how enforcement can also be made borderless.

I'll just add some promising signs not directly in AI. You're seeing countries like Australia and Spain banning 16 16 year olds and below with mobile phones. I think we can see again I started by

saying nothing is inevitable as well. And I think it's and when we talk about kids interacting with AI chat bots, I I would be very critical. I mean when you look at these companies, how many of

their own kids would be using these chat bots themselves? So I think we should really look at it with a critical eye and go like now governments are taking a sledgehammer approach because as you as

you said it's not working and it's the the pace it's it's outpacing the ability to legislate. So I think we need to look at some bigger picture sort of um ways to tackle this as well. Utra

&gt;&gt; I want to take the opportunity of your question to respond to something the EG recommendation sort of alluded to right and this is uh on using tech the payments sort of analogy to children's

online safety to my mind and and this is an early thoughts right my my several reaction to that is that children's online's harms are not a transaction between one account and another account

right it is inherently about behavioral relational harms occurring in the real world and therefore in my opinion in finitely more complex that interaction between platforms and the real world

right and therefore actually I think the question the gentleman asked is profound uh I think that we're going to be dealing with a landscape of complex and a and an order of complexity that's

difficult for us to work with which is why I think it's important for us to have laws that are almost entirely focused on children's online safety as an exclusive subject matter as opposed

to using horizontal legislation on online content or privacy to tackle something as complex as this. &gt;&gt; No, this is this is a amazing way to end this which is children's well-being is

not a transactional thing, right? So, I'm going to sorry Char let's end it. Why? Why don't we do this in Indian context? &gt;&gt; Okay. Yeah. Just quickly Indian context

DPP act has come which says if you don't give your details you'll be penalized 10,000 which is very bad for children's safety because it's leading to what is called as digital gerrymandering. Those

who apply for scholarship are actually kept on the wrong side of digital divide. They are treated as marginalized and outliers which I think is a by sorted. Thank you.

&gt;&gt; Yeah. Again even even a better point to end. Sorry. Let's let's close this. Uh thank you very much. I learned a lot. So thanks a lot. [applause] &gt;&gt; Thank you so much all the panelist. I

would request you all before leaving to just uh be here for the group photograph. And I'll also invite Chitra ma'am and Miss Zoe please come and join us for the photograph. He's I think

Gorav Aarwal is not here. &gt;&gt; Yes. So &gt;&gt; and meanwhile I would like to thank you IGP team for making this happen and uh

Ashi sir for being very good uh moderator and at the same time partners who had joined us for this panel discussion child lights I spread and space to grow.

And we would be sending out momento in a while to our guests because we are getting ready with it. I'm so sorry. &gt;&gt; So I would request our audience to be here for the next session that we are

going to host. It is on AI powered ports.
