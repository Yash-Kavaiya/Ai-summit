# Quality Control of AI Applications in Healthcare

**India AI Impact Summit 2026 ‚Äî Day 2 (2026-02-17)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 14:30 ‚Äì 15:30 |
| üìç **Venue** | Bharat Mandapam | West Wing Room 4 B |
| üìÖ **Date** | 2026-02-17 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/Ah1jrXirVsQ?feature=share) |

## üé§ Speakers

- Lt Col Parikshit Sanyal, AFMC Pune
- Nisheeth Srivastava, IIT, Kanpur
- Tapan K Gandhi, IIT Delhi
- Taruna Madan Gupta, ICMR
- Vivek Hande, DGAFMS

## ü§ù Knowledge Partners

- Armed Forces Medical College

## üìù Summary

Deploy AI models in healthcare through phased pilots, using clinically validated datasets and bias audits. Establish strict data governance, patient consent, and privacy compliance. Implement human-in-the-loop oversight for all clinical decisions. Mandate regulatory approvals, model explainability, and documentation. Conduct continuous performance monitoring, post-deployment audits, and retraining. Define clear accountability, cybersecurity safeguards, and standardized quality benchmarks to ensure safety, accuracy, transparency, and trust in real-world clinical use.

## üîë Key Takeaways

1. Deploy AI models in healthcare through phased pilots, using clinically validated datasets and bias audits.
2. Establish strict data governance, patient consent, and privacy compliance.
3. Implement human-in-the-loop oversight for all clinical decisions.
4. Mandate regulatory approvals, model explainability, and documentation.
5. Conduct continuous performance monitoring, post-deployment audits, and retraining.
6. Define clear accountability, cybersecurity safeguards, and standardized quality benchmarks to ensure safety, accuracy, transparency, and trust in real-world clinical use.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/Ah1jrXirVsQ/maxresdefault.jpg)](https://youtube.com/live/Ah1jrXirVsQ?feature=share)

---

_[‚Üê Back to Day 2 Sessions](../README.md)_


## üìù Transcript

Keith will be shortly joining us. So uh to get the discussion rolling uh so uh we have among us uh Dr. Karthik Arappa who is from the WHO from the

digital health division of the WHO sir is a medical professional and a former and an is officer as well sir as well as one of the key personal developing the national AI strategy for health which

will be released today. So I invite uh Dr. Karthik in the panel as well sir. Sir please sir. So uh to get the discussion started

we'll have uh uh the clinical uh view first and uh the questions I have uh for uh Vive Kandesha is uh the three points uh if we could have the presentation of Chhattage. the

first presentation. So the three questions uh for uh the clinical lead that is one patient outcomes in a clinical setting. How do you distinguish between a high

performing model and a safe model which are not often the same? And then edge cases what are what are the most frequent blind spots which you have encountered or are likely to encounter

in using AI models? There are lot many. and coverage. Do you see AI models covering the entire gamut of the medical practice or uh uh in a certain fraction they're going to come up early or later

in some of the stages? Yeah. So uh I'll just request uh surgeon vice admiral Vive Kandes sir to take over and uh dwell on these questions. Sir over to

you. &gt;&gt; Hello. Uh good afternoon. It's a delight to see full house. Um this I think um is a very important subject. The need for quality control of AI applications in

healthcare. You know, I see I am probably got the most amount of gray hair in this room. So, I probably represent a bridge between uh an old-time clinician and a younger

physician who is more invested perhaps in uh AI tools. But at the same time, I must confess that as one evolves and one grows, one realizes the importance and the essence of AI tools to help

healthcare. So there is a requirement of a basic clinical instinct, a gut instinct, your clinical knowledge, your soft skills. But at the same time, the AI tools are there and they're going to

remain and they're only going to grow. But my concern of course is with the requirement of the quality control in terms of the AI tools. Is all that is provided by the AI tools adequate, safe,

consistent? Can I have the next slide? In my opinion, to a large extent, AI without quality control is nothing but modern-day sorcery. You know, I may be overstating what I want to say, but the

fact of the matter is AI without quality control is absolute hogwash. Can I have the next slide? Now, why are we talking about AI in healthcare? Now &gt;&gt; because of its rapid growth at times

unregulated there is a absolute you know the AI tools are involved in every stage of the diagnosis management triage and everything. You have a whole lot of AI tools some perhaps authorized. The US

FDA authorizes more than 600 close to 700 almost enabled medical devices but the large part of the devices are still not authorized. There is a lot of pressure

by various health mechanisms and everyone to adopt this AI tools to improve so-called efficiency. The next one please. Now what are the type of quality problems if you may in medical

AI? One of course is the data issues. You know your AI is as good as your data but at times the data could be biased. It could be incomplete. It could be non-representative data sets. Then of

course you have models which may not be the right fit. Usability issues I think is another important paradigm here. Then of course are the issues of transparency and cyber security and privacy. All of

these make the quality assurance in AI health tools a very important paradigm. Can I have another slide please? Now why is quality control essential in the first place?

You know as far as a physician is concerned as far as a doctor is concerned the core of medical practice is primum non noare which means that do no harm to a patient. So patient safety

is at the center of AI health tools. So if you got to have AI health tools you got to ensure that the patient remains safe with the usage of that. There's also the issue of equity and ethics and

of course trust. Now trust is something which is not gained easily. It takes a lifetime to gain a physician's or rather a patient's trust for the physician. Are these AI tools good enough to be trusted

both by the physician and the patient? Then of course are the issues of regulatory and various risks. Now there is an evolving regulatory landscape and I do understand that and we have nobody

more qualified than Dr. Karthik here from the WHO who will talk about it. You know there is evolving regulatory landscape but at the same time if you just see the European Union AI act

classifies most medical AI as continues to be high risk requiring a very strict quality and monitoring. The requirements therefore are AI specific QM data governance human oversight mechanisms

and so on. And many of these AI tools unfortunately fall outside the traditional device regulation increasing the need for a local quality control. There are certain core dimensions of AI

quality in healthcare. One of course is clinical effectiveness. Obviously these tools have to be clinically effective. They have to be I cannot reiterate this more. They have to be safe and they got

to be robust. There has to be fairness and bias control. The issues of privacy and security need to be factored. And then of course most importantly transparency and accountability. Now I

have a dual kind of a shall we say uh role that I play. One of course I'm a gastronologist. I'm a clinician. I'm also a uniform person. I deal with data. I deal with security issues. So I'm a

little more concerned about whether this data which we use increasingly also has security risks. It's a question of national security as well. Can I have the next slide?

So there is definitely a develop requirement of a quality control through the entire cycle of the quality of the AI life cycle whether it is terms of design and development whether it is

terms of validation deployment post deployment monitoring and of course the feedback loops. So while AI I would dare say is essential and is going to be increasingly so is perhaps make to make

our practice of medicine more scientific but at the same time we have to be I would say judicious in the use of AI we shouldn't grasp too much too soon too quick and that may land us in more

trouble than benefits so can I have another slide so if you just see a practical framework it obviously has to be strategically aligned there has to be an ethical evaluation. There has to be

usefulness and effectiveness. It has to be fiscally prudent and it has to have a risk tired approach. Now this is basically also to do with various American heart association uh you know

procedures but it works very well for the AI model it well as well. So therefore to implement AI quality control in health um shall we say environments you have to develop an AI

governance policy and that is something which I understand is going to be released by the health minister in just about an hour. The AI governance model for the country. One has to maintain an

AI inventory which includes embedded tools, devices and cloud solutions. One has to standardize procurement. One has to start I think you got to start small. We got to probably look at AI tools

which are kind of factored to your own uh patient sets, data sets before we kind of go on scale and then gradually it has to be built up. The capacity has to be built up that I think would be a

good method to implement air quality control in healthcare environments. Another slide please. So therefore my key takeaways as far as a clinician as far as a physician would be to the next

slide to AI you know can definitely significantly improve diagnosis monitoring efficiency. I do agree I concede quality control has to span the entire life cycle. Poor quality AI can

harm patients worsen inequities and actually expose organizations to serious regulatory and reputational risk. No AI without QA would probably be the bottom line. So therefore one each of us have a

stake in this. Each of us has to be involved in every step whether it is the scientists or the innovators who are developing the AI tools whether it is the clinicians to some extent the

patients as well. Each of us have a role to play in this. So I would definitely feel that if quality control is not applied to AI machine intelligence could be the last invention that humanity will

ever need to make. Perhaps a very drastic statement, but I do believe that quality control is critical and central to the development of AI tools in healthcare. Thank you.

[applause] So uh that uh I hope that uh uh covered the problem statement and convey to the audience that what we are really facing with while dealing with AI because what

my realization and uh my perspective on this from the very beginning of the the when when I planned this session we were doubtful on whether AI models would cause some amount of insecurity

among doctors not like the professional jobs would be gone but because if an AI model tells something whether to ignore it or go along with it in spite of my clinical gut feeling so uh this is a

question I guess most doctors would find difficult difficult to answer so moving on we have among us Dr. Karthik today who seen all three worlds uh the medical, the technical and as well as

the administrative side of things. So we're very lucky to have him today as he joined in the literally the very last second and uh to Dr. Karthik I'll be asking sir

if we are to devise a QC model for AI applications three things is there something beyond accuracy and F1 score and ROC's that is one second is uh if the models which are static we have

which have standards weights and biases we can still have some control over them but some models maybe which are not written by us we are not sure about the source code their weights biases might

change over time. They might drift. They might learn something new from the available data which we did not intend to teach them. Drift detection, how do we deal with it? And third is a very key

question which is very close to all of our hearts. That is when we train an AI model, we say we need 1,000 images or 10,000 images or 1 million images or whatever data set that we have. But

that's just quantity. That's just a number. What is the measure of variability? what is a measure of uh how variable our training data set is? Is there a way to quantify? Sir, please.

&gt;&gt; Uh firstly, thank you so much for having me in in such a short duration. I know some of uh these are very interesting questions and and I can actually bore people to death because they're also

very technical nature. But let me let me try and make it simpler just because the audience could have various uh interests to essentially state two things, right? So

eventually what you want is how do you build models on the performance metrics the metrics that we talked about accuracy precision scores F1 scores rock curve scores we've gone beyond that

especially in healthcare how do you build a model with all these performance metrics there is enough science to build them enough science to know how do you build these models in a fairly

controlled environment so today when you think about AI in health more in a research space purely from a technical standpoint. How do you have the model work with very very controlled data sets

with a very very clean data set is something that we've truly mastered. So when you look into the scores that you hear from the large language models or agents working in very controlled space

saying you've cleared an USML score or you've got a a good accuracy on making diagnosis. One has to be very clear that is that these codes are very specific to a very context that are that is where

the models have been trained and fine-tuned. But when you bring these models to the real world, when you bring them to the clinic, there's a deep cliff there. So essentially what is going to

happen is if 100 application that have worked in control settings in the lab move to the clinical environment where it's going to be much more noisy the data is going to be much more noisy all

these model performance metrics will certainly fall down right so your accuracy scores will dramatically reduce in your clinical environment and then imagine you take them to a population

level then it's going to be even more dramatic so what is going to be important is one on what are things which are important for me evaluating these AI models. It's not the

performance metrics. We've been beaten to death in lot of journal publications for the past decade or so on the performance metrics and therefore today AI in health has to go towards more on

outcomes. So when we use AI machine learning models are they truly improving health outcomes? So it has to be outcome based. Second is where I'm sure we'll have a colleague from ICMR talk about it

which is on the health tech assessment right so essentially is there an ROI on this tool is these tools truly cost effective very often we see that we probably don't need an AI tool the

problem today is there are people building tools without having a problem statement so they come with tools and say tell me a problem rather than identifying the problem and building for

them and that's the problem that you have in AI in healthcare as well. So the second thing that we want is to ensure that this AI evaluation is truly not just based on performance metrics but on

health outcomes to the importance of post monitoring which which has just been highlighted right so uh Dr. Dr. Vive Khanda highlighted the importance of post monitoring and auditability of

AI machine learning tools. So the importance of this is going to be much more because when you have post monitoring audit post monitoring uh work when you do a continuous continuous

evaluation of these tools what is going to happen is when it when you have the model drifts and the data shifts you will be continuously evaluating not just during the design but development and

the post evaluation as well. And that is where these tools are going to be most important where when evaluation is continuous it is based on outcomes and importantly the methods of how we

evaluate which has to be completely based to local settings. Very often the culture is to bring tools which have worked in very different contexts and try and deploy them into a different

context without doing a local validation. So what we've seen globally around is tools that get developed locally and validated locally tend to serve better the clinical outcomes that

we want to accomplish. So I will stop here with just this because I know there are other panelists but I want to just leave these two thoughts which is as we build these variety of AI machine

learning tools what is going to be important is to build the skills of local validation and AI evaluation frameworks frameworks that are contextualized to the local settings and

contextualized to the clinical priorities that we have. &gt;&gt; Thank you sir. uh [applause] I I understand these are all very pertinent points which will generate a

lot of discussion. We have kept some time for that. So moving on u I'll ask uh Taruna ma'am from uh ICMR. ma'am is the head of development research at ICMR. And uh the questions for Tarunam

is one the kill switch should there be any for an AI model is there any way to turn it off or sent it back to a previous version through a version controlling system when should should it

be automatically triggered or it should be manually triggered things like that. Second, data integrity. Again, this is a long question I understand. And the controls, this is the most important

part because I'm from a lab person. We have positive, negative, and three-level controls for everything. Ma'am is from the lab. Ma'am knows this very well. So, what kind of controls can we have for

daily running in a in a hospital or a clinical lab? Ma'am, &gt;&gt; thank you so much. &gt;&gt; Thank you. So you would like me to uh show it from

my presentation? &gt;&gt; Yes ma'am. Yes ma'am. &gt;&gt; Thank you. Can I have? &gt;&gt; So the first question was the kill switch till they are loading the things.

So uh I would like to apprise all the audience here that the regulator will call an AI tool as a product only when it has stopped learning. It will be a fixed thing that will be used as a

product. That is the definition of a product by the regulator. So if you want the AI tool to learn, you can collect that information but that product will be functioning as a product but the

simultaneously it will gather data for you to develop another new version. So that product that has been given a license by the regulator will not change. You will be coming out with a

newer version and you will again have to go to the regulator to get approval of that newer version. So your tool although is learning simultaneously as it is doing its job but the product that

is doing the job is going to be the fixed one. So that is the most important thing that is the kill switch we have. Yes sir. &gt;&gt; Okay. So the question that was given to

me was how we can have secure and quality control deployment of AI models in the healthcare and that Dr. Hyundai already mentioned that it is a multi-layered approach that we require.

We definitely require all the cyber security tools that is very very critical to us. That is what will build a trust in the public health and then we have adherence to the regulatory

frameworks. So our regulator has adopted these frameworks has come out with its own version in October. The draft was launched and is open for all the stakeholders for suggestions. You are

most welcome to give suggestions to CDSO and they will come out with a final framework in few months time. So that is what India as a regulator is developing. Then continuous performance monitoring.

So this is very very important and the continuous performing monitoring uh framework has to be built in within the AI tool because it will keep monitoring itself whether it is doing the right

things or not. And this task has been given to our center for excellence six IITs who are developing related frameworks and we call it Nirman. And then we have rigorous validation

framework that we have in the real world setting. So ICMR has a network of clinical trial sites which is 84 clinical trial sites across the country and they have very specific population

uh which is uh they are handling with. So for example if you want to do a trial for malaria so we have malaria in neic areas. So any tool AI tool which is used for uh malaria diagnosis or prediction

of uh uh uh you know susceptibility to malaria. So these tools will be developed by those centers which are having those endemic populations. So this is just an example. We have 84

sites catering to different diseases and ailments including mental health. And then we have this framework of 170 medical colleges which have the uh modern multidisciplinary research units

and then we have model rural health research units in the rural areas. So total of 170 this framework is under department of health research and is catering to our field feasibility and as

Dr. Karthik mentioned that we do have health technology assessment by department of health research. It's an attached office where all these AI tools once they are licensed by the regulator

for the public use they will be evaluated for their clinical evaluation data visav the uh standard of care. So the standard of care is the diagnosis that is being made by the clinician in

the conventional way and then you have the data or the diagnosis coming from the clinician again but assisted with an AI tool. So these two comparative there will be randomized clinical trials to be

done and these RCTs would be supporting uh the trust building because that is the robust framework for evaluation. So these RCTs as we heard in our last sessions that India is lacking in these

RCTs on AI tools and that is what we are aiming for and within a year you will see from ICMR several tools coming out. We are already in process for many of them. Thank you.

So uh now uh professor Nishit sir is from IIT Kpur and his primary interest from from since the time I know him and uh it seems to

be thought itself how entities think that entity might be human or machine and uh to that end u I have have few questions for president Nishid because since eventually if we deploy AI models

in healthcare we have to ascribe these thoughts to some entity some agent at some point who is thinking it or is that thought being generated in random in a fluff so uh questions for professor

Nishit uh whether we should uh establish an accountability chain how we do it whom do we ascribe incidents to who takes the blame that is the bottom line in all the

clinicians hearts here who are sitting here. Second is uh is is this to be done in a punitive way or in an incentivized manner? Again, incentivized manner works better. And third is uh uh whether we

should uh follow the open-source model in AI like rolling release or have strict version control for a fixed period something like that. Which ones do you prefer sir? So three questions

for professor Nishit sir. Okay. Um, so you can get my slides up, but in the meantime, I'll answer your questions while you get the slides up. So, the first question of accountability

uh is not all that complicated because at the end of the day, any trust that the patient is going to place is going to be on an institution. That institution could be a family doctor.

That institution could be a a large hospital chain. That institution could be a government hospital. But at the end of the day, whichever institution is on the letterhead that finds the

prescription, that is the institution that has to have accountability no matter what tools um are part of the chain. Um the question that I thought was the

most interesting one was the third one which is do you go the open-source approach or do you do something more systematic? And in healthcare in particular, there's this massive problem

that people that work with regulatory agencies come across from time to time, which is something I want to spend more time on, which is that in live deployments of AI tools in health,

you're primarily using them for decision support, which means that you're saying, does this patient need to be looked at separately or do we let them flow in the regular operational chain. So false

negatives are hard to come by. Let's say that you say that this patient's X-ray shows TB and you triage them and you say some human has to look at this. Now what that means is if they have a if they do

have TB that's a true positive and you get to say this model is working well or if it's a false positive and the patient does not have TB then you say yes the model is not accurate. So the precision

is low. But if you say this patient does not have TB and it the patient actually does have TB, that is not going to float through your doctor. If that was going to float through your doctor, then you

might as well not use your AI model. Your AI model is being used to reduce the doctor's effort. That means that at some point you're going to say these normal um patients or something that the

these normal X-rays or what the model is is calling normal, these are just going to go through with minimal scrutiny. And so the ex to the extent that you trust your AI model to replace the human, your

false negatives are going to be invisible to you. And so what happens is that you have to test um your models over and over for distribution drift as Pik was talking

about um at the outset and you do that with control samples. So you have samples of dis of let's say chest X-rays that you know are TB positive and from time to time you're going to float them

through the um model and you're going to see how well it does. So in this case you're doing a control test and you're seeing if the model is still doing just as good a job as before. Now the problem

with this is that if you have rare conditions, a single hospital might have five of those cases in a year. And so if you say these are the control samples, there's

no way for you to get them at this the sort of scale that you need to actually reliably test your model. So this is the answer to the question that Pikett asked which is you have to have cooperation

across these institutions which is what the open source model is. multiple people cooperate or multiple institutions cooperate because you need to crowdsource control samples,

particularly positive control samples across different ideologies that can be complicated and rare to come by in individual institutions. However, open source has a massive

accountability problem. In open source, you can choose to contribute, you can choose to not contribute. It's the wild wild west. So there's some aspects of open source that we need which is the

crowdsourcing aspects but there's a strong regulatory incentive here which is that the hospitals themselves don't want to share data because they don't want to be responsible for data leakage

um privacy compromise and other complications. Therefore uh the regulator's role is very important. So if you'll skip um so this is me doing a little bit of

advertising for the session that the health minister is going to be uh gracing in an hour's time where he's also going to release in addition to the AI strategy document this platform that

we've put together for the National Health Authority that tries to solve this problem. How do you give the regulator the ability to crowdsource data

um but to do it in a way that is rigorous and that is not unaccountable in the way that opensource is. So the sort of answer to your question Pikshit is that you have to wait for that

session to get the full answer. Um just skip forward. I'll just do my advertising properly. Go forward. Go forward. Just go skip forward. skip forward at the very end. Yeah. Here. L2

audi 430. You'll get the answer to that question. [laughter] &gt;&gt; Um and if you can repeat your second question, I've forgotten. &gt;&gt; The second question was about the blame.

Sir, the blame. Blame [clears throat] &gt;&gt; whom to blame? &gt;&gt; Oh, that was the first question. Accountability. &gt;&gt; The first was okay. So, that was first

question was rolling release. Second question was the blame and the third one. Yeah, that one is incentivizing quality or punitive action. &gt;&gt; I see. Um so

incentivizing quality or punitive action it or you always have to have both because institutions cannot work with only carrots and only sticks. You have to have both. Um hospitals, doctors you

cannot really punish because doctors are a scarce commodity. &gt;&gt; Um so you have to incentivize them. But the institutions themselves you have to punish because that is how you force

them to make structures that get doctors to behave properly. So the answer is you have to have both. You can't have just one. Thank you sir. And uh uh I guess all of the speakers uh they have uh

elaborated the matter uh to the heart's content of the audience. We have kept the last 15 minutes uh for question and answers. So I guess we have come to the most interesting part of the session. Uh

there are clinicians out here, scientists out here and technical leads, CEOs, funders and uh founders out here. So yes, the house is open for any questions.

&gt;&gt; Before we take a question, I want to make a comment here. You know, as a clinician, I'm currently surrounded by a lot of engineers, I'm sure, and everything is not algorithmic in the

practice of medicine. sadly or sadly or you know in a way u you know I've been in clinical medicine for 40 years now a trust is built up with a patient over you know over a lifetime

u while I do accept that these AI tools are here to stay and they will probably help us but at the same time you know there are any number of patients who question me today and say why is there

something so artificial in the terminology ology itself when you are coming with your health with your personal problems why are we increasingly talking about something so

artificial the practice of medicine is something very natural you know I try to explain to them that this is in an effort to try and make things safer better but he's he or she says that I

have trust in you so it's not just an institutional trust it's also the trust which you kind of maybe I'm playing the devil's advocate here but I'm not too show where you know and what the points

which everybody is making again and again is that you can't trust AI blindly. It has to be validated locally to your local settings. Only then will it have relevance. So as a kind of an

old-time physician who is trying to bridge the gap to AI, I still feel that the connect with the patient, the touch with the patient, the trust is, you know, this algorithms kind of scare me

at the moment and I am probably an old-fashioned physician here and I still think that there is a lot which natural intelligence has to offer rather than artificial intelligence. just my

perspective on the whole affair. &gt;&gt; Sir, I think that was a great opening to our discussion session sir. So yes sir. &gt;&gt; Uh thank you Perishits and the panel for an extremely uh engaging debate. So

couple of points I think uh there are more than one devil's advocate here. So uh just a couple of points like we very conveniently say uh drift in terms of models it's very important to understand

that diseases also drift health drifts. So for example if you train a model on data today it may not be really valid in the future if it were to be so we would still suppose if we had AI around 100 or

200 years years back we would have sworn by bloodletting till today because blood letting was the standard treatment. The pursuit of research and excellence in medicine is something which AI can

never never address because we cannot predict this drift. This drift in medicine can only be acquired by intelligence and having an algorithm actually defeats its very purpose.

That's number one. Uh number two is also that see it's also something uh where it deals to training. We are all teachers also here. I'm a YT surgeon who's a professor. What does happen is that when

you are training, you use the example of X-rays and you said the normal X-rays will not be seen by a person because they are normal. How do you train the next generation?

This next generation does not get trained in reading what is right from wrong. Number two, when it does that, you see what it does it it induces a lot of cognitive uh sort of what I'll say if

you start using spell check, you don't remember your spellings. It's very straightforward as such. What does happen is in the longer run you do have a set of clinicians who were trained on

AI to validate AI. They have never been trained clinically. So that's a huge gap which we have to address if you want to look at quality control when it comes to AI tools. And I

think to that extent um the medical field is far more complicated than a simple mechanism at a larger community level and all definitely may have behaved

differently but at an individual level I fully support what sir also said the human being is far more complex and the drift in human disease is far more complicated than a drift in a AI model.

So let me come in between now because we have clinicians, we also have a computer scientist. I'm both. So I'll give you my perspective. Uh which is on how do we use AI and I'm I'm not somebody who's

been working on AI at least for 2 three years. I've done it about a decade now. There are two concepts that I want to introduce. First is what what are some good use cases where

we could use AI? Well, now for example in a higher center in a center like yours in AFMC or other places one of the rare commodities that Nishid talked about is a very trained clinician

like professor Vive Hundai right what is going to be important is as a novice learner if I can actually learn if AI can actually learn how does he make a diagnosis for example as a classic

radiologist a neuroraiologist if I can put an eyetracking goggle See when I give an image where does an experienced neuro imaging radiologist look at and what are the tools that he

works with so if I can actually train AI models that work like experts to novice learners because they can't get access to professor Vive Hyundai these AI AI models can help train them in the

absence of these experts that's one second the other interesting use case is for example I work in the space of radiation oncology and what is important is with every image that you create for

a patient there's a process called contouring so when you have a tumor you need to contour for that specific image for the for the specific tumor this contouring is a very specific task that

a physician does. Now very often what happens is you do the image the contouring and imagine you give entire radiation onto that contour but contouring has a lot of interposition

variability. So what you have as a quality measure in large academic hospitals is you go through a peerreview process. So you have other experts who also peer review the contouring that

you've done. But what we've seen over the last decade or so is when there was this contouring this peer review when it is done in person physicians would actively engage and and give comments on

the contouring. But once they went into virtual space when the peer reviewing moved to virtual physicians stopped engaging and so the the inputs that they were giving on the contouring process

dramatically reduced and this is not in one single center it is in all the large academic centers in the US. So what has happened to that is to counter this problem what we've done essentially is

we've built an AI AI models that have got trained on a large amount of data and today in the workflows when this peer review process happens we first allow the current workflow which is we

present the model other physicians if they don't comment then the AI model comes in and nudges them so it's like a nudge to say professor Hyundai 10 years back 5 years back 2 years back one year

back on a very similar patient profile you did the contouring like this the AI model will come and nudge and say but on this patient why did you not give the comment so essentially what we're trying

to say now it is not AI trying to even supplement it is trying to nudge people to have a conversation on an existing plan so it is it is therefore very important that even at higher centers

today what is called a role separation framework is essentially emerging that when you have large amounts of data or large amounts of images that are available, we just don't have the

manpower, the experience manpower to even handle the amount of information images that are coming to big academic centers both in the US or even in high resource centers as well as in low

resource centers. And that is where where does AI do well? We put AI there where we think we need experts like you all. We will of course have you placed there first as such. So what is going to

happen is for 4.3 billion human beings that we have in this world, we just don't have those many experienced physicians. Physicians and experienced physicians are always going to be a rare

commodity. We'll always continue that. So therefore what is going to be important for hospitals is to develop what are called role separation frameworks on where is AI going to do

well? Where is experienced physicians going to do well? Reduce the burden for them. So I think the science of AI is of course a very evolving one. In 5 years back the way we were doing AI machine AI

model development has dramatically changed in just the one month itself. So I think what is going to happen is it's not that I want to be very conscious also for the fact that AI is a

generalpurpose technology. Two it's always an errorprone technology as well. That is something that I want to admit to start with itself which basically will come down to this fact that it is

all about how do you deploy where do you deploy and when you deployed you have the science of how you build them effectively and ensure that you follow the science of AI development which is

going to be very important. &gt;&gt; I just wanted to add to what uh you said uh Dr. Karthik is that the aim of Indian government is to make the health care affordable and accessible to the last

man in the queue. Right? So that last man we can reach with the help of AI. So we are not questioning here whether AI is replacing clinicians. Not at all. If you see the intended use statement of

the AI tools that have been approved by the regulator, all of them make a statement that this tool is going to assist the clinician in making a faster diagnosis or faster prediction for the

participant. And that's how as he said that roles and responsibilities you are actually using these tools to reduce the workload of the clinicians so that they can perform to the best of their

efficiency. If they are overloaded and they are not even looking at the patient eye to eye they're just saying okay you come you're right you go this is not going to help the health care system. So

what we want is an assisted uh uh you know health care system where the clinicians definitely take the decision but we have assistance not only the human assistants but the AI assistance

to help them. Thank you. &gt;&gt; So yes I see a few hands there. Yeah. &gt;&gt; Uh hello. Hi. Uh so I'm an AI healthcare entrepreneur making an AI product. Now the hardest part for me as an engineer

as a scientist on the tech front was to collaborate with the scientist with the doctors. So it's not about like deployment I completely agree it has to be role based and all but while making

it are for me the hardest part is to get the insights of the doctors the experience of theirs and you know get the data collection the data labeled then validation and

then the acceptance and somebody mentioned the incentive structure. So can you tell me what ICMR is doing or what government is doing for the incentivization of that because we

understand that doctors have so much load that they do not have extra work of data collection data labeling all that stuff. So yeah &gt;&gt; so to tell you very honestly the first

thing was to have the way he said the contouring of the uh you know uh the cancer tissue and that contouring is the data repository which we have created. Okay. So with Indian Institute of

Sciences, Bangalore, we have created uh gold standard data sets the images that are coming from glyobblastoma as well as breast cancer and oral cancer and these have been onloaded on the AI kosha of

mighty and these are available these annotated curated and with metadata all these images are there for you to build on as a entrepreneur. Thank you. &gt;&gt; Second thing the uh grants that we are

giving you. So we have actually initiated first in the world challenge grant where you will develop these AI tools and we support you end to end. Not that you go to TRL 3 and then you are

again lost who is going to fund me the next. So we are going from TRL1 to TRL8 in the same frame. The only thing is it will be done in three phases. So first phase is proof of concept. Second is

prototype and the third is your product. So these are the three phases. We have a validation network which is going to validate your technologies as I mentioned and therefore take you further

in the uh in your product reach out to the masses. &gt;&gt; Uh could you speak to the incentivization of the doctors as well because they feel that we you know they

they are not getting out of it. So the clinical trial network we have so if you come to us we have built a portal actually I could not see some of my slides if you can put the last one of my

slide I would like to show that &gt;&gt; so there is a portal which we have initiated 2 years ago on 25th of December 2023 under the guidance of NITI Aayog and uh with the help of uh CDSCO

our regulator. So we joined hands collaborated to make this medtech mitra which honorable health minister rightly said that it is a medtech highway for the innovators where any valley of death

you are standing near. We will handhold you and take you across. Be it financial support be it technical support be it clinical evaluation support we help you develop your clinical evaluation

protocols which are regulation compliant. So this is a board of more than 18 stakeholders and we have one-on-one interaction with every innovator. We have so far helped this

slide is a pretty old one. We have supported more than 900 uh you know innovators. So far we have helped them to get regulators licenses for test batch for manufacturing licenses the

licenses that you require for doing a clinical study. the clinical trial protocol development. So all that we have on board. You just have to register on this portal and knock at our door and

we will be there after you then till you develop. &gt;&gt; Thank you. Thank you. I think we have time for one more question. I Yeah, please.

So going back to uh Dr. Hyundai and the gentleman and talking about the devil's advocate. uh I'm primarily an educationist and I have been interested in the learning part of what Dr. Karthik

was talking about. I think an important aspect is the cognitive debt that is raised when we are thinking about carrying forward the experts knowledge and making a machine learn and then

trying to make somebody else learn from that. So the process of becoming an expert is filled with drudgery. It's it's a long process. It's it's a tenuous effortful process and we're trying to

compress that just as coaching institutes actually do. So they hard spoon they spoon feed you you all equations at the end of the day you cannot think on your own because you're

dependent upon something else. So what is happening is there is a short circuiting of the learning process itself. So that's the major thing that I guess that I mean of course everybody is

quite competent here and understands this very well that learning is is a it's a very long tedious process the debt that is created the cognitive debt AI is actually perhaps it's increasing

that debt so when we are learning we have to be very very cautious just &gt;&gt; I think the caution is the main word I mean one is as a clinician one is not against AI We very much uh do believe we

have already started using it in a [clears throat] large way. I'm an endoscopist. The AI tools tell us when I'm doing an endoscopy certain things it helps us. It assists us. But my only

concern is too much too soon unregulated. So that is the problem. I think if we go cautiously as what madam just said, if we have the regulatory mechanisms in place and we take stepwise

incremental steps, I don't think you know we should fear AI. I'm not saying we should be fearful of that. The only thing is that we have to be cautious. I definitely, you know, because this is

not um we're not talking about uh we talking about human life and the human life being so precious, we can't afford it to be us. So it cannot overtake us. The patient has to be at the center of

everything. I mean that has to be the bottom line. So with all these regulatory mechanisms and incremental steps and perhaps adapting a large model to a local setup with you know better

tools and all we'll probably enhance I mean none of us as clinicians I have a very senior radiologist from the armed forces here he's nodding his head very vigorously he's already using uh you

know AI in a very big way but what the only thing that one would say is to practice it with caution and incremental steps. So I would like to add one more thing here which is that a lot of this

um concern about patient health um and the centrality of this the medical profession seems very overblown because most of our health depends not on going to the doctor but on the food

that we put in our mouth every single day. So if you want to talk about artificiality an easy way to talk about this is artificial nutrition. The food that we put in our mouths is all junk.

It's full of preservatives. It's ultrarocessed. It's filled with stuff that makes it taste good. And everybody knows that it's damaging for your health and everybody just eats it every single

day. We don't care about our health. We don't care about being healthy. So the incentives for artificial intelligence in health are exactly the same as for artificial nutrition in

health. Corporations have to make money. Managers have to manage people. People have to have jobs. And we keep doing this. I'm supposed to

be the artificial intelligence expert. I've been doing it for 20 years. I will never go to a doctor that uses AI tools. I will go to a doctor that can spend time with me and I'll pay him because I

value my health. So in the same way that FSSAI is responsible for regulating adulteration of nutrition, the health regulators have a

responsibility to regulate &gt;&gt; artificial intelligence. &gt;&gt; On on that note sir uh because uh I had got one bonus question for each participant. So I'll claim my right

there and u I'll ask my bonus question to professor Tapan Gandhi who's just joined us. So the bonus question and the surprise question here is that uh regarding if if we eventually apply AI

models to healthcare and in a decision making role should we uh go into because there are ways and means we can examine it why certain answers came up and why certain answers didn't. we go to that

foreign path or we should uh go to a more regulatory pathway regarding that sir thank you giving uh this opportunity to answer this question so I'm really sorry

that I got late for some some reason so uh just one thing I'd like to tell of course I mean the two part again it depends on us so the first thing let me clarify few things because when I talk

to the a expert so I always feel that I mean whether we truly understand the meaning of AI or not. So we should not have the fear that AI will take over everything. No, it's not like that. And

of course in the healthcare we should use AI is a tool and when I'm talking the tool so to the best of my understanding I always believe this AI is nothing but the s and the some sort

of automation and we are putting intelligence on top of it. So the big question is why we are talking about AI. First two things we all need to remember those who work in the AI because first

thing is that we need to see that where we are not good at first thing is that we human being we get easily tired so we don't like the repetitive job so that's why we all thought of I mean we I mean

thought that can we come up with some model or gadget or some automation which can do this job for us right second thing is that always we need the accuracy and the speed that how quick we

can make the decision right and why why we feel that the machine can do better because if we look at the speed then of course the electronics or the AI is faster than the human because this is

also the limitation of the human our electricity is because of the ions whereas in the semiconductor the electricity due to the electrons that's why they are very very fast compared to

the human mind and because of this two reason we we all are adopting AI which can give us the I mean which can give the diagnosis or the uh prediction whatever you are talking about in a

better way. So coming to this regulatory mechanism and of course science always you know just like the friction is a necessary friction is good for us in the same time friction is also not good for

us likewise this is a tool and it's up to us that how we are going to use it and of course in this clinical setting or the healthcare health is the very important thing so we should not give uh

all the control to the AI AI can help us just like our slave it can uh it can increase speed of our diagnosis or it can uh guide us that okay maybe this is what is happening and we as a human we

need to take the final uh judgment because I always believe that whatever the you talking about if we not keep the human in the loop then it is not successful so that's why I mean when you

talk about the regulatory or the ethics or we are talking about the quality everything we need to first ask ourself that why I need this tool for which purpose and if we understand it better

then I I think we can get all the answers. &gt;&gt; I think that sums up the session and uh I would uh like to thank uh uh the DJFS ma'am and as well as the office of uh

DJFMS the commander FMC Pune all the participants especially Mr. and matey and our audience. Uh I hope we have made our point that we need a QC guideline at least we have acknowledged

that maybe we'll work towards that and uh from the bottom of our hearts from all the panelists and uh thanks to this audience for this wonderful discussion that we had and thanks to all the

panelists again. Thank you all.
