# Best practices from the International Network for Advanced AI Measurement, Evaluation and Science.

**India AI Impact Summit 2026 ‚Äî Day 3 (2026-02-18)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 11:30 ‚Äì 12:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 18 |
| üìÖ **Date** | 2026-02-18 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/Qkh3YRjbi6w?feature=share) |

## üé§ Speakers

- Adam Beaumont, UK AISI
- Austin Mayron, US CAISI
- Chris Meserole, Frontier Model Forum
- Sara Hooker, Adaption Labs
- Wan Sie Lee, Singapore AISI

## ü§ù Knowledge Partners

- UK AI Security Institute

## üìù Summary

This panel discusses best practices from the International Network for Advanced AI Measurement, Evaluation and Science, focusing on shared learnings, evaluation challenges, and how governments and industry can collaborate and share knowledge globally.

## üîë Key Takeaways

1. This panel discusses best practices from the International Network for Advanced AI Measurement, Evaluation and Science, focusing on shared learnings, evaluation challenges, and how governments and industry can collaborate and share knowledge globally.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/Qkh3YRjbi6w/maxresdefault.jpg)](https://youtube.com/live/Qkh3YRjbi6w?feature=share)

---

_[‚Üê Back to Day 3 Sessions](../README.md)_


## üìù Transcript

forunate to have like four very you know esteemed kind of colleagues on the stage. Um we have Jan Lee who's head of safety in in IMDA. Um we have uh Adam Bowmont um who's director of the UK AI

security institute and we have um Austin Meron who's director of the uh US uh center for AI standards and innovation. Uh and then we have Sarah Hooker who's co-founder of an amazing new uh AI

startup called Adaption Labs. Um so we're very fortunate to have them each. Uh I'm going to ask uh in turn for them to say a little bit about kind of what the current state of play is with with

their own work and and their own um organizations. So Adam, I may start with you. &gt;&gt; Thank you Chris for those um remarks. Really appreciate that. Thank you for

facilitating the panel. Um and thank you to India for hosting this summit. So as you said I am director of the AI security institute in the UK. We were founded at the Bletchley Park AI Summit

back in 2024. Um, and I want to say a little bit about the network and I'll say a little bit about um, AC. So, firstly, just to call out, we have other network members in

the room as well as the the panel. So, you might just wave in the front row some of the uh, maybe a little ripple. [applause] So the network is comprised of 10 expert

organizations that are evaluating AI. We're now known as the network for advanced AI measurement evaluation and science or names. um that that brings together uh those

who have invested quite heavily in their AI safety and security institutes or capabilities and who are doing technical work and we exchange that and work how do we share that with the rest of the

world. Um I'm really delighted that we're able to be here and participate in this panel but also to contribute to some of the key deliverables from the summit like the commons where UKAC we

are making available our inspect platform our framework for doing evaluations that is available uh via open source and used by many other organizations both in the frontier AI

companies but also in institutions in in government that we're really really proud of think really helps move us forward uh a couple of opening remarks I guess

on the importance of of measurement uh the theme of our talk today where being able to measure AI models and capabilities is is vital to our ability to be able to understand them without

the right tools we can't see what the models are capable of and we won't be able to make the best use of them and so AC's mission is focused on building that scientific understanding uh for

governments with an S not just the UK but we really care about this international work too. Uh and we're doing this alongside a new investment in the national physical laboratory uh

which is a center for AI measurement uh aimed at accelerating the development and deployment of secure and transparent AI and the kind of adoption field. Uh I might pause there just to pass over

and then maybe we'll come back to some some questions. Thank you. Um onie do you want to go? &gt;&gt; Hi everyone. I'm on from um Singapore. So I I lead the work on air governance

and safety in the infocom media development authority and as well as the Singapore AC in terms of the policy and international engagements. Um just a quick introduction about the Singapore

AC. It stands for AI safety institute. Um and uh we it was set up also in 2024ish. Yeah. Um together when we were all talking about ACS at that time um I

think around the soul summit. So you have batchley and then there was so and then at the soul summit we announced that we have an AC as well and then shortly after that's when the network

was sort of put together. Um what the Singapore does is also very much focused on evaluation. Um both conducting evaluations as well as um research into the science of uh evaluations, right? Um

evaluation of models. Um we think that it's going to be very important um to invest some of these resources in this space because still a very much emerging um science.

Um we're constantly finding new things that uh we need to tackle. Um and having an ability a technical ability within the government as well as together with uh research institutions in Singapore to

do that is important for us. Um so a lot of our AC work is also cited out of the Nanyang Technological University um as part of the digital trust center. Um so we find that it's important to be able

to tap on um capabilities from research um to deliver the promise that um that what AC is meant to achieve. So I'm just going to pause here so that we can get into a bit more of the details later. Um

just quick introduction of ACS. Yeah. And as um Adam said um we have fellow um ACS here. I just want to just call them out so they are not looking at their phones. Japan, Australia and Korea.

Thank you. We're thrilled to have everybody here uh from many of the other AC's as well. Um uh um I think moving kind of along Austin, it would be great to to hear

from you a bit about the US Casey and all the work that you're doing. &gt;&gt; Yeah, absolutely. First off, I want to thank you Chris, my fellow panelists, the other ACs and the UK and its

coordinator role for the network and our host in India for having us here today. My name is Austin Mayor and I serve as acting director of the US Center for AI standards and innovation. As folks know

or may know, in June of last year, Secretary Howard Lutnik of the Department of Commerce rebranded our organization from the US AI Safety Institute to the Center for AI standards

and innovation. This wasn't just a name change. It was a total change in our mission. We've moved from a focus on AI safety and we're now focused on innovation and fostering adoption of AI.

Casey is housed within the National Institute for Standards and Technology, which has a long history of working with industry to develop the underlying measurement science tests uh and

standards to unlock the adoption of new technologies. Following this tradition, Casey works to develop the science of AI needed to understand AI systems. One story that I I really like is that

um during the physicist Albert Einstein's lifetime, a a book was published, a hundred authors against Albert Einstein. And when Einstein was asked how he'd respond to the authors of

that book, he said, "Well, if I were wrong, you would have only needed one." And that story is very powerful to me. Um, because I see that as part of Casey's mission. Our job isn't to be a

hundred voices telling people they're wrong or they can't do this or they can't do that. Our job is to be the one voice and to work with with our our fellow members in the network so that

they can be the one voice in their country that delivers the correct answer because that's the thing when it comes to things that are scientific or technical is there's a right answer and

there's a wrong answer and these things can be measured and we need to figure out how to do it. Um so the work that we're doing is especially important to policy makers as we focus on AI

adoption. At Casey, we conduct our own evaluations of US models and of um foreign models to understand their capabilities, their vulnerabilities, and the state of global competition. Um just

recently, we published draft guidelines in the form of a NIST publication on best practices for AI automated evaluations. However, we cannot and should not do this work alone. Aligning

with like-minded countries through the network is critically important to ensure that we don't end up with a fragmented ecosystem. And just one way to think about that is if 10 countries

evaluate a model and they view it as having a certain capability and then one country evaluates it and comes to another conclusion. Well, how do we have a conversation about did we do the same

evaluation? Are we measuring the same thing? Are we taking into account the same considerations and questioning the same assumptions? So, we think that's incredibly important for innovation. We

think it's important for adoption. Uh we're doing this work internally and we're doing it through the network and we're excited to be here today. &gt;&gt; Thanks so much, Austin. Um, I you know,

in addition to having many of the the network members here today, we're also fortunate to have some industry as well. And so, um, Sarah, I'd love to to hear a bit about you and what you all are

doing. &gt;&gt; Okay. Excellent. Yes, I'm the only rogue nonAC person on this panel, so I feel very honored to be included. Um, so I suspect I'm here to bring maybe more of

a um, industry or computer scientist perspective. So, over the last 10 years, I've been part of several frontier labs. I was at deep mind and then I led a research on cohhere and now I'm working

on adaption. Um but I'm a computer scientist. So maybe I'll share um I guess two two perspectives to get us started. One is around benchmarks in general and evaluations. The other is

around the role of government in those. Um for benchmarks I think we're at um what I would call the muddy middle. So there's a lot of benchmarks that are not that useful anymore mainly because

they're quite uh static. They are very academic in nature. They capture snapshots in time that are quickly uh either overfitit to if they're open source. So typically a lot of a lot of

frontier models train on these benchmarks and so they become worthless. They're not worth the effort of curating or they don't capture the frontier and how it's moving. And so I think that's

worth having a conversation about. How do we return to private test sets? How do we uh really maintain like true calibrated estimates of performance? This relates to the second category. So

there's both what uh industry can do and also academia. But frankly I think there's two things that are interesting about the role of government. One is thing that does make me optimistic is

that this is the first wave we talked about. I mean you're talking about Bletchley and being a few years ago that for me was very encouraging because a lot of it was about attracting technical

talented people to government. Um, and this has typically been quite challenging. Um, and I think that one of the things that I'm most optimistic about is that ACS have been able to

attract uh, talent and that's important for a few reasons. Government has to have the ability to shape a growing ecosystem of technology. Um, my career has been odd because the thing I chose

to study exploded and transcended research conferences into the real world and the ramifications of that are important to think about and there has to be a royal government. The other

thing I'll put out before we open up is that I do think government has this really unique position to curate private test sets uh to have more calibrated estimates of model performance to insist

what languages are covered by a model which are rarely reported in full detail but also to talk about much more specific evolving risks that typically governments have a a a better view of

like cyber security which I think is actually very credible and um I think there's few people positioned to tackle it. So, I think that's also fun to think about here because we typically um I

think you were hinting at it, but there might be some pessimism, but I also think there's different roles for benchmarking and we should be talking about why it's important to bring

technical talent into government. &gt;&gt; It's a great point and I I think there there's been a lot of progress in in kind of drawing in talent into government. Um there's been a lot of

progress on evaluations as I was mentioning earlier. Um, it's partly why this this kind of uh, you know, panel is so is so important. And I I but I think it would be worth turning now, I think,

to to talking a little bit about just what are some of the um, challenges we're now seeing when it comes to uh, evaluations. We've got I think it's it feels like after two years, we've got a

bit of, you know, talent in the system. we have some infrastructure in place to start kind of building on, you know, what are the the challenges in the AI um evaluation space that that you all are

kind of wrestling with and in in particular, how can the network help like what can the what role can the network play in in facilitating um uh the ability to to overcome some of those

challenges? Um I may start with with you and then we can kind of work through. &gt;&gt; Sure. I I think all of us can agree that um um as AI capabilities develop very quickly, I think one of the big

challenges how do we catch up in terms of evaluating them and and assessing them for reliability, robustness, security, you know, safety and so on. Um and that's one one big challenge in

terms and that's why the network is is helpful because I think we can draw on each other's uh one another's um strengths and capabilities. we all look at slightly different things um and some

of it's the same and that's those are some areas where we could work together um but I think having the the network uh members come together um to share uh methodologies experience um of how

they're tackling some of these issues is important so I just give you a couple of examples um we talk about um in India here especially um a very important aspect is how do you ensure that models

address um languages and and and and culture for example. So making sure that um you do not then expose um critical um concerns um when when the models cannot cater for um different types of

languages. Um and I think the network has been a wonderful um team and resource for us to tap on because we have different members who can then bring um you know their different types

of languages to participate in um testing exercise together. So we've done joint testing. Um, the reason why I can call out my uh fellow aces is because we've done a lot of work together and

we're kind of friends and we know each other well. Um, and I forgot to mention actually Kenya is also in the room. Hi Stephanie. Yeah. Um, so um, so it's it's that's one value that the network

brings. The other value is also then besides the language element is also where we focus on slightly different things, right? as I mentioned um and we're interested in some of the harms

that um citizens deal with on a day-to-day basis like data disclosure um we're looking at what does a agentic um um deployment look like for and how does that in in impact citizens um and

whereas you know in the UK for example you look at some of the frontier issues around you know uplifting of cyber capabilities um surn and so on so I think that allows us to look at

different things and then bring our own sort of um value together. Yam, do &gt;&gt; you want to &gt;&gt; Yeah, I might expand on that a bit. So

um in the UK, firstly to your talent point, we've now got about 250 people in the UK AC of which 100 are our technical staff. Um and you're right on one see we've been

very focused on frontier AI systems and evaluated about 30 different frontier AI systems for a variety of different risks ranging from cyber security or chemio risks or impact on society.

um some of the challenges in that space and why the network is very helpful is because we need more people to work on these issues, right? And um it's what one of the things I've really loved

about the summit so far is just seeing how many people are interested in working in AI and particularly the participation of so many students in the conference that makes me very optimistic

for the for the future because we've got a lot of work to do. I think on on evaluations, the things that I am concerned about at the moment are how how do we start to evaluate agents

really effectively. So when I think about our cyber evaluations where we started with kind of capture the flag type um approaches, we're increasingly using cyber ranges.

But when we move towards having AI systems working together on these problems, I think our approach will need to change. and I'm in the market for ideas and learning from other network

members because I think this will be quite uh quite difficult. Um and then I guess the last challenge with evaluation I really wanted to agree with Sarah's point around there are things only

governments can do because we hold either the data &gt;&gt; or the powers to be able to do something about different uh risks. So there is a question around what do we publish what

should we share and so much of AC's work in the UK is public and we blog about it post papers share that and and we've tried to take that approach when we've been coordinating the network with a a

blog post on some of our challenges that we've shared but equally some of our work needs to be private and is either shared with companies privately or is shared with governments privately. It's

a great point. I think there's a a lot of work to do as well about setting norms about what should be public and what should be private just given some of the the information hazards and

attention hazards of of some of this. Um you mentioned agents earlier. I know um Austin there there's been some some great work happening within the US Casey on agentic security as well. Would love

to draw you in on on evaluations uh more broadly too. &gt;&gt; Yeah, absolutely. And and I'll touch on both. So the first in terms of the challenges, I mean the main challenge is

that this field is so new and it's evolving so quickly. There are new capabilities and new things to test and measure every day and it's keeping all of our organizations focused on the

thing we're trying to measure and not the new capability that's exciting that we also want to measure. Um is is I think a task given how rapidly the technology is evolving. In terms of what

we do through the network and what we're doing at Casey, we're trying to build a shared vernacular, a shared set of practices that we can use to communicate the work that we do in terms of

evaluations. There's three areas in particular. We've noticed that there's a need for us to build best practices and try to develop consensus. Um, the first is about figuring out exactly what we're

measuring and what the import of that is. I think folks, you there's a lot of things that AI can and will do, but how do you measure that with a benchmark? And is a benchmark the appropriate tool

to evaluate that concept? In in the field, there's a concept called chart crime, which is when you give a graph and the graph doesn't actually depict what the graph may depict at first

blush. For instance, the y-axis isn't scaled or or or other features. I remember one early presentation with my team. They gave me a chart in the Y-axis was model performance and one model was

higher than the other and it's like well what is model performance and then you know they actually then had to walk me through exactly what they contended went into that and the methodology but that

methodology and everything that goes behind it. That's incredibly important because when we talk about a lot of these commercially available benchmarks they'll measure performance on graduate

level or undergraduate level science questions. Well, just because a model can perform well on a, you know, biology 101 exam doesn't mean that it can actually help a researcher, you know,

develop a novel vaccine, which could be something that, you know, somebody would want to model to do. And understanding those capabilities, particularly as now we're getting to the the agentic world,

is is increasingly important. The other two, which are areas that the network worked on, including at our meeting in San Diego, are reporting out results of evaluations. Um, how do you do that? If

you run a benchmark 10 times, do do you tell people that you did it 10 times? Do you do you pick the best run? Do you take an average? What information do you have to disclose so that another person

can reproduce that work? In the US, the the president issued an executive order on gold standard science that we want science that is reproducible, that's transparent, where people explain the

assumptions they're making, the configurations they're setting. And I think we've really taken that to heart. We want to communicate that information to the rest of the ecosystem, to our

fellow network members, um, and to the people who review our products so they understand. Um, and the process of going through and identifying what should be reported out, I think, has been

incredibly useful, at least for us at Casey. If you look, we have the the publication um, best practices for automated benchmark evaluations. It's NIST AI 8002. We'd love if you read it.

Give us your comments and thoughts. So, I'll put that plug in. But in that document, we identify um some practices that we call emerging practices. So these are practices that we haven't seen

necessarily take hold across the evaluator ecosystem, but we think they would be helpful. Um and one of those practices that stands out uh to me is, you know, publishing transcripts so that

other evaluators and other, you know, members of the network can see exactly what happened with, for instance, the benchmark that you ran. And we flagged it as an emerging practice because we

ourselves don't do that yet. And so we're hoping through identifying it as emerging practice and hopefully doing it ourselves, we can help shake the ecosystem. Um,

so so yeah, that's &gt;&gt; great. &gt;&gt; That's great. And I want to come back to kind of where the network is at and some of the things that came out of San

Diego. But before getting to that, Sarah, how are you thinking about and conceptualizing evaluations at the moment? Just what some of the the lay the land is when it comes to the

challenges involved. &gt;&gt; Yeah. And I'll just say that was quite fun cuz you said the NIST report and actually a lady was nodding energetically in the audience which is a

very great great reaction to a full N report of of um different landscapes. So I mean I'll throw out three things which I think are very interesting here which are the core challenges I think of of

where we are with benchmarks. one is that um what I would call the influx of capital into this field. And so let me just be candid. When I started working on machine learning, all the benchmarks

were open because no one cared about what we did and everyone thought we were slightly foolish for working on what we did. And so it was a smaller community. Um in the last 10 years, we've had

massive influx of capital and that is the conflation with marketing. So when there's a huge amount at stake, people care about benchmarks in a way that is often um it's not about measuring

progress, it's about stating you've arrived somewhere. And what this means is it creates a conundrum. We can't use open benchmarks anymore because they're immediately gamified and trained on. We

private benchmarks, you would think, don't have these issues, but we did a very thorough, you know, I'm quite proud of this work. We did something called the leaderboard illusion where the most

widely used benchmarked has been widely gamed. I mean I think you alluded to this with like number of goes you get um or actually I think you were talking about number of goes you get to submit a

model to something. If someone can submit 34 models and choose the best score it's a bit cheeky you know like oh what are we doing here? So that's the first thing is that we're in a conundrum

where the difficulty of benchmarking is partly because there's huge pressure on all the stakeholders involved and uh this has to be acknowledged because it means we have to go back to private test

sets. That's important. It means we have to not give notice of when things are being benchmarked. That's important if it's to be credible. Um and it also means that if you're in the open

community you need to have holdouts or you need to have some way of like evolving your benchmark. Um, and that's fun because then you can catch people over fit to it, right? If you kept some

data back and then you can do a cheeky like report a year later. I think that's quite fun to think about. Number two is very powerful which is that um our benchmarks are not capturing real world

harm in a meaningful way. And this is quite sincere. I think we sometimes organize safety gatherings and it's all about the most clever way you can break a model. But the truth is like a lot of

the harms today are about you know phone calls which are fraudulent which are uh scaled because of voice AI is getting so good. Uh it's about the ability to uh frankly create fake resumes very

readily. So it's interesting like we're hiring aggressively for a team. I'm seeing this firsthand. But it's also about just the subtle ways in which people um may be often like I think this

is not talked about in these circles but it's it's AI companionship and the vulnerability of that and we don't I think it's very interesting because that I'm talking about now computer science

focus or academic focus that is often not not treated to the same degree. It's not seen as like um something that we should dedicate a lot of resources to or treat in compos and that has to change

because now that our technology that we build is being used everywhere. We have to think about the system and the implications. So that's the second component is that our our benchmarks

have to take into account not just the algorithm but the system and the interaction. Um and then the third thing I'll say is this and this is maybe a cheeky thing. I use my first comment to

say why I think it's so important that AC's exist and they have this purview. So the whole point of benchmarking is to guide decisions and if you benchmark and you don't make a decision afterwards

you're just collecting data and the truth is I think that's the major challenge ahead is how I think if we look back and the years have passed how has the benchmarks informed decision-

making um and I think that's the ultimate test of whether AC's have permanence in government is whether they can influence the shape of the ecosystem and I think that's worth talking about

because that is a core challenge for how benchmarks are crafted. Otherwise, you're just collecting reports um and they're sitting there. &gt;&gt; I think that's a a great point. Um and

certainly one that I think um I'd actually like to build on in in the next question. I'll also say um uh we're going to try and open up the the conversation in a bit. This will be kind

of the last round. Uh I'm going to definitely draw in some of the other members of the AC network as well in a minute. Um uh but before we get to that um I I want to kind of talk over um you

know within uh uh the San Diego conference that happened a little over a month ago now. Um there was a lot of great discussion as I mentioned earlier about evaluations. We've talked a little

bit about challenges so far but it'd be great to hear a little bit about you know what kind of alignment there was um across members of the the international network um when it came to you know

practices for evaluations. you know what some of the the conclusions were. And the reason I I appreciated Sarah's framing as well is for there to be alignment on some of the evals, there

would probably presumably also need to be some alignment on like what these evals are for and like what are the decisions that we're we're trying to to move towards. So, um, uh, to the extent

that you can speak to to to those questions, it would be great to kind of draw out a little bit, um, you know, how how those conversations went and where the different members within the network

are thinking of of of pushing these conversations. Uh, Adam, you know, given the kind of great role that the UK is now playing in that, I I'll kind of turn to you to to to lead this round of

questions. &gt;&gt; Sure. Thank you. And just to give a bit of context on what happened in San Diego. So um this was uh in in San Diego you had the Nurips uh conference with

many many different AI researchers all gathering in one place uh in the US and we held a network event in the margins of that to be able to bring together uh both network members but also industry

practitioners and experts to have a workshop to explore these questions and look at well what are the best practices that we're noticing in in evaluations? What open questions do we still have?

Um, UKAC, we published a blog post about this last week that summarizes some of the different things that we noticed where there was common agreement there around things like having a clear

objective and purpose for an evaluation. To your point earlier, Austin, like what is it that we are actually measuring and have we scoped that question accurately to get an answer from the evaluation?

Um, being able to reproduce results. Uh so these can be like verified by other members in their their context and having the right amount of transparency. Um but we also had a range of like open

questions. One thing we explored was uh what format should reports take? Do we want to kind of standardize around a common approach to that? Uh what kind of principles we have about when to be

public or or private. Um, so these are these are some of the kind of the the areas of common agreement but also open questions that we had. And then on on actions taken because I didn't want to

leave Sarah's point hanging around. what decisions do these kind of drive? I guess in the UK I can see this perhaps most clearly in the in the realm of cyber security and how our understanding

of frontier AI capability in AC combined with places like our national cyber security center. Uh when you bring those two things together, then that can turn into decisions that are things like um

the the European Etsy standard for secure AI deployment. Uh that enables people to build things in a a more secure way or or it factors into the advice guidance of products in in cyber

security because it has a good we have a better understanding of frontier AI risks and how that manifests. That's one example, but I think it cuts across many different domains.

&gt;&gt; Austin um does that kind of match your reflections on on some of the conversations that have been taking place? &gt;&gt; Yeah, absolutely. And and echoing Adam,

I think in the meeting we had in San Diego, we had a terrific amount of alignment around sort of the core principles of what we're doing and they track the gold standard science

principles I was discussing ear earlier. We want evaluation science that's transparent, that's reproducible, that's reliable. It is an immense scientific and technical task for us to actually

measure the capabilities of these models. It's not quite the same as measuring, you know, how long is a meter or how heavy is a kilogram. These these measurements, it's a lot more involved.

There's a lot more subtle decisions and assumptions that you have to make when you do these evaluations. And I I think all of our all of our networks, all the AC's were very interested in um those

aspects of the science and also communicating clearly. I think each each member of the network, they may have policy makers in their home country that want to take policy in a certain

direction, but I I think we're developing sort of the core of measurement science and the the language we can use to communicate with one another about um AI evaluations and

assessments. Um and at least from my vantage point at Caseia, I think just doing that work in and of itself has been a terrific value ad for the US government. We have a small team of

experts that's very skilled at what they do in terms of understanding models, how they perform, and we've been able to leverage that and engage with partners across the US government to inform a

wide variety of decisions. We've talked various organizations about adoption and procurement. You know, how which model do you buy? Um, that's a massively important decision for the US

government. It's an important decision for for governments across the world. And that may be informed by things like model performance. And to bring it all full full circle to talk about some of

the work we're doing as as AC's and Casey is for instance benchmarks. If somebody says I scored um a certain amount on a benchmark well uh how many tokens did you use? What was the budget?

What's the cost efficiency of it? Um that matters a lot for people who are using adopting deploying AI and so is that something that should be reported alongside your benchmark result. Did you

get a better score because you used significantly more resources to get there? Doesn't mean you didn't get the higher score, but it could inform a lot of decisions. Um, and so sometimes we're

we're we're starting sort of in a a first principles uh state where we say, what even should we be measuring? Should we just be measuring the benchmark? Are there other things that may be relevant

to folks as they as policy makers as they make their determinations? Um, and so as members of the network, we've been able to really communicate about what should be measured, what should be

reported, and how do we do it. Great. Onie, would you like to to add some color? And just be before you start, if if anyone who's also part of the network would like to speak or come

in, please raise your hand as well and we'll we'll get a microphone to you. But um on &gt;&gt; Yeah, I'm just going to be a little bit cheeky here. Um one, I agree we agreed

in San Diego that it's important that we focus on the science and developing the methodology for eval. I think that's super important. Um and and focusing on how to do this well, right? Um and I

think and then and really also to address Sarah's point earlier that for ACS to be relevant and to be useful, we do have to meet a certain need within our governments, right? And all of us

have I think slightly different need within our governments. Um from Singapore's point of view, it is about yes um evaluations of models for policy makers in in Singapore. At the same time

is also building methodologies that we can then translate into useful best practices and tools that the rest of the community the rest of the industry um learn from. I think that's also another

mission that we have that for us it's um it's very important. Um but every AC has a slightly different need and I think that's and that's really also answering Chris's question. Do we agree on what we

want to measure together? Uh no I don't think that's also that's I don't think that's what we are trying to do in the network. um it's what we agree on is we should be building out the the science

of evals and the methods and that's something that we can come together collectively to do well and so then where is the where I think is is the benefit of the network coming together

is not just in terms of you know developing methodologies you know building out the science and so on but it's also something that I've saved quite a lot of is that element of

creating this network of trust I I think events will change, needs will change, objectives will change. Um, but having a group of institutions that you could work together on because you have

done work together over for now I think two years already. We've done joint testing exercises together. Go to the Singapore AC website. You can read some of the joint testing reports that we've

published. Uh what are some of the methodologies? What are some of the uh outcomes of that? I think that that you give you a sense of the kind of work that we did together. Um but having a a

group of institutions that you could because you have built a relationship with because you you talk to every other now maybe less often but at one point every week right I think it's uh very

useful when you do need to start to think about you know when when um when you have to do more together when you have to pick up the phone and says this thing has happened can I get a sense of

what's going on on your end I think that trust is uh really important important and this is one of those networks well I think it's the only network where we have a technical conversation with one

another at a government level that allows us to exchange information in a trust manner and I think that's really the value of the network that doesn't get spoken enough of y

&gt;&gt; I really like that kind of framing of the network in terms of the trust of of the information exchange and in particular the the kind of emphasis on the technical and scientific nature of

that exchange less about what specific risk is that being studied, but more how we should actually do these evaluations, how we can standardize some of them, etc. Um, we want this to to be a kind of

participatory uh conversation. Um, so we uh can hopefully kind of open this up to the floor a little bit for for some questions. There's also if anyone in the AC network would like to to comment uh

or the international network rather um uh would like to kind of offer additional comments or questions as well um uh uh please feel free to raise your hand. Um I may open the floor to I think

what I may do is take two or three questions at once and then we'll kind of turn to the to the panelists to to be able to to speak. Um so yes I'll start with you.

&gt;&gt; Yeah. Hi thanks for the conversation really helpful. Just want to understand as you're moving from say static models to more agent deployments uh where behavior

&gt;&gt; Yeah. So I'll just repeat my question. And I just want to understand as we moving from static models to agent deployments where behavior is more emerging from the interaction with the

model, the tools, the memory, how exactly is evaluation evolving uh to better measure that. Okay, great. Thanks. Uh if we can have a couple of other uh questions as well. I

see a hand right there um in the middle of the the row and then another hand Uh hi, thank you for your insights and I just wanted to ask that uh with the growth of AI in the modern era, many

countries and many private institutions uh they are making new models but every country's resources uh they are on a different scale and their budgets for AC is different. [snorts] So how does the

network help these countries and uh to keep par uh with the evaluations and everything and in terms of resources uh is AC helping other countries apart from the network I would say.

&gt;&gt; Thank you. &gt;&gt; Okay great um thank you and then we'll take um one more if you can pass it right across the the row there. Yeah. &gt;&gt; Thank you. Thank you very much. U just

wanted to understand uh how does geopolitics affect your work? uh you know the evaluation metrics you know you are a bunch of experts but is you know there is geopolitics around AI how does

that affect your work &gt;&gt; great so we've got uh kind of three starter questions um they they kind of range they cover a range of of issues from you know how eval are evolving to

um you know resources and budget uh constraints and and kind of taking into account the fact that there's kind of differential uh uh you know opportunityities across some of the

different ACEs and then obviously just some of the broader context which I would just kind of immediately frame as as part of the value of some of these tie in scientific and technical

exchanges are to try and keep it focused on the on the core science but uh does any of you uh would um whenever you like to kind of take the lead in in offering response yeah Adam

&gt;&gt; I might just answer the resourcing point because I'm I'm conscious UKAC went first and um we are like are the largest concent concentration of of talent in government that's looking at this kind

of issue. Our approach is um to publish and to share is our essentially our goal. So things like both sharing software like the inspect framework to do evaluations but also control arena

inspect cyber with a bunch of these different tools we've been trying to publish and open source and share and getting good take up um around the world and in different companies and and

governments but also to share what we are learning. So in December, we published the Frontier AI trends report that kind of summarizes two years worth of evaluations across 30 different um

models that we think it's helpful to share with others to then inform policy making inform informed decisions. Um, so it's it's not the case that this is sort of like a closed club, the only place

where learning happens. And I think the approach of sharing information, technology, tools, methodologies is a way that many people can learn and and scale.

&gt;&gt; One, do you want to come? &gt;&gt; I can take the agentic one. Yeah. And then Austin, you can do the geopolitics one. [laughter] [gasps]

&gt;&gt; Um, anyway, jokes aside. Okay. Um the agentic one it's a great question actually. Uh yes uh it is a new area of evals. Um we are all figuring out how to do it. Uh we've tried it with within the

network. Actually we did a joint testing exercise last year on just a very initial kind of thinking about how would you test for agents. Um and we realized that some of the key differences number

one the scaffolding is very different because you're not just looking at the output you're also looking at what happens in between. you have to find ways in which to collect the data of

what uh what tools are being called are the right tools being called uh uh undesirable uh requests being turned down and so on. So there's quite a lot of um things that have to be done in

terms of scaffold um and as well as we we were quite ambitious and we tried to do a multilingual agentic evaluation in that joint testing exercise. Um and we realized that actually that it's very

complicated when you think about you know using different languages you have when you're translating the the data sets that um that used to call the agents actually you have to also worry

about things like um tool calling the what kind of languages should in you know do I in in Japan in Japanese actually the data set we still use the English names but in another uh language

you have to translate the two call into a different language you know and so on. So a lot of areas um and opportunities actually for us to develop out agentic testing and I think that's where again

coming back I sound like a broken record but coming back to this network in terms of how we could help each other um to build this out it's important but also I think to the earlier question about

engaging with the broader community I think we're also keen to learn from everybody else who's doing this work that's why we have people like Sarah you know third party testers working with

the labs and so on to really see how we could augment one another's efforts and over to you Austin. &gt;&gt; Well, I'm just going to echo that I think the network is an incredibly

important forum for science and and technical exchange. I I think when governments interact sometimes you have the the subject matter experts who are close to the ground and then it gets

filtered up through layers of bureaucrats and diplomats before it gets communicated to other countries. And what the network does incredibly well is we put our experts together and they

talk as experts do. Sometimes they talk on Slack, sometimes they talk on email. Um, I don't use Slack. I'm not an engineer, but I understand people in the computer science field. They all they

grew up on Slack. They use it all the time. And I really do enjoy the fact that my staff tell me that I was just talking to so and so from another AC on Slack because the the field is moving so

quickly. We want our experts exchanging ideas and talking and sharing papers and saying, "We saw this new problem. We saw this new paper. How should this change the way we're doing our work?" And so

the network is an incredible forum for that type of exchange. And I think we we want to keep it that way. And then we also are trying to be as transparent as we can. We love to come to events like

this and talk about the work that we're doing. We published a quite detailed blog post in the follow-up from San Diego and our work in San Diego led to 8002 the publication I spoke about

earlier which really goes in depth about some of the things we're looking at in terms of automated benchmark evaluations. So I think all of our organizations we're trying to put our

information out there. This common language we're trying to build around AI evaluations and assessments. It doesn't work unless we share it. And so we have a very open collaborative environment

and our our materials are out there and we really encourage you to engage. I think each of our nations engages in different ways but you uscy we have requests for comment requests for

information out to industry when we do publications. We publish drafts or comment and we solicit comments from you know any anybody in the ecosystem who has something to add. And so we want to

keep that going. &gt;&gt; Great. I think we've got time for two more questions and then we'll kind of have a rapid fire uh close out. Uh there's one gentleman over here and then

another gentleman right there. Um &gt;&gt; yeah hi uh first of all thank you so much uh probably from the perspective that you are making AI safer day by day. I have a technical question around this.

So most of the frontier models they develop capabilities and after that we have evals catching up to make it safer. Now the challenge here is that if you see from an architectural perspective

superposition, poly semanticity are inherent in the architecture of these models. How are we probably innovating to essentially rectify that so that the frontier models don't develop these

capabilities and I had one add on to the geopolitical &gt;&gt; very quickly and then we need to go to the the last question after you. I had one point on the geopolitical thing was

that uh specific to us uh so very recently there has been an executive order from the perspective of American leadership in AI would that have impact on AI safety and if you can just

elaborate and help us understand that. &gt;&gt; All right. Um last question here and then we'll we'll go to the audience or go to the the panelists. My my question is uh regarding control uh and the lack

of development of of measurements and benchmarks on particularly intervenability and processes and procedures to roll back to last known good. Um so I'm wondering perhaps if

Sarah if you if you know privately you have that but it it certainly doesn't exist at the AI safety level. I may um so there's a lot put on the table in terms of just some of the next

generation I think topics within uh eval um I might reframe it in that in that regard like is there kind of we've only got a couple minutes left here so just some brief thoughts about where uh the

eval space should go in the future uh and then we can kind of close out um close out the discussion &gt;&gt; and start with you I'll throw out two things because there were two I mean one

thought is this idea that evaluations happen after models I think that's mainly a facet of access to evaluations. So typically frontier labs continuously evaluate across different life cycle

stages. So training is incredibly expensive. A lot of your capital is in it. So you are almost certainly measuring many attributes over the course of uh pre-training, post-training

uh and then test time scaling. Uh so I I think that's mainly a facet of when third party auditors get access and that's worth a conversation. Um I I think few third party auditors do have

access before often it's often just before release frankly. So uh there's a question as well like does that have enough intervention point if something is wrong. Uh there's also an implicit

question of you know many uh frontier labs set projections or uh thresholds for different degrees of risk and there's a question of how is that um the bending of those depending on how

progress has has actually come about. So I think that gets to one of the first questions. The second one about rolling back, I I think that the notion of rolling back, I'm not quite sure I

understood. So typically most frontier labs have multiple models at any one time. Um and multiple generations and actually for many many reasons including cost, you may not be getting the model

you actually think you're using. Just to put it out there. Uh and so rolling back typically um it's a pool of models. And so the idea of a monolithic model is not quite true anymore. Um and then the idea

of rolling back maybe is not as clear because um you can roll back in training time. So typically you checkpoint aggressively but yeah go ahead. &gt;&gt; Oh yes go. Let me open up. Yes. Yes.

Yes. Good. Good poke Chris. Yes. What do others think? Go for it. Yes. &gt;&gt; Just any kind of quick wrap up. &gt;&gt; This is being an excellent [clears throat] moderator. Well done.

[laughter] &gt;&gt; Uh we've got about one minute left. I just wanted to offer kind of a you know quick 20 or 30 second intervention for the the rest of the panelists about the

future of of eval where they'd like to see things go. I &gt;&gt; I'll just say quickly in response to one of the questions we at Casey we do have agreements with many of the frontier

labs and are working to get agreements with the others to enable us to do pre-eployment evaluations and I think that's an incredible tool that Casey has in terms of understanding the models and

it's something we look forward continue to continue to do with our industry partners. Yeah, I don't the same is true in in the UK and there's good news and bad news

here. I think in our frontier AI trends report we noted like safeguards are improving but we still are able to get universal jailbreaks for many of the risks that

we're concern concerned about and and that picture will look different between sort of responsible deployers that are implementing safeguards in proprietary models um is a different kettle of fish

fish for openweight models that present different risks. Maybe very quickly I think um next step uh we're going to focus very much on agentic AI evals I think that's an area

that uh needs a lot of work on one two I think something that we've never talked about and I don't think maybe we will do it as part of the network but certainly something that's important is societal

impact assessment I think that's something that's that we need to spend a lot more time on it's not your technical eval it's not benchmarking you know it's not great teaming or anything like that

but it's societal impact eval that I think or assessments that I think will inform form on some of the policies that we want to make. Thank you. &gt;&gt; All right. Well, thank you for a very

rich discussion. Um uh please uh follow me in congratulating just all the great panelists and the work that's happening uh in the international network for science. So thank you so much and and

look forward to continuing the conversation. Okay. something funny. &gt;&gt; Uh he's running a bit late. Wanted to

check like by when the panel will start and &gt;&gt; we going to start now. &gt;&gt; Let him join late. &gt;&gt; Okay.

&gt;&gt; We'll start with the questions with uh &gt;&gt; check check. Thank you. &gt;&gt; Let him join late. We'll ask his questions when he comes. I have just noticed that his name plate

is also not there. &gt;&gt; That's coming in. That's coming in finance. &gt;&gt; And there is one person. Good afternoon everyone. Thank you all

for making it today here. I appreciate your time that you're here today. I know the and the Delhi weather is like now as a Bangalore weather today. So I'm happy to have see that. So uh this is Bachand

BK. uh I lead startups and innovation for Karnataka digital economy mission and I would like to take take this opportunity to thank the India AI impact summit 2026 for giving us this

opportunity to convene this uh wonderful panel discussion where we have uh government leadership from Madhya Pradesh Odisa and government of Karnataka and also Telangana. So without

further ado uh I will take this opportunity uh and uh to inviting um our guest for this panels. I would first request u Sri Vishal Kumar Dave the additional chief secretary electronics

IT enit of Hodisa government is an officer of 1996 batch belonging to the Odisha Kedar as of early 2026 is a pivotal figure in Odisa's administration recently elevated to the rank of

additional chief secretary I beg your pardon it is mentioned as principal secretary on the slide but he's an additional chief secretary um he oversees the state digital

infrastructure AI implementation and cyber security and he has has lot of accel so I don't want to mention it you can go Google it but second I would like to

invest I mean I would like to invite uh Dr. Mangala in the IAS officer of 2002 batch Karnataka.
