# Responsible AI in Practice: A Data Perspective

**India AI Impact Summit 2026 ‚Äî Day 3 (2026-02-18)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 13:30 ‚Äì 14:30 |
| üìç **Venue** | Sushma Swaraj Bhawan | Nalanda Banquet |
| üìÖ **Date** | 2026-02-18 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/FyJq-bWCkoI?feature=share) |

## üé§ Speakers

- Dr. Richa Singh, IIT Jodhpur
- Dr. Shruti Nagpal, Sony AI
- Ms.  Aprajita Rana, AZB & Partners
- Shri Nitendra Rajput, AI Garage, MasterCard

## ü§ù Knowledge Partners

- Sony AI, Sony Research

## üìù Summary

The session will begin with a 15-minute introduction outlining the research we have been leading at Sony AI, after which we will open into a moderated panel discussion. The aim is to examine how responsible AI can be meaningfully advanced by addressing data practices, evaluation standards, and governance considerations.

## üîë Key Takeaways

1. The session will begin with a 15-minute introduction outlining the research we have been leading at Sony AI, after which we will open into a moderated panel discussion.
2. The aim is to examine how responsible AI can be meaningfully advanced by addressing data practices, evaluation standards, and governance considerations.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/FyJq-bWCkoI/maxresdefault.jpg)](https://youtube.com/live/FyJq-bWCkoI?feature=share)

---

_[‚Üê Back to Day 3 Sessions](../README.md)_


## üìù Transcript

in practice and uh it is going to be more from a data perspective. We've been hearing this term AI responsible AI since the past 2 days a lot. Everybody is talking about it. But I think we must

also talk about conversation drive these conversations about how we can do it more responsibly. So I am Shri and I'm a researcher with Sony research. I'll briefly talk about

some of the research we've been doing here in this space and then we'll have some of our really nice and cool panel members join us and share their opinions.

So while I wait for our clicker I will tell you and we'll begin. So at Sony we've been asking this very fundamental question of what if fair fair fairness can begin at the data set

level like we talk of fairness we talk of models we talk of bias mitigation but what if we actually start it from the data level itself and sorry And over the past 2 days you would

have seen we've seen a lot of human centric applications being talked about. We've seen robots, we've seen healthcare, we've seen face, we've seen biometrics

and we know that human centric AI is here to stay. It's not going anywhere which makes us developing AI responsibly even more important. Now what does responsible AI really mean

and what are we trying to do at Sony? So we started questioning what is ethical AI? What is responsible AI specifically for human centric terms and what we realized it is that we need to take care

of certain key aspects something like consent it is a non-negotiable people are not talking about consent enough. If we have consent we need to talk about consent revocation. So if someone is

okay with sharing the data and tomorrow they say that no we don't want to share our data anymore we want to revoke consent there should be an option to do this which is already being talked about

but it is not yet actually made practical or implemented globally diverse representation this is again a non-negotiable we need data to be as diverse as possible we need copyright

protection uh we want to prevent unauthorized use and the most important but very less talked about is fair compensation for everyone in the pipeline be it people who are

contributing their data who are working with the data who are annotating it so fair compensation is another very important topic to talk about and in order to address all these

challenges we came up with this data set called phoebe In short, it stands for fair human centric image benchmark and it's the first globally diverse consensually collected fairness

evaluation data set for a wide variety of human ccentric computer vision tasks. And when I say wide variety, I mean this is one data set which will let you do nine different computer vision tasks

from a single data set. So you don't have to go around running pillar to post finding one data set for face recognition, one for person segmentation. This is one data set you

can use it. It has dense annotations. It is built in an ethical and robust manner. So what I mean by that is all data has consent. It is ethically sourced. It's fairly compensated. This

is the first data set which is publicly available and GDPR compliant. Moreover, this is a data set which will enable you to do fairness and ethical evaluations. So, it can enable you to

diagnose bias. It will give you a large number of human- ccentric tasks you can work on, which I already talked about. And when I mean richer annotations, I mean that each image here has 40 plus

annotations. self-reported, quality checked by QA workers, quality checked by a bunch of researchers. And these annotations are specific to images, specific to

subjects. They also capture environmental conditions and they also capture the device information. So what camera, what focal length, when I say environment, outdoor, indoor, where is

the light source coming from? And uh for like image specific it's pertaining to the subject. So it's more like what is my actual skin tone? What is my apparent skin tone? Do I have facial hair? Do I

not have facial hair? And it's not a binary data set. So it has a lot of annotations which go into the specifics. So is it a goatee? Is it a goatee and mustache? Is it white hair? Is it blonde

hair? Is it brown and black hair? So there's also an option to list multiple uh features in the annotation space which makes lives for researchers also very um valuable and important and

easier because we can then use this information to actually diagnose bias. What is the purpose of doing all that is that we release an open-source data set which will enable people to actually do

fairness evaluations. It's a first of a kind. Nobody has tried to tackle this problem where we are actually having a data set which is so dense is GDPR compliant is fairly

compensated. So it's a blueprint for anyone looking to do ethical data collection and of course the reason why we all are here it is fostering responsible AI. So it is to motivate

people to talk about responsible AI create awareness and of course start building AI more responsibly. This is a link to like you can download the data set, take a look, play around,

scan the QR and there's a link as well which um shares all the information and now I will call upon some of our panelists on stage um to continue this conversation of responsible AI

specifically in the data space and how we can build AI more responsibly. Yes. So I will begin with Aprajata. Um Apraja thank you so much for joining us. She is a partner at AZB and partners

specializing in AI technology law and data governance. She's qualified in India, England and Wales and New York and has been recognized by chambers and legal 500 for her work in technology and

law. Thank you so much for joining us. Next we have Ankor from Mastercard. He's a VP at um AI garage Mastercard and he leads the AI center for excellence at Mastercard for EMA regions and he has 20

plus years of experience in AI and building products. Thank you so much. And I think we can start now. We have one panel member who's stuck in traffic. We've all been there. So we will wait

for him and introduce him when he's here. &gt;&gt; So I think I'll begin with a few open questions and then we'll go on to more specific questions. So Ankur and

Aprajata both of you have been working with technology in technology with AI um and we've been hearing this term responsible AI safe AI a lot over the past couple of days and maybe months as

well. So in your opinion how would you define responsible AI and in specific to your domain or more broadly and what do you think are the key components in it? Cool. Thanks. Thanks for the question.

So maybe I would like to correct you. People are talking about AI not responsible AI. So uh for us uh like you know we operate in the financial uh industry domain. Uh

so since it has to do you know all the matters related to money. So trust is the most important aspect you know which we have to deal with. In fact you know if I have to put it uh like you know

what Mastercard does. So we uh at Mastercard we consider ourselves as the guardian of trust. So our business is to create trust in the financial ecosystem and then to preserve it on an ongoing

basis. Now coming uh to the topic of responsible AI. How do we ensure that you know whatever AI systems we are designing, developing or deploying are responsible. So the things you know uh

it's the principle which we follow it's called security by design. So we ensure all our AI systems they are fair a uh they are transparent B there is some kind of you know accountability which

has been assigned to different components and then at the same time these are compliant as well compliant and secure so maybe I'll touch upon all these four or five different aspects

first is uh fairness so I need to ensure that all my models like whether it has to uh do some kind of transaction transaction approval or decline or let's say you know uh deciding for a loan

application it has to be fair. It can't discriminate between the customers on the basis of let's say uh race, ethnicity, gender, income group. So that's fair. Second is uh it has to be

transparent. By transparent I mean you know there has to be some explanability which can be associated with the decisions your system is making. In finance every decision has got you know

has to have some kind of defensibles. For example, every customer he has a right to know why his application has been rejected. There may be instances where regulator

can come to you and ask for you know certain decisions which you have made in the past. Maybe he'll come to you after a couple of years asking for why did you do that. So things like those you need

to have proper explanation in place and then there has to be some kind of audit trail which you can use at a later point of time. Third is around security which is very specific and very relevant to

the topic. So all AI systems they feed on data like without data data is the oil on which this entire machinery runs. So I need to ensure the data I'm ingesting is from the true source. The

veracity has been verified of the data sources. I'm allowed to use the data. The consumer has given me the consent to use it for my uh for my model training. Then at the same time I am the custodian

of the data. So I need to ensure it is not being leaked anywhere. there is no cyber you know cyber instance or cyber heist of of my data. So that's also there and at the same time depending

upon the domain or let's say country or region you are operating in uh you also need to be compliant by some of the local guidelines. For example in Europe uh we recently had uh the EU AI act. So

I need to ensure that my AI systems are compliant to that as well. So this uh in fact I would say uh if I have to summarize like you know AI systems are essentially scaled intelligence

responsible AI is scaled intelligence with some you know with there is no bias it is fair accountable and compliant to the regulations. &gt;&gt; Thank you.

&gt;&gt; Thank you so much for your insights. I think you corrected me at the right time to say that we've been talking about AI but it's also important to start &gt;&gt; that people are waking up to realize the

importance of &gt;&gt; responsibility u a prajata from your point of view how would you define it &gt;&gt; I think of course I have a lawyer's perspective onto the AI conundrum and I

do agree with you both that uh especially I think in a lot of developing countries including India there is a lot of conversation about using AI and making it beneficial and

really adapting it to absolutely any kind of business scenario. But um even yesterday at the summit you know I was on a panel where we were discussing port infrastructure and they were talking

about making it completely autonomous ports and making the shipping industry go completely autonomous and uh for me it's a very interesting conversation because you can I mean deploying

autonomous AI tech onto a critical infrastructure is a very very nuanced and complicated conversation which I feel is not happening because uh the the interest is more in let's adapt to it.

So I think from a legal perspective, responsible AI is really building onto the kind of work that you know technologists like you and him have already doing because I think there are

many people who are engineers who are technologists who understand the tech really well and they have you know figured out what should be the responsible AI critical elements at the

time of model deployment or even at the time of building it. So I think there's a lot of discourse that has already happened. There's a lot of research. There is a lot of reliable information

and methodology that one can adapt including across diverse sectors like fintech financial sector for him for healthcare and all the you know the critical sectors wherein people have

concerns about when AI is adapted onto it. I think for a lawyer I for me what I have noticed is that the responsible AI conversation is failing to discuss what is going to happen when these

responsible standards let's say become too automated or there is an automation you know uh lethargy as a result of humans simply depending on the automation process is a bit too much and

forgetting to use you know their own discretion or their own judgment when the tech is being used because when those kind of incidents start happening in India at least We don't really have

any kind of accountability or any kind of remedies to protect users as well as companies. For example, if I am a hospital and I have have an open-source some model that I'm using from some

developer and I'm using it for my billing for my you know prescription for my all my kinds of uh the work that I do in my industry. If something goes wrong in that AI deployment, there are very

limited means that are available both to B2B entities and also to users like us who would be impacted by that adaptability. So I think there needs to be more conversation as well as I think

um legislative amendment. you know we need to have laws we need to have some sort of guidance on what would happen when it comes to accountability especially in all these diffused AI

models where there will be one entity that will come up with a responsible AI standard but it's there's lack of clarity on which all entities that are part of that AI ecosystem are actually

following that standard because it's it's very good for a bank to say that I have a responsible AI standard but all your sub vendors and your agencies do they even know what you're following and

they are the ones who are actually going to deploy your model at a mass scale And if there's an accountability issue then we need to know who is responsible whether you as the entity that's

developing the model or the entity that's deploying the model that as well as enforcement remedies. I think currently under Indian law as I mentioned we don't have any remedies in

the event there is any negligence there's any harm caused as a result of EI deployment and uh in fact yesterday I was speaking to a legal tech uh you know developer and he was asking me that

they're building this tool you know for law firms and lawyers to use and get research answers and things like that and he asked me that you know how do you guys actually advise clients on what to

do when there is a liability concern because he's like because there are no cases in India. I said yeah there are none. So what we do is we read foreign case laws that are similar jurisdiction

like we India is a common law country. So we try to see what's happening and then we develop an argument. And I told him that well the the good luck is that we haven't had those kind of mass

liability and infringement claims happening in India but when it does you will see a lot of people getting affected and there aren't going to be inadequate remedies under law. So I hope

that you know what happens once you deploy the model there is a lot more conversation about that uh because I think there are a lot of smart people already aware of how to set up a model

and launch it. Thank you Apraja. So we have our last panelist he's finally braced the traffic and he's here uh thank you so much Dr. Mang for joining us.

Um yeah so I think uh we can continue the conversation. Um so &gt;&gt; just let me apologize first. It's a really bad traffic but you're right on

time. &gt;&gt; So Dr. I was just asking the other panel members what in their opinion is how would they define responsible AI and

specific to their domain how what are the components or how do they define responsible AI what does it mean for them so we would like to hear something from you

&gt;&gt; so I'm sure some of these things the panelist may have already &gt;&gt; uh Uh I hope I'm I'm audible now. Okay. So so um since everyone would actually start with data mean I'm sure everyone

would have talked about uh start with data start with uh what about responsible models and all. I'll take actually one step before even data right so so so um uh think of the core design

itself like we we all talk about and I've seen truth's post also that it has to be uh uh by design itself and when I say by by design like you have to talk about the problem statement

like think of the responsibility at the problem statement definition level itself like can I solve this particular problem responsibly and that's where actually start with can I collect the

data responsibly? Can I create a model which is responsible model? Can I take the decisions which are responsible decisions? So, so, so if you actually go back to the uh the the the root point

itself saying that can I define the problem statement in a more responsible fashion probably that can then uh uh have a rainfall effect with respect to every component of of AI or machine

learning models. Thank you so much. &gt;&gt; Thank you so much for your perspective and I fully agree with you and I'm sure like everybody in the room agrees to you

um agrees with you about how we need to actually start thinking from the design level and not work in a post talk manner. Um that okay now we've done this now how can we make it better? we need

to start asking the question, how do we make it better before we even start building it? Um, but to your point, I know you've you work with students, you train the next generation

um of minds and people who are working in AI. So to that do you think the current academic system adequately prepares the students to think about responsible AI beyond just model

performance and accuracy and adaptation or thinking of applications and are we training students to actually talk about data governance trustability security safety and just responsibility as a

whole &gt;&gt; uh actually it's a very good question um see uh when when I was in triple A to Delhi first time uh we started a new program on Btech in computer science and

artificial intelligence um jointly with IIT Hyderabad that was the first India's first AI program at undergrad level um at that point of time we did not think about it right but then subsequently

when when I moved to IT JPUR and I created another program there undergrad program in AI and data science we started thinking about that you know the students uh uh should not only know how

to create models or how to evaluate models. Uh they should also understand the basic principles of uh being responsible uh in terms of usage even even application usage uh uh domains. uh

you you pick a particular um theme and you explain that you know again like men our work on face recognition where we start with face recognition being fair or not fair right and and when we talk

about Indian context um it should not happen that someone coming from north India uh versus northeast versus south India should should not get mclassified because of their skin tone or or or the

facial appearance. So these things we start teaching slowly we have started teaching to students and over the years I have seen that almost all the BTE programs that at least I have reviewed.

Um they have a program on a particular course on on dependable AI, trusted AI or responsible AI you name it and and and these courses are there. uh in fact this semester we have started a new

course at IIT JPUR on um MLDDL ops where we are actually teaching them uh at design level at operations level how we can uh do a pre hawk or a post hawk responsible AI. So slowly things are

going in that direction and probably this impact summit is also making that particular statement that um AI should be for the people and therefore uh we have to be more responsible. So

hopefully things will uh be more mature as we go forward. &gt;&gt; That's good to know and very reassuring that we have the next gen of researchers actually thinking and learning about

responsible AI from like the grassroot level. So that's good. But moving on to Ankor um at the same theme do you think fair like how is it in in the industry you're working with Mastercard? Do you

think fairness is ever traded off for model performance or deployment pressures, timelines and &gt;&gt; if tensions arise, how do you tackle

them and how do you tackle conflicts? &gt;&gt; Yeah, sure. Uh so first of all, I would like to enforce the point which Dr. Mayang just made. So when I started my career, so the only thing model

governance team used to do is to do feature importance. Now you have a list of let's say 20 features. you need to go with this light with the feature importance and why do you think is the

rational for the you know for the model to have picked that particular feature. So that's the practical explanation of the features which have been picked by the model based upon uh variable

importance. So that was the only model governance we used to do like 20 years back. Now as people are realizing you know these systems they are intelligent but there can be some inherent bias in

the data or the training method. So people are you know kind of you know uh realizing it that it has to be done at the design level which you highlighted. Uh and I also highlighted you know what

we practice um at Mastercard is security by design. Uh so for your question like you know is it a hindrance at at number of times you know wherein you have to you are under pressure to deliver your

project but at the same time you have been asked to do all these things which may or may not bring that kind of economic impact. Okay. people are just you know uh just worried about uh their

econom the the economic value which is going to come out in terms of impact when you move ahead with the deployment of the AI product. So what we do uh at Mastercard so we have got something

which is called studio process. Now that process has got different gates. So gate one is when you are doing your due diligence. Okay due diligence is what's the pro uh what's the problem objective?

How you are going to solve it? Very basic questions. What are the data elements or the data sources you are going to use? Is there a possibility of having some kind of inherent bias in

that data which can creep into your model and can affect the final result of that? So these are preliminary discussions but are very important because many ideas they got killed at

this particular stage because of you know non-feasibility of that moving into production system because there is some kind of you know uh nonadherence to the protocols or the guidelines. So that

becomes very important and this is uh not a topic which is only discussed at the time of you know uh doing the initial discussion which is gate one we call it. This happens at every stage. So

this is an ongoing uh ongoing I would say change in thought process. Earlier model governance used to happen at the last step like when you are done with doing all the data injection creating

data pipelines training the models doing all the due diligence and then you go to the AI governance team. But now what happens is now data strategy team, AI governance team, they are working with

the data scientist and the product manager and the business people right from the word go. So this is now you know teams are not working in silos anymore. It is uh I can talk about the

mature organizations maybe some other smaller companies uh they might be working uh in a different fashion but they will also you know get to that level once they scale uh because as we

speak today there are many companies uh they still don't have any chief data officer so this becomes a side hustle for someone else some other sea level executive oh he's chief AI officer and

also the data strategy officer but data strategy and data related uh due diligence is as important important as you know any other uh associated uh field I would say as far as the final

deployment or the final uh delivery of the AI systems is concerned but uh particularly to your question it's not a trade-off for us like if I have to meet my timely deliverables then

I need to plan for it in advance and I need to ensure that I have all the governance approvals in place before the launch so it's no compromise &gt;&gt; thank Thank you so much for your

perspective and sharing how you do it at Mastercard Arajata. I'll bring you into the conversation and u we've been talking about how we do it in tech. Dr. Mayang told us how it's done in academic

institutes. An uncle told us how we do it in large organizations. But you interact with a lot more people and you see a lot more cases coming from the background you have in law. Um what

according to you is one thing which um researchers and technologists often miss out on when talking about responsible AI. I think uh uh there are in my view I

think there are two things that um obviously the market and the environment are very different so there are many points of non-commonality but I'd like to just cover two and uh it's it's very

heartening to hear I think the examples that ankur was giving because much of that is what we think should be an ideal uh you know like a multi-disiplinary conversation between lawyers ethicists

technologists as well as governance professionals who are in charge of running affairs of a company. So the number one thing that was missing for quite some time and I think it's being

slowly fixed but we would obviously like to see it happen at a much larger scale is the whole data governance issue because uh for quite some time in India because we didn't have a very robust

data protection law. it was quite common to uh essentially use a lot of public data for AI training and we are seeing some litigation you know taking place in India for example ANI versus open AI

which is based on this exact point but um so obviously AI training companies have relied a lot on public data and that's the way they would like to proceed but there is also a lot of

personal and sensitive data that is used for data training purposes now that we have a more specific law which is the DPP act this law asks you to have consent and I think that's where the

kind of research that you've done shi is very interesting because you're trying to address those kind of legal compliances within the kind of you know the data quality that you're submitting

but in addition to having consent and proportionality and purpose limitation principles at the point of origin and collection this law also requires that after 2 years if I no longer want you to

have my data then you should have those facility in place in place to delete it and uh at least in my experience barring the very top you know AI developers ers there is one reluctance and two

ignorance about the fact that this compliance is a very real compliance in India. So yes, I think the data governance issue needs to be discussed at a more holistic level because uh just

because a technology makes it challenging to erase data after a certain point of time that's not really you know that's not really a response to a very active legal obligation that

exists and uh the second point that I want to cover is really about uh in my experience what we are seeing is that now a lot of CIOS you know chief technology officers they are suddenly

being appointed in CEO positions they are becoming board members there are many small startup up organizations where they don't want a CEO, they're more than happy to have a CTO because if

you have a very technology ready organization, they feel that our product, our company is tech and let this person run the company. I think there are a lot of concerns there

because uh currently the the conversation that doesn't happen you know between lawyers as well as technologists is about governance and accountability. So just building on my

earlier point because today for example if you're a director in an Indian company you don't really know that to what extent should you be aware of the kind of tech that you're using what is

the risk of that technology do you have good downside protection in the event there's a different law that comes in so this kind of conversation is is very missing because nobody wants to think

about a future perspective but I can tell you in the legal fraternity there is a lot of conversation that today if you do an M&amp;A deal which is about a AI company the kind of downside protection

that one should have and the lack of clarity ity on what exactly you need to protect yourself from and how much is uh it it scares a lot of people because it's all I mean a lot of it is just very

predictive and uh non-deterministic kind of conversations. So I think we'd like to see more awareness at a much larger scale about data governance because your AI is really as good as your data. So it

should start at that point and subsequently to that there should be a lot more conversation about the very realtime scenarios in which AI is being deployed. I think on Ankur's point and

uh you know uh Mang's point as well the fact is that you can develop an AI tech but you are putting it in a very real organization where there are speed concerns there are profitability

concerns there are shareholder concerns and how do you really align that because no matter how responsibly perfect your product is when you are you know defeat you're debating between shareholder

interests shareholder interest will always take primacy so then you need to understand how that responsible index is really going to float when time was of essence and economic concerns are of

essence because that is actually how this technology is going to be used and uh you need a more practical mindset which steps away a little bit from the academic point of view.

uh thank you so much for sharing your perspective and I feel that's why it's more important to have more of these interdisciplinary conversations not just in the CEO's office but beyond outside

classrooms discussion rooms coffee chats everywhere so that there is more awareness around the kind of issues that you know come from a more legal perspective and policy

perspective and likewise what are the concerns or technologies which can be understood on the other side. So we need to have this conversation more openly to understand each other's point of view.

But to your point, I would like to ask you, should responsible AI be enforced through regulation or can voluntary frameworks suffice? &gt;&gt; I think uh I think it's a combination.

Certainly there is merit in India being a AI you know an a country that really wants to deploy and there it doesn't make sense to really stop innovation for example while EU has come up with the EU

AI act there is already news coming in that they are thinking of amending the law they have realized that the the legal requirements there are so prescriptive that they would be left

behind in the AI adaptability globally so uh there is a reason why India has not you know followed that approach so I I think uh the honest answer to that is it should be a combination because uh

when it comes to voluntary compliance and voluntary standards we're already seeing so much good work from the private sector as well as the public sector content governance model

explanability norms what kind of training boards should have what should be the AI governance standards for you know people who are in positions of management there is a lot of uh

practices that are already being you know adapted in the market and I think one should let uh the industry do that especially when it comes to I think uh very uh rapidly adapting technology like

genai because the technology itself is so accelerated in terms of its possibilities that the law can never really catch up you know regardless of what kind of guidelines you come up with

so I would say that it's important to have voluntary compliance and allow the private sector to come up with their own standards in areas that are more technically focused and you know audit

and red teaming and these kind of criteria but when it comes to high-risk AI use which is healthcare, financial sector, insurance sector, you know, critical information system. It is

important to come up with the law that provides one some sort of accountability, you know, which is building on my previous point. when you are adapting an AI system onto a

particular sector who is exactly responsible for what and then we also need to update our laws so that users like us who will be let's say experiencing this technology without

knowing what is the data that's fed in without knowing having much awareness of the technology they need to have proper legal remedies to provide them protection so I think a combination of

that would be good to see in India because that will provide a more protective environment compared to what it is right now which is very guidance and advisory

based. &gt;&gt; Thank you. Um, so Arajata mentioned Genai and I think it's everywhere nowadays. We have it in our phones and we are pretty much dependent on it. We

use it like our personal assistants. Um, but coming to Dr. Mayen, I would like to hear your thoughts on given that foundation models are trained on massive and often very opaque data sets. Can we

meaningfully talk about responsible AI if training data largely remains undisclosed? &gt;&gt; Um yes and no. Um and then I'll explain the reason. In fact uh you talked about

DBDP act and the right to be erasure. uh see if if a model is trained on a very large corpus of data and we expect um someone actually tomorrow coming and saying hey you know my data is being

used uh let's say let's say you have trained on on on traffic data like uh millions and millions of samples from traffic uh signal that the cameras that that are there and I know I have crossed

this uh let's say uh crossing five times uh almost every day so so my data is there now under DPDP act I can go and Hey, if you are able to recognize me, if you're using face recognition to

recognize me, remove my data. So, it's not only the removal of the data, it's the removal of that information from the model that has been trained on that particular data. Right?

So if if I if I truly talk about responsible AI and and implementing DBTP right to erasure this means that not only removing from my databases like from the image sample sets I have to

also go to the model space and implement unlearning or forgetting uh aspects over there. So truly speaking if I if I have to implement DPDP act in true sense I need to have the mechanism to unlearn

these generative AI models right now. Now now If you if you if you think of uh uh how truly we can be in terms of being responsible starting from data to model um

it's a long way. We are not there yet. uh and and and and the very first thing that we should think of is um ensuring that the models that are being trained in fact a lot of models have been

inaugurated in the last two days over here right so so u um but I I did not see any model card or data card right no one actually said that here is my model here is the model card here are the data

cards like on which data that we have trained what is the property of the data at least release that so So, so, so at some point of time if we have to start thinking in in a true sense implementing

being responsible we have to start think like we have to start with the process of declaring that what kind of databases you have used here is a model card and and you have to standardize these model

cards also and and that would actually help in the right directions towards uh being responsible in in the development phases be generative AI be discriminative AI or be even agentic AI

right I mean that's another Pandora box when we talk about agentic AI um and and what do we mean by responsible agentic AI that's like another probably panel alto together

yeah &gt;&gt; yeah thank you um and I agree with you I think model unlearning is pretty critical but it all begins with knowing a little more and standardizing

processes and data cards and model cards even though they do exist I don't think a lot of people in the community use it very actively. So, um, bringing the conversation back

to Ankor, Ankor, you talked about processes at Mastercard and having conversations early on. So, you're familiar with how you know you've you've done it at Mastercard where you have uh

processes to enable more responsible AI. From your experience and perspective, how do you think India should position itself in shaping global responsible AI standards rather than just simply

adopting frameworks which somebody else comes up with? &gt;&gt; Yeah, sure. Uh so I think as far as uh regulations are concerned, so I believe that regulations should not throttle

innovation. So there has to be a sweet point wherein you know uh you are allowing the companies to do innovation but within you know with some kind of guardrails around it. For example like

Aprajetta talked about you know she is still waiting for some data related cases to be filed in India so that you know she can uh also build a business around it. But the thing is uh our

regulators they also need to be proactive and at least have some umbrella guidelines to be followed for AI responsible AI generic applications and then things can change. For example,

agentic we are not ready for it. Uh yesterday only we uh demonstrated our capabilities to do agentic AI uh transactions. So the first question which most of the uh visitors were

asking what will happen in case you know I raise a dispute who will handle it? Let's say in case uh there has been a faulty purchase which has been made or the item was found to be detective

defective at a later point of time who's responsible for that my agent merchants agent mastercards agent. So these all things so these the like the rules of the games are yet to be defined and yet

to be followed. Uh at the at this point of time most of the tech players they are kind of you know readying the ground for the launch but it is regulator's job to define uh like uniform rules which

are applicable to all the participants in the ecosystem and to ensure the trust which we have built over the years and decades. It stills kind of you know uh follows the legacy and people feel

equally comfortable while interacting with agent even uh you know if even if they are not aware of you know these agents working behind the scenes but they should be feeling they should feel

comfortable about it. India specific things we have a super DPI digital uh infrastructure but the thing is there are no guidelines on how to use it and most of the things u like the

customers the consumers like who uh who are ready to share that data without knowing the consequences they also need need to be sensitized about you know sharing their details so in India I have

seen many people sharing their information over social media like that because they are not you know aware of the consequences what kind of issues they can get into if you know they don't

follow the data hygiene um uh properly over especially over social media. So this is one thing a we need to educate and sensitize the consumer side you know when they they they provide their

consent to uh their data to be shared and used in downstream processes but at the same time for the companies who are leveraging those data to come up with some kind of intelligence to learn

certain uh aspects of behavior and monetize those there has to be some defined you know uh framework for that as well &gt;&gt; yeah and I agree but maybe a praada

doesn't I think there's always a cat and mouse game we develop, we do research and regulators are figuring out how to catch up and then they come up with something and then there's a push back.

So I think there's always that back and forth which is really essential to actually uh research innovation as well as doing it responsibly. So before we close, I would like to hear from all of

you if there's one major structural change, not just incremental but structural change that must happen for responsible AI to move from principle to practice. What would that be?

&gt;&gt; Um so we have already started that uh process. we haven't started the actual implementation is introducing a course on artificial intelligence at class 11th and 12th level. I'm right now writing

the textbook. I'm leading that particular effort for NCRT. And our hope is that uh uh if the textbooks from class 11th and 12th starts teaching kids what AI can do, what AI can't do and

what is the meaning of ethics in artificial intelligence probably we'll have right set of people to ask questions to develop right set of tools and technologies and use it more

responsibly. So so so so the basic at least coming from academics this is what we can do teach from the basics itself from from and and and this course is going to be available for every student

who can with be from art side or or humanities or commerce or from science. So any student and every student would take this about our hope is that about 2 cr students every year would actually be

taking this particular course and hopefully this will just elevate and have more such dialogues and discussions. I'm really glad to see that dialogues have started happening about

being responsible and all probably with this one step uh that we are right now taking uh that would actually increase uh the the the fast forward in that direction.

&gt;&gt; That's really amazing to hear Ankur. &gt;&gt; Yeah. So how about changing our job titles like from chief AI officer to chief responsible AI officer, AI scientist to responsible AI scientist.

But jokes apart uh so a uh this uh implementing responsible AI it's a continuous process okay it need not be like you know one touch point at the end of the model development cycle or the

deployment cycle it is uh it is a continuous cycle and it has to be made an integral part of the overall product development that's one so I'll go back to my uh definition which I gave at the

start so responsible AI is nothing but scaled intelligence which has got fairness transparency Y and trust &gt;&gt; Arajata. &gt;&gt; I think uh for me the main thing would

be that some of the principles seeing them applied in practice and consequences if they're not followed. So certainly we look forward to having at least a definition of entities that are

involved in an AI system so that there's more clarity and then understanding that whoever is implementing responsible AI who has who has to be hauled up if something goes wrong. I think you know

we look forward to at least some clarity in the law so that it gives I think comfort to you know to students to people in the sector that there are some remedies in the event of mishap and and

harm. &gt;&gt; Thank you so much for such an engaging discussion and sharing your thoughts. Um I think it is clear um Dr. again highlighted it that responsibility

cannot like responsibly cannot be retrofitted. It must be embedded in design. Rajata told us that accountability must be clear. So that's one key aspect which has come up

multiple times and it's really essential and like Ankor mentioned responsibility must be shared. So it's no one person's job. It has to be throughout the pipeline at different stages. Thank you

so much and um I would like to thank you all for joining us. We have Sony Research India's director and head here and he would give you all a token of thanks from Sony's side

and please wait for a group photo as well. [applause] Nice. Thank you.

&gt;&gt; So I would like to thank all the attendees and speaker of this session. I told
