# Closing the Loop: Building Global AI Incident Monitoring and Response Capabilities at the India AI Impact Summit

**India AI Impact Summit 2026 ‚Äî Day 3 (2026-02-18)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 10:30 ‚Äì 11:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 16 |
| üìÖ **Date** | 2026-02-18 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/SZ3NIjgzJbo?feature=share) |

## üé§ Speakers

- Akiko Murakami, Japanese AI Safety Institute
- Caio Vieira Machado, Berkman Klein Center at Harvard University
- Elham Tabassi, Senior Fellow at Brookings Institution; Former Chief AI Advisor at US NIST
- Hugo Valadares, Department of Science, Technology, and Digital Innovation (DECTI) of the Brazilian Ministry of Science, Technology and Innovation
- Marko Grobelnik, Digital Champion of Slovenia at the European Commission
- Niki Iliadis, Global AI Governance at The Future Society

## ü§ù Knowledge Partners

- The Future Society Inc

## üìù Summary

AI incidents are emerging as early warning signals of systemic, cross-border risk - but the global infrastructure to 
capture and learn from them remains fragmented. This session brings together policymakers, and 
technical experts to explore how interoperable AI incident reporting and monitoring can strengthen prevention, early 
warning, and coordinated response. Participants will discuss institutional designs ‚Äîfrom shared taxonomies to secure 
data-sharing‚Äîthat can turn incident data into actionable governance, standards, and risk-management insights.

## üîë Key Takeaways

1. AI incidents are emerging as early warning signals of systemic, cross-border risk - but the global infrastructure to 
capture and learn from them remains fragmented.
2. This session brings together policymakers, and 
technical experts to explore how interoperable AI incident reporting and monitoring can strengthen prevention, early 
warning, and coordinated response.
3. Participants will discuss institutional designs ‚Äîfrom shared taxonomies to secure 
data-sharing‚Äîthat can turn incident data into actionable governance, standards, and risk-management insights.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/SZ3NIjgzJbo/maxresdefault.jpg)](https://youtube.com/live/SZ3NIjgzJbo?feature=share)

---

_[‚Üê Back to Day 3 Sessions](../README.md)_


## üìù Transcript

Hi, good morning everyone. Welcome to our session, our main summit session on AI incidents organized by the future society, my wonderful colleagues at the future society with contributions from

the OECD AI expert group on AI incidents. I'm going to just take a few moments to set the scene and then we'll get started with a high level panel. So I'll ask the panelists to join us in

just a minute on stage. But um I wanted to start us off by acknowledging I think yesterday I was actually part of one of these conversations and I believe many of you in this room may have been part

of similar conversations either bilateral ones or small group discussions where there seems to be one concept that always enters the table but I've never heard it on stage. So I

wanted to be the one that threw it on there. And that's the concept that some people are saying. It's a hypothesis, a theory there really that if if an AI incident were going to happen, if an AI

incident that's either severe enough or serious enough or big enough or catastrophic enough, you can choose the adjective of your liking. But if there was some AI incident that were going to

happen, that would serve as a wake-up call for governments really for decision makers to to take AI risk seriously and to start doing something about it. And uh we we often in the AI space love

analogies, right? So we have an analogy obviously with the not obviously but we have an a a an analogy with the aviation space and a lot of people talk about how aviation is a highstakes industry and

there are very clear safety requirements as part of that industry. Why can't we do something similar in AI? And others will say yes, but it took Boeing many times to fail for that to actually

happen. And you could say the same thing for social media, right? It wasn't until Cambridge Analytica happened that a lot of people took a lot of the same issues, same concerns that we still have around

social media seriously. So, bringing it back to AI, I was thinking, is is that what we need? Is it an AI incident? But then I started looking at the numbers and the evidence and um well I don't

want to be the one that breaks it to you guys but AI incidents are happening right there's a lot of facts around that we see AI incidents they're increasing in scale they're increasing in severity

they're increasing in frequency right there's in just in the last couple of years we've seen AI enabled cyber attacks on critical infrastructure we've seen manipulative systems really harming

individuals especially vulnerable groups of our societ society like children and there's even been claims from some right that some models are portraying are uh demonstrating deceptive behavior

refusing to shut down. So AI incidents as I said before are happening. What I don't think is what we need is another AI incident. What we need is to build crossborder infrastructure to actually

prevent this AI incidents to be able to classify them and really analyze alarming patterns to be able to share incidents when it makes sure it makes sense to do so across jurisdictions

and to be able to learn from those AI incidents. So uh before I wrap up, we actually about two months ago at the future society held a presummit uh event called the Athens roundt and as part of

this event one of the conversations that we had there was around serious incident prevention and preparedness and so I wanted to just kick us off with some of the takeaways of at least how I

understood the takeaways from that dialogue and what were some of the common themes. The first one is around inadequate detection. So uh a lot of people were mentioning that right now

our incident monitoring platforms that we do have rely on capturing incidents actually using AI. So we scrape mostly media stories to understand what has gone wrong and uh that's that's you know

it's not a perfect solution. It's a solution towards or at least there's something but I do think we need more systematic approaches to get there. There are many jurisdictions that are

starting to create reporting requirements. There's notably the EUAI act uh in California where I'm based there's the SB43 but those are still at very early stages and it's to be seen

what happens when it comes to enforcement right and from a crossber perspective we're also thinking there's so many different definitions of AI incidents within those again serious

incidents critically safe incidents you call it what it is that it's there's an issue definitely arising around interpability then the second topic is really one that relates to

accountability. When we think about AI incidents, one one would question, okay, who is responsible for this a and because the value chain is so complex, it's so global, it's very hard to assign

responsibility. At least we haven't figured out how to build that liability regime yet. And I I should probably mention here a lot of times when we talk, I probably just made that mistake.

We say AI is posing the risks, right? And I think it's an important reminder to keep in mind that it's individuals behind this AI systems that are developing and deploying the actual

technology. And it's individuals that are enabling I guess the system to happen without having adequate checks and balances or without creating new incentive structures. And the third

point uh after accountability that comes up is around preparedness. I think that's a buzzword that we keep on hearing over and over at least in many conversations that I've been part of in

the past year uh when we're thinking about response capacity specifically to to incidents and I think this is a space that we just need to do a lot more um have a lot more coordination and

progress on. So obviously if you add to all these elements the cross border border layer knowing that AI is really uh realized as I said before in a global supply chain and everything is so

interconnected it's very easy to see how much we need crossber incident infrastructure. So I'm going to pose some questions and then invite the panelist and our moderator my colleague

Kyio to uh to join us on stage and I'll pose the questions and hopefully the panelists can give some answers to those questions. But the first one is around how how as an international community

can we come together can we better organize can we better act to actually have AI incidents serve as that wakeup call for policy makers to have stronger governance mechanisms and then related

to that what are the actual technical and policy components needed for this incident infrastructure is that there's already some shared reporting taxonomies how do we scale those tonomies how do we

build information sharing protocols before be uh between jurisdictions how do we make sure there's robust data governance structures in place and the other part of the question what are the

right institutions or what are the appropriate actors I should say that should take a role in this the last question I want to leave the with is how do you link AI incidents to red lines

there's a lot of work that the future society does on that and we often think how do incidents serve as those early warning signals that tell us when we're reaching a certain risk threshold at

least or a dangerous territory. So how do we make that connection and feedback loop? So with that I will invite uh our moderator for the day Kyle Machado to the floor. He will be uh he will be

moderating uh what we have in panel really full of AI experts when we come to AI incidents. So please let's give a round of applause and let's get started. [applause]

Good morning. &gt;&gt; Hello. Hello. &gt;&gt; Good morning everyone. So I'll invite our speakers to come up. I'll introduce you in a second. I've also been asked by

the organizers first to start off with a picture. So instead of a picture at the end already started with one. Nikki will need we'll need you for that. Uh oh there's our photographer.

&gt;&gt; So should we sit? Should we stand? Yeah. So, a quick a quick pause for &gt;&gt; We've got the We've got the picture. We can go now. Um I'll introduce our speakers today. So I'll be very brief

and I'll [snorts] uh if there are any points from your experiences that you would like to bring up during your speech, please do so. Um so I'll start with Alhim Tabasi who's

director at of AI and emerging technology initi initiative at and senior fellow at the Brookings Institution and former chief AI adviser at US NIST followed by Miss Ako Murakami

who's the executive director director of the Japan AI safety institute followed by my um fellow Brazilian Ugo Valadaris who is from the ministry of science and

technology. He is the director of the department of science and technology and digital innovation. And finally, Michael Grabelnik who's the AI champion for Slovenia and an AI researcher who

co-chairs the AI incident working group uh at the OECD. So, good morning to you. It's very nice to have you. Thank you once again uh coming all the way to India for us to discuss. Um I'll start

with Marco at the at the other end. Um guard. Absolutely. &gt;&gt; Let's go. &gt;&gt; Um &gt;&gt; so you have been working a lot uh in

with the OECD incident group and really advancing u the discussions around definitions and and working to establish um this cooperation across mult multiple nationalities. And I'm interested from

your global vantage point, what are the gaps we have in developing incident monitoring right now? &gt;&gt; Well, obviously uh uh obviously the major gap is uh sensing the the

incidents, right? So at the moment we have just mainstream news more or less, right? Not even social media because social media got locked, right, a few years ago. Uh so we

we have mainstream media. which means that we detect pretty much everything what kind of spills over which which which is significant enough so that it's news worthy as journalists would say

right uh which mostly it's okay right mostly it's okay uh but um it's not systematic right um so this is one um major gap right on the sensing part now there are gaps along the the whole

pipeline right I would uh say so at the moment we collect this data we organize it we uh structure it we classify it so we have all these nice OECD schemas um and you can search

across this and so on we can count this so we get statistics and so on all fine but you know uh uh what Nikki was mentioning before right the feedback loop right is is missing right so the

feedback loop AC uh back to the policy makers at the moment policy makers can okay see the incidents but then use pretty much imagination uh what to do right uh so one extension what we are

working on at OCD right now is uh how to provide a little bit more proactive um um recommendations mitigations um uh policies ahead of time right so a little bit of this landscape around

particular incidents so that policy makers would at least uh uh get ideas right maybe slightly out of box ideas uh how what to do how to do and so on we know policy makers are slow

okay fine uh that's why let's say we would speed up so this I would say two major gaps right so sensing now we have mainstream media social media I'm not sure if would would be

useful anyway Uh it might be that in the future we'll get this obligatory reporting from the companies. Not that companies would be extremely incentivized to to report their

failures, right? Uh uh but okay, this may happen. Um and the second gap is uh closing the loop, right? As the title of the panel.

&gt;&gt; All right, weaving in the title of the panel. Thank you. Thank you. Um and just a quick followup, have you seen while working with uh well I understand the incident monitoring has global coverage

and we're seeing stuff all over the world. Uh this is a great point that we're tied to that lens of what's media reportable, what's interesting and so on. But are there any particular

capabilities or type of incidents that are showing up uh that are drawing more attention? Yeah. So [clears throat] uh well this would be maybe five 10 minutes lecture right so that I

would show these graphs but somehow we we cluster the incidents into I think 14 groups or something u and some some types of incidents go up some go down and some are more like this incidental

like I don't know uh incidental ones would be elections right so just before elections you would get spikes right so and then goes down and and then you get spiked later on as well, right? Then um

let's say an interesting one is autonomous cars accidents, right? So this used to be extremely well reported a while ago, right? Maybe 10 years ago, you know, it was one accident, right?

And everybody was writing about this one accident. Now nobody cares anymore because there are just too many uh accidents and cars and people got used to it, right? Um so this went down,

right? Uh and then there are a couple of types of accident incidents which kind of go up uh especially yeah these fakes now if you if you look how AI evolved in the last maybe just six months you don't

need to go much further right three six months uh we got couple of crazy good tools like nano banana and a couple of others right suddenly you can make uh you can make deep fakes for talking here

on the mic right I could produce extremely well um extremely good deep fakes for instance, right? The cost is close to zero, right? U so these are these would be like three types, I would

need to show the u the graph maybe just the last sentence, right? Uh since we have this OECD taxonomy of incidents u and hazards and hazards, right? So

there's one type of incidents which uh didn't happen yet right l thanks god right catastrophic incidents right we we left the space for this possible catastrophic ones right so seems like AI

doomers are begging for one right because every day you hear oh well it will happen it will happen didn't happen yet right didn't happen yet but uh it may it may you know some people will be

happy uh because they invested so much energy in scaring people It's obviously, right? You know the cartoon Monster Inc., right? We scare because we care. So, this is the the AI

doomers roughly are of that kind, right? I hope I didn't insult anybody here. &gt;&gt; It's that pleasure of saying I told you so. Right. [laughter] I told you so. I didn't want it to

happen, but I told you so. But I I'll I'll take a point you you mentioned which is um how in certain incidents in the beginning it happened once and it generates outcry and then

people get used to it. So there is a diff difference between the public perception of a risk and a harm and eventually bringing a taxonomy and working with that. So I'm using this as

a hook to to come over to Elam and oh Elam you have as we mentioned experience as the chief AI adviser at MIT which I understand uh had a lot of work on risk management framework and I'm interested

in hearing uh what are the limitations and the challenges of the framework especially when you have real life incidents and crossber incidents um have you been faced with situations where the

taxonomies are challenged. Um, yeah, if you could elaborate a little bit. &gt;&gt; Um, first of all, thank you very much for having me here. It's great to be in this panel. Uh, Marco, I hope that

scenario never happens because we are all working towards making sure that we have a sort of responsible trustworthy AI. I just want to start by saying that uh we should take it seriously that AI

doesn't happen to us and it's all on all of us and all of the efforts that we do to make sure that it it progress in a safe way. Uh going back to your question and u I don't know your question was

around a mf or just current governance and risk management practices but I'll try to address both of them. I think um uh the there first of all I I should say that the technology is moving much

faster than any of these frameworks and policy and standards can keep up with them. uh but uh some of these mismatches that I see is uh on the two dimension of temporal and jurisdictional that the

jurisdictional one has been already um mentioned. So uh what they mean by the temporal is that majority of the uh risk management processes governance processes that we have uh they uh they

are heavily on uh study for the pre-eployment which is also necessary but they assume that the risk and incident can be identified pre-eployment and can be managed uh within certain uh

um national organizational uh border and jurisdictions. Um I want to say that IRMF is maybe a little bit exception to that that it it emphasizes post

deployment monitoring continual monitoring. Uh but the problem is that uh uh we don't quite know how to do the evaluations in the right way. But going back to again incident reporting is that

uh the problem we uh emphasis emphasize in the uh pre-eployment is that the majority of the incident or serious incidents that we have to be worry about and uh report them and have a way of uh

dealing with them uh are those that happening post deployment and there are crossber there are emergent uh and many of these pre-eployment testing that are based on red teaming benchmarking um uh

thread modeling analysis are important and necessary But they cannot reliably predict how the models are going to behave and function in real messy u uh context of the use

that they haven't been really seen uh before uh being deployed there. Uh where the um uh it's open-ended use um uh you have cascaded interactions and many other things that cannot be uh really

thought of during the uh pre-eployment. Um uh and uh compounding all of these challenges is that uh as we all uh reading, hearing and learning that uh particularly agents uh they can

differentiate between the operational and the uh development or testing environment uh so they can behave completely differently in this environment which is quite a challenge

on how to design a test to for the behaviors and for the functions that that that we don't know. Um so um uh to kind of summarize that the real incidents that we want to follow and and

um not follow but kind of we watch for them um they they they are emerging you know their failures emerges when the AI is in uh use at scale you know a lot of the misuses we only see them when they

are at scale uh and um and also some non-AI aspects of this you know system integrations cascading interactions that you don't see them and you cannot really

design for scenario testing before that. Uh on top of that the uh the issue of the discovery has been mentioned the we uh we don't have a systemic way of monitoring the systems to figure that

out. We are relying on the external uh researchers and media to report that. So there's a lot of blind spot there too. uh then there is this um uh cross jurisdiction uh problem that AI systems

can be trained in one jurisdiction, hosted uh used uh deployed uh in other jurisdictions and uh uh so the visibility to what happens across the different stages of the AI life cycle uh

may not be readily available for for anybody to look at that. So it causes uh it causes some challenges there. The issue of the uh you know when incident Then the um um structural weaknesses

start showing up like as it was mentioned not having a shared uh taxonomy and definitions for the reporting not having a um uniform threshold for where an incident uh

should be reported across the different jurisdictions. Um so I think all of them uh uh makes the current structure having blind spots that um we should be worry about uh because we don't want the

serious incidents go undetected. Um so I just end by saying that um assurance centric uh governance risk managements are really important and we should pursue that but maybe and well not maybe

as as we talked about it in AI risk management uh we should also augment that with post-eployment monitoring with um maybe resilience uh centric governance after the post- deployment

for all this. Yeah, that's very interesting. And this dual duality between what we can assume pre-eployment and what we can do post deployment. I'm thinking here even post uh

decommissioning right AI systems in the world they operate then a company stops stops existing you turn it off um but the model might be out there variations of the model. So you you no I just want

to say that what does the decommissioning mean? How do we know that we have actually took everything off the system? So &gt;&gt; amazing. Um Kiko like to move on to you.

Um you're as the head of the Jet. &gt;&gt; Can you Yeah. Sorry. I tend to project my voice so I forget if the microphone is on or off. But you can all hear me at the back. All good. Great.

So, Ako, as the head of the Japan AIC Institute and now operating in a global network of where you're you're getting colleagues from all over the world. Um, but it seems to me that each safety

institute has a variation of mandates of governance designs. Some try to be independent, other within a ministry of science and technology, others are multi-ministerial.

Um I'm curious to your views as the AI Safety Institute's capability of uh being the place to monitor incidents and promote preparedness at a global scale. &gt;&gt; Thank you for the question and thank you

for having me today. So my opinion is that the as you mentioned that the uh the definition of the AI safety institute in the each country are the totally different. So for instance the

in Japan the uh we are being the hub of the information about the AI safety inside and outside Japan and also that we are now expanding our mission to not only for the information hub but also

the having the capability to examine the AI models and systems by ourselves. But they uh the the some of the AI institute have the same kind of the capability but the uh the but the the another type of

the institute do not have the capability. So it depend on the the comp country situation and also the the each company has the uh the the not all of the AI safety institute or the the

relating to the institute having the the skill set of that examination of the the models or the monitoring the system and but the our the network's mission is that they not only for the not the for

the the doing the monitoring by ourself but also the discussing what's the kind of the benchmark or the what kind of the line for the AI safety in the world. So because of the um the talking about the

air safety is sometimes very complicated uh the apart from the national security so that's why that we are focusing on the technical issues and uh the what kind of the the and do not want to uh

talking about the ethic or human rights by ourselves. So that's the so many uh the organization talking about the AI ethics or the human rights like the OECD or the GPI are doing that kind of the

discussion and the the thanks for the such kind of the discussion the AI safety institute network focusing on the technical issues how to the operate operate the such kind of the uh

benchmark the realizing they we can uh the understanding the this uh AI is a safety or not. So that's the uh most important part of the mission of the AI safety institute network. I do believe

so. &gt;&gt; Amazing. Thank you very much. Um I'm going to move on to Google and then we'll do a second round and uh the second round if you guys want to comment

on the questions as well. um Hugo uh from the perspective of a country that's building its governance framework um and will have to interact with the space which Kiko was mentioning uh and each

country has its own design its own governance framework. How do you see first of all nationally uh what how will Brazil may be prepared because we don't know if the law is going to be approved

uh to respond to incidents and how will Brazil be prepared to dialogue with other countries in responding to AI incidents. So first of all, thank [clears throat]

you very much Kaio, my friend, Brazilian. [laughter] It's very nice to be here and uh to discuss this very important subject. Um first of all, I I I believe it's

important to mention uh how Brazil is dealing with the subject of AI. uh since 20 and 24 we launched uh the Brazilian plan of artificial intelligence and this plan uh present five axis and in this

axis we have 54 actions uh to discuss all the dimensions related to AI of course every day you have to put another action because uh it's impossible to to forecast what is going to happen

tomorrow. And uh but now uh I'm sure that uh first of all we were thinking about uh three great uh subjects uh human resources uh sovereignty of the data and hardware

supercomputing. But now we are very sure that we have uh the fourth uh uh uh dimension I mean cyber security because it's not possible to do anything without security in this

sense uh we are working uh in all that dimensions together because you have to do everything simultaneously and you know this year is national elections president Lula President Lulu here the

day after tomorrow and uh he's very concerned about that because the fake news and the the generative AI I like very much generative AI but it's dangerous sometimes so we also concerned

and uh but we are doing a lot of stuff uh and of course it's not regarding uh the fake news and something like that uh some some months ago we have a fin an unpreced attack in our uh system. Uh Dr.

Ka knows PS and probably you know what I mean. Uh three attacks and we have a very great produce and uh that time uh we are uh in investing actually in many of uh the the dimensions I I told to

you. So first of all um we are buying a system actually uh uh 100 million he eyes to investigate for our researchers uh about uh we need to take care our childhood

that our children uh the minors especially in the internet environment. So we are investing a lot of money to re research about that. We also uh uh joined with the ministry actually is the

cabinet of institutional security. Uh we are creating uh sensors to detect the attack previously and this is a very big um uh project we are doing with the colleagues with some colleagues for

Spain and I mean usually the attacks we we not prevent but we act after they happen. So we are trying to create a system to uh solve this kind of problem before and

uh but I mean the main the main the main subject here in our opinion is international cooperation. We have to do everything together. It's not possible to a country to do everything by

themselves by him itself. So what's the main gap in my opinion not in my opinion Brazilian opinion the government opinion the political process the bureaucracy is very slow compared

uh to the technology speed so we have to do a lot of things yesterday for tomorrow and sometimes we spend years withus and dialogues but that's the word and we have to do something. So this

very moment we are working to uh expand our cooperations with the countries. Brazil has has a lot of friends of country friends in the world. So we are we are working on this kind of uh stuff

and we are very happy. I believe we are going to to find a very good uh solutions to to mitigate this kind of uh attack and this is the the current word we have to work on that

&gt;&gt; amazing thank you Google thank you very much and I I'll I'll move to Elim and exactly on this point of international cooperation that Ugo was mentioning and which is so critical

oh just as a parenthesis mentioned pix which is for context it's a Brazilian payment system uh developed by the central bank public and just today bricks announced uh bricks payment

system so Brazil, India, China, South Africa um and other countries that join later. So this is getting pretty big and having such a payment system involving all blocks uh being attacked with the

use of AI is a great risk to many nations and is again the issue of international cooperation. So, Elhub uh when incidents do occur and I hope it won't happen to the payment system

or at least when I have debt maybe then it could happen. What should a well functioning u AI incident reporting pipeline look like? Uh what are critical or at least minimal

elements that we should have in all legis jurisdictions that Brazil could incorporate for example? That's a very good question. So, uh when I think about this and u you know

learning from our panelists and the point that they made uh I think uh three three things comes to my mind. Uh one is that uh a a monitoring system and incident reporting needs to uh generate

uh outputs and results and technical facts uh that are relevant for uh decision making and decision makers. Uh so having that uh that lens on this. Then the second thing that it needs to

be um I think um lightweight enough that uh it can be adopted and used or not heavyweight that prevents uh using this. Uh and then the last thing is that some sort of a standardized uh reporting uh

that uh and if you do that right um it is not um kind of tied to one jurisdiction. So it's flexible as other jurisdictions as the different rules and regulations uh the reporting can be

applied and be helpful for for those uh other regulations. So what we need to get there is uh it was mentioned Marco mentioned that we need some sort of a you know beyond sh understanding but

more sort of a standardized u definitions and taxonomy for uh what is incident and that's what the work that OECD did uh with uh the definitions of incidents uh and accidents and all that

and uh uh I was privileged to be part of that work and uh uh there are other work too that you know for example US AI uh uh incident databases or MIT has a series of work. So how to bring all of

them together in a more standardized way. Um the second thing is that make sure that our approaches I'm a big fan of risk based outcome based approaches that prevents us to from getting uh too

prescriptive. Um uh so an outcome based approach that uh uh uh triggering for when an incident should be reported is based on the severity of harm uh that uh uh uh that

can happen can be real realized if an incident happens or is very likely to realized versus the system categories uh or model capabilities because those definitions can change over time and can

change over jurisdictions. Um uh it has been mentioned and I mentioned that too. I I'll say it again that we need have to have some sort of a standardized reporting uh that's

flexible enough for different jurisdictions but beyond that also allows for mechanisms for sharing those information. Not all of the informations about the incidents uh can be in public.

So then we need to think about uh ways that the nations can share those informations and also think about some sort of a perhaps uh uh stage timing that when incident is reported something

happens quickly um but make sure that we follow it up with enough technical information to understand that to make sure that it doesn't happen again. Uh and the last thing I will add is uh uh

make sure that there is in there is enough incentives for the people and the uh entities to actually actually do this uh you know uh there's usually they compare AI incidents with cyber security

incident reporting that is you know that has have a lot of benefits but I think there's also some some sort of a subtle differences that when you when there is a cyber security incident reporting it's

often at the same time or very quickly followed by a patch so so they can take the credit for solving it with AI is a lot more difficult you just report that something went gone uh gone wrong and

there's a lot of you know reputations and other things to worry about. So how to find the right incentives uh so that they uh report that and don't cut corners in reporting.

&gt;&gt; Thank you very much. Um Marco, I want to loop you in in in this conversation as well because I think you you'll have thoughts on what sort of information we should be sharing when it comes to cross

jurisdictional coordination. Um What would be the bare minimum again that we we'd need to think of thresholds to respond uh disclosure sorry threshold to trigger international cooperation uh

what should companies disclosed to what parties do you have any take on that? Yeah, it's a complicated question, right? Because uh it's a lot of consensus which uh needs to be or many

parties which need to be aligned uh if you go across jurisdiction, cross country, cross uh culture if you want um even cross technology, cross business. Right? So there many crosses here right

now. Um there several sub questions which you asked u well what would be useful uh in terms of level of details right level of details would be so we would need to know why the accident

happens so at the moment we we don't have causality any notion of causality in this incident uh detection analysis reporting and then feedback loop right so the concept of

causality that it needs to be uh brought in and then this would allow actually the the policy feedback loop. This is where what I mentioned at the end of my first intervention, right? So causality,

right? Then uh next thing which is extremely important. So AI as we knew like two years ago or one year ago or today or the AI which will experience in one year from now is a different AIS

right similar but fairly different uh these days I don't know we have this getting we are getting there to to have this complex agentic structures right this is so much different compared to

you know some small poor LLM where you drop in little bit of text and you get in response something and maybe ah it was insulting but then people were saying oh this is really ugly and so on

now we have no clue anymore where where things happen right where things happen why they happen and so on so it's getting way more complicated what Adam was mentioning the real time aspect is

also right so mitigation [clears throat] too late is better not even to mitigate right almost uh So uh reaction reaction time right uh this is an important one. So sharing um yeah so there's one

important element right so these AI products generally they are coming from companies right these companies usually would like to keep the secret right business secret and so on now sharing

all the details they don't they're not really in incentivized to explain this happened because we using that component and this component well it was so much cheaper so that's why we introduced it

but it's a little bit buggy Okay, some troubles came out but generally it runs fine. That this is very usual response. Uh how to convince them? Yeah, maybe by policy makers and so on but unclear. Uh

certainly incentives are not there at the moment. uh uh and just uh to conclude right so cross culture cross country cross jurisdictional um there's no AI which would run hardly any AI

which runs in a single country okay maybe US China a little bit different but generally AI runs all over right so and why because it's well everybody's developing this either

agents or something else or uh because clouds are everywhere tons of things right um how to share unclear I mean of course it's very easy to talk about this but in a practical

implementation setting I'm not sure if this will uh be implemented in an easy way in the foreseeable future likely not maybe we will get some samples but systematically you No, likely not.

Currently at this OCD AI uh monitor, right? AIM as we call it. Uh you can go to OCD.AI/inccidents, right? So this is the portal. Um so per day we have few tens of few tens of

incidents right of different kinds. Um and uh across all the languages, cultures and so on. of course mostly on the western sides of the world. Uh but still have about eight minutes. So finer

questions are to Aiko and and then Google. I'll ask you to keep about 44 minutes. uh as long as we're talk we're discussing this AI incident reporting

life cycle right what we need to report what are the challenges I know Japan released um an incident response playbook recently I couldn't read the kaji but I was very interested in so

maybe you could share a little bit about that with us and also discuss a little bit of how your experience with collaborating with the companies in private and public partnerships uh have

worked to build trust and share this responsibility. &gt;&gt; Thank you. Thank you so much. So the the instant is the as you mentioned that the technology is changing very rapidly. So

that's why the if we release the the concrete the response about the each incident is that the the book is the being the old as soon as possible. So that's why the our the guide book is not

only the not for the the each incident uh the the treatment but also the framework of the how to they react to the incident itself. So the framework is not changing rapidly because of the um

the uh organization how to the monitoring the periodically the uh freshness data fresh or something like that is the keeping in the the very long time. So that's why we released the the

very framework of the how to react to the accident. So that's why the uh the our and also that we had now the replacing the very frequently because of that this is a very uh living document.

So that's why the uh even if we do not have the concrete the incident react uh appointment but the uh the just the releasing the framework but the the we change the periodically uh of the

document uh keep the living document. So that's the uh the the one of the topic and also the you may they ask me about the how to the collaborate with the private sector to the how to keep the

such kind of the incident reaction. So the the in Japan AI safety institute have the the working group for the the realistic the uh problem uh for the the we have the two types of the working

group. The one is the vertical uh the working group for the each industry sector that we started from the healthcare and robotics and we are going to expand the the vertical layer for the

each industry like the financial sector or uh the other kind of the industry and the another types of the working group is that the horizontal one so the the horizontal means that the it's a common

layer of the AI safety like the uh data quality or the uh how to inspect the AI model or something like that. So that's two the types of the working group is focusing on the how to the real reaction

for the AI uh incident. However, the it's not the doing uh by ourselves the government or the AI safety institute. So that's why they we uh the uh collaborate with the private sector uh

starting from the domestic Japanese domestic company but we would like to expand our activity to the internationality. &gt;&gt; Amazing. Thank you very much. And

finally Uber to to bring us home. Um &gt;&gt; let's go. Um, I'll ask two questions actually. One, one is more of a gossip I've heard and maybe you can share some with us. I hear Brazil has been working

on a transparency institute. &gt;&gt; I don't know if there's anything that can be shared there, but just throwing it out there uh which would be potentially working with the uh AI

safety institute. But my my main question and my true question is I'm very interested in in understanding how Brazil Brazil sees the possibility of working with other middle powers uh to

respond to incidents and and well to monitor and respond to incidents. Um so the last to talk last to speak is easier because all the intelligent people already said so it's easier for

me [laughter] the the difficult questions was answered so thank you for my colleagues but I mean uh everyone in here uh we are we have a a very good convergence about

what we should do and uh first of all regarding what Brazil is doing. We we already started to talk to other countries especially in South America and Latin America uh because of course

we have uh how can I say similar problems you know so we have to do especially of course we are right now you are right we are creating our AI safe institute and I

it's my pleasure to announce that professor Vagner who is here he is from uh federal university of Mina he is one of the coordinators of the proposal and please don't leave uh Akiko without talk

with Dr. &gt;&gt; [laughter] &gt;&gt; we must to change many uh uh ideas in here and this is a very important place to do that. So I believe in a few months

two or three months maximum our institute is a public institute but without uh government interference. This is uh multis sectoral uh uh participation of course academia uh

industry big tax everything everybody must be there because we have to dialogue with everyone and uh understand the the concerns of each one. So uh as I said Kaio to finish we have just two

minutes u I mean we are we have at this very moment manyus signed with many other countries of course the there are countries that are in the state-of-the-art in the current uh way

to to mitigate the problems with with this but there are a lot of of countries that has nothing no infrastructure no people to work in the subject. So we are all the time trying to work together

especially with our neighbors. And then uh finally I mean I think uh the most the the biggest problem is the time of the politics the bureaucracy. Uh sometimes we'd like to to do something

in a very short time but we are unable. So what we are doing first solving the Brazilian problems working together and discussing with the world and our presence here Dr.

Merro came uh next next meeting is about AI safety in England I think yeah Dr. came to discuss with the the the the colleagues from England. What can we do? Of course, we have a a a main line. We

know almost the the what we should do, but of course to to to change ideas to exchange ideas is absolutely important in in this kind of subject. So I I think we are in the right way and uh listen my

colleagues I believe we think similarly thank you &gt;&gt; amazing and with exactly a minute to wrap up I'd like to thank you all the of the speakers uh we managed to what's the

lay of the land what can we detect right now with instruments such as incident u the incident monitoring at the OECD what are the challenges in terms of what information do we need to acquire? What

are the biases biases of the information we have? And we discussed the whole life cycle of incident monitoring. Uh what are the pipelines? What are the thresholds we should have? What we

should have? What are the next steps? and bringing this infrastructure to beyond countries that have already established AI safety institutes but to other nations who will also have to are

already dealing with AI and the full response will uh incident response monitoring preparedness will only happen once we have buyin from everyone in a shared infrastructure otherwise our

problems are just going to go forum shopping they're going to skip around and they're going to persist and exactly at zero I thank you all for attending Big round of applause. [applause]

It's amazing.
