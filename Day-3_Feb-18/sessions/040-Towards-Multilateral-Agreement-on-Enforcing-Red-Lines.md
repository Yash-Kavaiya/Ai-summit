# Towards Multilateral Agreement on Enforcing Red Lines

**India AI Impact Summit 2026 ‚Äî Day 3 (2026-02-18)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 10:30 ‚Äì 11:30 |
| üìç **Venue** | Bharat Mandapam | West Wing Room 4 A |
| üìÖ **Date** | 2026-02-18 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/K_Ts6D2yZAA?feature=share) |

## üé§ Speakers

- Anita Gurumurthy, IT for Change
- Cam Rincon, Ada Lovelace Institute
- Gaia Marcus, Ada Lovelace Institute
- Guilherme Fitzgibbon Alves Pereira, Ministry of Foreign Affairs, Brazil
- Marielle Mumenthaler, ¬∑ Federal Department of Foreign Affairs of Switzerland
- Rumman Chaudhury, Humaine Intelligence

## ü§ù Knowledge Partners

- The Ada Lovelace Institute

## üìù Summary

This panel will bring together governments and experts to discuss the potential for multilateral agreement on AI governance, the need for agreement on unacceptable risks and harms across the AI value chain, and for establishing multilateral commitment on enforcement mechanisms that address them. Panellists will explore the desirability of red lines, how international consensus on governance could be built, options for feasible and proportional enforcement, and practical ways forward given geopolitical and technical constraints.

## üîë Key Takeaways

1. This panel will bring together governments and experts to discuss the potential for multilateral agreement on AI governance, the need for agreement on unacceptable risks and harms across the AI value chain, and for establishing multilateral commitment on enforcement mechanisms that address them.
2. Panellists will explore the desirability of red lines, how international consensus on governance could be built, options for feasible and proportional enforcement, and practical ways forward given geopolitical and technical constraints.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/K_Ts6D2yZAA/maxresdefault.jpg)](https://youtube.com/live/K_Ts6D2yZAA?feature=share)

---

_[‚Üê Back to Day 3 Sessions](../README.md)_


## üìù Transcript

And now I hand over the stage to the our knowledge partners. Thank you. Hello, I am Gia Marcus, the director of the Ada Love Lace Institute. Uh, thank you so much for being here with us

today. I hope the summer is treating you well. Um I am really excited about the panel conversation that we'll be having today and I think it will be quite a special conversation.

So today we are living I would say in interesting times and in this these interesting times many of us here in this room are understanding how to ensure that AI best

serves people and society in the midst of a changing geopolitical context differently implied voluntary commitments and technical barriers to both verification and enforcement.

I think we're gonna find out what's happening with the sound. Um, &gt;&gt; is the sound okay with people in the room? We can really hear the conversation behind us. But can you can

you all hear us? Great. Okay, we are professionals. We will we will continue in that case. Uh, we'll start again though. Hi. So, that was the dress rehearsal. Thank you for joining us for

said dress rehearsal. The mics are working. We're warm. We're ready. Uh I am yet again guy Marcus the director of the Ada Love Lace Institute and I'm I would really like to say this is the

kind of panel that you only can get at a summit. So I'm really excited that we're all here. So reflecting on today we reflected that these are very much interesting times. But within these

interesting times, many of us are here to understand how to ensure that AI best serve people in society in the midst of changing geopolitical context, differently applied voluntary

both enforcement and verification. Now, at the Ada Love Lace Institute, which is an independent research organization focused in ensuring that data and AI work for people and society, we approach

this question in many different ways. We tend to center the voices, experience and hopes of diverse publics and peoples. We build evidence of the real world impact of data on people and

society. And we convene diverse voices to tackle the big questions this all throws up. Now for us, one of the real big questions is how we ensure that like-minded countries work

together to ensure that AI is properly governed with risk and responsibility with the right actors. and I am joined by a truly one-off panel to kick the tires of this question. We're here to

look into four key questions. Is it desirable and indeed possible to reach international consensus on red lines for AI? What might the vehicles be to drive this consensus? What are the options for

feasible and proportional enforcement? And what are the actual practical ways forward given the geopolitical context that we're in? Now what we're going to have today is a series of opening

remarks and provocations from our speakers who I'll introduce as they come to speak and then a mediated conversation between all of us. We will then open up to Q&amp;A towards the end. So

please if you have any burning questions do keep them ready. I think one of the benefits of this room is we should be able to have quite an open dialogue between ourselves and the speakers. Uh

so to kick us off today is Roman Chowry, executive director and founder of Humane Intelligence. Thank you. Thank you. Um there we go. We're on. I will I will also project a little bit

with my voice. Um so one thing I'm very excited about just to start framing the conversation around red lines and what red lines means is first we have to have an approach of how we want what is the

mechanism by which we arrive at red lines. And there's really two models for us to think about and I think there's something for this panel to digest as well and maybe for us to dig into. So

the first is a sort of traditional model of uh risk assessment and risk evaluation that is more common to the corporate model and and in this world what you do and this a lot of the work I

do um with with enterprise companies is we help companies map their risk appetite. we help we help them understand the risks being introduced by AI models and then you sort of map said

appetite to said risks and say okay there's this threshold after this threshold we will not be using the system or we have you know sort of mechanisms that come into play so that's

one model often the red lines conversation though is really about extreme risk so extreme harm things like manipulation or runaway AI systems or or bionucle nuclear and biological weapons

and actually in that case there is sometimes a different model and and it's you know sort of known as like the vaccine model. So just in this scenario you could have a disease that kills five

out of 100 people. You and can go in a lab a scientist can go in a lab and let's say create a vaccine for that disease that um only has a mortality rate of one in a 100 people. But

actually what happens is that vaccine will never be mass- prodduced because there is a hard red line to say that this vaccine actually needs to be 100% safe before we are able to release it.

Even though we know that there is an additional four deaths due to the existence of this disease and you know in in expectation you would go down to only one person. So those are two very

very different approaches to drawing red lines. Why is that important here? because that changes the mechanism by which actually frankly most companies work in identifying risk and also

changes the mechanism by which we even think about uh test and evaluation which is specifically what I work on. How does that change? Well, first then the kind of testing and evaluating you're doing

is not actually holistic testing to generally see what the impact of the model will be. Then you do very traditional red teaming which is pushing the model to its most extreme

circumstance. So for example, if you are interested in CBRN, biological, radiological, nuclear weapons, you don't worry about what is the probability that somebody might be able to get some

ingredients. What you do is try to make one instance of it happen. And if you're able to create one instance, then you say, well, then this model cannot move forward. And again, I think because we

have not had clarity on which mechanism of red lines we want to draw, we cannot then back into clarity on how we can even have multilateral agreement on testing or enforcement.

&gt;&gt; Fantastic. Thank you. We're now going to hear from Gilliam Fitzgen Alves Pereira who's the special adviser on AI for the Ministry of Foreign Affairs of Brazil. Thank you.

&gt;&gt; Thank you, Gia. Just a quick adjustment here. Uh so I want to take a a slightly different look from what Roman just just said. We're going for here from the policy side on on how we consider red

lines uh in uh Brazilian internal and foreign policy. In this case, uh we are currently working on a on an AI draft uh on a draft AI law in Brazil that's currently being discussed and it takes a

slightly different approach to what red lines are actually are. In this case uh usually the discussion around red lines is uh is centered around runaway systems

and extreme risks but in our case we are taking a slightly different approach in terms of applications uh we don't want uh AI no matter whether it's runaway or or extreme or any other

uh technical quality to it uh that changes its actions. uh we we are taking a look into what kind of applications might be uh unsuitable or unacceptable [snorts]

uh in certain con uh in certain uh contexts. For example, uh we've been advancing the domestic regulation through through this bill and the the the approach classifies systems

according to their potential impact. [snorts] Uh it's somewhat similar to the EU AI act but with some uh sectoral adapt adaptations for us. so that sectors can be uh more or less averse to

risk. [snorts] And there are some things that we consider to be unacceptable risks uh such as the use of AI to on uh on healthbased systems or threats to safety and security, threats to

democracy and to information integrity or threats to fundamental rights. Those are the kinds of risks and red lines that we are defining through the AI uh through the AI bill. And uh there's a

specific point on the on the on the bill that mentions a uh using AI for manipulative behavioral systems or so to speak mass manipulation and also for predictive criminal profiling,

autonomous weapons, and mass biometric surveillance. Those are the red lines that are being defined in the law. So we're not taking a a very specific approach as to how AI does things, but

where it's what it's doing that doing those things to. It's a slightly different approach [snorts] and uh this also uh forms the base for a multilateral

uh approach to red lines as well. Although it's a very very young uh a very young discussion in at the UN or UNESCO or or CCD or other multilateral fora. Uh it's something that's been

showing up and we've been trying to take this also sort of a so more of a socially applied approach also as well. Um this is all based on on our own conception of digital sovereignty and

how how this is necessary for full collaboration. We need to be able to define our own priorities to define our own uh our own wishes or on what to do with AI and uh that's what forms a

conception of what we are trying to work with red lights. &gt;&gt; Thank you. &gt;&gt; Thanks so much. uh a pleasure to be part of this conversation and thanks to Ada

Loveies um I think we need to consider as technology evolves its functionalities and behaviors um manifest so in some sense what's

prohibited may not always be possible to anticipate and freeze for eternity and therefore a list of use cases which is the approach broadly that the law takes at this point

in time may not be illustrative of imaginable harms that are egregious. So this is something we have to hold on to because uh today's green lines may be tomorrow's red lines and that way the

EU's AI act article 5 has provided a set of red lines and prohibited AI practices or behaviors but nobody anticipated the development of AI tools like Grock that generate non-consensual explicit

images and Grock is a civilizational crisis. So at some level this has generated a huge debate in the EU among the policy community, civil society, uh gender equality advocates and others and

the commission has currently instituted an inquiry but that is under the digital services act. We do not really have a human rights benchmark to really evaluate this. So in the case of the EU,

the providers of [music] generalpurpose AI systems that are seen to carry systemic risk, which is what the law says, have such obligations. But my submission would be that given the ever

expanding affordances, functionalities and use cases of AI innovation. All AI technologies may need a baseline of uh fundamental rights assessment and postmarket monitoring. You have to watch

AI so that it doesn't watch you. So this baseline is as important as defining behavioral red lines. I have a second point to submit to which is about upstream concerns which is about the

production of AI. So we're not only looking at the application of AI and the behavior of AI once it's produced. We need to look at patently harmful stuff which is patently harmful for society

and ecology. For instance, how do we bring labor, nature? In fact, in many Latin American countries, the rights of nature is actually constitutionally provided. So,

we actually have to think about these enshrined constitutional rights when we think global so that this kind of homogenized westernized human rights regime is informed by constitutionalisms

of the south. How do we bring labor? How do we bring nature? How do we bring bodies? How do we bring human dignity? How do we bring the public domain of civilizational knowledge into

discussions of governance on international uh AI um regimes? So AI production counts. This calls for baselines. So from red lines to baselines that account for a higher

threshold of freedoms and red lines that account for justice. So we are not only talking about freedom from harm but we we must ask for freedom from harm to be read with freedom to flourish. So we're

talking about negative freedoms of course but we're also talking about positive freedoms. So we're talking about exploitation of labor, the ecological costs of data centers and all

the social economic ramifications. And this becomes really essential. Let's not look at AI just as a product to be tested before consumption. AI is a systemic technology that reorganizes

society and we need to pay equal attention to whether it's creating slavery, exploitation and injustice that's extractivist and neocolonial and therefore I would really like to end by

saying that and um you know your initiative to really think about multilateral agreement is absolutely essential. We know that the you know multilateral system is enabled at this

point in time but we have no alternative other than to rebuild it. We know self-regulation is not working. And for those of you I wasn't there at Seol when the first AI summit took place but I

just wanted to say that I while researching I came up to a commitment that was signed by many people in Seol. It's called the SEAL commitment, formally pledging to set out thresholds

at which severe risks posed by a model or system unless adequately mitigated would [music] be deemed intolerable. And companies like Anthropic and OpenAI signed onto these commitments. And lo

and behold, just the other day, we saw that Anthropic Cloud has been suspected to have had a role to play in the invasion of another country. I will stop there.

&gt;&gt; Thank you so much, Anita. So this is Anita Gurami from the uh the executive director of IT for change. Now we have ambassador Phil Tio, special envoy in technology for the Republic of Kenya.

Over to you Philillip. &gt;&gt; Thanks so much Ed. And it's good to speak almost last when everybody has said anything everything. So I think it's a cool thing but I think I just

wanted to place a context of where Kenya and potentially Africa is coming from. I don't see anybody from the African continent. So I'll be allowed to speak at least for the continent in this

particular part. I think we start from the premise that the current international order has been exclusive. Um to the extent that a lot of countries in the global majority have not been

part of the conversation around AI uh and and and in most cases its impacts on on society and community. We we neither build artificial intelligence systems. Neither do we own them. But we we are

the most we actually the largest users of AI. I think it's on record that Kenya is the largest user of charge GPT for instance. And so I think for me that speaks to a

red line and to the extent that um yes you have no control in terms of its built and impact but then you bear the brand uh of of its negative implications but then the first the first use of AI

in Kenya is emotional advice and I think for me potentially that is a problem I think in terms of red lines. Um secondly I think for us we we look at red lines from mine to model. um because in Africa

for instance and I agree with with with Anita's point here that u part of the challenge we see uh is is the upstream um implications around AI and I've been talking to a couple of folks here

they're not really on model evaluations but some people in agreech some people in climate some people on land rights and so it's to the extent that how do you ensure that part of the conversation

is not to have this multilateral agreement that only talks about technology but actually talks about how technology in this technologically enhanced world has implications ations

on real world um you know community rights, land rights, conflict, governance risks, structural extractivism and a lot of those competence that we see. I think the

second point of course is this midstream piece and we're seeing this uh in my continent for example around energy demands water stress uh all the biggest data centers being announced right now

in the African continent. uh and potentially remember we neither build AI we still have only 1% of data centers in the continent and we have no no how do I say no power to sort of even

engage I think for me so the second red line for me is this idea of power and how power is reorganizing itself around this new imagined capability around artificial intelligence the third part

is a downstream part around waste um but also I think in terms of just the the extreme arms around this and I can stand here and I was in SEO but before that there was Bletchley right I was also

there and potentially the the challenge with these issues are that yes we have a capable team that can work with Roman's team around red teaming you know um but I think for me the problem is that if

you do not have the capabilities and you're just an outsider we call it window shopping by the way because you're just standing from the side and there's this uh implied um participation

or inclusion uh piece that you're there and I think for me so really my thesis is that yes we are looking at the entire life cycle of artificial intelligence we can participate across it but there

fundamental structural issues one around is there real participation two is there real uh notion or or or even ability for them for the companies to want to change the last part of this dichotomy between

AI in the military domain including lethal autonomous weapons and civilian use and when it comes to multi agreements that becomes the challenge because we are separating the security

aspects of AI and its management and the civilian use of AI and we we don't see the difference. Thanks. &gt;&gt; Thank you so much. And we now have our last provocation uh from Mariel Muala

who's the co-lead of digital and tech policy selection at the Federal Department of Foreign Affairs of Switzerland. Thank you. &gt;&gt; Thank you very much. So I'll try and sum

up what my uh predecessor said. [laughter] &gt;&gt; [gasps] &gt;&gt; Um, thank you so much. It's an honor to be part of this discussion. Um, many of

you will rightly emphasize the importance of human rights, fairness, and inclusive participation in AI governance. From the Swiss side perspect from the Swiss perspective, I'd like to

focus on how we turn broad principles into practical enforcable action and what Switzerland can contribute, especially in the leadup to the AI summit in Geneva in 27.

AI poses risks throughout its development and supply chain from data collection and model training to deployment in areas such as health, finance, and public services. These

include threats to privacy and non-discrimination, safety, and reliability concerns and the risk that governance remains fragmented across borders.

Addressing these risks requires clear red lines, but red lines that are vague or hard to enforce. To be meaningful, they must be precise, measurable, and grounded in shared

technical standards. This is not just a matter of good intentions. It determines whether we can monitor, audit, and hold actors accountable. Because AI systems span multiple

countries and jurisdictions, multilateral cooperation is essential and Switzerland has long supported international collaboration on technology governance whether through

the OECD, the Council of Europe's AI convention, which we see as a success or processes at the UN. We see these efforts as building the foundations for interoperable approaches that reduce

fragmentation and support consistency. Looking ahead to the AI summit in Geneva in 27, Switzerland aims to help shape a practical multilateral agenda that bridges technical expertise, regulatory

experience, and diverse geopolitical perspectives. Geneva is home to key actors and we are committed to using that ecosystem to advance shared definitions, monitoring tools and

capacity building mechanisms. We also recognize geopolitical realities. A single binding global treaty on every aspect on AI may be unrealistic and in the short term.

That's why Switzerland supports incremental progress, coordinated standards, shared incidents reporting, mutual recognition of compliance regimes, and collaboration on public

procurement strategies that reinforce red lines through incentives as well as rules. In these efforts, Switzerland offers neutrality, convening capacity, and an

inclusive platform for dialogue. Our goal for 27 and beyond is to help achieve consensus on what matters most and to translate that consensus into governance frameworks that work in

practice. &gt;&gt; Please do join me in thanking our speakers for their opening provocations because I thought it was a really wide ranging [applause]

perfect introduction to the concept of red lines and how we take it I think from the slightly rarified boardrooms that these conversations on red lines are often had into practical

enforcement. Um we are now going to have a a bit of a conversation for 10-15 minutes with the panel and then I will be bringing the rest of the room in. So please do start gearing up with your

questions. Roman, you and I have been in many rooms recently where red lines are very differently discussed and I wondered if you could give the audience a bit of a feel for what you're hearing

when people talk about red lines and maybe the delta between what we've heard so far and what you've seen. &gt;&gt; I'm really grateful for that question Gaia. So most of the rooms that for

those of us who have been working in whether you call it safety, security, responsible use, it has many ethics, it has many different names uh over the past 10 plus years. um the rooms of like

the more extreme risk people that that's where the red lines conversation started and the red lines conversation started around things like runaway AI systems self-replicating AI systems AI systems

that you couldn't shut down ones that would you know set off nuclear weapons uh you know build boweapons things like that um and this is why I started with you know a provocation about what

exactly are we testing for now bringing in the perspectives of my very practically applied people on colleagues on this panel. Um it seems that what people are actually looking for red

lines on is navigating these difficult intertwined sociotechnical conversations. So whether these are ongoing conversations about whether we are protecting nature which I very much

you know appreciated you bringing up the idea that in some places um you know the the the rights of nature and animals are protected. This is not the red lines conversation that happens in many of

these rooms. But it seems like we know whether it is from uh a global a global majority perspective or even just a policy perspective on writing sensible regulation or just saying you know how

do we how do we uh further goals of having better international conversations. It seems like what we need to do is really push that conversation towards sociotechnical

analysis and think through what red lines look like. And then I'm going to go back to my riskmanagement model versus vaccine model. Right? So in this extreme risk story, we adopt a vaccine

model where we say you cannot even once be able to do thing X. What we're talking about here is frankly the more complex and I think for a lot of us a more satisfying narrative of how can we

intertwine multiple competing incentives, stories, uh priorities, all of which have significant value and embody them in an AI system. &gt;&gt; Fantastic Philip. A lot of us today will

now be going to AI Safety Connect. Well, we're probably having slightly different conversations for the room. What do you want us to take from your experience in Kenya and

the wider continent? What do you want us to be thinking of when we hear red lines? &gt;&gt; No, I think it's both and and I'll come back to Roman's point. Uh I think the

first thing we need is to increase the tribe of folks who are watching the front lines. I think the front end still needs to be watched because we always say that we'll never have a second

chance if Roman's work does not get done. We may not be aware of it but it can happen. If AI today is runaway, some of us may not know about it because we're looking at social technical stuff

but we don't have a second chance if this happens. Right? For example, if AI became selfaware and presses a nuclear but would not happen. So I think exactly we don't want that. As much as we're not

aware of it, it needs to happen and I think so we need to invest in that part of work. I think that's one. Two, we still need also to expand the understanding of red lines to where

society and community basically understands it because I think part of what has happened in the multilateral order is the erosion of those specific rights, especially when it comes to

technology and innovation. &gt;&gt; Guam, I know that you've been I know that Brazil has actually had a lot of international leadership recently of having these conversations at the

international level. What are you seeing where where do you see the kind of conversation moving in the in the directions that Philip is is discussing more about people about bodies about

nature what's important for Brazil? &gt;&gt; Well, we uh we've been trying to bring this the discussion on AI policy and governance mainly through the United

Nations. uh as much as it's been in some cases rightly criticized in terms of of uh how it works and how effective it is uh we still think that uh discussing any matter of that pertains to AI governance

should be should be done through the UN and that includes also the discussion on red lines uh but as Ran said we should take also the social technical perspective to expand uh the view on red

lines to incorporate matters like uh the use of uh uh the use of AI to manipulate people to undermine information integrity uh or excesses in application of AI that actually uh cause

irreversible damage to environment or the use of AI to violate human rights. Those are all this is on a big uh a big rethink of how red line should be thought because as much as we think that

a runaway Skynet scenario would uh would probably be irreversible and it most certainly would be uh there are there are certainly smaller applica let's say smaller

applications of AI that are just as damaging to to humanity as a whole only over a longer period of time. Uh I mean we we can see the effect on this with people using Chhatty PT or Claude or

Grock as emotional support. This is causing them to have psychosis episodes. This should be a red line. &gt;&gt; So So we have to expand we have to expand the discussion a bit beyond uh

what uh what people think was once science fiction. It probably won't be anymore, but it's not coming anytime soon. So &gt;&gt; Roman, looks like you want to double

click on something. &gt;&gt; I do. I'm so glad you've brought up things like psychosis, over reliance, and this is exactly what I mean about better measurement tools, right? We use

this term over reliance. We're now using new terms like psychosis, uh cognitive offloading. All of these things need to be quantified. They need to be operationalized at the very least or at

least we need to make measurement tools around them. So, I'm going to use the word over reliance. You may have heard that over reliant on technology. What is the baseline that is reliance, right?

Like an I'm so glad you brought up baselines because for some someone to be over reliant we have to have an idea of what the appropriate amount of reliance is and we don't know what that is.

Usually when people say, "Oh, kids are overrely reliant on their iPhones." We just mean relative to us, right? Like we are the old people and now we are looking at young people. Maybe the way

our parents looked at us and said, "You spend too much time watching TV or you're on AOL chat too much, right?" And that's how they feel when we say you're over reliance on talking to JT GPT. But

the reality is there is psychosis. There are children developing parasocial relationships. We do know that children's brains are not sufficiently formed at a young age to discern this

manipulative fake reality from the real world. They think AI friends are the same as real friends because literally their brains are not formed. So there is a truth you know a seat of truth but

again in order to understand identify and mitigate that we have to have an understanding of what the appropriate amount of use is. Fantastic. Anita, I think your your concept of baselines has

really sparked some thought. Do you want to tell the audience a bit more and what you're seeing and what you want this to then do to feed into the red lines conversation?

&gt;&gt; I think I'm also coming from something that is uh profoundly uh disempowering for so many of us in the global south. International trade agreements, for instance, prevent the opening up of

algorithmic source code in our jurisdictions. And considering that we are dependent on foreign code to run our systems even in public systems, electoral systems for instance or social

security systems. It's impossible to open up anything that leaves us with such a state of disempowerment. our public authorities are really not able to do much. Our state is not able

to implement its duty of development and maintaining the baseline of human rights. So this is really the real politic of trades, trade and investment regimes and we have to understand that

we might think about no-go areas but even to have a modicum of control on that is not possible. We have enough evidence and it's really like the tobacco industry. The tobacco industry

went on and on and on and after generations of people and after incidents of cancer just shot up. You know, now you have Scandinavian countries come up and say

2030 no tobacco in the country. Now we don't have to we have to learn from the mistakes of history. We are playing with fire here. It's playing with fire. And I think if red lines do not include and I

have five points, dehumanizing work, the abuse of appropriation of bodies into the circuit of capital, the plunder of natural resources, the theft of people's knowledge and the elimination of fair

markets and the erosion of the public. We cannot avert the compounded harms over time over time that create widespread inequality and injustice. So we already know what we want. We already

have the evidence, but it's really like the myth of tobacco. So yeah, [laughter] Marielle, I know that for Switzerland, ensuring that AI is governed and

understood in ways that protect the fundamental rights and this I'd say explosion of what we understand fundamental rights as being that we've seen on this panel is really important

for you. But I know that you also believe in incremental change. What can we expect to start seeing around enforcement, do you think? What would you like to see from the international

community? What steps do you think we can start taking together? &gt;&gt; I think maybe I can just offer a practical example of where Switzerland stands. So, we uh signed the AI

convention of the Council of Europe. Um, which for us is a is a good opportunity to, you know, to share common ground with other countries. Um, and now we're in the process of translating that

international law. And so there's a multistakeholder process going on saying what does it mean? There's a discussion around red lines and at the moment there's a lot of consensus on it's the

consequences they have to be technology neutral. So we don't have an a bill as such but we look at the consequences on fundamental rights. So if my right to privacy um is violated what are my

judicial means? How can I go forward? And that is a very I think interesting discussion we have and we're looking forward to um also exchange with with other countries but it's I'd say a lot

more practical than the big thresholds which we also need to discuss but I think from from an everyday uh perspective of individuals we are currently focusing more on on that

approach. &gt;&gt; So Philillip I know that you're bringing the fight of what can be done &gt;&gt; to the highest levels. Is that fair to say? Yes.

&gt;&gt; Can you give us a bit of what what can we expect? What would you like to see? &gt;&gt; A couple of things and and as you mentioned, we've been part of all the UNI resolutions including the global

digital compact. I I think for us is is is one is to and and and I love this panel because the first thing we've been thinking about is beyond the data and the evidence um how can we provide some

policy recommendations that are very specific one around yes reforming the current international financial system. We are talking about UN UN 2.0. I agree Brazil that the UN currently at least

especially for the global majority remains the only space that we can actually even have a conversation. It's not perfect. We don't get a lot of things agreed. We fight a lot but at

least there's a space for people to articulate their issues. Right? So I think we need this UN 2.0 to think about this. I think second thing is this mapping beyond the baseline. I think

there's this mapping of risks uh beyond beyond adverse risks in the in the sort of the technical space to understand the risks around society. For example, the risks around democracy for us is a big

deal. Uh and we are seeing how AI is manipulating elections. Uh one person recently claimed they manipulated Belgium's elections, right? One guy, right? So, so I think for us, I mean,

how do we do this mapping at the highest level? Third point I think is to be bold enough and I think it will take civil society, academia, governments especially in the global south and Kenya

we are happy to always take the lead in that is to begin to introduce these conversations um in in in those spaces in the United Nations and to include companies because it's not enough for

them to sign at an AI summit. This is not a binding space. We need to move this conversation to a legally binding space. Um Kim, are you would you like to come

in on you were you were not Brazil was mentioned? What what is your response to that? &gt;&gt; Okay. Uh well, we are in agreement. The UN is still the most legitimate uh part

and probably will be. uh which is funny saying that in a in an AI summit that's not UN based but uh but there there is enough I mean it's a fragmented [clears throat]

excuse me it's it's a very fragmented stage for the debate on on anything related to AI nowadays everyone wants to have a say and that's legitimate I mean everyone should have a say at least

because it's something that's potentially very uh very upsetting to to to certain uh certain aspects of of of society of the economy. Um but there should that again coming back to the

baseline there should be a certain baseline that usually that remains at least in the international international diplomacy arena that remains to the UN and it probably will be for the

foreseeable future. [snorts] Um I mean spaces like this like the AI summit they're important for us to actually uh expand our perspectives and include companies include uh NOS's

include academia include the developer include developers because the UN is very closed in within itself into itself. So uh uh it's a very statebased uh forum. So it's important for us to

have these uh these uh spaces as well to sort of air out the discussion a bit and get new ideas. &gt;&gt; Fantastic. And that is an unplanned but brilliant segue to the audience. If we

wanted to air out the discussion, can I get just a a show of hands? Who has questions? You sir moved very quickly. There we go. One, two, and three. And then we'll go for more questions. So

brief, tight, and we'll do three in a row. Thank you. Can you hear me? &gt;&gt; Oh, oh, thank you. Thank you for this great panel. Um, my name is Alejandra

Moral. I'm the co-executive director of Access Now. And, uh, one one thing that, um, uh, we are right now evaluating is, of course, we are conveners of RightsCon. And one one thing that uh has

happened is in these multistakeholder uh mechanisms especially the last year a lot of the mechanisms that we used to work with are not working anymore. So what from your perspective from

governments but also from civil society what is our role as conveners? How what are the mechanisms that are still relevant to create some changes in this AI governance and with everything that

is going on? Fantastic gentleman in the middle there. Quick. &gt;&gt; Hi uh Alex Reed with the Interp Parliamentary Union. It seems like the discussion on red lines operates at two

levels. One is on catastrophic risk and the other one is on near-term harms and risks that are emerging in societies worldwide. So my question is how do you set these red lines at societal level

because a number of the panelists said that they needed to be culturally specific and so on. For me I think it needs a public conversation and for this to feed into setting red lines that are

then put into legislation. But what is the mechanism that you seek to democratize democratize this conversation on red lines? &gt;&gt; Hi um my name is Ishida. I'm from the

Australian delegation in the startup world down under. Um my specific um inquiry is around this idea of generalized AI versus specialized AI. And I think a lot of the benefits that

we see in specialized applications and ag climate techch are positive. But when we have a generalized impact with open AI anthropic, it's quite dangerous because there's really there's really no

sector left untouched. So what are the kind of governance frameworks and how are we separating the positive AI from the negative AI to ensure that the red lines don't um impede the technological

innovation for good but ensure we are protecting society and humanity at its core. &gt;&gt; Fantastic. Who would like to go? So we have what is the role of of forest such

as rightscon given that nothing else is working. Uh how do we democratize the conversation at red line paraphrasing and let's not throw the bad out with the good or vice versa.

&gt;&gt; I'll check this &gt;&gt; Philip go [laughter] for it. &gt;&gt; He know you're you're funny. So I I I think and and and this is the thing. So Ritscon normally invites people like us.

I think I've been invited a couple of times at rightcon. So I think a couple of things are happening. One of course is you've had the scientific panel that has been announced by the United

secretary general, United Nations Secretary General 40 40 experts uh good space uh happy to support how rightscon can fit into that. Second part is a global dialogue uh that is coming in in

July. Uh third is a network of tech and voice. So Australia, myself and Denmark uh are part of the convening of this. We're actually having a dinner tomorrow. So if you can give me something, we're

actually planning on how we can power uh this space with a tech envoys. I think we have capability in my view and we are quite senior in government. So there you are and hopefully we'll have a white

paper from this panel &gt;&gt; Russia. &gt;&gt; I can I can talk to the second point unless other people want to address the first question.

&gt;&gt; We have five minutes. &gt;&gt; Okay. &gt;&gt; Okay. Okay. Uh all right. So to the second question like this is why I meant in the very beginning about this sort of

risk management model versus the vaccine model. Extreme risk calls for the vaccine model. It cannot even happen once. Right. what we're talking about and what a lot of the policy

conversation is about is how do we navigate you know identifying this point now like I'm actually a political scientist by background so uh I'm going to talk a little bit about constructing

policy to people who actually construct policy um one of the problems is if you set a hard and fast threshold or hard and fast line then literally what happens just incentivizes company to

innovate one under that line if you say you can't have more than 100 million parameters guess what every model is built at the same level of sophistication at one minus 100 million

parameters, right? That is literally just how that is actually the story of deepseek, right? The story of deepseek is given a construct constraints, they innovated and created this new method.

So my concern about drawing red lines around sociotechnical harms is it is just an invitation to say you can just do one less than, right? And is that really acceptable? Are we okay with only

3% of children having AI psychosis or something? You know what I mean? Like that just sounds ridiculous to say, but that's the world we end up in. So my suggestion would be um doing test and

evaluation based on as I mentioned like that risk management model where you're identifying threshold of risk as well as risk appetite and then mapping that to some uh like um malleable red line. So

it it does like this is why the term red line is not something that is used in the sociotechnical community because of that problem. So red line implies a hard and fast line. But policy cannot be

written around hard and fast lines. The world changes, right? Uh the the technology changes, the application changes. We cannot write a law today that 15 years from now sounds not only

ridiculous but is actually quite limiting. So my suggestion would be if you're using the language of red lines, use it more as um uh as a suggestion and and have whatever that line is, that

threshold is be something that is consistently revisited and re-evaluated and updated. &gt;&gt; Just a quick uh a quick try to answer the third question. Uh the Brazilian

draft AI law is doing exactly that. uh we're expect if it's if it's approved and we expect it to be approved this semester uh the idea is that the law will create a system where uh specific

sectors have different regulations regarding AI. So for example we have this the vaccine scenario where using AI in the health context is much uh it's much more complicated and it

should be far more regulated than using it in a in a commercial uh setting for example. uh you should you can use AI to sort of like process data from logistics or something like

that but you cannot use it to do biometrical recognition. So we are taking more of a application based and sectoral based regulation uh where it's possible to do sandboxing of certain uh

certain applications where people can try out uh innovative solutions uh in a specific context uh but doing but doing so not within a more I mean there are some general

uh some general red lines so to speak on what cannot be cannot be used what AI cannot be used for in any circumstance but for everything else it depends on sectoral regulations so for that we have

different agencies different ministries different even uh industry organizations who participate in the in the regulation for that so uh it's the main message we went we want to convey and that also

applies to our foreign policy in this matter is that uh we don't think regulation and innovation uh are mutually exclusive. On the contrary, they have to work together. If one if

innovation wants to thrive, it needs regulation. It needs something to sort of take the edge out of the risk. &gt;&gt; Fantastic. We're going to hand to Anita and fastest three hands get the last

three questions. You gentlemen were very fast. Anyone else? Anita, over to you and I'll watch out for other questions. So, we've got two more questions pending.

&gt;&gt; Thanks. I think that in our quest to really open up uh the debate and have conversations in the past 15 20 years in the digital rights domain we've somehow used the term multistakeholderism

through which we have somehow managed to decouple democracy from public interest. I think democracy and public interest as two sides of the coin and if we really want to make multistakeholderism

meaningful we have to make sure the twins are in the room and I think the problem is that by bringing in very powerful actors into the room we have somehow uh deceived ourselves into

believing that means agenda setting power but it doesn't happen that way so we're realizing it pre late because we have been very a theoretical you know we have to be very theoretical critical.

The second is um I agree completely that we need to um have uh sectoral um conversations but I would also like to add that the debate can only be democratized if we bring constituencies

who are voices of descent the food justice movement the labor rights movement people's movements for right to information all of these folks talking really about what data is to them so

that the question of AI cannot be separated from data justice. So I think to link these two and keep the conversation going is really important. And finally to your point about

malleability I think the whole issue of malleability is very important because we're talking about a baseline of norms where questions of dignity the right to truth you know Tarun Khan who is a

scholar has been talking about the constitutional right to truth in fake uh you know democracies of our times. So I think that there are a bunch of things to be done where we really have to go

back to the ground and reexamine what democracy means in the moment of AI. &gt;&gt; The mic's off. What happened? &gt;&gt; We're back. We're back. Thank you so much. Every time you speak, I get

goosebumps. It's fantastic. You had a question. Yeah. &gt;&gt; Uh I think we might be out of time. If someone has a very quick question, &gt;&gt; go for it. Go. You go. You go.

&gt;&gt; Okay. Um second question. Uh first of all, thank you for for for the uh very insightful panel. So I'm I come from the Philippines and I lead um AI governance and AI delivery for the Ministry of

Education. So you might think it's quite unique cuz um my role is first of all uh innovating for speed but also ensuring everything is secure, right? And um I actually came in with this question

prepared for the for the panel. Um so I'm actually curious how developing and third world countries can sort of strike a balance between um inclusive innovation in AI. So for example, making

sure our people actually get a seat at the table when it comes to international discussions, right? But also respons enforcing responsible AI red lines as discussed here so we sort of don't fall

behind in in the global race. &gt;&gt; That is a fantastic question to end on. Anita, you're so ready. You go. And then I think we're going to just go around the line for last reflections from

everyone. &gt;&gt; The hedgemony of the narrative is what we need to challenge. So we need to define what we think is valuable. So this is a very feminist perspective. The

larger growthoriented economy defines certain things as valuable. So Peter Theal will make money, but not indigenous peoples in the islands of the Philippines. So we need to define what

value is. I mean, I'd actually be happy finishing there, but Marielle, should you go down the line? Last thought to leave this room with and thank you so much everyone.

&gt;&gt; Thank you so much for this um discussion. Um we're really looking forward to continuing this and uh see you in Geneva next year. &gt;&gt; All right.

&gt;&gt; I think I'll be quick and and part of it is and to your point, I think the UN still remains the space for us to engage in these issues. I think with the geopolitics right now, it's very

difficult for any global south country, for example, to take on a 5 trillion uh dollar company. I think that's that's not realistic. &gt;&gt; I I just want to defer to whatever Anita

says. That's fine. &gt;&gt; I know, right? I was like, we're done, guys. It's over. Um we have 43 seconds, so I'll take chairs privilege. Thank you so much to the work and our colleagues

on the red UNESCO Redline subgroup for being part of this. Thank you so much to our speakers for what has been like, this is not my I'm a bit old to say this. This has given me life, guys. This

has been great. Um, we're so looking forward to continuing the conversation. I would like to give a massive shout out to Cam who has done all the work uh and none of the glory yet. And please

do, this is the beginning of a conversation for us. We're hoping to have it in Geneva soon. We're hoping to have it in any other international AI gathering. We see you coming forward.

Please do scan that QR code. Please be in touch. We've been the ADA Love Lace Institute. Please again say thank you to my fabulous panel. Thank you. Yeah. Uh so uh I request

India team Mr. Capil, Mr. Amul to please handle the moments. I I request speakers to be please stay on the stage. Speakers uh ambassador. Yeah. So uh yeah, she she's needed. Yeah. So

Correct. &gt;&gt; Yeah, there.
