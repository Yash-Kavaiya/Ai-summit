# Safe and Trusted AI: The Ethics and Governance Perspective

**India AI Impact Summit 2026 ‚Äî Day 3 (2026-02-18)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 09:30 ‚Äì 10:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 14 |
| üìÖ **Date** | 2026-02-18 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/EMETwQ32yJQ?feature=share) |

## üé§ Speakers

- Dr Amit Kumar, RIS, New Delhi
- Dr B K Murthy, Amity University, Noida
- Dr Geetha Vani Rayasam, CSIR-NIScPR, New Delhi
- Dr K Ravi Srinivas, NALSAR, Hyderabad and IIT-Madras
- Dr Neethu Rajam, National Law University of Delhi
- Dr Nupur Chowdhury, JNU, New Delhi
- Dr Roli Mathur, ICMR, New Delhi
- Dr S K Varshney, RIS, New Delhi
- Dr Titipol Phakdeewanich, Ubon Ratchathani University, Thailand
- Mr Saurabh Kapil, BioSky
- Prof. Sachin Kumar Sharma, RIS, New Delhi

## ü§ù Knowledge Partners

- Research and Information System for Developing Countries (RIS)

## üìù Summary

Anchored in the principles of People, Planet, and Progress, this session examines how safe and trusted AI can be advanced through ethical safeguards, accountability and effective governance. Bringing together national and global experts, the discussion explores practical pathways for inclusive, equitable, and human-centred AI development and deployment across sectors, highlighting the role of international cooperation in strengthening trust and responsible AI adoption.

## üîë Key Takeaways

1. Anchored in the principles of People, Planet, and Progress, this session examines how safe and trusted AI can be advanced through ethical safeguards, accountability and effective governance.
2. Bringing together national and global experts, the discussion explores practical pathways for inclusive, equitable, and human-centred AI development and deployment across sectors, highlighting the role of international cooperation in strengthening trust and responsible AI adoption.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/EMETwQ32yJQ/maxresdefault.jpg)](https://youtube.com/live/EMETwQ32yJQ?feature=share)

---

_[‚Üê Back to Day 3 Sessions](../README.md)_


## üìù Transcript

which is safety and trustworthiness. Governance cannot be discussed alone or separately. So ethics and governance has to go together. So that was the logic of curating this session and I'm so

grateful that our director general uh encouraged us and supported us throughout this endeavor. Without his support and by our constant encouragement, this could not have been

possible. So it's my proud privilege again to welcome you all and I invite our director general professor to deliver the welcome remarks. Sir thank you.

&gt;&gt; Thank you Amit. I think I encourage you more 11th speaker in one hour. [laughter] So first of all thank you for being here uh for this very important session and

few words about why we are doing this. When Amit came to me and we were discussing about this AI impact then we thought about the topic we discuss

about then we thought that this is the one issue about safe and trusted AI. Nowadays if you see uh whether a student or a businessman across we are using the AI across all the sphere no doubt but

sometime we also not ready to accept that we are using the AI sometime we are preparing our speeches by using the AI that even then sometime we become too ethical to accept this that okay we are

not using the AI and even uh in uh uh in a research tank think tank I uh meet across a lot of people from universities and I know that many of the student and

even the faculty member they use AI they dig out uh all their report in minutes or one hour then they spend five hours to show that it's not look like AI so these are the real challenges whether

we need to use AI across if we even even if the government is saying that okay we need to uh encourage AI in the schooling if that is a thing then what what are the limit what are

the ethics what are the governance issues are there that's the important thing and the way we are using the AI across whether in the medical field or uh even

education research it's really uh sometime it's very nice that it's facilitate but it's also challenging for us as Because now if you see ourself we are I

think uh AI know much more about us even when we are writing the email we are bit cautious but when we are interacting with the AI we discuss lot of personal things and if someone has your AI lo

login and pass password we know you about your personality what you think what you had done how you are thinking about uh uh this issue. So this is the one aspect and the second aspect is that

uh given the fact that we need to use the AI whether we have the governance structure in place and if that is place uh whether this governance structure would be inclusive

these are the important things like AI need to be like a UPI it should not be like a nuclear technology only the selected people should have or only the urban elite should have and that's what

why we have one center Dshin voice of global south. The discussion here why I wanted to take uh it to the global south because uh we had the MOU with more than 157 think tank across 90 countries. So

whatever you are discussing it we come out with a report and uh the whole purpose is that we want it that this AI should be inclusive it need to be trusted and whatever the governance

structure need to be there it should encourage from grassroot to the urban elite all should be inclusive thank you very much and I request uh Mr. Please moderate this.

Namaskar. Uh good morning. So Dr. Sachin Sharma has already laid the background why we are having this session and we are so blessed that we have got a panel which

is mix of both the governance and users and uh with their experience. Uh we would understand both aspects uh how it is governed and how uh what are the challenges in governance how

users find it and what are the challenges or easiness if I could say to the users and then obviously what Dr. Sharma concluded how not to make it look like AIdriven that's also so I think we

start from the first last point first and I would like to invite my first panelist Dr. Gita uh Dr. Gita is director of national center for science communication and policy research and

her organization publishes lot of scientific journals. So Dr. My question to you would be to help us understand. You see these days lot of authors are using AI to write their scientific

manuscripts and even editors are using AI tools to detect how much is the plagarism, how much is the AI content. So this is describe us this scenario. How do you

see it as a director Nisper and in your opinion how authors, editors and even publishers should make uh ethical use of this tool? Please you can

&gt;&gt; uh good morning everyone and firstly my uh thanks to the organizers rais for having me here today and I think on a very very critical aspect and uh as highlighted by Dr. Sachin

so safe and trusted AI and I don't need to sell the importance of AI you see the huge participation and for AI uh like the Dr. Vashnney talked about. So we are publishing 15 research journals at our

institution and also popular science magazines uh spanning you know from physics, chemistry, biology, intellectual property, traditional knowledge and so on and so forth and

this is a really an acute problem today. So AI is a bone and a brain like we all talked about it and uh so how many of you have uploaded any report which comes to you for analysis into AI maybe you

can raise your hand probably each and every one of us at some point have done it &gt;&gt; but do you know you're violating the privacy that is something which many of

us are not aware because when a report is coming to you or a manuscript is submitted it's confidential but that is sometimes we are not aware I think it's lack of awareness because

Somebody may want to patent and somebody has uh want to take a copyright. The minute you put it into any AI and upload that document, it becomes a public document because AI is training on those

data and it is exposing. So you're breaching confidentiality. So many of us are not aware of these ethical issues. So this is just one example I'm giving and uh AI is here to assist us not

replace us. You cannot leave everything to AI. So I will tell you what are the dos and don'ts what we understand from the publication and this is not made only by us it's being followed globally.

Each publisher if you look at it has different variations of the policy. So if you are publishing a paper or submitting a manuscript make sure that you know what is the publisher allowing

you to do because each publishers have their own uh formats and what can be allowed and what is not allowed. So that awareness is really really important and as I said data privacy is very very

critical. So please ensure that you don't upload things onto those things and many of us Sharma said a quick summary we want or something and you know many of you may have heard about

hallucinations I think you all are aware of the hallucinations. So be very very aware of it and especially many times it quotes fictional references. You have to doubly verify that okay that the and

this is very very critical. It may sound very logical because the AI is very smart. It makes it sounds very plausible and logical but you do not know whether it is actually supported by some

reference or by facts and although we may use AI one of the most important things is accountability is the human. So you cannot say that the AI gave this product so I am not responsible for it.

Ultimately each and every one of us who is AI we are liable to any plagarism or any data breach of data data privacy rest with the humans. So I think this is another ethical aspect you have to keep

in mind and again uh we all know that AI is training on the existing data and by no means existing data is comprehensive. people from India know like you know clinical trials when they are done many

times data may not be from Indian populations and even in India maybe only some populations are represented so there's a lot of bias within the data and within the models and the output

you're getting so you cannot uh take it as it is because there is an inherent bias and that is something one has to be mindful uh when you are using AI but at the same time not everything is bleak so

I'm not here to say that do not use AI but use it responsibly so when I say what is responsible AI? So responsibility means you need to know what tool I'm using, what are its pros

and cons and when to use it and how to use it. I think these are responsible usage is absolutely critical and as I said there has to be human oversight for the AI and if you're using Grammarly or

for editing that's perfectly fine that nobody questions it. If you're using it for editing that's fine but you write a whole paragraph using AI that is not allowed and many publishers and many you

have to be transparent about the usage of AI. If you are using AI you have to declare that you have used AI and in what context you have used. In fact some publishers go to the extent that you

have to tell what is the prompt you have used and what section you have highlighted. But again that varies from publisher to publisher. But what is common is that you can use it for

editing and you know of things but not for entire drafting of a manuscript. You cannot give a prompt and say write a paper this is my data that you can't do. So these are some of the ethical

considerations when you're writing a manuscript. And again coming to images and graphics especially if it's research data you may have a figure you cannot use AI to you know embellish it or make

it look better. That's again not acceptable. And but if you have large amount of data and you have to then you have to declare it. That's one of the things and I said confidentiality

is of supreme importance and again originality you have to be responsible for whatever you're using and he talked about editors editors you're right editors are also using AI because see we

all want to use AI in a safe way but that is used for only for detection of plagarism and so on again editors cannot when you're doing a peer review of a paper somebody says a paper and you are

a peer review you just cannot upload that and from the that is again not acceptable. So these are some of the critical issues when it comes to AI and ethics uh in

publishing and I think most important is we should not forget that AI like recently we had a workshop on human centered AI and the focus was it is for the humans we are generating AI for the

humans and it should benefit the humans not replace the humans. So and if we work together uh humans and AI we can really accomplish a lot. Thank you. Thank you ma. Just one side question. Uh

for the commoners who are putting their scientific manuscript for uh plagarism or uh AI content check sometime they commit mistake they leave it on the platform itself so that anybody who

subsequently sees it it looks like plagarized. So how to avoid such recurrences if you could advise to the commerce. No, you're absolutely right. Many times I was just reading even US

constitution if you put it in the AI it says it's already existing and it is 100% AI generated. I think that's one of the challenges one is facing but I think when it is 100% matching or something

people are aware that it's a copy based on that. I think &gt;&gt; no even like you put your one script and you want to check it second time, third time then it would

&gt;&gt; but I think there are now new additions of softares which are coming which can kind of address that. &gt;&gt; Thank you. Thank you Gita for such a wonderful remarks and explanations and I

hope we understand that in when you are writing how much it is per permissible and as editor uh and as reviewer uh we need to make very very responsible use of this tool. Now let us go to the

governance to the person who is involved in shaping it up in India. So I would like to invite Dr. Morti and for all of us. Dr. Morti was head of the research and development unit of ministry of

electronics and information technology and currently he dean of uh entrepreneurship uh in Amiti University. So Dr. Morti when we are talking about AI in India we

are talking about fairness, accountability, transparency, safety and privacy within AI life cycle particularly uh ensuring human centricity and

compliances within mighty guidelines. So my question to you is how does government focus on mitigating biases? Gita mentioned about that uh in diverse data set establishing local

accountability frameworks and implementing regulatory sandboxes for safe innovation. How best to deploy AI in responsible manners. &gt;&gt; So first of all we need to thank you

very much for inviting me for this session. Uh I'm a third generation AI guy. So my first my PhD thesis is in 1992 to96 from Delhi and it is on how to use neural networks for naturally

large language models that was the first of its kind. Okay fine. Uh having said that I was in the first of national.

So now having seen the participants here in the diverse area of academicians and developers and users and the people who are deploying these systems I would like to touch upon a brief uh overview of

what is the framework when you want to take any use case for deployment and how to take care of what are the issues that you need to consider that's that's overall a framework for governance of

trusted AI systems while deploying in the system and for the users for especially for the public use. So when you're talking about responsible AI, it it includes safety, transparency, trust,

trustworthiness and accountability that is these are the major concerns of any person who is deploying or who the system and when you are talking about the

responsible AI that's what it's a common term for this why these things are coming first of all we need to understand that any AI algorithm in any AI system does not guarantee any optimal

solution. It gives only an expert just like I'm asking an expert for for example you are going to a doctor whether the doctor is giving his full knowledge on that and then diagnosing

that or not still we are also getting the doubts that's why you go to for second opinion third opinion like so here also when you are using AI system we should understand that it is a system

developed by humans it may have some inherent errors or it may be mis miscalculations may be in that that's what we need to understand that so that's why these issues will come on the

trust issue now while doing this system while deploying the systems broadly I categorize into three categories one is highrisk systems second one is lowrisk system and third one is no risk

or medium risk system if there are medium risk systems and low risk or no risk systems that is different and now the human inter is required in all the three cases. We should not leave the

final decision to the uh AI system. So for example in the highrisk systems human has to have a complete control and command on the systems. Okay. And second if it is a medium risk system human

should be inside the system. That means before the final delivery to the uh to the user your human has to verify that validity of the service. And if it is no risk or low risk lowrisk

system obviously human could be little bit back. So when there is no risk for the user ultimate user or the deployment of that system will not harm. So that's what we need to consider. Now while

developing all these systems I consider even though my guidelines talks about seven &gt;&gt; sutras I consider it as one more system that

addressing that. So ethical purpose, social benefit, accountability and transparency and explainability that's very important and safety and reliability and open data and fairation

and privacy that is more important and AI and intellectual property is another important thing that we need to add to that seven sutras which will come when you are talking about who are using it

or any without taking much time I will talk about How to select an AI system for what purpose and how to deploy it and who will be the user what all the

condition that you need to take. I take this into four steps. The whole framework of AI deployment will be categorized into four steps. What is the first step? First step is you need to

focus areas. What is the use case that you are going to use it and who will be the ultimate users and what is the weather that is related to business goal assignment just like

everybody is using I want also use it for this system that does not work here it is like okay hammer in the jail scenario so review internal policies whether this is required or internal

policies can change because of EI systems that are coming in that is that is one more thing risk assessment design and you need to do technical and socioeconomic

whether it is feasible the system or not and whether you have enough manpower to design such a AI system. So that is the first thing that you have while considering the design system all the 12

tenets of the trust need to be taken into account assuming that the AI system will not give you an optimal solution. So next one is the data because for any AI algorithm data is the most important

one. So the data sourcing and cleansing and if where how to store the data that is one of the important thing that you need to consider testing and quality testing for quality and biases if the

data is biased your work results will be skewed. So access rules and compliances whether you are following all the data security rules data privacy rules or not and data audit is also needed whether

the data provided to you is correct or not or it is just manipulated AI generated data is also will be there nowadays it is abundant so how you are taking that data that is the second step

while considering the second step you need to talk take care of that concerns whether the consensus has been taken from the source that you are taking the data fair and quality and privacy and

security need to be considered these are the characters that in the second step third step is algorithm so what sort of AI algorithm that you want to use it I think some of the people are talking

about the algorithms which were designed 2015 2016 or 2010 may not be relevant in today's context only those algorithms which are coming after 2020 maybe a little more user user friendly and may

be useful for you and even some of them may not give you the explanability which is essential. So that we need to consider whether these are required or not and design the logic logic flow is

very important. Test the data sets thoroughly. Don't just go for a shortcut and if any algorithm you take a small data is there you will get some solution. That is not the end of the

story because we need to have complete transparency and accountability and the completeness of the total algorithm. That is important. So and model the whole algorithm and then output you need

to check for biases if there are any. And the more training data is available and the more testing is done the more robust the final algorithm will be and the last step is deployment and

governance. This is very important for those people who are organizations and government who are deploying these systems for public use. Testing release into production thoroughly test it and

then release it to production and man manage and monitor mitigate the risks and model this risks and monitor the hacking and third party breaches if there are any. So most of the data

systems today AI systems can be easily hacked. So how to take care of that? Get incident response immediately and you should have a separate incident response team for this AI systems if you are

using it if it is medium risk or high risk system and change management is very important. Training of the employment for EI systems is very very important and getting the right manpower

for this is important. So for doing this transparency and accountability for each system for each case who will be the accountable that policy has to be designed while governing in the

governance mechanism of the AI systems and these are the things that you need to find out but unfortunately when compared to the classical systems AI systems requires thorough training of

almost everything. Why? Because the data it depends on the data. So whatever the data that you have collected from that date onwards, whatever the data that is generated will not be accounted. Suppose

some of the chart GPT of 2022 or any open AI systems which was trained in 2023 or 25 for example, whatever the latest data, it will not provide you answer. Expecting that answer solution

from the open AI systems is not a good thing. So that's what we need to deal that is why there is a continuous development of this is required when compared to any other classical system.

So with that I will &gt;&gt; yeah thank you Dr. Thank you for sharing [applause] journey or evolution of AI adoption and its governance and how it is progressing

and what are the possible checks we should take and one of the thing he said is it should be human centric and when we talk about human centric we have a person uh Dr. Title Fak Dinavich who is

director of center for human rights uh in uh Thailand and I'll request him Dr. Tul that uh bridging the gap between rapid technological advancement and protection of fundamental human

rights. How would you recommend use of AI for fairness, equity, non-discrimination of the heterogeneous society like

Thailand or India. &gt;&gt; Thank you for having me today. And um firstly I think uh what we have share in common is the issue of privacy and how to use AI and to protect the the rights

of the people. But uh perhaps I would start with reflecting how human rights is interpreted. That is the main challenge for the use of AI because in the context of Thailand I think it's

similar to many other Asian countries because domestic perception is very different from the universal declaration of human rights and that is one of the very main challenge in Thailand. back

before actually in Thailand we have what we call personal data protection act PDPA of 2019 and before this one of the main example that I would give you like for example

when the university was discussing about linking student um ID with the bank because to facilitate student payment and some of the the staff didn't actually pay attention to that that's

fine why why can't we just give them straight way. But when you look at the data, the information of the students containing a certain level of personal information which many people see is

fine because and partly is a reflection of the societal perception in Thailand as well because I think uh many Asian country have similar perception like uh hierarchical structure. The adult tend

to have the rights and authority and this also translated into the the the mechanism of the states as well when the organization feel that they have authority or over student to to transfer

information and this is one of the main challenge for uh the protection of the rights you know if this kind of uh interpretation of the rights is not uh accepted as universal like when we talk

about the language of universal declaration of human rights And that is one of the main challenge in Thailand to protect that. So we when we talk about this kind of safe and trust AI use I

think it is very important that the mechanism of the state should interpret this strictly not just in a cabux form or take authority to um to authorize everything by themsel and this is one of

the main thing that that I think we we have this challenge and the other thing that I would also reflect from the the problem that we have is the uh in Thailand is now I think you might have

heard a lot about the um cross cross border criminal crimes in Thailand and we have the big problem with on the both side with Cambodia and Myanmar and then [clears throat] the leakage of

information personal information is very important that many people are concerned about this and we also have a problem with deficit trust in public uh institutions. So how can we actually

control and have a kind mechanism to control the leakage of informations because a numbers of cases has been um published that the personal informations from different ministry in talent has

been leaked to certain organization. And when we look at this at the local level and people are very concerned and we if you look at the implementation of this policy it's like um how can the

government implement the PDPA and that is very main challenge. I think it is very important to open up for public scrutiny but how can we ensure that civil society can get involved because

they tend to take authority in Thailand. So and many civil society group in Thailand are trying to ensure that the government would actually have facilitate them or provide channel for

civil society to get involved to scrutinize the use of the AI and how can the um the collecting process of information be um perhaps open for more scrutiny rather than providing um

authority for the the operator. to to use it directly. &gt;&gt; Thank you. Thank you. I think this is very important that people across uh different

uh nationalities and how different uh even genetic variations there and people are looking so diverse and they are subjected to these rights we should look for. When we are talking about diversity

actually health care is very very important and uh in fact currently most of the data sets are trained on western data sets. So how they affect India or country like India's so I would like to

ask Dr. Dr. Roi Matur who is head of bioeththics unit in Indian council of medical research. What are the ICMR guidelines for uh bioeththics and how we are following?

A very good morning and thank you for uh for this invitation and I'm very happy to be speaking a little bit about ethics because uh what we have been seeing is that uh we talk about responsible AI and

uh but we somehow are not discussing I feel I mean I have a bias because being a bioethicist that we need to talk a little bit more about ethics uh in forums like this because what is

happening is that artificial intelligence is uh developing in at a break neck speed. Really the amount of speed of innovation is something that we cannot match and uh healthcare is

handled by doctors and AI technology is handled by technologists. So we have to really make sure that they connect with each other, they understand each other because uh in the in the in the race

there's a pursuit of you know there's also commercial interests involved and there are a lot of you know things that are u you know angles to it. So it's very important that they understand each

other's perspectives talk a little bit more to each other. uh a tool can only be good as very rightly pointed out by Dr. Morty that the tool can be good if the data is good

&gt;&gt; of course. &gt;&gt; So uh like how do you really ensure that the data that is going is robust enough? Is it enough? Is there any oversight? And that is something that is not being

talked that much development. We are only talking about responsible use mostly in terms of deployment you know after the tool comes for healthcare but we have to talk about the whole life

cycle of the tool at the beginning. How how is it going to be prepared? What is the oversight? Uh and uh how is it quality controlled and optimal? Because this is not about controlling traffic.

This is not the application for our day-to-day other activities. This is talking about human diseases, health, human life which is probably more important than uh you know many of the

other things. So um uh therefore you know in ICMR guidelines ICMR came up with this guideline document in 2023. This is the document. I wanted to show it to everybody because uh there is a

document that came out in 2023 and it talks about the whole life cycle. It talks about development. It talks about the importance of validation. It talks about deployment and it also talks about

that after you know monitoring oversight and translation to make sure that the technology is going to be used for the people. It has given out 10 principles of ethics uh which are related to the

patient autonomy for example or even uh the data sets are going to be used from people. Now what kind of autonomy is being respected or not for them &gt;&gt; or um while the data is being used and

how much it is going to be used how is it going to be uh used for their benefit the benefit beneficence non-maleficence justice which are the first primary principles and then talking about

inclusivity privacy and confidentiality uh transparency explainability accountability and we have to also talk of liability if something happens to a patient in the course of use then how

and who is going to be liable is it the doctor or is it the technologist or how do you really make sure that the humans are not the ones who suffer and how do you maintain the robustness so this

document has actually laid down the architecture for ethics review of AI development and uh where it has suggested some kind of uh safeguards and uh the guardrails have to be slightly

higher And uh then also it talks about the social impact and inclusion because we would like it to be a local uh you know importance so that we understand our own

cultural diversity and our requirements also the to the tool is not for the elite few who are in the urban areas but it has to go to the roots and it has to be affordable. Uh so all these things

have to be seen uh you know as it is developing and uh so guidelines has actually suggested that there has to be an ethics committee that reviews and monitors and has an oversight. Uh but I

really want to urge everybody to have a look at the guideline and see how it can be implemented. &gt;&gt; Thank you. Thank you Dr. Roi. And in fact at this point I would like to also

thank her for inviting Nepali bioeththics delegation to this meeting. Welcome you all. So we discussed about the technology its governance

its ethics and also the commercial interest. But where does it where do they all sit in the legal framework? So I would like to invite uh Dr. Keshinas K Ravish

Shinas who is adjunct professor Nalar and also associate faculty fellow in the center for responsible AI in IIT Madras. So Dr. Ravi uh what are the ethical and professional guidelines for AI adoption

in legal frame and how human in the loop oversight is maintained? What are your view on sovereignity and sovereign AI? &gt;&gt; Good morning. Uh I thank for inviting me for this uh important meeting.

Basically, whatever the profession you are in, ethical frameworks, ethical guidelines are always there. AIA is not going to substitute them. A is going to implement them. Particularly when you

look at AI in the legal field, you all know that the professional responsibility of lawyers and then judges is very huge in the sense that it's a question of life and death in

some many instances and today's newspaper you would have noticed that the Supreme Court judges have also highlighted that how people tend to take things very lightly and then begin

citing uh cases that are not there or imputing judgments which are not there. So fundamentally the problem is very simple. Fundamentally the problem is ethical responsibility cannot be simply

wished away or cannot be washed off by using AI. rather it adds one more layer of your ethical responsibility in in that sense that you have to deal with [clears throat] your professional

responsibility much more categorically much more significant way for example if I'm a lawyer if I'm filing a petition or if I'm using [clears throat] AI to base my arguments in a court I cannot simply

turn around and say that it's a problem with LLM or it's a problem with the software tool it is not rather I should ensure that whatever the thing I do I totally recheck everything,

realign everything with my professional integrity and ethics and then I do it. Then the second problem which we people always think is that AI is the best solution that can take up that can

handle many things efficiently. Yes, efficiently it can do but the ethical responsibility cannot be delegated cannot be wished away. So that is something that cannot be simply ignored.

But the real question is how do we match the ethical responsibility of the professional with the efficiency of the AI. That is where the professional

guidelines come in and every sector is now struggling with it in the sense that legal profession is also struggling with that. That is why you would find that in many countries there are a lot of

updates to the legal ethical professionals guidelines. Courts have come up with that courts have clearly mandated that by using AI you are not abdicating your responsibility rather

you are adding to your responsibility. So the challenge here is that how do we responsibly develop ethical guidelines for different professions including legal professions. How should

judges should use AI? How should advocate should use AI? How should clients first think that by telling that okay sir I mean AIA will take care of anything is not true. So the

responsibility will cannot be delegated accountability cannot be delegated. It is there but then we are also finding ways and means to overcome that in the sense that you know if you are a trained

legal profession if you're a trained legal professional you you know how ethical issues will arise what sort of checklisting you need to do and more importantly you cannot simply say that

my junior did it or it came up just like that because a wasnetting so we need to figure out ways and means to come to gs with ethical issues, professional ethical issues in dealing

with EI. That is an issue where everyone is struggling. Different legal organizations, different codes have come up with specific guidelines, specific in the sense that methodology to how to

overcome some of the hallucinations. How do we understand that citations which are not there have to be rectified and all. So it's a long struggle which all of us know all of us in any profession

will have to deal with. Thank you. Thank you Dr. I think uh it was very clear that uh although the technology is there but uh each profession needs to draw its own boundary line and uh legal is also

not far from that and we need to follow that. Now let us take a divergent go back to the research. You know India is known as a country of three seasons summer winter and rainy. But of late we

are experiencing lot of sudden events like cloud burst clout uh uh landslides flash floods lake busts those kind of stories. So in such a model uh I would like to invite Mr. Sor

Kapil uh he is the co-founder and director of a startup called Biosky. So one of the young guy uh who is in this profession and who is using AI for the modeling. So Sorab my question to you is

how does AI model perform during extreme unprecedented or black swan weather events compared to the historical data we have got? What are the failure models? failure modes of the model and

how these failure modes are communicated to the end user like meteorologist, emergency managers or maybe even farmers. Firstly, thank you so much for having me

here. It's a privilege. Uh and let me first introduce u Biosky Space Innovations. Uh we are based out of IIT Delhi Research Park and we are a space native AI company that is uh solving for

extreme weather events particularly impacting the power and critical infrastructure sector. Uh now a very important question that's asked that how exactly does AI look into

black swan events or extreme weather events. Now if you look at typical standard AI models, they are built on statistical u assumptions that the future would

statistically replicate what has been what has happened in the past. So let's say if there's a cyclone event which probably happened 100 years ago and if our training data set is only for let's

say 50 years or 70 years plus we also know that the quality of the data set is also not up to the mark. So of course when we are looking at at phenomenas like weather etc it's very difficult uh

to predict them with purely just on statistical based models. Now what's asked that how exactly are we looking at extreme weather events or the black swan events. Now the core problem here is

that uh just these statistical models will not work. These models have to be physically has to be physics constrained. Now what that essentially means is that if the statistic does not

resemble the the past trend but the physics is correct that you know in in such a a particular situation in a particular phenomena this is going to be the outcome the physics takes over and

that's where when it comes to extreme weather events physics constraint models have a very high uh accuracy as compared to just a numerical or or a stat statistical based model. Um secondly now

in terms of the failure u modes what u what is what is asked here and I think a lot lot many other panelists also mentioned about the data hallucination so AI training is first of all of course

there's a lot of data hallucination that that happens in addition to that what also happens is the quality of data what we feed into the AI models has to be very very accurate now if you're looking

at the tropical nations or if you're looking at the global south we do not have the right set of satellite right coverage. We do not have the right set of onground sensors. Most of the weather

models are also built uh in mid- latitude countries of course largely western countries. So when we just simply take a take those models and try to impose them on tropical or global

south that's where these models tend to fail and that's where the innovation has to happen where we build on top of those models. We constraint we physics constraint the data sets that we have so

far and then we build our own hyperloized models that are very relevant for for the industry as well as any any stakeholder uh that is that is involved here. So it's really important

to have that quality data and of course then comes the compute and the lag part. These are very heavy models. So of course we need real time uh intelligence for people on ground and all the

stakeholders that are impacted. So of course that is al something which is which is being uh you know being built. So that's why if you would see so far we rely on a probab probabilistic outcome.

You would see that there's a 80% chance that this would happen. Uh nobody so far tells you that 50 mm of rainfall will happen at 125 p.m. Right? Most of the models that u show you the result right

now are purely probabilistic because the data that we have so far is not really uh you know that that evolved. So it's really important that we build on top of these existing uh you know models which

are which are which are there but it has to be very hyper localized. It has to be for the region and we need to in in addition to the statistically statistical based model we also have to

physics constraint them. &gt;&gt; Thank you. Thank you sir. uh so I think uh advice is very clear that we need to work on super localized models also in addition to the large data set which we

have got but one other thing he raised and also one other speaker raised is about hallucination of the data so I would like to come to Dr. Nitu Rajam who is uh from National Law University and

I'll request her maybe you may have to come here uh she's one of the she joined us little late so my question to her is uh Dr. Nitu that in case of faulty legal advice

or hallucination generated by AI who bears the liability the lawyer the firm or AI developer. &gt;&gt; Hello um thank you sir for um asking me this question and uh sorry everybody I

joined a bit late um got stuck in traffic and so on. Um but to um answer your question is not that um simple as we uh think it is. Um and I would like to start from a quote which was just uh

recently delivered by uh justice Nagaratna um while um dismissing a special petition. So she asked this question that artificial intelligence is different thing but natural intelligence

doing things artificially is something which we cannot condone. Because of artificial intelligence the lawyers and judges uh we have an additional duty and responsibility to see whether it is real

orde fakes which are being produced. So uh a special leave petition was in fact filed which is uh often filed only for question of law. Uh and this was filed with fake uh citations and with

paragraphs from fake um you know um uh cases uh and that's when she talked about this. So she has identified although primarily from a legal background, she has very well identified

the uh need for um uh human uh oversight into matters especially when it leads to um any kind of um legal drafting uh legal uh pleadings are being made or if there is any uh communications which are

being um made by the lawyers to their clients. Now um if you specifically look at the um advice which is being given by the um lawyer to uh the clients um one of the thing that we can say that is

that the clients are obviously they do not know what the law is all about. Forget about the citations or whatever. So they completely have this trust on the person whom um you know they are uh

approaching for the sake of um getting the legal advice or uh getting their um uh the plaint being filed uh and therefore there is um a kind of fiduciary relationship that exists

between the um the uh lawyer and the client um which as per the um recent standpoint from the uh Indian judiciary uh it doesn't come under the purview of the um consumer protection act but it

will be seen as um uh uh contract um uh of uh service uh which is being rendered by the uh lawyer to his client. Now uh the question is under such circumstances um we cannot claim any kind of defective

um you know uh liability from the uh lawyer if he is giving a fake advice or if he is giving any kind of wrongful advice which is misguiding the client itself um and therefore as um Dr. Shinas

has uh aptly pointed out uh one has to rely on the uh ethical stands which are taken uh in the field of law particularly the um advocate advocates act and also the ethical guidelines for

the uh lawyers which are present. Uh but the ethical guidelines are also uh focused on um you know how an advice is particularly given and it is not focusing on the idea of technology being

used. uh same uh for the matter of uh IT act as well which talks about any kind of interference with the uh computer technologies as opposed to um using AI to give some kind of a hallucinated um

uh you know version in terms of the advice. So um uh in um 2020 uh3 the last quarter of 2023 uh 24 the American Bar Association has in fact um very thoughtfully drafted um um you know u

guideline which is um known as u the um 512 opinion. uh of course it has um um no validity in terms of compliance but it is just a guideline which talks about the fact that um you know while using

generative AI so they recognize that generative AI can be used and it has to be because that's the future that we are all uh envisioning and also with the summit that um we have uh talked so far

um and so while using those in the legal profession uh it is also important to ensure that um the Um uh the lawyer takes responsibility completely of the legal advice which is being given by uh

examining any kind of advice by scrutinizing any kind of documents which are being um you know presented before the courts not to mention any advice which is also being given to the

clients. And um an interesting um fact about this 512 opinion is also that um in uh if you if you know the practice in um US there there is uh the billing being charged hourly. Uh and so they

brought in this guideline that when you are charging hourly rates for your clients uh it also means that when you are using AI as a tool uh it cuts shorts on your timeline. So uh that means you

can do things faster. So what you need to essentially bill is with regard to the um you know uh the scrutinizing of the document. That means you have to give certain input to assess whether um

you know the uh case laws or whether the citations or whether the legal paragraphs which are being uh drafted um using the legal tech is right or not. So therefore uh by recognizing this even in

the billing uh of the um you know the um lawyers charges they have recognized the fact that that human oversight is essential. Now if you look at the um the uh law firms because pretty much the

practice is no longer solo um and we run law firms which um also tend to have these um AI based tools in place and they also train these uh lawyers sometimes. So if that is the case there

uh exists some kind of a vicarious liability on the part of the um the um firm as far as the uh use of these um tools are concerned. Now um with uh the last point being the AI developers um uh

you know perspective whether they have any liability uh primaca um this is the standpoint which has been taken globally um in most prominent jurisdictions with the case laws that have been coming up

that um uh the uh AI uh developer is not primarily liable for any kind of um you know um hallucinations which are there for the um software the legal tech which is there but at the same time unless uh

there is some kind of a follow-through. Let's say there is a um some kind of a adverse um event which happens following the launch of a particular kind of software and it has not been followed up

by the um you know AI developer. So they have this um you know uh this database which OECD has also promoted uh which um highlights those challenges uh with regard to particular um types of um uh

you know u legal um um u legal tech and if that is being identified through the common database which is being shared. It is also a responsibility of the AI developer to do the patch which is

necessary to in fact overcome those challenges and if at any point they fail to do so then of course liability arises and uh this is the uh standpoint which has also been adopted under the EU act

as well. Yes, &gt;&gt; thank you. &gt;&gt; Thank you Dr. Nitu to say that human oversight is most important and the person who is doing it is the

responsible person for its uses. So coming to the last panelist last but not the least panelist she comes from she's Nup Chadri Dr. Nup Chadri she comes from center of study for law and governance.

So uh when we are talking about legal governance, I would like to talk about specific amendments which you feel are required for the existing Indian laws to address legal gaps on AI accountability

and liability. &gt;&gt; Uh thank you very much for having me here. So uh just very briefly because we have uh very less time. So uh firstly we need to understand what can be the

possible harms what do we need to regulate right the first thing is that of misrepresentation which is essentially the data poisoning as it is known the data is not representative

enough therefore there is misrepresentation so whatever models that you're making and when you deploy those models essentially they will discriminate so the whole the whole

discussion about fairness is about discrimination and what does discrimination mean whether it's uh what You said for example an adverse event or it can if you actually deploy AI in very

critical sectors say health right employment all these decisions it will lead to inaccess right those who could access will not be able to access right so that is the kind of harm that we are

looking at right so we need to therefore then identify sectors like what was mentioned earlier like you mentioned high-risk sectors what are the high-risisk sectors right uh so

Currently the regime that we have uh essentially does not look at sectors right the EU uh AI act does but the digital personal data protection act does not actually identify sectors what

it does is that essentially it gives more responsibility to scale off operations what is called significant data fiduciaries have a higher responsibility but it's not sector

specific so one aspect is to actually look at critical sectors identify them and whether or not the scale of operations is huge the uh responsibility should be the trigger of the

responsibility should be in which sectors the AI is being deployed rather than the scale of operations of the deploying entity. So that's one aspect. Now the in terms of the ethical

standards, it's fairly clear. You have a duty of care and you have a duty of no harm, right? So one would if you look at the AI act the the kind of language it uses is that of a trust, right? So it

seems to establish a duty of care. You have something like a data fiduciary, a data principle, but actually there is no explicit provision which actually establishes a duty of care. you have

this use of this language but there is a uh there is no mention of a duty of care right so that is one aspect the second very important aspect is that for most liability to actually be triggered right

for harm to be understood right you need uh it's a black box right so what you need in this kind of a regime is mandatory public disclosure of information or say you you were to make

a connection it's something like financial you know data developers or AI deployers might say it's confidential information if we give it out we lose uh you know uh

market resource or whatever you want to call it one aspect is that you have to look at AI deployers as something like what SEBI does right it's conf financial information is confidential but you do

have oversight about about it because you understand that the kind of damage it might happen if there is market collusion for example similarly given the kind kind of uh risk of uh you know

uh uh discrimination. we can have uh you know uh public mandatory public information disclosure so that audit can happen right audit cannot happen in a blackbox scenario right so that is

something that we need to think about and these are very specific aspects that we need to move ahead we've decided as a sovereign entity not to have a larger AI act right but once the deployment takes

place we need to develop standards of care for sectors that's that's Just uh one minute I would ask Dr. Morti as a person who developed the governance model for AI in the country. Just one

minute. Yeah. How would you respond? &gt;&gt; Yes. I will take only 30 seconds what she has mentioned about uh while designing the rules of IT rules or IT act. These sectors was not mentioned

high risk or see the sectors are not I will classify as high risk or low risk or medium risk. But the application that you are taking is required to be analyzed whether it is high risk or

medium risk or low risk. Like for example in the healthare &gt;&gt; suppose you have pathological tests already done by some pathology lab and after that you there is an AI app where

you can feed in all that you will find out what is the diagnosis. &gt;&gt; Yeah. &gt;&gt; So that is only medium risk or even low lower low risk I can say that because

anyway you are going to your doctor to consult for that. So these systems healthare but if you are talking about robotic surgery using AI &gt;&gt; then that may be high risk.

&gt;&gt; So the application is high risk but not the sector that is why the sectors are not mentioned in that. &gt;&gt; Yeah thank you Dr. Morti and I would like to invite DG Dr. Sachin Sharma and

Dr. Nitu to please join us first for the group photograph &gt;&gt; and and yeah and we can give a a token of appreciation. Thank you. I request

What are you doing? Thank you everyone. Thank you so much.
