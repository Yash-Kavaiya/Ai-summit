# Building Sovereign Deep Tech for a Resilient Future: Solutions from Finland and India

**India AI Impact Summit 2026 ‚Äî Day 3 (2026-02-18)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 10:30 ‚Äì 11:30 |
| üìç **Venue** | Bharat Mandapam | L1 Meeting Room No. 18 |
| üìÖ **Date** | 2026-02-18 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/NnlmqeOd-CA?feature=share) |

## üé§ Speakers

- Antti Vasara, Foreign Ministry of Finland
- Atte J√§√§skel√§inen, Finnish Innovation Fund Sitra
- Kimmo L√§hdevirta, Ambassador, Finland to India
- Manjunatha Kukkuru, Infosys Center for Emerging Technology Solutions
- Mari Walls, Research Organization Collaboration and Academic Partnerships, CSC IT Centre for Science
- Pasi Toivanen, Nokia
- Petteri Orpo, Government of Finland
- Sethu Saveda Suvanam, Reorbit

## ü§ù Knowledge Partners

- Embassy of Finland

## üìù Summary

This panel discussion explores pioneering deep tech solutions emerging from innovators. Through examples presented by industry leaders, the panelists discuss how AI integrated solutions can enhance competitiveness, security, and societal resilience. The discussion further explores how research and innovative public-private partnerships can strengthen strategic autonomy and create globally competitive AI ecosystems. The event is hosted by the Embassy of Finland in New Delhi, the Finnish Innovation Fund Sitra and the Ministry of Transport and Communication of Finland

## üîë Key Takeaways

1. This panel discussion explores pioneering deep tech solutions emerging from innovators.
2. Through examples presented by industry leaders, the panelists discuss how AI integrated solutions can enhance competitiveness, security, and societal resilience.
3. The discussion further explores how research and innovative public-private partnerships can strengthen strategic autonomy and create globally competitive AI ecosystems.
4. The event is hosted by the Embassy of Finland in New Delhi, the Finnish Innovation Fund Sitra and the Ministry of Transport and Communication of Finland.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/NnlmqeOd-CA/maxresdefault.jpg)](https://youtube.com/live/NnlmqeOd-CA?feature=share)

---

_[‚Üê Back to Day 3 Sessions](../README.md)_


## üìù Transcript

incident. But then I started looking at the numbers and the evidence and um well I don't want to be the one that breaks it to you guys but AI incidents are happening right there's a lot of facts

around that we see AI incidents they're increasing in scale they're increasing in severity they're increasing in frequency right there's in just in the last couple of years we've seen AI

enabled cyber attacks on critical infrastructure we've seen manipulative systems really harming individuals especially vulnerable groups of our society like children and there's even

been claims from some right that some models are portraying are uh demonstrating deceptive behavior refusing to shut down. So AI incidents as I said before are happening. What I

don't think is what we need is another AI incident. What we need is to build crossborder infrastructure to actually prevent this AI incidents to be able to classify them and really analyze

alarming patterns to be able to share incidents when it makes sure it makes sense to do so across jurisdictions and to be able to learn from those AI incidents. So uh before I wrap up, we

actually about two months ago at the future society held a presummit uh event called the Athens roundt. And as part of this event, one of the conversations that we had there was around serious

incident prevention and preparedness. And so I wanted to just kick us off with some of the takeaways of at least how I understood the takeaways from that dialogue and what were some of the

common themes. The first one is around inadequate detection. So uh a lot of people were mentioning that right now our incident monitoring platforms that we do have rely on capturing incidents

actually using AI. So we scrape mostly media stories to understand what has gone wrong and uh that's that's you know it's not a perfect solution. It's a solution towards or at least there's

something but I do think we need more systematic approaches to get there. There are many jurisdictions that are starting to create reporting requirements. There's notably the EUAI

act uh in California where I'm based there's the SB43 but those are still at very early stages and it's to be seen what happens when it comes to enforcement right and from a crossber

perspective we're also thinking there's so many different definitions of AI incidents within those again serious incidents critically safe incidents you call it what it is that it's there's an

issue definitely arising around interpability then the second topic is really one that relates to accountability. When we think about AI incidents and because the value chain is

so complex, it's so global, it's very hard to assign responsibility. At least we haven't figured out how to build that liability regime yet. And I I should probably mention here a lot of times

when we talk I probably just made that mistake. We say AI is posing the risks, right? And I think it's an important reminder to keep in mind that it's individuals behind this AI systems that

are developing and deploying the actual technology. And it's individuals that are enabling I guess the system to happen without having adequate checks and balances or without creating new

incentive structures. And the third point uh after accountability that comes up is around preparedness. I think that's a buzzword that we keep on hearing over and over at least in many

conversations that I've been part of in the past year uh when we're thinking about response capacity specifically to to incidents. And I think this is a space that we just need to do a lot more

um have a lot more coordination and progress on. So obviously if you add to all these elements the cross border border layer knowing that AI is really uh realized as I said before in a global

supply chain and everything is so interconnected it's very easy to see how much we need crossber incident infrastructure. So I'm going to pose some questions and then invite the

panelist and our moderator my colleague Kaio to uh to join us on stage and I'll pose the questions and hopefully the panelists can give some answers to those questions but the first one is around

how how as an international community can we come together can we better organize can we better act to actually have AI incidents serve as that wakeup call for policy makers to have stronger

governance mechanisms and then related to that what are the actual technical and polic policy components needed for this incident infrastructure is that there's already some shared reporting

taxonomies. How do we scale those tonomies? How do we build information sharing protocol before be uh between jurisdictions? How do we make sure there's robust data governance

structures in place? And the other part of the question, what are the right institutions or what are the appropriate actors I should say that should take a role in this? The last question I want

to leave the with is how do you link AI incidents to red lines? There's a lot of work that the future society does on that and we often think how do incidents serve as those early warning signals

that tell us when we're reaching a certain risk threshold at least or a dangerous territory. So how do we make that connection and feedback loop? So with that I will invite uh our moderator

for the day Kyle Machado to the floor. he will be uh he will be moderating uh what we have in panel really full of AI experts when we come to AI incidents. So please let's give a round of applause

and let's get started. &gt;&gt; Good morning. Hello. Hello. &gt;&gt; Good morning everyone. So I'll invite our speakers to come up. I'll introduce you in a second. I've also been asked by

the organizers first to start off with a picture. So instead of a picture at the end, we'll already start with one. Nikki, we'll need we'll need you for that. Uh

oh, there's our photographer. So should we sit? Should we stand? &gt;&gt; Yeah. So, a quick a quick pause for &gt;&gt; we've got the we've got the picture. We can go now. Um, I'll introduce our

speakers today. So I'll be very brief and I'll uh if there are any points from your experiences that you would like to bring up during your speech, please do so. Um so I'll start with Elhim Tabasi

who's director at of AI and emerging technology initi initiative at and senior fellow at the Brookings Institution and former chief AI adviser at US NIST followed by Miss Ako Murakami

who's the executive director director of the Japan AI safety institute followed by my um fellow Brazilian Ugo Valadaris who is from the ministry of science and

technology. He is the director of the department of science and technology and digital innovation. And finally, Michael Grabelnik who's the AI champion for Slovenia and an AI researcher who

co-chairs the AI incident working group uh at the OECD. So, good morning to you. It's very nice to have you. Thank you once again uh coming all the way to India for us to discuss. Um I'll start

with Marco at the at the other end. Um you have been working with guard. &gt;&gt; Absolutely. &gt;&gt; Let's go. &gt;&gt; Um

so you have been working a lot uh in with the OECD incident group and really advancing u the discussions around definitions and and working to establish um this cooperation across mult multiple

nationalities. And I'm interested from your global vantage point, what are the gaps we have in developing incident monitoring right now? &gt;&gt; Well, obviously uh uh obviously the

major gap is uh sensing the the incidents, right? So at the moment we have just mainstream news more or less, right? Not even social media because social media got locked, right, a few

years ago. Uh so we we have mainstream media. which means that we detect pretty much everything what kind of spills over which which which is significant enough so that it's

news worthy as journalists would say right uh which mostly it's okay right mostly it's okay uh but um it's not systematic right um so this is one um major gap right on the sensing part now

[clears throat] there are gaps along the the whole pipeline right I would uh say so at the moment we collect this data, we organize it, we uh structure it, we classify it. So we have all these

nice OECD schemas um and you can search across this and so on. We can count this so we get statistics and so on. All fine but you know uh uh what Nikki was mentioning before right the feedback

loop right is is missing right. So the feedback loop AC uh back to the policy makers at the moment policy makers can okay see the incidents but then use pretty much imagination uh what to do

right uh so one extension what we are working on at OCD right now is uh how to provide a little bit more proactive um um recommendations mitigations um uh policies ahead of time, right? So

a little bit of this landscape around particular incidents so that policy makers would at least uh uh get ideas right maybe slightly out of box ideas uh how what to do how to do

and so on you know policy makers are slow okay fine uh that's why let's say we would speed up so this I would say two major gaps right so sensing now we have mainstream media social

media I'm not sure if would be useful anyway. Uh it might be that in the future we'll get this obligatory reporting from the companies. Not that companies would be extremely

incentivized to to report their failures, right? Uh uh but okay, this may happen. Um and the second gap is uh [snorts] closing the loop, right? As the title of the

panel. &gt;&gt; All right, weaving in the title of the panel. Thank you. Thank you. Um, and just a quick followup, have you seen while working with uh, well, I

understand the internet monitoring has global coverage and we're seeing stuff all over the world. Uh, this is a great point that we're tied to that lens of what's media reportable, what's

interesting and so on. But are there any particular capabilities or type of incidents that are showing up uh, that are drawing more attention? Yeah. So &gt;&gt; [clears throat]

&gt;&gt; uh well this would be maybe five 10 minutes lecture right so that I would show these graphs but somehow we we cluster the incidents into I think 14 groups or something um and some some

types of incidents go up some go down and some are more like this incidental like I don't know uh incidental ones would be elections right so just before elections you would get spikes right so

and then goes down and and then you get spiked later on as well, right? Then um let's say an interesting one is autonomous cars accidents, right? So this used to be extremely well reported

a while ago, right? Maybe 10 years ago, you know, it was one accident, right? And everybody was writing about this one accident. Now nobody cares anymore because there are just too many uh

accidents and cars and people got used to it, right? Um so this went down, right? Uh and then there are a couple of types of accident incidents which kind of go up uh especially yeah these fakes

now if you if you look how AI evolved in the last maybe just six months you don't need to go much further right three six months uh we got couple of crazy good tools like nano banana and couple of

others right suddenly you can make uh you can make deep fakes for talking here on the mic right I could produce extremely well um extremely good deep fakes for instance, right? The cost

is close to zero, right? U so these are these would be like three types, I would need to show the u the graph maybe just the last sentence, right? Uh since we have this OECD taxonomy of

incidents u and hazards and hazards, right? So there's one type of incidents which uh didn't happen yet right l thanks god right catastrophic incidents right we we

left the space for this possibly catastrophic ones right so seems like AI doomers are begging for one right because every day you hear oh well it will happen it will happen didn't happen

yet right didn't happen yet but uh it may it may you know some people will be happy uh because they invested so much energy in scaring people It's obviously, right? You know the

cartoon Monster Inc., right? We scare because we care. So, this is the the AI doomers roughly are of that kind, right? I hope I didn't insult anybody here. &gt;&gt; It's that pleasure of saying I told you

so. Right. I told you so. I didn't want it to happen, but I told you so. But I I'll I'll take a point you you mentioned which is um how in certain incidents in the beginning it happened

once and it generates outcry and then people get used to it. So there is a diff difference between the public perception of a risk and a harm and eventually bringing a taxonomy and

working with that. So I'm using this as a hook to to come over to Elam and oh Elam you have as we mentioned experience as the chief AI adviser at MIT which I understand uh had a lot of work on risk

management framework and I'm interested in hearing uh what are the limitations and the challenges of the framework especially when you have real life incidents crossber incidents um have you

been faced with situations where the taxonomies are challenged. Um, yeah, if you could elaborate a little bit. &gt;&gt; Um, first of all, thank you very much for having me here. It's great to be in

this panel. Uh, Marco, I hope that scenario never happens because we are all working towards making sure that we have a sort of responsible trustworthy AI. I just want to start by saying that

uh we should take it seriously that AI doesn't happen to us and it's all on all of us and all of the efforts that we do to make sure that it it progress in a safe way. Uh going back to your question

and u I don't know your question was around a MF or just current governance and risk management practices but I'll try to address both of them. I think um uh the the first of all I I should say

that the technology is moving much faster than any of these frameworks and policy and standards can keep up with them. uh but uh some of these mismatches that I see is uh on the two dimension of

temporal and jurisdictional that the jurisdictional one has been already um mentioned. So uh what they mean by temporal is that majority of the uh risk management processes governance

processes that we have uh they uh they are heavily on uh study for the pre-eployment which is also necessary but they assume that the risk and incident can be identified pre-eployment

and can be managed uh within certain u um national organizational uh border and jurisdictions. Um I want to say that IRMF is maybe a little bit exception to

that that it it emphasizes post- deployment monitoring, continual monitoring. Uh but the problem is that uh uh we don't quite know how to do the evaluations in the right way. But going

back to again incident reporting is that uh the problem we uh emphasis emphasize in the uh pre-eployment is that the majority of the incident or serious incidents that we have to be worry about

and uh report them and have a way of uh dealing with them uh are those that happening post deployment and there are crossber there are emergent uh and many of these pre-eployment testing that are

based on red teaming benchmarking um uh thread modeling analysis are important and necessary But they cannot reliably predict how the models are going to behave and function

in real messy u uh context of the use that they haven't been really seen uh before uh being deployed there. Uh where the um uh it's open-ended use um uh you have cascaded interactions and many

other things that cannot be uh really thought of during the uh pre-eployment. Um uh and uh compounding all of these challenges is that uh as we all uh reading, hearing and learning that uh

particularly agents uh they can differentiate between the operational and the uh development or testing environment uh so they can behave completely differently in this

environment which is quite a challenge on how to design a test to for the behaviors and for the functions that that that we don't know. Um so um uh to kind of summarize that the real

incidents that we want to follow and and um not follow but kind of we watch for them um they they they are emerging you know their failures emerges when the AI is in uh use at scale you know a lot of

the misuses we only see them when they are at scale uh and um and also some non-AI aspects of this you know system integrations cascading interactions that

you don't see them and you cannot really design for scenario testing before that. Uh on top of that the uh the issue of the discovery has been mentioned the we uh we don't have a systemic way of

monitoring the systems to figure that out. We are relying on the external uh researchers and media to report that. So there's a lot of blind spot there too. uh then there is this um uh cross

jurisdiction uh problem that AI systems can be trained in one jurisdiction, hosted uh used uh deployed uh in other jurisdictions and uh uh so the visibility to what happens across the

different stages of the AI life cycle uh may not be readily available for for anybody to look at that. So it causes uh it causes some challenges there. The issue of the uh you know when incident

Then the um um structural weaknesses start showing up like as it was mentioned not having a shared uh taxonomy and definitions for the reporting not having a um uniform

threshold for where an incident uh should be reported across the different jurisdictions. Um so I think all of them uh uh makes the current structure having blind spots that um we should be worry

about uh because we don't want the serious incidents go undetected. Um so I just end by saying that um assurance centric uh governance risk managements are really important and we should

pursue that but maybe and well not maybe as as we talked about it in AI risk management uh we should also augment that with post-eployment monitoring with um maybe resilience uh centric

governance after the post- deployment process. &gt;&gt; Yeah, that's very interesting and this dual duality what we can assume pre-eployment and what we can do post

deployment. I'm thinking here even post uh decommissioning right you release AI systems in the world they operate then a company stops stops existing you turn it off um but the model might be out there

variations of the model so you you &gt;&gt; no I just want to say that what does the decommissioning mean how do we know that we have actually took everything off the system so

&gt;&gt; amazing um Kiko I'd like to move on to you Um, you're as the head of the Jet. &gt;&gt; Can you Yeah. Sorry. I tend to project my voice so I forget if the microphone is on or off, but you can all hear me at

the back. All good. Great. Um, so Ako, as the head of the Japan AIC Institute and now operating in a global network of where you're you're getting colleagues from all over the world. Um but it seems

to me that each safety institute has a variation of mandates of governance designs. Some try to be independent, other within a ministry of science and technology, others are

multi-ministerial. Um I'm curious to your views as the AI safety institute's capability of uh being the place to monitor incidents and promote preparedness at a global scale.

&gt;&gt; Thank you for the question and thank you for having me today. So uh my opinion is that the as you mentioned that the uh the the definition of the AI safety institute in the each country are the

totally different. So for instance the in Japan the uh we are being the hub of the information about the AI safety inside and outside Japan and also that we are now expanding our mission to not

only for the information hub but also the having the capability to examine the AI models and systems by ourselves. But they uh the the some of the AI institute have the same kind of the capability but

the uh the but the the another type of the institute do not have the capability. So it depend on the the company country situation and also the the each company has the uh the the not

all of the AI safety institute or the the relating to the institute having the the skill set of that examination of the the models or the monitoring the system and but the our the network's mission is

that they not only for the not the for the the doing the monitoring by ourself but also the discussing what's the kind of the benchmark or the what kind of the red line for the AI safety in the world.

So because of the um the talking about the air safety is sometimes very complicated uh the apart from the national security so that's why that we are focusing on

the technical issues and uh the what kind of the the and do not want to uh talking about the ethic or human rights by ourselves. So that's the so many uh the organization talking about the AI

ethics or the human rights like the OECD or the GPI are doing that kind of the discussion and the the thanks for the such kind of the discussion the AI safety institute network focusing on the

technical issues how to the operate operate the such kind of the uh benchmark the realizing they we can't uh the understanding the this uh AI is a safety or not. So that's the uh most

important part of the mission of the AI safety institute network. I do believe so. &gt;&gt; Amazing. Thank you very much. Um I'm going to move on to Google and then

we'll do a second round and uh the second round if you guys want to comment on the questions as well. um Google uh from the perspective of a country that's building its governance framework um and

will have to interact with the space which Kiko was mentioning uh and each country has its own design its own governance framework. How do you see first of all nationally uh what how will

Brazil may be prepared because we don't know if the law is going to be approved uh to respond to incidents and how will Brazil be prepared to dialogue with other countries in responding to AI

incidents. So first of all, thank you very much, Kaio, my friend, Brazilian. [laughter] It's very nice to be here and uh to discuss this very important subject. Um

first of all, I I I believe it's important to mention uh how Brazil is dealing with the subject of AI. Uh since 2024, we launched uh the Brazilian plan of

artificial intelligence. And this plan uh presents five axis and in this axis we have 54 actions uh to discuss all the dimensions related to AI. Of course, every day you have to put another action

because uh it's impossible to to forecast what is going to happen tomorrow. And uh but now uh I'm sure that uh first of all we were thinking about uh three great uh subjects uh

human resources uh sovereignty of the data and hardware supercomputing. But now we are very sure that we have uh the fourth uh uh uh dimension I mean

cyber security because it's not possible to do anything without security in this sense uh we are working uh in all that dimensions together because you have to do everything simultaneously and you

know this year is national elections president Lula President Lul beu here the day after tomorrow and uh he's very concerned about that because the fake news and the the generative AI I like

very much generative AI but [sighs] it's dangerous sometimes so we also concerned and uh but we are doing a lot of stuff uh and of course it's not regarding uh the fake news and something like that uh

some some months ago we have a fin an unpreced attack in our uh system. Uh Dr. Ka knows PS and probably you know what I mean. Uh three attacks and we have a very great produce and uh that time uh

we are uh in investing actually in many of uh the the dimensions I I told to you. So first of all um we are buying a system actually uh uh 100 million he eyes to investigate for our researchers

uh about uh we need to take care our childhood that our children uh the minors especially in the internet environment. So we are investing a lot of money to re

research about that. We also uh uh joined with the ministry actually is the cabinet of institutional security. Uh we are creating uh sensors to detect the attack previously and this is a very big

um uh project we are doing with the colleagues with some colleagues for Spain and I mean usually the attacks we we not prevent but we act after they happen. So we are trying to create a

system to uh solve this kind of problem before and uh but I mean the main the main the main subject here in our opinion is international cooperation. We have to do

everything together. It's not possible to a country to do everything by themselves by him itself. So what's the main gap in my opinion? Not in my opinion, Brazilian opinion, the

government opinion, the political process, the bureaucracy is very slow compared uh to the technology speed. So we have to do a lot of things yesterday for

tomorrow and sometimes we spend years with dialogues but that's the word and we have to do something. So this very moment we are working to uh expand our

cooperations with the countries Brazil has has a lot of friends country friends in the world. So we are we are working on this kind of uh stuff and we are very happy. I believe we are going to to find

a very good uh solutions to to mitigate this kind of uh attack and this is the the current word we have to work on that &gt;&gt; amazing thank you Google thank you very much and I I'll I'll

move to El and exactly on this point of international cooperation that Ugo was mentioning and which is so critical oh just as a parenthesis Google mentioned pix which is for context it's a

Brazilian payment system uh developed by the central bank public and just today bricks announced uh bricks payment system so Brazil, India, China, South Africa um and other countries that join

later. So this is getting pretty big and having such a payment system involving all blocks uh being attacked with the use of AI is a great risk to many nations and is again the issue of

international cooperation. So, Elhub uh when incidents do occur and I hope it won't happen to the payment system or at least when I have debt maybe then it could happen.

What should a well functioning u AI incident reporting pipeline look like? Uh what are critical or at least minimal elements that we should have in all legis jurisdictions that Brazil could

incorporate for example? Yeah, that's a very good question. So, uh when I think about this and u you know learning from our panelists and the point that they made uh I think uh three three things

comes to my mind. Uh one is that uh a a monitoring system and incident reporting needs to uh generate uh outputs and results and technical facts uh that are relevant for uh decision- making and

decision makers. Uh so having that uh that lens on this then the second thing that it needs to be um I think um lightweight enough that uh it can be adopted and used or not heavyweight that

prevents uh using this uh and then the last thing is that some sort of a standardized uh reporting uh that uh and if you do that right um it is not um kind of tied to one jurisdiction so it's

flexible as well as other jurisdictions as the different rules and regulations uh the reporting can be applied and be helpful for for those uh other regulations. So what we need to get

there is uh it was mentioned Marco mentioned that we need some sort of a you know beyond sh understanding but more sort of a standardized u definitions and taxonomy for uh what is

incident and that's what the work that OECD did uh with uh the definitions of incidents uh and accidents and all that and uh uh I was privileged to be part of that work and uh uh there are other work

too that you know for example US AI uh uh incident databases or MIT has series of work. So how to bring all of them together in a more standardized way. Um the second thing is that make sure that

our approaches I'm a big fan of risk based outcome based approaches that prevents us to from getting uh too prescriptive. Um uh so an outcome based approach that

uh uh uh triggering for when an incident should be reported is based on the severity of harm uh that uh uh uh that can happen can be real realized if an incident happens or is very likely to

realized versus the system categories uh or model capabilities because those definitions can change over time and can change over jurisdictions. Um uh it has been mentioned and I mentioned

that too. I I'll say it again that we need have to have some sort of a standardized reporting uh that's flexible enough for different jurisdictions but beyond that also

allows for mechanisms for sharing those information. Not all of the informations about the incidents uh can be in public. So then we need to think about uh ways that the nations can share those

informations and also think about some sort of a perhaps uh uh stage timing that when incident is reported something happens quickly um but make sure that we follow it up with enough technical

information to understand that to make sure that it doesn't happen again. Uh and the last thing I will add is uh uh make sure that there is in there is enough incentives for the people and the

uh entities to actually actually do this. Uh you know uh there's usually they compare AI incidents with cyber security incident reporting that is you know that has have a lot of benefits but

I think there's also some some sort of a subtle differences that when you when there is a cyber security incident reporting it's often at the same time or very quickly followed by a patch. So so

they can take the credit for solving it with AI is a lot more difficult. you just report that something went gone uh gone wrong and there's a lot of you know reputations and other things to worry

about. So how to find the right incentives uh so that they uh report that and don't cut corners in reporting. &gt;&gt; Thank you very much. Um Marco, I want to loop you in in in this conversation as

well because I think you you'll have thoughts on what sort of information we should be sharing when it comes to cross jurisdictional coordination. What what would be the bare minimum

again that we we'd need to think of thresholds to respond uh disclosure sorry threshold to trigger international cooperation uh what should companies disclose to what parties do you have any

take on that &gt;&gt; yeah it's a complicated question right because uh it's a lot of consensus which uh needs to be or many parties which need to be aligned uh if you go um cross

jurisdiction, cross country, cross uh culture if you want um uh even cross technology, cross business. Right? So there many crosses here right now um there several sub questions which you

asked u well what would be useful uh in terms of level of details right level of details would be so we would need to know why the accident happens. So at the moment we we don't have causality

any notion of causality in this um incident uh detection analysis reporting and then feedback loop right so the concept of causality needs to be uh brought in and then this would allow

actually the the policy feedback loop this is where what I mentioned at the end of my first intervention right so causality right then The next thing which is extremely important. So AI as

we knew like two years ago or one year ago or today or the AI which we'll experience in one year from now is a different AIS right similar but fairly different uh these days I don't know we

have this getting we are getting there to to have this complex agentic structures right this is so much different compared to you know some small poor LLM where you drop in little

bit of text and you get in response something and maybe ah it was insulting but then people was saying oh this is really ugly and so on now we have no clue anymore where where things happen

right where things happen why they happen and so on so it's getting way more complicated what Adam was mentioning the real time aspect is also right so [clears throat] mitigation

too late is better not even to mitigate right almost uh So uh reaction reaction time right uh this is an important one. So sharing um yeah so there there's one important element right so these AI

products generally they are coming from companies right these companies usually would like to keep the secret right business secrets and so on now sharing all the details they don't they're not

really in incentivized to explain huh this happened because we are using that component and this component well it was so much cheaper so that's why we introduced it but it's a little bit

buggy Okay, some troubles came out but generally it runs fine. This this is very usual response. Uh how to convince them? Yeah, maybe by policy makers and so on but unclear. Uh certainly

incentives are not there at the moment. uh uh and just uh to conclude right so cross culture cross country cross jurisdictional um there's no AI which would run hardly any AI which runs in a

single country okay maybe US China a little bit different but generally AI runs all over right so and why because it's well everybody's developing this either

agents or something else or uh because clouds are everywhere tons of things right um how to share unclear I mean of course it's very easy to talk about this but in a practical

implementation setting I'm not sure if this will uh be implemented in an easy way in the foreseeable future likely not maybe we will get some samples but systematically you No, likely not.

Currently at this OCD AI uh monitor, right? AIM as we call it. Uh we can go to OCD.ai/inccidents, right? So this is the portal. Um so per day we have few tens of few tens of

incidents right of different kinds. Um and uh across all the languages, cultures and so on. of course mostly on the western sides of the world uh but still have about eight minutes so finer

questions are to a kiko and and then I'll ask you to keep 44 minutes uh as long as we're talk we're discussing this AI incident reporting life cycle right with what we need to

report what are challenges. I know Japan released um an incident response playbook recently. I couldn't read the kanji but I was very interested in. So maybe you can share a little bit of

about that with us and also discuss a little bit of how your experience with collaborating with the companies in private and uh public partnerships uh have work to build trust and share this

responsibility. &gt;&gt; Thank you. Thank you so much. So the the instant is the as you mentioned that the technology is changing very rapidly. So that's why the if we release the the

concrete the response about the each incident is that the the book is the being the old as soon as possible. So that's why the our the guide book is not only the not for the the each incident

uh the the treatment but also the framework of the how to they react to the incident itself. So the framework is not changing rapidly because of the um the uh organization how to the

monitoring the periodically the uh freshness data fresh or something like that is the keeping in the the very long time. So that's why we released the the very framework of the how to react to

the accident. So that's why the uh the our and also that we had now the repressing the very frequently because of that this is a very uh living document. So that's why the uh even if

we do not have the concrete the incident react uh appointment but the uh the just the releasing the framework but the the we change the periodically uh of the document uh keep the living document. So

that's the uh the the one of the topic and also the you may they ask me about the how to the collaborate with the private sector to the how to keep the such kind of the incident reaction. So

the the in Japan AI safety institute have the the working group for the the realistic the uh problem uh for the the we have the two types of the working group. The one is the vertical uh the

working group for the each industry sector that we started from the healthcare and robotics and we are going to expand the the vertical layer for the each industry like the financial sector

or uh the other kind of the industry and the another types of the working group is that the horizontal one so the the horizontal means that the it's a common layer of the AI safety like the uh data

quality or the uh how to inspect the AI model or something like that. So that's two the types of the working group is focusing on the how to the real reaction for the AI uh incident. However, the

it's not the doing uh by ourselves the government or the AI safety institute but so that's why they we uh the uh collaborate with the private sector uh starting from the domestic Japanese

domestic company but we would like to expand our activity to the internationality. &gt;&gt; Amazing. Thank you very much. And finally Uber to to bring us home. Um

&gt;&gt; let's go. Um, I'll ask two questions actually. One, one is more of a gossip I've heard and maybe you can share some with us. I hear Brazil has been working on a transparency institute.

&gt;&gt; I don't know if there's anything that could be shared there, but just throwing it out there uh which would be potentially working with the uh AI safety institutes. But my my main

question and my true question is I'm very interested in in understanding how Brazil Brazil sees the possibility of working with other middle powers uh to respond to incidents and and well to

monitor and respond to incidents. Um so the last to talk last to speak is easier because all the intelligent people already said so it's easier for me the the difficult questions was

answered so thank you for my colleagues but I mean uh everyone in here uh we are we have a a very good convergence about what we should do and uh first of all regarding what Brazil is doing. We we

already started to talk to other countries especially in South America and Latin America uh because of course we have uh how can I say similar problems you know

so we have to do especially of course we are right now you are right we are creating our AI safe institute and I it's my pleasure to announce that professor Vagner who is here he is from

uh federal university of Mina he is one of the coordinators of the proposal and please don't leave uh Akiko without talk with Dr. mirror [laughter] we must to change many uh uh ideas in

here and this is a very important place to do that. So I believe in a few months two or 3 months maximum our institute is a public institute but without uh government interference. This is uh a

multis sectoral uh uh participation of course academia uh industry big tax everything everybody must be there because we have to dialogue with everyone and uh understand the the

concerns of each one. So uh as I said Kaio to finish we have just two minutes u I mean we are we have at this very moment manyus signed with many other countries of course the there are

countries that are in the state-of-the-art in the current uh way to to mitigate the problems with with this but there are a lot of of countries that has nothing no infrastructure no

people to work in the subject. So we are all the time trying to work together especially with our neighbors. And then uh finally I mean I think uh the most the the biggest problem is the time of

the politics the bureaucracy. Uh sometimes we'd like to to do something in a very short time but we are unable. So what we are doing first solving the Brazilian problems working together and

discussing with the world and our presence here Dr. Mero came uh next next meeting is about AI safety in England I think yeah Dr. came to discuss with the the the the

colleagues from England. What can we do? Of course, we have a a a main line. We know almost the the what we should do, but of course to to change ideas to exchange ideas is absolutely important

in in this kind of subject. So I I think we are in the right way and uh listen my colleagues I believe we think similarly. Thank you. &gt;&gt; Amazing. And with exactly a minute to

wrap up I'd like to thank you all the speakers. Uh we managed to what's the lay of the land? What can we detect right now with instruments such as incident u the incident monitoring at

the OECD? What are the challenges in terms of what information do we need to acquire? What are the biases biases of the information we have? And we discussed the whole life cycle of

incident monitoring. Uh what are the pipelines? What are the thresholds we should have? what we have, what are the next steps and bringing this infrastructure to beyond countries that

have already established AI safety institutes but to other nations who will also have to are already dealing with AI and the full response will uh incident response monitoring preparedness will

only happen once we have buyin from everyone in a shared infrastructure otherwise our problems are just going to go forum shopping they're going to skip around and they're going to persist and

exactly at zero. I thank you all for attending. Big round of applause. [cheering] [applause] It's amazing.
