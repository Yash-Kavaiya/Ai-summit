# Future of Fair Tech: Addressing Equity, Safety and Accountability in a Rapidly Evolving AI Landscape

**India AI Impact Summit 2026 ‚Äî Day 3 (2026-02-18)**

---

## üìå Session Details

| | |
|---|---|
| ‚è∞ **Time** | 09:30 ‚Äì 10:30 |
| üìç **Venue** | Bharat Mandapam | West Wing Room 4 B |
| üìÖ **Date** | 2026-02-18 |
| üé• **Video** | [‚ñ∂Ô∏è Watch on YouTube](https://youtube.com/live/-Ayuqiwf0Mg?feature=share) |

## üé§ Speakers

- Damar Juniarto, KONDISI and PIKAT Demokrasi
- Lipika Kapoor, NABU Sciences | MIT
- Niki Iliadis, The Future Society
- Pauline Charazac, Center for AI Safety (CeSIA)
- Tess Buckley, techUK
- Vidhi Sharma, Future Shift Labs Foundation

## ü§ù Knowledge Partners

- Future Shift Labs Foundation

## üìù Summary

This session will examine how India can translate fairness, safety, and accountability in AI governance as AI systems expand across public service delivery, platform regulation, and economic decision making. The discussion will focus on accountability frameworks for high impact AI, gaps in safety standards and enforcement, and structural inclusion challenges affecting women, rural populations, and marginalized communities.

## üîë Key Takeaways

1. This session will examine how India can translate fairness, safety, and accountability in AI governance as AI systems expand across public service delivery, platform regulation, and economic decision making.
2. The discussion will focus on accountability frameworks for high impact AI, gaps in safety standards and enforcement, and structural inclusion challenges affecting women, rural populations, and marginalized communities.

## üì∫ Video

[![Watch on YouTube](https://img.youtube.com/vi/-Ayuqiwf0Mg/maxresdefault.jpg)](https://youtube.com/live/-Ayuqiwf0Mg?feature=share)

---

_[‚Üê Back to Day 3 Sessions](../README.md)_


## üìù Transcript

jurisdictions, sectors, and power asymmetries. Today's discussion brings together perspectives from across regions and regulatory traditions to examine what works, where gaps exist,

and how cooperation can be made credible. It is my pleasure to introduce our panel. We're going to start with our lovely moderator, Miss Vidhi Sharma. She is a

digital policy and international affairs professional. She's also the head of responsible AI at Future Shift Labs. Can we give her a round of applause? [applause]

Joining us as our distinguished panelist is Nikki Elidius, director global AI governance, the future society. Nikki oversees policy engagement with intergovernmental bodies including the

OECD, the UN and UNESCO. Can we give her a round of applause and welcome here today? [applause] We have with us Pauline Charizac who is the head of policy engagement CESAI

France. She leads efforts to advance responsible AI policym and adoption. Welcome Pauline. Furthermore, we are joined by Damar Juan Juanerta,

co-founder condi and pitcat democracy. Damar is a prominent voice in digital democracy, AI, media integrity with a strong record of shaping national and international policy. Can we give him a

big round of applause? [applause] We are also joined by Lipika Kapoor who is the co-founder of Nabu Sciences where she focuses on advancing AI transformation and helping organizations

build human centered AI capabilities. And with that firstly let's give her another round of applause. And with that we'll start our session over to you Vidhi.

&gt;&gt; Thank you Anaya for that kind introduction. While uh you were speaking about a misty morning, I think uh I'm going to do something different today. We usually end sessions with gratitude

and thank you. But I'm going to start with a gratitude note for all of you uh being here today. Thank you for being here for this important conversation which is not about compute only but

about future of fair and our presence today on day three early in the morning gives us hope about it. &gt;&gt; [snorts] &gt;&gt; Um

I I'll set aside slight context and then we'll dive straight into questions with our wonderful panel that we have today. So a lot of conversations about responsible AI or fair tech still

operate at the level of principles. We talk about fairness, safety, accountability as if they are just universal values that simply need agreement. But I feel the reality is

slightly messier. These ideas are being translated into systems, standards and incentives in real time and outcomes are uneven. So today with this powerful panel we will focus on two main

characters of the story. One is some shared principles and values and second character is infrastructure and what happens when they meet. uh there are you know three tensions

globally uh that are present today and today we are here to discuss that the distribution of AI capability is unequal and that shapes who gets to define what fairness or safety even mean and

accountability frameworks are emerging but they remain fragmented often reactive and difficult to operationalize across borders third trust and trust is not abstract it is experience at the

point where AI meets whether in welf welfare systems, loan approvals, media ecosystems or enterprise tools. So rather than asking what fair tech should look like, I think we have established

that enough in last few years. Uh this panel asks more practical questions. What is actually working? What is breaking down? And what where are we still relying on assumptions instead of

reality or evidence? So I want to begin with you Pauline. We often assume that fairness and safety are technical challenges. But in practice they seem to be shaped by markets, institutions and

infrastructure. So from where you sit, what is the biggest factor? What are the different factors today that determine whether AI systems end up being fair in practice or not?

&gt;&gt; Thank you so much VD and uh as you said uh thanks a lot for having us today in Delhi. It's wonderful to be back after the wonderful G20 uh in 2023 under Indian presidency and it's a pleasure to

be part of this wonderful panel especially with an exciting first question and we're getting really deep into the topic right at the beginning. Um as you mentioned uh fairness is a

technical topic but it's not only a technical topic. It's uh about first accessibility and second about safety. And maybe to kickstart the conversation I'd like to uh put some facts and

figures so we share a common understanding of the matter. Um on accessibility, I think we can uh refer uh to the wonderful uh 2026 international AI safety report that was

just launched a few days ago. And if you look at uh one of the one of the graph and figures being um being highlighted in the report, you can see that accessibility to AI systems is not share

equally around the world. you will see that the countries who are building the technologies are the one benefiting from the technologies and this is not what something we could call fair. On the

contrary um many emerging economies uh be it in Asia, Africa, Latin America have an adoption rate very low um around 10% for most of those country amongst the working population. So it says a lot

of thing about what is the reality as you say from a pragmatic um perspective uh on the safety aspect and again this report is showing that the risk are rising. So we need to have a proactive

approach to tackle this issue. So overall um and being very realistic about this, I do not see a world where tech is safe without uh the tech being uh fair, accessible and uh included in

some framework where accountability is very important. You also mention a word that is I think at the basis of what we are discussing today is trust. We need to have trust in the AI systems. We need

to have trust in the providers. We need to have trust in the governments that are regulating those uh those systems because um this is not a simple tech evolution. This is really a structural

change for our economies, for our societies and for the way we live, work and think. So this is a really uh over technical matter. This is almost an existential matter.

&gt;&gt; Absolutely. Thank you. And I'm going to come back to you on the proactive approach uh that you mentioned but I'll jump to Lipika first on the matter of trust as you mentioned Lipika. Last

night I was reading your LinkedIn post which said what we are dealing with today is not a capability problem anymore. It is a trust problem. People don't hesitate use to use AI because

it's weak. They hesitate because they haven't built the judgment to know when to trust it and when not to. And you further went ahead and mentioned it's the same problem everywhere. So where do

you think that trust actually breaks first in real world is it institution developers government or our citizens or users have not uh yet developed the judgment muscle.

&gt;&gt; Thanks uh thanks so much firstly for having me here. It's a great pleasure to be uh sitting here with this panel. So um I think as you mentioned trust right I think

It does flow down through all the various levels from you know whether it's where it's getting built at the developer level and uh when you know your skilled talent are trying to deploy

it um and versus you know the last level where citizens are actually trying to use it. I would say on on judgment essentially the problem becomes that you are engaging with the AI. The question

becomes how are you really learning AI right and uh from what we have observed we've done like bunch of workshops with uh different groups of people including um executives governments students and

what we have found that people actually learn AI in four stages stage one is where you're using AI as an um advanced Google search you ask questions you get answers and then as you do that you move

to stage too and now you are actually using AI to create outputs you are using it to summarize your emails create some pictures videos and whatnot and I think this is the space where you're like do I

actually trust this summary like if I'm actually asked you know to summarize a financial model like are the numbers accurate do I trust the analysis and insights are there any madeup numbers

and I personally have experienced that the models are improving every single day. I think I heard someone say this that the AI is only going to be better tomorrow, it's worse

today. And I think that's pretty much true. And I I think as you progress in your learning stage from creating outputs, I think that's where you're trying to, you know, jump to a stage

where you want to develop judgment and understand where AI can actually help you to augment either, you know, your task, productivities versus where it cannot. So I think it's also an identity

shift as professionals when you're moving from someone just creating outputs as someone who's now trying to architect workflows around it. So when you actually design workflows using AI

tools etc. I think that's where the need for judgment really arises and um and I think it is at the individual level I would say a lot of it uh and there is a need to u need a desire to sort of

develop trust um around all the tasks that you are doing um but I would say that judgment is something that's more coming from you but it all it is also coming from how the tools are

interacting with you So for example, if a farmer is trying to apply for a loan and if they are rejected then they need to be told in the language they will understand that why that happened and

what more they can do you know if they have to try again tomorrow. So I think it is a two-way street. Uh so I would take that. &gt;&gt; Yeah. Thank you. I think it's it's a mix

of many things as you said. It it is at individual level but policy, literacy, product design, everything becomes a part of it and and then goes into the individual uh uh aspect of it. Uh

Pauline I'll come back to you on that point that there is a strong push around better data sets and we were talking about individual level and policy as a solution to fairness gaps in general but

from an implementation standpoint how much of the equity problem is really about data or to what extent better data sets alone can address these gaps if you know local structure or incentives are

weak or as you know Lipika is mentioning that at individual level the judgment is weak that how does how does that work? &gt;&gt; Thank you so much for this follow-up question. Yeah, you're totally right

that uh better data sets are going to improve the inclusiveness of the models that uh most of the people around the world are using. It will show the complexity of the world. This is very

true. But at the same time, this is probably not enough to ensure that tech is becoming fairer for everyone. Um once again I do think that governance is the key issue. Um we need to set some rules

that are agreeable uh between everyone and when I say everyone I will want to mention three types of actor obviously governments but the industry I think the industry has a really big role to play

uh when uh when ensuring that those uh AI systems are fair and also civil society. So I see those three actors being the main component of this discussion. Um I'm sure that Nikki will

discuss about some of the initiative that we have been leading together uh with the future society and the center for AI safety on implementing and establishing some AI red lines which are

key to ensure that we are not um moving forward with some risk that we believe they're unacceptable. So I won't go any further on on this um but I do think that yes having this conversation with

everyone at the table is really important and having it in very pragmatic uh approach and very um very willing approach to actually tackle uh these big challenges. If we don't do

this uh there is a high chance that something could go wrong. Um, one of the recent studies have shown that uh out of a survey uh from more than 2,000 uh AI system developers, more than half of

them do think that there are 10% chances of more that something could actually go wrong with AI systems. So you could think well it's only 10% chance that's not that big but think about it in

another way. For example, you get into the plane and the people building the plane, half of them are telling you, well, there is 10% chances that you're probably not going to land with this

plane. Well, are you really going to take the plane? I am not too sure. So, this is the kind of question we need to ask ourself to make sure that we are getting at the table. We are discussing

uh this important matters. And when I say we, I say once again the country building the systems but also those one using the system. We are in India um the the most populated country around the

world. We need uh definitely emerging countries at the table. We need also the EU at the table. And very recently um on uh the Republic Day of India, the fact that the EU was the uh guest uh of owner

and the signing of the agreement on the 26th of January is a very strong sign of what are the dynamics of middle powers and of emerging economies and I think this is going in the very good

direction. Thanks. &gt;&gt; Great. So perfect segue into now that we have established a little bit on safety and trust. The next word comes into mind is democracy. And I'm going to bring

Damar into the picture now. Namar, your work at Pikat looks at how AI is reshaping democracy from the ground up, especially in Southeast Asia and Indonesia. From your lens, is AI

changing democracy or balance of power as we say it or are we already at that tipping point? &gt;&gt; Yeah. Uh, thank you for having me here. Um I'm Damar uh from PIKAT. For those

who haven't um engaged with with Pikat, Pikat is the center of AI and tech innovation for democracy. We are starting in 2024 uh in observing the general election in Indonesia and so the

uh how AI transform our election result. So um back to your questions. Um this is something that I really concerned about. Uh you as we understand democracy is very important

for our lives. Uh for those coming from uh global south countries we struggle to have democracy because we have a long history of uh having not no democracy at the at that time but then we free oursel

and uh become independent. So democracy is uh very related with our lives uh especially when we talk about how media the four pillar of democracy is really hold important uh part of our lives

including election. So those two are uh the one that Picat uh uh work with and what I saw when we talk about AI actually uh we are the target of the AI uh AI

already transforms the election and already transform the media ecosystem. That's the the fact that we had to understand uh in election for for instance uh Indonesia is very uh

dynamic. We have our first AI consultant political consultant two years ago who's giving the power for those who were able to access the services uh to win the election. So that's that's some

something not not uh I'm I'm pretty sure it's not happening in other place. Um, and I don't know uh why Indonesia have this kind of idea. Probably power that something uh AI is something that they

believe can uh really transform the uh the the election uh people's want to get in power. So that that's uh the important part. But the thing with this uh um AI that being applied is not being

really scrutinized uh in terms of whether this is the right part or do we need uh the first AI political consultant in Indonesia. So that's uh one thing that uh what what we did uh

two years ago. But the latest one um I'm going to tell you what the aspect of democracy related to the press. As I said uh as as a matter of speaking in Indonesia, the Indonesia ecosystem, the

media ecosystem is already uh affected by the AI. Um no longer economic power uh in our media because AI AI now uh provide all the information with the AI summary with the LLM that

provided uh information for the readers. So readers uh now have a zero click uh uh phenomenon. They don't go to the sources. They don't go to the media. They just need reading the summary

instead. And the direct impact of that situation is the media uh get uh lost a lot of traffic. uh if you reading the research institute uh report the average is 43%

in the near uh 3 years but in situation in Indonesia because I'm working on the media uh quality journalism uh one of the facteing media uh lost their uh viewers

uh from uh two 20 thou 2,00s uh a into 80 18,000s. So they lost 90% of the their traffic

means that the media is no longer uh appeal for the advertisers. Uh so they didn't go any campaign uh through the media and they give just give it to the digital platform. It means create uh

inequality in in in my uh point of view on on economy. how the digital platform has uh more power now in shaping the information and through the ecosystem uh through the

economic side but that that that is not the uh worst part. The worst part is uh the role of the media is also diminishing on in political side because now with AI summarize everything uh for

the readers means that the readers cannot read uh what is the context of the uh information right and um we I I just read about the P the political overturn window um um research uh

regarding ing how LLM actually doing the filtering doing the narrowing the the democ democratic views on uh how they did the summary the how they did the summary and I'm I'm uh

afraid that uh if there is no um standard or the or there is no like uh option for the readers means that they will be uh shaped the information already shaped uh to them the

information that they get is actually not the real one. The information that they get from deepseek for instance because deep uh is only have 1% political overton window. It's very

small numbers. They didn't give you uh much space to think about uh everything uh different different perspective. Then you will have uh readers who will probably only uh knows uh small

information and is easy in in in uh in our situation like this polarization is easy to be uh shaped because of the AI summary. Uh I'm hoping that uh there is uh a way for for us to uh push the uh AI

developers uh who develop this AI summary to talk with the media uh uh industry the press industry to make sure that we don't lost our fourth pillar of democracy to make sure that the first uh

the AI companies pays uh money uh as they taken all the data from the media industry but they didn't give back to the media industry in order to make the media ecosystem survive. That's number

one. The second thing is uh is also like uh to make the uh AI developers and uh give option for the readers and um at least more bigger room for them to have debates um because

the AI summary should be more than just 1% at least like I don't know like 35% probably that will be something that I think ideal then we then have a a nice conversation in in our societ society

about what happened in our world is not being shaped by the AI then we have unhealthy democratic situation where people can debate as always and not define everything uh by the AI. So

that's um two things that I think um um my observation on uh on talking about democracy during this AI era. &gt;&gt; Thank you and thank you for that perspective. uh on this and now that

we've you know established this context very strongly I think we've patiently waited to hear Nikki and uh I'm pretty sure it's going to be worth it. First of all, Nikki, I think the future society

is doing some amazing work at the global level and from where you sit at the center stage in the US having a view around the globe and track of global regulations. In your view, when does

fragmentation become harmful or starts affecting fairness and safety and on the other end when does it actually allow some useful experimentation as well?

So first, hi everyone. Thank you for joining us here today and thank you for the invitation. Um, I think your question really hits the nail at the head. We were part of a round table

conversation yesterday. &gt;&gt; Can you hear me better now? Okay. &gt;&gt; I was saying that the question really hits the nail on the head. There was a round table that I was part of yesterday

and we were talking, we didn't call it fragmentation, but we were calling it the coordination challenge. And I really think AI governance and a lot of people call it the AI governance crisis that

we're now expering Singh but that can be described as a coordination challenge. Coordination across countries, coordination across institutions, you name it. And the truth is, you mentioned

this right. We have a lot of principles already established and there are governments that are working to develop rules for how these systems are actually developed and how they're deployed. But

these rules are being established separately. they're not being done in a coordinated manner. And the truth is we talked about this AI is such a complex and such a global supply chain that it's

hard to think about it through such a siloed approach. Right? I always say this, you may have heard me before, but a a system can really be developed in country A. It can be based on critical

resources from country B, C, D. It can be trained in country E. and I'm forgetting my alphabet so I might have skipped a letter but then it can be modified in country XYZ whatever it is

down the line what I'm trying to say is that this is already an interconnected system so just a national approach to solving them is not going to be effective it's not going to work down

the line and we've heard a lot in the past year talk around sovereignty and I think sovereignty is very important right but we should also confront the reality that for a lot of countries the

truth is if you don't look at alliances if you don't look at blocks you um there's no capacity at the moment right now to develop your own AI systems and really to be sovereign over them and I

think a lot of countries are accepting that reality and forming hence even soft alliances in some cases uh more more formal ones through multilateral fora like the bricks for example

uh I I guess I want to add with also putting a plug since pen mentioned it I think from different conversations we're seeing two very specific opportunities for international coordination

The first one is around AI red lines. When we say AI red lines, I think for a lot of people that seems to be kind of a concept, a concept that seems extreme or novel, but in many ways AI red lines,

even if they're not called that, do exist. So these are very clear limits on how we don't want AI to be developed or deployed. Just to give some examples, this might be AI enabled cyber attacks

on critical infrastructure or they might be manipulative systems that are harming individuals uh especially vulnerable groups of our society like children. Right? So we want

to come together as an international community really understand what is the lowest common denominator what are the risks that were not okay for our humanity for our planet to experience

and to come up with risk thresholds to understand if we're reaching these red lines and to be able to stop them. The next step would of course also be to create the oversight mechanisms, the

enforcement tools, the the verification tools in order in order to actually make this an operationalized system. And the second uh opportunity which I think we've heard quite a bit about actually

this has been a common theme that I've heard at the AI India impact summit which I'm excited about is coordination around incident prevention. Uh, a lot of people talk about AI harm like it's

something in the future, but we have a strong evidence base. It's AI incidents that are actually manifesting all around us. And I don't know why I'm saying this smiling that it's really sad. There's a

lot of people that are actually being harmed right now today. And I think we need to pay attention to do that. What we're not doing well is really detecting this AI incidents and having crossber

infrastructure to prevent them or at least to react to them and be able to stop them from escalating into something larger. So I do think there's an opportunity there, a very concrete one

for jurisdictions to come together to create uh whether it's information sharing channels for example to share with each other information when an incident manifests or share tonomies to

report things in the same ways or feedback loops between the actual evidence of AI incidents and then actual policy action that happens afterwards. And uh yeah, I I'll end with I I I

appreciate that multilateralism is uh is not exactly thriving in 2026. And I realize that it's this isn't easy, right? But I think and it's more than a safety challenge, right? It's a it's a

political challenge. Yesterday we were talking a lot about the political will of individuals coming forward and shaping the narrative in such a way that we can do it. But as I mentioned

earlier, I am hopeful. I think there's we are at a point where we can turn the pendulum back and make sure that we do start coordinating better together and hopefully break this fragmentation gap.

&gt;&gt; Thank you. And I'm going to ask you to zoom in slightly on this perspective. You know a lot of countries in global south including India are navigating through this um you know trade

development and strategic alignment at the same time. So how do we make sure that global cooperation does not become one sizefits all model

sort of a thing and instead leaves room for certain economic realities or local priorities. I'll start off by saying there that I think a lot of times when we talk about

AI governance issues, we tend to put things one against the other. And I think about the AI governance regime, well, you can think about it as a puzzle, right? Safety is one puzzle

piece, but it's not the only one, right? Economic development, trade, equity are other pieces. And I think to get this right, we have to have this holistic approach where we think about it

together. something that uh we could probably do better at collectively, right, is to shape our narrative, for example, that it's not that you either have a governance approach that promotes

economic development or you have one that promotes AI safety. It's not one or the other, right? You could actually do them both together. There's that fin like in regulation stifles innovation. I

genuinely believe that's not true. We've seen it in history that that's not the case, especially if you're talking about long-term economic development, right? So, so that's one point. But to your

question around a one-sizefits model, I I agree. I don't think there is such a thing as a one-sizefits model. Again, we've seen it in other spaces. We um I think we need to differentiate the types

of risks and harms we're talking about and the kind of solutions we're looking at. Some of the things could be looked at a very local and a national level, right? So, what could work, I'm

originally from Greece. What could work in Greece doesn't necessarily have to work here in India or in the United States. when it comes to how we approach um AI adoption in education for example

that that's one issue but then there are some risks to what I was hinting at earlier that are truly crossborder and cross um and sociotechnical I guess on a global level and that's where

coordination I think is really needed so for that I would lean towards for us such as the ones that already exist obviously the United Nations OECD we need to revisit this institutions think

if we need to create new ones and see how we can really strengthen those than to um to promote what I was saying earlier, coordination. &gt;&gt; Absolutely. Thank you. And I could

probably relate a lot. I think day before yesterday on day one at a very informal dinner, we were having this very passionate conversation about why is this a conversation about innovation

versus regulation? Why is this a versus topic? It it is it should be innovation with regulation. regulation helps systems last longer and stronger rather than working against them. So yeah,

thank you for bringing that up. Um I'm going to bring you know Lipika in and maybe Damar can add on to it later that when we generally think about fair tech and fairness seems today less about bias

in models and more about two you know who controls that information layer that goes in and which then indirectly or directly connects to the topic of trust again and safety and democracy. So I I

would like to know your thoughts about it about that information layer. So um I feel about the um information layer right like the way it gets um so in democracy like the

important thing is that people can trust information and they can use that information to make their choices and decisions and I feel when when you're when when people are in a situation

where the information itself becomes either like questionable and I think I'm going to bring in the perspective of judgment again like how do I know that the information that's kind of being

sent my way uh one it's set up in the right context that makes sense for me and um two like it it it actually addresses my worries and my concerns about about the issues that I wanted to

address. So I feel like and and and that last mile trust I think uh becomes pretty essential and that's where um I think when there is a gap in in the judgment and people are trying to

figure it out I think it impacts the uh democratic regime also in that sense because then when people don't have information that they can use and so for example I I I reached Delhi like few

days ago and I met few of my mother's friends and they said, "Oh, what's happening in US and uh it's it's all about AI but I don't think it means anything." So, and that just stuck with

me. Why? Because this kind of like tells me that AI is happening in the world, but it's not for me. And I feel like that's the layer of uh trust or rather like mistrust that is already happening

because it's not meeting people where they are. I mean while there is the absolutely necessary investments happening on the on the infrastructure layer um uh which is which is absolutely

necessary but I think at the same time it needs to tie in to um and then we have the advantage I think as a country of like you know the people like there is more than 500 million people in the

informal sector and the way that AI is currently being developed is um is is a lot coming from the west the the foundational models are coming from the west it's based on English as a primary

language and uh while we have you know the initiators like the bhashini which is trying to incorporate multiple languages but I think um there is a need to uh fill in that gap um which can help

bring the last mile onto the platform where people can also you know start where they feel like they are part of the process and it's not that the process is kind of being brought to

them. Thank you Mark. Would you like to add anything onto that? &gt;&gt; Yeah. Uh thank you. So if we look at the supply chain mode, right? Uh in information should be AI as the supplies

is coming from the media because the media collect all the information for us right and then uh AI uh uh apparently being part of the the one who distribute the information to to to the society.

But it was the ideal thing. &gt;&gt; The reality now people us I mean uh didn't get the information directly from the media. We've been uh the AI become the

intermediaries. The AI become the one who sort the information for you. Uh China has developed the AI that can personalize to yourself to individuals right. um I don't know uh other other

model but the idea that AI already filter everything that fits you is something that uh makes the AI become the uh not becoming the uh not following the supply chain

actually AI now uh dictate what we can what we read what we uh uh what we search and then uh slowly um shaping our understanding of what is uh

currently happening in the world. So in this situation uh I'm I'm going to say that AI uh creates information decay in our society. That's something the risk uh and related to safety but we usually

underexplore on that uh around that idea because we thought that is uh not happening at the moment but it's already happening now in in Indonesia for instance.

So because the media now become um weak because of the AI uh AI is become the one who is uh playing uh much role on shaping uh the information. I think the one thing that we need to address uh

pretty soon is how to make sure that our society able to uh to understand the systemic problem with the AI. If they are depending only one model for instance they are depending on deepseek

we need to literate them uh to read other model as well the summary coming from other model to understand that there are there is an opportunity that this AI didn't give you the information

needed as uh as fast as you need to make uh to to uh make decision so you need uh to read other model uh but also So I think in outside the literacy the government also

has has a bigger part to protect the media industry. The media ind industry now uh feels that AI become uh too powerful. Yeah. Uh I love what Anthony Gut said AI is not mean fostering

inequality right. Uh so should be there is a way that the the mechanism the the government um um protect the media industry in national

level or even um if I recalls what happened 70 years ago uh during um Asia Pacific Asia Africa uh conference in Bandong in in Bandung in Indonesia we

had a movement called the information decolonization means that we don't want the west uh the west dictate the information that uh we uh get from uh uh concerning what what

happened in in our uh uh global news right and then we started to have our own television we started to have our own uh media uh uh producers now I think it

it it it is uh important that the information is is uh being colonized &gt;&gt; by AI. So means that the governments should step in and then working together with the uh press industries in the

region in Asia, Africa to to um to block this uh digital digital colonialism, information colonialism coming from the AI uh uh developers to make sure that we uh get the good information, we get the

health healthy information from the media not being uh filtered not being um uh shaped by the AI then uh in the end we we are suffering because we get we didn't get unhealthy information we

don't get good journalism because of uh AI already uh uh uh uh uh already pick uh anything that according to them is is fits but we are not actually uh fit with the information that we provided by AI.

&gt;&gt; Yeah, absolutely. Um I I want to leave last 5 minutes to bring back the energy uh and get some rapid fire questions to the panelists. But before that just quickly pollen to you and maybe if uh

Nikki wants to comment from a policy angle on it. Is there any empirical evidence linking or what you know Damar has been talking about or what Lipika spoke about on on the concepts of

transparency trust or information lay requirements to measurable safety outcomes. &gt;&gt; Thanks so much. &gt;&gt; Thank you so much Vidia for this really

good question and really um I want to emphasize on what Nikki just say about innovation and regulation. I think we cannot be track trapped any longer into this innovation versus regulation

dilemma. It actually goes end in end and as you were asking do we have any empirical um statistics or evidence showing that uh when you regulate well the sector then the sector can actually

innovate. The answer is clearly yes and we don't need to look very far in history to show this. So two example the first one is the aviation sector. It's a highly regulated sector. That being

said, today uh I think we see a record number of people flying because they trust the system. We trust the plane and if a plane is not compliant with the safety regulation, then the plane

doesn't fly. This is as simple as that. Another example that shows that innovation and regulations are two pieces of the of the same coin is uh most probably the banking sector. We've

been through a major world financial crisis in uh 2008 and um it appeared that the system was lacking of regulation. So um collectively we went with uh baso 1 2 and three to set up

some supervision rules. Uh since then no major financial crisis has happened. We've been through the covid everyone was frightening that it might happen but the banking sector has been very

resilient. Not only resilient but also highly profitable. So it really showed that innovation and regulation are really going together. &gt;&gt; Perfect. Thank that gives me hope and it

gives me hope with some sort of evidence backing it. Uh last 5 minutes. Let's probably get some energy back into the room and uh we'll do a rapid fire. So these questions are to all of you maybe

in one sentence u or one word. Uh what is one signal that would tell you AI is becoming more fair or safer and not just more powerful? Rapid fire guys come on bring in the

energy AI red lines because I think that's key from an international perspective. Yeah, for me if AI able uh want willing to sit together with the media ecosystem before they doing any

innovation related to the uh AI summary &gt;&gt; I would say like the more and more um usage you see in like the rural part of the world as well as like with lower income households I think that's where I

would feel it's become fair &gt;&gt; and I think adoption rate seeing the adoption rate in both emerging countries and developed countries being uh equal uh shell will be a really good sign of

that we are going towards a safer system. &gt;&gt; Mhm. What is one risk in AI today that you think is underestimated? &gt;&gt; Do you have to pick one? [laughter]

&gt;&gt; Information decay. &gt;&gt; Sorry, what was the question is risks that &gt;&gt; uh what is one risk in AI today that you think is underestimated or underplayed,

underrated? [snorts] I'm going to skip that one. I think there's just too much to [laughter] say in like a rapid.

&gt;&gt; I will say from a general public perspective, probably the risk related to defense are probably not um well discussed on the on the mainstream media. For example, I'm thinking about

botterism, cyber attack, uh autonomous weapon. This is some kind of risk that are emerging that are um obviously existential risk. uh we don't discuss it enough I believe uh with the general

public. &gt;&gt; Okay. So if you could redesign one accountability mechanism tomorrow to make governance or fair tech more real and less performative, what would that

be? &gt;&gt; Can I take that? Absolutely. Um I think before any AI being implemented in the politic usage uh there should be uh uh sandboxing for the use uh right

sandboxing means that you check whether it's uh really fits with the uh context cultural context but also social context. uh sunb is also make sure that the AI being applied being

uh used is uh safe for for everyone. I think I would say like um I think while there is a lot of focus sometimes that u you know is AI going to you know become this big superpower and you know

u be a problem for the humanity versus but I think the focus also needs to be on more I guess local uh day-to-day problems that people are trying to solve and I feel like it can add a lot of

value um so I feel like a commitment to try and find and actually spend resources and time on more of those use cases is which are more day-to-day problem and like just like can

accelerate or like improve productivities for people. I think those are the use cases I think where we need I think more accountability on than rather just like the world shifting um

narrative that often gets sets up &gt;&gt; and uh I think if I can compliment on what my fellow uh speakers just said from a governance perspective maybe my hope and dream for 2026 is really having

all countries around the table and this is something that I believe the prime minister of Canada said it very rightly at Davos a couple of weeks ago that um during a negotiation if you are not at

the table that means you're on the menu and I don't want to see any country being on the menu of AI for this year. Something that makes me also hopeful is uh the G7 this year under French

presidency. Um France has been announcing the key um pillars of the um digital and AI track. AI safety is the number one priority and it says black and white that uh France is aiming at

fostering and establishing an international consensus on AI safety. So this is something that uh is a really um positive sign for for the year. &gt;&gt; Perfect. Thank you. And that brings me

to end of my list of questions. uh and uh I just want to you know summarize this morning uh which wherein we covered almost I think all aspects that we'd like to see in future of fair tech

safety trust democracy and Nikki spoke about global cooperation um seeing you everyone here today and uh our panelists from their respective countries I think this gives us hope uh evidencebacked

hope as I said for uh future of fair tech and uh Yeah, thank you everyone. May I request uh Mr. Saga Vishnoi to kindly felicitate our panelists and thank you everybody to uh for coming

today. It's not a very uh easy weather out there. In addition to this, I see a lot of people had joined later. So, Future Shift Labs has uploaded a YouTube live link on their uh LinkedIn page

which will help you navigate through the different conversations we had before you join. Thank you.
